{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "mount_file_id": "1iE1F3AERJnTFRSer9ThzD5l-ZpdKKTtT",
      "authorship_tag": "ABX9TyMgzKwU3fANRmp7tgEh05zj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/handsome1201/AI_Challenge/blob/main/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "UXgew5s6Sw7V"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import easydict\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import SGD, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "args = easydict.EasyDict({\n",
        "    # device setting\n",
        "    'device': 0,\n",
        "    'seed' : 123,\n",
        "\n",
        "    # training setting\n",
        "    'batch_size' : 128,\n",
        "    'num_workers' : 2,\n",
        "    'epoch' : 50,\n",
        "    'num_cls' : 100,\n",
        "    'resample' : True,\n",
        "\n",
        "    # optimizer & criterion\n",
        "    'lr' : 0.01,\n",
        "    'momentum' : 0.9,\n",
        "    'weight_decay' : 1e-4,\n",
        "    'nesterov' : True,\n",
        "\n",
        "    # directory\n",
        "    'data_path' : '/content/drive/MyDrive/소프트웨어/Data',\n",
        "    'save_path' : '/content/drive/MyDrive/소프트웨어/Save',\n",
        "    # etc\n",
        "    'print_freq' : 10,\n",
        "    'threshold' : 0.5,\n",
        "})\n",
        "\n",
        "def setup(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        np.random.seed(args.seed)\n",
        "        torch.random.manual_seed(args.seed)\n",
        "    return device"
      ],
      "metadata": {
        "id": "NISPzPEmSy_r"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data_preprocessing module\n",
        "sys.path.append(args.data_path)"
      ],
      "metadata": {
        "id": "ExGGeqCaTG6C"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path, train_transform, test_transform):\n",
        "    start = time.time()\n",
        "    train_dataset = CIFAR100(root=data_path, train=True, download=True, transform=train_transform)\n",
        "    test_dataset = CIFAR100(root=data_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "    end = time.time()\n",
        "    sec = end - start\n",
        "    print(f\"Completed Loading dataset at {str(datetime.timedelta(seconds=sec)).split('.')[0]}\")\n",
        "    return train_dataset, test_dataset"
      ],
      "metadata": {
        "id": "XwfHK1N4TIq5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "train_dataset, test_dataset = load_data(args.data_path, transforms.ToTensor(), transforms.ToTensor())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LPxBDSLTLLx",
        "outputId": "a0445495-aea7-426e-a953-b4281edcbbe7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Completed Loading dataset at 0:00:05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check dataset shape\n",
        "args.num_features = train_dataset.data.shape[1]\n",
        "args.num_classes = len(np.unique(train_dataset.targets))\n",
        "print(f\"Train_shape: {train_dataset.data.shape}\")\n",
        "print(f\"Test_shape: {test_dataset.data.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(train_dataset.targets))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAOcw1Q3TNAR",
        "outputId": "075a851b-c49d-4d6a-cd3b-4a2c4e738bad"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_shape: (50000, 32, 32, 3)\n",
            "Test_shape: (10000, 32, 32, 3)\n",
            "Number of classes: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_balance = [np.sum(np.array(train_dataset.targets) == i) for i in np.unique(train_dataset.targets)]"
      ],
      "metadata": {
        "id": "5o6rmfPKTOOT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,7), dpi=100)\n",
        "ax = fig.subplots()\n",
        "ax.bar(range(len(np.unique(train_dataset.targets))), num_balance, color='red')\n",
        "ax.set_title(\"Imbalanced Dataset\")\n",
        "_= ax.set_ylabel(\"Number of data\")\n",
        "_= ax.set_xlabel(\"Classes\")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "x7gEren3TPks",
        "outputId": "14486cc6-4e57-4fcf-e89e-70228518b73a"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1IAAAJwCAYAAACDNVCOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABFUElEQVR4nO3debzVdZ0/8Ndlu7LdewEVZFTEFVE0w0lvLpUykqFl0KJDikbZ2MWNUmNyL6Vo1AbHZZpHiVM5Nq4lpWhgOCqaa+GGu5jIkgoXUFm/vz96cH7eQONr4D3A8/l4nEf3fN6fc877nM/3kbwe362mKIoiAAAArLU2rd0AAADAhkaQAgAAKEmQAgAAKEmQAgAAKEmQAgAAKEmQAgAAKEmQAgAAKEmQAgAAKEmQAgAAKEmQAmCNJkyYkJqamjz44IPr7D2PPfbYbLfdduvs/VrLiy++mJqamkyYMKG1WwGglQhSABuB9RF6+Pv97ne/S01NTeVRW1ubnj175uMf/3guvPDCzJs3732/9xNPPJFzzz03L7744rpr+O9wzTXX5Ic//GFrtwHwgRGkAGA9O+mkk/LTn/40P/rRj3Laaaele/fuOeecc7LrrrtmypQp7+s9n3jiiZx33nmCFEAradfaDQDAxu6AAw7I5z73uRZjf/jDH3LIIYdk2LBheeKJJ7LVVlu1UncAvB/2SAFspI499th06dIlM2fOzGGHHZYuXbrkH/7hH3LZZZclSaZPn56DDjoonTt3Tp8+fXLNNdes8X3efPPNfO1rX0uPHj1SV1eXY445Jm+88UaLOb/85S8zZMiQ9O7dO7W1tdlhhx3yne98JytWrPibff7bv/1bPvrRj6ZHjx7p2LFjBg4cmOuvv361eTU1NRk1alRuvvnm7L777qmtrc1uu+2W2267bbW5r7zySkaOHFnpp2/fvjnhhBOydOnSypz58+fnlFNOyTbbbJPa2trsuOOO+f73v5+VK1e2eK/58+fn2GOPTX19fRoaGjJixIjMnz//b36vv2XPPffMD3/4w8yfPz//8R//URl/6aWX8vWvfz277LJLOnbsmB49euTzn/98iz1PEyZMyOc///kkySc+8YnKoYO/+93vkqz9ejzzzDMZNmxYevXqlc022yxbb711jjzyyCxYsKDFvJ/97GcZOHBgOnbsmO7du+fII4/Myy+/XKl//OMfz69//eu89NJLlV42hnPhAN6LPVIAG7EVK1bk0EMPzYEHHphx48bl5z//eUaNGpXOnTvn29/+doYPH56hQ4fmyiuvzDHHHJPGxsb07du3xXuMGjUqDQ0NOffcczNjxoxcccUVeemllyrn/yR/+Yd9ly5dMnr06HTp0iVTpkzJ2Wefnebm5vzgBz94zx7//d//PZ/+9KczfPjwLF26NNdee20+//nPZ+LEiRkyZEiLuXfffXduvPHGfP3rX0/Xrl0zfvz4DBs2LDNnzkyPHj2SJLNmzcpHPvKRzJ8/P8cff3z69euXV155Jddff33efPPNdOjQIW+++WY+9rGP5ZVXXsnXvva1bLvttrn33nszZsyYvPrqq5VD1IqiyGc+85ncfffd+Zd/+ZfsuuuuuemmmzJixIh1sj6f+9znMnLkyNx+++254IILkiQPPPBA7r333hx55JHZeuut8+KLL+aKK67Ixz/+8TzxxBPp1KlTDjzwwJx00kkZP358/vVf/zW77rprklT+d23WY+nSpRk8eHCWLFmSE088Mb169corr7ySiRMnZv78+amvr0+SXHDBBTnrrLPyhS98IV/5ylcyb968XHrppTnwwAPzyCOPpKGhId/+9rezYMGC/OlPf8oll1ySJOnSpcs6+Y0AqlYBwAbvqquuKpIUDzzwQGVsxIgRRZLiwgsvrIy98cYbRceOHYuampri2muvrYw/9dRTRZLinHPOWe09Bw4cWCxdurQyPm7cuCJJ8ctf/rIy9uabb67W09e+9rWiU6dOxdtvv92ipz59+rSY99evXbp0abH77rsXBx10UIvxJEWHDh2KZ599tjL2hz/8oUhSXHrppZWxY445pmjTpk2L32KVlStXFkVRFN/5zneKzp07F08//XSL+re+9a2ibdu2xcyZM4uiKIqbb765SFKMGzeuMmf58uXFAQccUCQprrrqqtU+453uvPPOIklx3XXXveucPffcs+jWrVvl+Zp+y2nTphVJiv/+7/+ujF133XVFkuLOO+9cbf7arMcjjzzyN3t78cUXi7Zt2xYXXHBBi/Hp06cX7dq1azE+ZMiQ1dYWYGPm0D6AjdxXvvKVyt8NDQ3ZZZdd0rlz53zhC1+ojO+yyy5paGjI888/v9rrjz/++LRv377y/IQTTki7du3ym9/8pjLWsWPHyt8LFy7Mn//85xxwwAF5880389RTT71nf+987RtvvJEFCxbkgAMOyMMPP7za3EGDBmWHHXaoPN9jjz1SV1dX6XvlypW5+eabc/jhh2fvvfde7fWr9qBdd911OeCAA9KtW7f8+c9/rjwGDRqUFStW5K677kqS/OY3v0m7du1ywgknVN6jbdu2OfHEE9/zO5XRpUuXLFy4sPL8nb/HsmXL8tprr2XHHXdMQ0PDGn+TNVmb9Vi1x2nSpEl588031/g+N954Y1auXJkvfOELLX6nXr16Zaeddsqdd95Z+vsCbCwc2gewEdtss82yxRZbtBirr6/P1ltvXQkV7xz/63OfkmSnnXZq8bxLly7ZaqutWpyz8/jjj+fMM8/MlClT0tzc3GL+X59v89cmTpyY7373u3n00UezZMmSyvhf95ck22677Wpj3bp1q/Q9b968NDc3Z/fdd3/Pz3zmmWfyxz/+cbXfZpW5c+cm+cv5SltttdVqh6ntsssu7/n+ZSxatChdu3atPH/rrbcyduzYXHXVVXnllVdSFEWl9rd+y1XWZj369u2b0aNH5+KLL87Pf/7zHHDAAfn0pz+dL33pS5WQ9cwzz6QoitW2gVXeGbABNjWCFMBGrG3btqXG3/mP9rU1f/78fOxjH0tdXV3OP//87LDDDtlss83y8MMP54wzzljt4g3v9H//93/59Kc/nQMPPDCXX355ttpqq7Rv3z5XXXXVGi9+sa76XrlyZf7pn/4pp59++hrrO++8c6n3e7+WLVuWp59+ukXwO/HEE3PVVVfllFNOSWNjY+rr61NTU5MjjzzyPX/LVcqsx0UXXZRjjz02v/zlL3P77bfnpJNOytixY3Pfffdl6623zsqVK1NTU5Nbb711jb+986CATZkgBcB7euaZZ/KJT3yi8nzRokV59dVX86lPfSrJX246+9prr+XGG2/MgQceWJn3wgsv/M33vuGGG7LZZptl0qRJqa2trYxfddVV76vXLbbYInV1dXnsscfec94OO+yQRYsWZdCgQe85r0+fPpk8eXIWLVrUIjTMmDHjffX3166//vq89dZbGTx4cIuxESNG5KKLLqqMvf3226tdKXBNe+yS8usxYMCADBgwIGeeeWbuvffe7Lfffrnyyivz3e9+NzvssEOKokjfvn3/Zrh8t34ANlbOkQLgPf3oRz/KsmXLKs+vuOKKLF++PIceemiS/7+X6J17hZYuXZrLL7/8b75327ZtU1NT0+Ky3C+++GJuvvnm99VrmzZtcsQRR+SWW27Jgw8+uFp9VY9f+MIXMm3atEyaNGm1OfPnz8/y5cuTJJ/61KeyfPnyXHHFFZX6ihUrcumll76v/t7pD3/4Q0455ZR069YtTU1NlfG2bduutoft0ksvXe3S5Z07d670+05rux7Nzc2V77nKgAED0qZNm8ohlkOHDk3btm1z3nnnrdZTURR57bXXWvSztoceAmwM7JEC4D0tXbo0Bx98cL7whS9kxowZufzyy7P//vvn05/+dJLkox/9aLp165YRI0bkpJNOSk1NTX7605+u1eF2Q4YMycUXX5xPfvKT+ed//ufMnTs3l112WXbcccf88Y9/fF/9Xnjhhbn99tvzsY99LMcff3x23XXXvPrqq7nuuuty9913p6GhIaeddlp+9atf5bDDDsuxxx6bgQMHZvHixZk+fXquv/76vPjii9l8881z+OGHZ7/99su3vvWtvPjii+nfv39uvPHG0oHh//7v//L2229nxYoVee2113LPPffkV7/6Verr63PTTTelV69elbmHHXZYfvrTn6a+vj79+/fPtGnT8tvf/rZyefdVPvShD6Vt27b5/ve/nwULFqS2tjYHHXTQWq/HlClTMmrUqHz+85/PzjvvnOXLl+enP/1p2rZtm2HDhiX5y5677373uxkzZkxefPHFHHHEEenatWteeOGF3HTTTTn++OPzzW9+M0kycODA/OIXv8jo0aPzj//4j+nSpUsOP/zw97OEABuGVrlWIADr1Ltd/rxz586rzf3Yxz5W7LbbbquN9+nTpxgyZMhq7zl16tTi+OOPL7p161Z06dKlGD58ePHaa6+1eO0999xT7LvvvkXHjh2L3r17F6effnoxadKk1S7PvabLn//4xz8udtppp6K2trbo169fcdVVVxXnnHNO8df/iUpSNDU1rbHvESNGtBh76aWXimOOOabYYostitra2mL77bcvmpqaiiVLllTmLFy4sBgzZkyx4447Fh06dCg233zz4qMf/Wjxb//2by0u9/7aa68VRx99dFFXV1fU19cXRx99dOXS4Wt7+fNVj/bt2xdbbLFFceCBBxYXXHBBMXfu3NVe88YbbxTHHXdcsfnmmxddunQpBg8eXDz11FNr/J7/9V//VWy//fZF27ZtW/zWa7Mezz//fPHlL3+52GGHHYrNNtus6N69e/GJT3yi+O1vf7taTzfccEOx//77F507dy46d+5c9OvXr2hqaipmzJhRmbNo0aLin//5n4uGhoYiiUuhAxu9mqJ4H2cWAwAAbMKcIwUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSG/ImWblyZWbNmpWuXbumpqamtdsBAABaSVEUWbhwYXr37p02bd59v5MglWTWrFnZZpttWrsNAACgSrz88svZeuut37UuSCXp2rVrkr/8WHV1da3cDQAA0Fqam5uzzTbbVDLCuxGkksrhfHV1dYIUAADwN0/5cbEJAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAkgQpAACAklo1SJ177rmpqalp8ejXr1+l/vbbb6epqSk9evRIly5dMmzYsMyZM6fFe8ycOTNDhgxJp06dsuWWW+a0007L8uXLP+ivAgAAbELatXYDu+22W377299Wnrdr9/9bOvXUU/PrX/861113Xerr6zNq1KgMHTo099xzT5JkxYoVGTJkSHr16pV77703r776ao455pi0b98+F1544Qf+XQAAgE1Dqwepdu3apVevXquNL1iwID/+8Y9zzTXX5KCDDkqSXHXVVdl1111z3333Zd99983tt9+eJ554Ir/97W/Ts2fPfOhDH8p3vvOdnHHGGTn33HPToUOHNX7mkiVLsmTJksrz5ubm9fPlAACAjVKrnyP1zDPPpHfv3tl+++0zfPjwzJw5M0ny0EMPZdmyZRk0aFBlbr9+/bLttttm2rRpSZJp06ZlwIAB6dmzZ2XO4MGD09zcnMcff/xdP3Ps2LGpr6+vPLbZZpv19O3ep5qaNT/U3r32bvUNpVaNv2k11aplnayvNVRr/bWwvtbX+m7c67sBadUgtc8++2TChAm57bbbcsUVV+SFF17IAQcckIULF2b27Nnp0KFDGhoaWrymZ8+emT17dpJk9uzZLULUqvqq2rsZM2ZMFixYUHm8/PLL6/aLAQAAG7VWPbTv0EMPrfy9xx57ZJ999kmfPn3yv//7v+nYseN6+9za2trU1taut/cHAAA2bq1+aN87NTQ0ZOedd86zzz6bXr16ZenSpZk/f36LOXPmzKmcU9WrV6/VruK36vmazrsCAABYF6oqSC1atCjPPfdcttpqqwwcODDt27fP5MmTK/UZM2Zk5syZaWxsTJI0NjZm+vTpmTt3bmXOHXfckbq6uvTv3/8D7x8AANg0tOqhfd/85jdz+OGHp0+fPpk1a1bOOeectG3bNkcddVTq6+szcuTIjB49Ot27d09dXV1OPPHENDY2Zt99902SHHLIIenfv3+OPvrojBs3LrNnz86ZZ56ZpqYmh+4BAADrTasGqT/96U856qij8tprr2WLLbbI/vvvn/vuuy9bbLFFkuSSSy5JmzZtMmzYsCxZsiSDBw/O5ZdfXnl927ZtM3HixJxwwglpbGxM586dM2LEiJx//vmt9ZUAAIBNQE1RFEVrN9HampubU19fnwULFqSurq6123n3S0AWhdq71ZI11zeU2qq62pprSXWsk/W1hmprriXVsU7W1/pW0++2odSS6linKooka5sNquocKQAAgA2BIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFCSIAUAAFBS1QSp733ve6mpqckpp5xSGXv77bfT1NSUHj16pEuXLhk2bFjmzJnT4nUzZ87MkCFD0qlTp2y55ZY57bTTsnz58g+4ewAAYFNSFUHqgQceyH/+539mjz32aDF+6qmn5pZbbsl1112XqVOnZtasWRk6dGilvmLFigwZMiRLly7Nvffem6uvvjoTJkzI2Wef/UF/BQAAYBPS6kFq0aJFGT58eP7rv/4r3bp1q4wvWLAgP/7xj3PxxRfnoIMOysCBA3PVVVfl3nvvzX333Zckuf322/PEE0/kZz/7WT70oQ/l0EMPzXe+851cdtllWbp0aWt9JQAAYCPX6kGqqakpQ4YMyaBBg1qMP/TQQ1m2bFmL8X79+mXbbbfNtGnTkiTTpk3LgAED0rNnz8qcwYMHp7m5OY8//vi7fuaSJUvS3Nzc4gEAALC22rXmh1977bV5+OGH88ADD6xWmz17djp06JCGhoYW4z179szs2bMrc94ZolbVV9XezdixY3Peeef9nd0DAACbqlbbI/Xyyy/n5JNPzs9//vNsttlmH+hnjxkzJgsWLKg8Xn755Q/08wEAgA1bqwWphx56KHPnzs2HP/zhtGvXLu3atcvUqVMzfvz4tGvXLj179szSpUszf/78Fq+bM2dOevXqlSTp1avXalfxW/V81Zw1qa2tTV1dXYsHAADA2mq1IHXwwQdn+vTpefTRRyuPvffeO8OHD6/83b59+0yePLnymhkzZmTmzJlpbGxMkjQ2Nmb69OmZO3duZc4dd9yRurq69O/f/wP/TgAAwKah1c6R6tq1a3bfffcWY507d06PHj0q4yNHjszo0aPTvXv31NXV5cQTT0xjY2P23XffJMkhhxyS/v375+ijj864ceMye/bsnHnmmWlqakptbe0H/p0AAIBNQ6tebOJvueSSS9KmTZsMGzYsS5YsyeDBg3P55ZdX6m3bts3EiRNzwgknpLGxMZ07d86IESNy/vnnt2LXAADAxq6mKIqitZtobc3Nzamvr8+CBQuq43ypmpo1jxeF2rvVkjXXN5TaqrrammtJdayT9bWGamuuJdWxTtbX+lbT77ah1JLqWKcqiiRrmw1a/T5SAAAAGxpBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoCRBCgAAoKR27/eFTzzxRGbOnJmlS5e2GP/0pz/9dzcFAABQzUoHqeeffz6f/exnM3369NTU1KQoiiRJTU1NkmTFihXrtkMAAIAqU/rQvpNPPjl9+/bN3Llz06lTpzz++OO56667svfee+d3v/vdemgRAACgupTeIzVt2rRMmTIlm2++edq0aZM2bdpk//33z9ixY3PSSSflkUceWR99AgAAVI3Se6RWrFiRrl27Jkk233zzzJo1K0nSp0+fzJgxY912BwAAUIVK75Hafffd84c//CF9+/bNPvvsk3HjxqVDhw750Y9+lO2333599AgAAFBVSgepM888M4sXL06SnH/++TnssMNywAEHpEePHrn22mvXeYMAAADVpnSQGjx4cOXvHXfcMU899VRef/31dOvWrXLlPgAAgI1Z6XOkvvzlL2fhwoUtxrp3754333wzX/7yl9dZYwAAANWqdJC6+uqr89Zbb602/tZbb+W///u/10lTAAAA1Wytg1Rzc3MWLFiQoiiycOHCNDc3Vx5vvPFGfvOb32TLLbcs9eFXXHFF9thjj9TV1aWuri6NjY259dZbK/W33347TU1N6dGjR7p06ZJhw4Zlzpw5Ld5j5syZGTJkSDp16pQtt9wyp512WpYvX16qDwAAgDLW+hyphoaG1NTUpKamJjvvvPNq9Zqampx33nmlPnzrrbfO9773vey0004piiJXX311PvOZz+SRRx7JbrvtllNPPTW//vWvc91116W+vj6jRo3K0KFDc8899yT5y6XYhwwZkl69euXee+/Nq6++mmOOOSbt27fPhRdeWKoXAACAtVVTFEWxNhOnTp2aoihy0EEH5YYbbkj37t0rtQ4dOqRPnz7p3bv3391Q9+7d84Mf/CCf+9znssUWW+Saa67J5z73uSTJU089lV133TXTpk3Lvvvum1tvvTWHHXZYZs2alZ49eyZJrrzyypxxxhmZN29eOnTosFaf2dzcnPr6+ixYsCB1dXV/93f4u73bRTuKQu3dasma6xtKbVVdbc21pDrWyfpaQ7U115LqWCfra32r6XfbUGpJdazT2kWSD8TaZoO13iP1sY99LEnywgsvZJtttkmbNqVPr3pPK1asyHXXXZfFixensbExDz30UJYtW5ZBgwZV5vTr1y/bbrttJUhNmzYtAwYMqISo5C9XFTzhhBPy+OOPZ6+99lrjZy1ZsiRLliypPG9ubl6n3wUAANi4lb78eZ8+fZIkb775ZmbOnJmlS5e2qO+xxx6l3m/69OlpbGzM22+/nS5duuSmm25K//798+ijj6ZDhw5paGhoMb9nz56ZPXt2kmT27NktQtSq+qrauxk7dmzpwxABAABWKR2k5s2bl+OOO67FRSHeacWKFaXeb5dddsmjjz6aBQsW5Prrr8+IESMyderUsm2VMmbMmIwePbryvLm5Odtss816/UwAAGDjUfr4vFNOOSXz58/P/fffn44dO+a2227L1VdfnZ122im/+tWvSjfQoUOH7Ljjjhk4cGDGjh2bPffcM//+7/+eXr16ZenSpZk/f36L+XPmzEmvXr2SJL169VrtKn6rnq+asya1tbWVKwWuegAAAKyt0kFqypQpufjii7P33nunTZs26dOnT770pS9l3LhxGTt27N/d0MqVK7NkyZIMHDgw7du3z+TJkyu1GTNmZObMmWlsbEySNDY2Zvr06Zk7d25lzh133JG6urr079//7+4FAABgTUof2rd48eLK/aK6deuWefPmZeedd86AAQPy8MMPl3qvMWPG5NBDD822226bhQsX5pprrsnvfve7TJo0KfX19Rk5cmRGjx6d7t27p66uLieeeGIaGxuz7777JkkOOeSQ9O/fP0cffXTGjRuX2bNn58wzz0xTU1Nqa2vLfjUAAIC1UjpI7bLLLpkxY0a222677LnnnvnP//zPbLfddrnyyiuz1VZblXqvuXPn5phjjsmrr76a+vr67LHHHpk0aVL+6Z/+KUlyySWXpE2bNhk2bFiWLFmSwYMH5/LLL6+8vm3btpk4cWJOOOGENDY2pnPnzhkxYkTOP//8sl8LAABgra31faRW+dnPfpbly5fn2GOPzUMPPZRPfvKTef3119OhQ4dMmDAhX/ziF9dXr+uN+0htBLWkuu6B4B4X67aWVMc6WV9rqLbmWlId62R9rW81/W4bSi2pjnXamO8jtcqXvvSlyt8DBw7MSy+9lKeeeirbbrttNt988/fXLQAAwAakdJD6a506dcqHP/zhddELAADABmGtgtQ777n0t1x88cXvuxkAAIANwVoFqUceeaTF84cffjjLly/PLrvskiR5+umn07Zt2wwcOHDddwgAAFBl1ipI3XnnnZW/L7744nTt2jVXX311unXrliR54403ctxxx+WAAw5YP10CAABUkdI35L3ooosyduzYSohK/nI/qe9+97u56KKL1mlzAAAA1ah0kGpubs68efNWG583b14WLly4TpoCAACoZqWD1Gc/+9kcd9xxufHGG/OnP/0pf/rTn3LDDTdk5MiRGTp06ProEQAAoKqUvvz5lVdemW9+85v553/+5yxbtuwvb9KuXUaOHJkf/OAH67xBAACAalM6SHXq1CmXX355fvCDH+S5555Lkuywww7p3LnzOm8OAACgGr3vG/J27tw5e+yxx7rsBQAAYINQ+hwpAACATZ0gBQAAUJIgBQAAUNJaBakPf/jDeeONN5Ik559/ft5888312hQAAEA1W6sg9eSTT2bx4sVJkvPOOy+LFi1ar00BAABUs7W6at+HPvShHHfccdl///1TFEX+7d/+LV26dFnj3LPPPnudNggAAFBt1ipITZgwIeecc04mTpyYmpqa3HrrrWnXbvWX1tTUCFIAAMBGb62C1C677JJrr702SdKmTZtMnjw5W2655XptDAAAoFqVviHvypUr10cfAAAAG4zSQSpJnnvuufzwhz/Mk08+mSTp379/Tj755Oywww7rtDkAAIBqVPo+UpMmTUr//v3z+9//PnvssUf22GOP3H///dltt91yxx13rI8eAQAAqkrpPVLf+ta3cuqpp+Z73/veauNnnHFG/umf/mmdNQcAAFCNSu+RevLJJzNy5MjVxr/85S/niSeeWCdNAQAAVLPSQWqLLbbIo48+utr4o48+6kp+AADAJqH0oX1f/epXc/zxx+f555/PRz/60STJPffck+9///sZPXr0Om8QAACg2pQOUmeddVa6du2aiy66KGPGjEmS9O7dO+eee25OOumkdd4gAABAtSkdpGpqanLqqafm1FNPzcKFC5MkXbt2XeeNAQAAVKv3dR+pVQQoAABgU1T6YhMAAACbOkEKAACgJEEKAACgpFJBatmyZTn44IPzzDPPrK9+AAAAql6pINW+ffv88Y9/XF+9AAAAbBBKH9r3pS99KT/+8Y/XRy8AAAAbhNKXP1++fHl+8pOf5Le//W0GDhyYzp07t6hffPHF66w5AACAalQ6SD322GP58Ic/nCR5+umnW9RqamrWTVcAAABVrHSQuvPOO9dHHwAAABuM933582effTaTJk3KW2+9lSQpimKdNQUAAFDNSgep1157LQcffHB23nnnfOpTn8qrr76aJBk5cmS+8Y1vrPMGAQAAqk3pIHXqqaemffv2mTlzZjp16lQZ/+IXv5jbbrttnTYHAABQjUqfI3X77bdn0qRJ2XrrrVuM77TTTnnppZfWWWMAAADVqvQeqcWLF7fYE7XK66+/ntra2nXSFAAAQDUrHaQOOOCA/Pd//3fleU1NTVauXJlx48blE5/4xDptDgAAoBqVPrRv3LhxOfjgg/Pggw9m6dKlOf300/P444/n9ddfzz333LM+egQAAKgqpfdI7b777nn66aez//775zOf+UwWL16coUOH5pFHHskOO+ywPnoEAACoKqX3SCVJfX19vv3tb6/rXgAAADYI7ytIvfHGG/nxj3+cJ598MknSv3//HHfccenevfs6bQ4AAKAalT6076677sp2222X8ePH54033sgbb7yR8ePHp2/fvrnrrrvWR48AAABVpfQeqaampnzxi1/MFVdckbZt2yZJVqxYka9//etpamrK9OnT13mTAAAA1aT0Hqlnn3023/jGNyohKknatm2b0aNH59lnn12nzQEAAFSj0kHqwx/+cOXcqHd68skns+eee66TpgAAAKrZWh3a98c//rHy90knnZSTTz45zz77bPbdd98kyX333ZfLLrss3/ve99ZPlwAAAFWkpiiK4m9NatOmTWpqavK3ptbU1GTFihXrrLkPSnNzc+rr67NgwYLU1dW1djtJTc2ax4tC7d1qyZrrG0ptVV1tzbWkOtbJ+lpDtTXXkupYJ+trfavpd9tQakl1rNPfjiQfmLXNBmu1R+qFF15YZ40BAABs6NYqSPXp02d99wEAALDBeF835J01a1buvvvuzJ07NytXrmxRO+mkk9ZJYwAAANWqdJCaMGFCvva1r6VDhw7p0aNHat5xjGNNTY0gBQAAbPRKB6mzzjorZ599dsaMGZM2bUpfPR0AAGCDVzoJvfnmmznyyCOFKAAAYJNVOg2NHDky11133froBQAAYINQ+tC+sWPH5rDDDsttt92WAQMGpH379i3qF1988TprDgAAoBq9ryA1adKk7LLLLkmy2sUmAAAANnalg9RFF12Un/zkJzn22GPXQzsAAADVr/Q5UrW1tdlvv/3WRy8AAAAbhNJB6uSTT86ll166PnoBAADYIJQ+tO/3v/99pkyZkokTJ2a33XZb7WITN9544zprDgAAoBqVDlINDQ0ZOnTo+ugFAABgg1A6SF111VXrow8AAIANRulzpAAAADZ1pfdI9e3b9z3vF/X888//XQ0BAABUu9JB6pRTTmnxfNmyZXnkkUdy22235bTTTltXfQEAAFSt0kHq5JNPXuP4ZZddlgcffPDvbggAAKDarbNzpA499NDccMMN6+rtAAAAqtY6C1LXX399unfvvq7eDgAAoGqVPrRvr732anGxiaIoMnv27MybNy+XX375Om0OAACgGpUOUkcccUSL523atMkWW2yRj3/84+nXr9+66gsAAKBqlQ5S55xzzvroAwAAYIPhhrwAAAAlrfUeqTZt2rznjXiTpKamJsuXL/+7mwIAAKhmax2kbrrppnetTZs2LePHj8/KlSvXSVMAAADVbK2D1Gc+85nVxmbMmJFvfetbueWWWzJ8+PCcf/7567Q5AACAavS+zpGaNWtWvvrVr2bAgAFZvnx5Hn300Vx99dXp06fPuu4PAACg6pQKUgsWLMgZZ5yRHXfcMY8//ngmT56cW265Jbvvvvv66g8AAKDqrPWhfePGjcv3v//99OrVK//zP/+zxkP9AAAANgU1RVEUazOxTZs26dixYwYNGpS2bdu+67wbb7xxnTX3QWlubk59fX0WLFiQurq61m4neberIxaF2rvVkjXXN5TaqrrammtJdayT9bWGamuuJdWxTtbX+lbT77ah1JLqWKe1iyQfiLXNBmu9R+qYY475m5c/BwAA2BSsdZCaMGHCemwDAABgw/G+rtoHAACwKROkAAAAShKkAAAAShKkAAAASmrVIDV27Nj84z/+Y7p27Zott9wyRxxxRGbMmNFizttvv52mpqb06NEjXbp0ybBhwzJnzpwWc2bOnJkhQ4akU6dO2XLLLXPaaadl+fLlH+RXAQAANiGtGqSmTp2apqam3HfffbnjjjuybNmyHHLIIVm8eHFlzqmnnppbbrkl1113XaZOnZpZs2Zl6NChlfqKFSsyZMiQLF26NPfee2+uvvrqTJgwIWeffXZrfCUAAGATsNY35P0gzJs3L1tuuWWmTp2aAw88MAsWLMgWW2yRa665Jp/73OeSJE899VR23XXXTJs2Lfvuu29uvfXWHHbYYZk1a1Z69uyZJLnyyitzxhlnZN68eenQocNqn7NkyZIsWbKk8ry5uTnbbLONG/JuyLWkum4m52aB67aWVMc6WV9rqLbmWlId62R9rW81/W4bSi2pjnWqnkiy1jfkrapzpBYsWJAk6d69e5LkoYceyrJlyzJo0KDKnH79+mXbbbfNtGnTkiTTpk3LgAEDKiEqSQYPHpzm5uY8/vjja/ycsWPHpr6+vvLYZptt1tdXAgAANkJVE6RWrlyZU045Jfvtt1923333JMns2bPToUOHNDQ0tJjbs2fPzJ49uzLnnSFqVX1VbU3GjBmTBQsWVB4vv/zyOv42AADAxqxdazewSlNTUx577LHcfffd6/2zamtrU1tbu94/BwAA2DhVxR6pUaNGZeLEibnzzjuz9dZbV8Z79eqVpUuXZv78+S3mz5kzJ7169arM+eur+K16vmoOAADAutSqQaooiowaNSo33XRTpkyZkr59+7aoDxw4MO3bt8/kyZMrYzNmzMjMmTPT2NiYJGlsbMz06dMzd+7cypw77rgjdXV16d+//wfzRQAAgE1Kqx7a19TUlGuuuSa//OUv07Vr18o5TfX19enYsWPq6+szcuTIjB49Ot27d09dXV1OPPHENDY2Zt99902SHHLIIenfv3+OPvrojBs3LrNnz86ZZ56ZpqYmh+8BAADrRasGqSuuuCJJ8vGPf7zF+FVXXZVjjz02SXLJJZekTZs2GTZsWJYsWZLBgwfn8ssvr8xt27ZtJk6cmBNOOCGNjY3p3LlzRowYkfPPP/+D+hoAAMAmpqruI9Va1vZa8R+Yarq3wIZSS6rrHgjucbFua0l1rJP1tYZqa64l1bFO1tf6VtPvtqHUkupYpyqKJBvkfaQAAAA2BIIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASYIUAABASa0apO66664cfvjh6d27d2pqanLzzTe3qBdFkbPPPjtbbbVVOnbsmEGDBuWZZ55pMef111/P8OHDU1dXl4aGhowcOTKLFi36AL8FAACwqWnVILV48eLsueeeueyyy9ZYHzduXMaPH58rr7wy999/fzp37pzBgwfn7bffrswZPnx4Hn/88dxxxx2ZOHFi7rrrrhx//PEf1FcAAAA2QTVFURSt3USS1NTU5KabbsoRRxyR5C97o3r37p1vfOMb+eY3v5kkWbBgQXr27JkJEybkyCOPzJNPPpn+/fvngQceyN57750kue222/KpT30qf/rTn9K7d++1+uzm5ubU19dnwYIFqaurWy/fr5SamjWPF4Xau9WSNdc3lNqqutqaa0l1rJP1tYZqa64l1bFO1tf6VtPvtqHUkupYp+qIJEnWPhtU7TlSL7zwQmbPnp1BgwZVxurr67PPPvtk2rRpSZJp06aloaGhEqKSZNCgQWnTpk3uv//+d33vJUuWpLm5ucUDAABgbVVtkJo9e3aSpGfPni3Ge/bsWanNnj07W265ZYt6u3bt0r1798qcNRk7dmzq6+srj2222WYddw8AAGzMqjZIrU9jxozJggULKo+XX365tVsCAAA2IFUbpHr16pUkmTNnTovxOXPmVGq9evXK3LlzW9SXL1+e119/vTJnTWpra1NXV9fiAQAAsLaqNkj17ds3vXr1yuTJkytjzc3Nuf/++9PY2JgkaWxszPz58/PQQw9V5kyZMiUrV67MPvvs84H3DAAAbBrateaHL1q0KM8++2zl+QsvvJBHH3003bt3z7bbbptTTjkl3/3ud7PTTjulb9++Oeuss9K7d+/Klf123XXXfPKTn8xXv/rVXHnllVm2bFlGjRqVI488cq2v2AcAAFBWqwapBx98MJ/4xCcqz0ePHp0kGTFiRCZMmJDTTz89ixcvzvHHH5/58+dn//33z2233ZbNNtus8pqf//znGTVqVA4++OC0adMmw4YNy/jx4z/w7wIAAGw6quY+Uq3JfaQ2glpSXfdAcI+LdVtLqmOdrK81VFtzLamOdbK+1reafrcNpZZUxzpVUSTZ4O8jBQAAUK0EKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJIEKQAAgJI2miB12WWXZbvttstmm22WffbZJ7///e9buyUAAGAjtVEEqV/84hcZPXp0zjnnnDz88MPZc889M3jw4MydO7e1WwMAADZCG0WQuvjii/PVr341xx13XPr3758rr7wynTp1yk9+8pPWbg0AANgItWvtBv5eS5cuzUMPPZQxY8ZUxtq0aZNBgwZl2rRpa3zNkiVLsmTJksrzBQsWJEmam5vXb7N/r/fqT23DrlVbP2rrtlZt/aiVr1VbP2rrtlZt/ait21q19aNWvvYBW5UJiqJ4z3k1xd+aUeVmzZqVf/iHf8i9996bxsbGyvjpp5+eqVOn5v7771/tNeeee27OO++8D7JNAABgA/Lyyy9n6623ftf6Br9H6v0YM2ZMRo8eXXm+cuXKvP766+nRo0dqampasbOWmpubs8022+Tll19OXV1da7fDBsA2Q1m2GcqyzVCWbYb3ozW3m6IosnDhwvTu3fs9523wQWrzzTdP27ZtM2fOnBbjc+bMSa9evdb4mtra2tTW1rYYa2hoWF8t/t3q6ur8Hw+l2GYoyzZDWbYZyrLN8H601nZTX1//N+ds8Beb6NChQwYOHJjJkydXxlauXJnJkye3ONQPAABgXdng90glyejRozNixIjsvffe+chHPpIf/vCHWbx4cY477rjWbg0AANgIbRRB6otf/GLmzZuXs88+O7Nnz86HPvSh3HbbbenZs2drt/Z3qa2tzTnnnLPaYYjwbmwzlGWboSzbDGXZZng/NoTtZoO/ah8AAMAHbYM/RwoAAOCDJkgBAACUJEgBAACUJEgBAACUJEhVqcsuuyzbbbddNttss+yzzz75/e9/39otUSXGjh2bf/zHf0zXrl2z5ZZb5ogjjsiMGTNazHn77bfT1NSUHj16pEuXLhk2bNhqN61m0/W9730vNTU1OeWUUypjthn+2iuvvJIvfelL6dGjRzp27JgBAwbkwQcfrNSLosjZZ5+drbbaKh07dsygQYPyzDPPtGLHtLYVK1bkrLPOSt++fdOxY8fssMMO+c53vpN3XtfMdrNpu+uuu3L44Yend+/eqampyc0339yivjbbx+uvv57hw4enrq4uDQ0NGTlyZBYtWvQBfov/T5CqQr/4xS8yevTonHPOOXn44Yez5557ZvDgwZk7d25rt0YVmDp1apqamnLffffljjvuyLJly3LIIYdk8eLFlTmnnnpqbrnlllx33XWZOnVqZs2alaFDh7Zi11SLBx54IP/5n/+ZPfbYo8W4bYZ3euONN7Lffvulffv2ufXWW/PEE0/koosuSrdu3Spzxo0bl/Hjx+fKK6/M/fffn86dO2fw4MF5++23W7FzWtP3v//9XHHFFfmP//iPPPnkk/n+97+fcePG5dJLL63Msd1s2hYvXpw999wzl1122Rrra7N9DB8+PI8//njuuOOOTJw4MXfddVeOP/74D+ortFRQdT7ykY8UTU1NlecrVqwoevfuXYwdO7YVu6JazZ07t0hSTJ06tSiKopg/f37Rvn374rrrrqvMefLJJ4skxbRp01qrTarAwoULi5122qm44447io997GPFySefXBSFbYbVnXHGGcX+++//rvWVK1cWvXr1Kn7wgx9UxubPn1/U1tYW//M///NBtEgVGjJkSPHlL3+5xdjQoUOL4cOHF0Vhu6GlJMVNN91Ueb4228cTTzxRJCkeeOCBypxbb721qKmpKV555ZUPrPdV7JGqMkuXLs1DDz2UQYMGVcbatGmTQYMGZdq0aa3YGdVqwYIFSZLu3bsnSR566KEsW7asxTbUr1+/bLvttrahTVxTU1OGDBnSYttIbDOs7le/+lX23nvvfP7zn8+WW26ZvfbaK//1X/9Vqb/wwguZPXt2i22mvr4+++yzj21mE/bRj340kydPztNPP50k+cMf/pC77747hx56aBLbDe9tbbaPadOmpaGhIXvvvXdlzqBBg9KmTZvcf//9H3jP7T7wT+Q9/fnPf86KFSvSs2fPFuM9e/bMU0891UpdUa1WrlyZU045Jfvtt1923333JMns2bPToUOHNDQ0tJjbs2fPzJ49uxW6pBpce+21efjhh/PAAw+sVrPN8Neef/75XHHFFRk9enT+9V//NQ888EBOOumkdOjQISNGjKhsF2v6b5VtZtP1rW99K83NzenXr1/atm2bFStW5IILLsjw4cOTxHbDe1qb7WP27NnZcsstW9TbtWuX7t27t8o2JEjBBqypqSmPPfZY7r777tZuhSr28ssv5+STT84dd9yRzTbbrLXbYQOwcuXK7L333rnwwguTJHvttVcee+yxXHnllRkxYkQrd0e1+t///d/8/Oc/zzXXXJPddtstjz76aE455ZT07t3bdsNGyaF9VWbzzTdP27ZtV7ta1pw5c9KrV69W6opqNGrUqEycODF33nlntt5668p4r169snTp0syfP7/FfNvQpuuhhx7K3Llz8+EPfzjt2rVLu3btMnXq1IwfPz7t2rVLz549bTO0sNVWW6V///4txnbdddfMnDkzSSrbhf9W8U6nnXZavvWtb+XII4/MgAEDcvTRR+fUU0/N2LFjk9hueG9rs3306tVrtYuvLV++PK+//nqrbEOCVJXp0KFDBg4cmMmTJ1fGVq5cmcmTJ6exsbEVO6NaFEWRUaNG5aabbsqUKVPSt2/fFvWBAwemffv2LbahGTNmZObMmbahTdTBBx+c6dOn59FHH6089t577wwfPrzyt22Gd9pvv/1Wu63C008/nT59+iRJ+vbtm169erXYZpqbm3P//ffbZjZhb775Ztq0aflPy7Zt22blypVJbDe8t7XZPhobGzN//vw89NBDlTlTpkzJypUrs88++3zgPbtqXxW69tpri9ra2mLChAnFE088URx//PFFQ0NDMXv27NZujSpwwgknFPX19cXvfve74tVXX6083nzzzcqcf/mXfym23XbbYsqUKcWDDz5YNDY2Fo2Nja3YNdXmnVftKwrbDC39/ve/L9q1a1dccMEFxTPPPFP8/Oc/Lzp16lT87Gc/q8z53ve+VzQ0NBS//OUviz/+8Y/FZz7zmaJv377FW2+91Yqd05pGjBhR/MM//EMxceLE4oUXXihuvPHGYvPNNy9OP/30yhzbzaZt4cKFxSOPPFI88sgjRZLi4osvLh555JHipZdeKopi7baPT37yk8Vee+1V3H///cXdd99d7LTTTsVRRx3VKt9HkKpSl156abHtttsWHTp0KD7ykY8U9913X2u3RJVIssbHVVddVZnz1ltvFV//+teLbt26FZ06dSo++9nPFq+++mrrNU3V+esgZZvhr91yyy3F7rvvXtTW1hb9+vUrfvSjH7Wor1y5sjjrrLOKnj17FrW1tcXBBx9czJgxo5W6pRo0NzcXJ598crHtttsWm222WbH99tsX3/72t4slS5ZU5thuNm133nnnGv8NM2LEiKIo1m77eO2114qjjjqq6NKlS1FXV1ccd9xxxcKFC1vh2xRFTVG843bTAAAA/E3OkQIAAChJkAIAAChJkAIAAChJkAIAAChJkAIAAChJkAIAAChJkAIAAChJkAIAAChJkAJgo1JTU5Obb765tdsAYCMnSAGwQZk9e3ZOPPHEbL/99qmtrc0222yTww8/PJMnT27t1gDYhLRr7QYAYG29+OKL2W+//dLQ0JAf/OAHGTBgQJYtW5ZJkyalqakpTz31VGu3CMAmwh4pADYYX//611NTU5Pf//73GTZsWHbeeefstttuGT16dO677741vuaMM87IzjvvnE6dOmX77bfPWWedlWXLllXqf/jDH/KJT3wiXbt2TV1dXQYOHJgHH3wwSfLSSy/l8MMPT7du3dK5c+fstttu+c1vflN57WOPPZZDDz00Xbp0Sc+ePXP00Ufnz3/+c6V+/fXXZ8CAAenYsWN69OiRQYMGZfHixevp1wHgg2SPFAAbhNdffz233XZbLrjggnTu3Hm1ekNDwxpf17Vr10yYMCG9e/fO9OnT89WvfjVdu3bN6aefniQZPnx49tprr1xxxRVp27ZtHn300bRv3z5J0tTUlKVLl+auu+5K586d88QTT6RLly5Jkvnz5+eggw7KV77ylVxyySV56623csYZZ+QLX/hCpkyZkldffTVHHXVUxo0bl89+9rNZuHBh/u///i9FUayfHwiAD5QgBcAG4dlnn01RFOnXr1+p15155pmVv7fbbrt885vfzLXXXlsJUjNnzsxpp51Wed+ddtqpMn/mzJkZNmxYBgwYkCTZfvvtK7X/+I//yF577ZULL7ywMvaTn/wk22yzTZ5++uksWrQoy5cvz9ChQ9OnT58kqbwPABs+QQqADcL73ZPzi1/8IuPHj89zzz1XCTd1dXWV+ujRo/OVr3wlP/3pTzNo0KB8/vOfzw477JAkOemkk3LCCSfk9ttvz6BBgzJs2LDsscceSf5ySOCdd95Z2UP1Ts8991wOOeSQHHzwwRkwYEAGDx6cQw45JJ/73OfSrVu39/U9AKguzpECYIOw0047paamptQFJaZNm5bhw4fnU5/6VCZOnJhHHnkk3/72t7N06dLKnHPPPTePP/54hgwZkilTpqR///656aabkiRf+cpX8vzzz+foo4/O9OnTs/fee+fSSy9NkixatCiHH354Hn300RaPZ555JgceeGDatm2bO+64I7feemv69++fSy+9NLvsskteeOGFdfvDANAqagoHawOwgTj00EMzffr0zJgxY7XzpObPn5+GhobU1NTkpptuyhFHHJGLLrool19+eZ577rnKvK985Su5/vrrM3/+/DV+xlFHHZXFixfnV7/61Wq1MWPG5Ne//nX++Mc/5tvf/nZuuOGGPPbYY2nX7m8f4LFixYr06dMno0ePzujRo8t9cQCqjj1SAGwwLrvssqxYsSIf+chHcsMNN+SZZ57Jk08+mfHjx6exsXG1+TvttFNmzpyZa6+9Ns8991zGjx9f2duUJG+99VZGjRqV3/3ud3nppZdyzz335IEHHsiuu+6aJDnllFMyadKkvPDCC3n44Ydz5513VmpNTU15/fXXc9RRR+WBBx7Ic889l0mTJuW4447LihUrcv/99+fCCy/Mgw8+mJkzZ+bGG2/MvHnzKq8HYMPmHCkANhjbb799Hn744VxwwQX5xje+kVdffTVbbLFFBg4cmCuuuGK1+Z/+9Kdz6qmnZtSoUVmyZEmGDBmSs846K+eee26SpG3btnnttddyzDHHZM6cOdl8880zdOjQnHfeeUn+shepqakpf/rTn1JXV5dPfvKTueSSS5IkvXv3zj333JMzzjgjhxxySJYsWZI+ffrkk5/8ZNq0aZO6urrcdddd+eEPf5jm5ub06dMnF110UQ499NAP7PcCYP1xaB8AAEBJDu0DAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAoSZACAAAo6f8B0mwfPmRy5pgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "\n",
        "    def __init__(self, dataset, type='train', indices=None, num_samples=None):\n",
        "\n",
        "        # if indices is not provided,\n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset.targets))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # if num_samples is not provided,\n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset\n",
        "        label_to_count = [0] * len(np.unique(dataset.targets))\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            label_to_count[label] += 1\n",
        "\n",
        "        beta = 0.9999\n",
        "        effective_num = 1.0 - np.power(beta, label_to_count)\n",
        "        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [per_cls_weights[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        return dataset.targets[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(torch.multinomial(self.weights, self.num_samples, replacement=True).tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = LambdaLayer(lambda x:\n",
        "                                        F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_s(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_s, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet50(num_classes=1000):\n",
        "    return torchvision.models.resnet50(num_classes=num_classes)"
      ],
      "metadata": {
        "id": "Q0gR0O1hTQew"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "    def __init__(self, header='', fmt='{val:.4f} ({avg:.4f})'):\n",
        "        \"\"\"Base Metric Class\n",
        "        :arg\n",
        "            fmt(str): format representing metric in string\n",
        "        \"\"\"\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.n = 0\n",
        "        self.avg = 0\n",
        "        self.header = header\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        if isinstance(val, torch.Tensor):\n",
        "            val = val.detach().clone()\n",
        "\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.n += n\n",
        "        self.avg = self.sum / self.n\n",
        "\n",
        "    def compute(self):\n",
        "        return self.avg\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.header + ' ' + self.fmt.format(**self.__dict__)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    epoch = epoch + 1\n",
        "    if epoch <= 5:\n",
        "        lr = args.lr * epoch / 5\n",
        "    elif epoch > 180:\n",
        "        lr = args.lr * 0.0001\n",
        "    elif epoch > 160:\n",
        "        lr = args.lr * 0.01\n",
        "    else:\n",
        "        lr = args.lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def train_one_epoch(model, train_dataloader, optimizer, criterion, epoch, args):\n",
        "    # 1. create metric\n",
        "    data_m = Metric(header='Data:')\n",
        "    batch_m = Metric(header='Batch:')\n",
        "    loss_m = Metric(header='Loss:')\n",
        "\n",
        "    # 2. start validate\n",
        "    model.train()\n",
        "\n",
        "    total_iter = len(train_dataloader)\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        data_m.update(time.time() - start_time)\n",
        "\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_m.update(loss, batch_size)\n",
        "\n",
        "        if batch_idx and args.print_freq and batch_idx % args.print_freq == 0:\n",
        "            num_digits = len(str(total_iter))\n",
        "            print(f\"TRAIN({epoch:03}): [{batch_idx:>{num_digits}}/{total_iter}] {batch_m} {data_m} {loss_m}\")\n",
        "\n",
        "        batch_m.update(time.time() - start_time)\n",
        "        start_time = time.time()\n",
        "\n",
        "    # 3. calculate metric\n",
        "    duration = str(datetime.timedelta(seconds=batch_m.sum)).split('.')[0]\n",
        "    data = str(datetime.timedelta(seconds=data_m.sum)).split('.')[0]\n",
        "    f_b_o = str(datetime.timedelta(seconds=batch_m.sum - data_m.sum)).split('.')[0]\n",
        "    loss = loss_m.compute()\n",
        "\n",
        "    # 4. print metric\n",
        "    space = 16\n",
        "    num_metric = 5\n",
        "    print('-'*space*num_metric)\n",
        "    print((\"{:>16}\"*num_metric).format('Stage', 'Batch', 'Data', 'F+B+O', 'Loss'))\n",
        "    print('-'*space*num_metric)\n",
        "    print(f\"{'TRAIN('+str(epoch)+')':>{space}}{duration:>{space}}{data:>{space}}{f_b_o:>{space}}{loss:{space}.4f}\")\n",
        "    print('-'*space*num_metric)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def prediction_submission(predict, args):\n",
        "    \"\"\"\n",
        "    테스트 데이터의 idx와 예측한 prediction label을 submission.csv로 저장\n",
        "    \"\"\"\n",
        "    submission = [[idx, label] for idx, label in enumerate(predict)]\n",
        "    df = pd.DataFrame(data=submission, columns=['id_idx', 'label'], index=None)\n",
        "    args.save_path = Path(args.save_path)\n",
        "    args.save_path.mkdir(exist_ok=True)\n",
        "    df.to_csv(args.save_path / 'submission.csv', index=False)\n",
        "\n",
        "def test_submission(model, test_dataloader, args):\n",
        "    \"\"\"\n",
        "    학습한 모델로 테스트 데이터의 라벨을 예측하고 그 결과를 submission.csv로 저장\n",
        "    \"\"\"\n",
        "    model_predict = [] # for submission.csv\n",
        "    for x,y in test_dataloader:\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "        output = model(x)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "        model_predict.extend(prediction.cpu().numpy())\n",
        "    prediction_submission(model_predict, args)"
      ],
      "metadata": {
        "id": "EF7ilBoGTSyd"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run(args):\n",
        "    start = time.time()\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    # 1. load train, test dataset\n",
        "    train_dataset, test_dataset = load_data(args.data_path, transform_train, transform_val)\n",
        "    train_sampler = None\n",
        "    if args.resample:\n",
        "        train_sampler = ImbalancedDatasetSampler(train_dataset)\n",
        "        per_cls_weights = None\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(args.resample is False),\n",
        "                                  num_workers=args.num_workers, sampler=train_sampler)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    # 2. create model\n",
        "    model = resnet50().to(args.device)\n",
        "\n",
        "    # 3. optimizer, criterion\n",
        "    optimizer = SGD(model.parameters(), lr=args.lr, momentum=args.momentum, weight_decay=args.weight_decay, nesterov=args.nesterov)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4. train & validate\n",
        "    for epoch in range(args.epoch):\n",
        "        adjust_learning_rate(optimizer, epoch, args)\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, epoch, args)\n",
        "\n",
        "    test_submission(model, test_dataloader, args)\n",
        "    end = time.time()\n",
        "    sec = end - start\n",
        "    print(f\"Finished Training & Test at {str(datetime.timedelta(seconds=sec)).split('.')[0]} ....\")"
      ],
      "metadata": {
        "id": "0LZqTCDsTU-6"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup(args)\n",
        "run(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6PGsznTWjs",
        "outputId": "24821550-4058-42b6-a8fa-75661a5eb5c2"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "TRAIN(086): [180/391] Batch: 0.0471 (0.0517) Data: 0.0269 (0.0191) Loss: 0.0615 (0.0657)\n",
            "TRAIN(086): [190/391] Batch: 0.0572 (0.0515) Data: 0.0180 (0.0193) Loss: 0.0146 (0.0656)\n",
            "TRAIN(086): [200/391] Batch: 0.0546 (0.0515) Data: 0.0151 (0.0191) Loss: 0.0697 (0.0661)\n",
            "TRAIN(086): [210/391] Batch: 0.0454 (0.0513) Data: 0.0270 (0.0191) Loss: 0.0386 (0.0656)\n",
            "TRAIN(086): [220/391] Batch: 0.0472 (0.0512) Data: 0.0272 (0.0192) Loss: 0.0570 (0.0661)\n",
            "TRAIN(086): [230/391] Batch: 0.0452 (0.0511) Data: 0.0258 (0.0193) Loss: 0.0886 (0.0665)\n",
            "TRAIN(086): [240/391] Batch: 0.0458 (0.0509) Data: 0.0268 (0.0195) Loss: 0.1922 (0.0667)\n",
            "TRAIN(086): [250/391] Batch: 0.0467 (0.0509) Data: 0.0262 (0.0195) Loss: 0.0406 (0.0666)\n",
            "TRAIN(086): [260/391] Batch: 0.0370 (0.0507) Data: 0.0273 (0.0195) Loss: 0.0724 (0.0672)\n",
            "TRAIN(086): [270/391] Batch: 0.0458 (0.0507) Data: 0.0252 (0.0196) Loss: 0.0778 (0.0671)\n",
            "TRAIN(086): [280/391] Batch: 0.0395 (0.0506) Data: 0.0271 (0.0196) Loss: 0.0425 (0.0670)\n",
            "TRAIN(086): [290/391] Batch: 0.0615 (0.0506) Data: 0.0137 (0.0196) Loss: 0.0485 (0.0671)\n",
            "TRAIN(086): [300/391] Batch: 0.0450 (0.0505) Data: 0.0284 (0.0197) Loss: 0.0349 (0.0674)\n",
            "TRAIN(086): [310/391] Batch: 0.0460 (0.0504) Data: 0.0259 (0.0198) Loss: 0.1155 (0.0677)\n",
            "TRAIN(086): [320/391] Batch: 0.0494 (0.0504) Data: 0.0237 (0.0198) Loss: 0.1167 (0.0679)\n",
            "TRAIN(086): [330/391] Batch: 0.0457 (0.0503) Data: 0.0270 (0.0198) Loss: 0.1220 (0.0685)\n",
            "TRAIN(086): [340/391] Batch: 0.0477 (0.0503) Data: 0.0166 (0.0199) Loss: 0.1090 (0.0687)\n",
            "TRAIN(086): [350/391] Batch: 0.0372 (0.0503) Data: 0.0267 (0.0199) Loss: 0.0497 (0.0691)\n",
            "TRAIN(086): [360/391] Batch: 0.0485 (0.0502) Data: 0.0237 (0.0199) Loss: 0.0215 (0.0692)\n",
            "TRAIN(086): [370/391] Batch: 0.0462 (0.0502) Data: 0.0172 (0.0198) Loss: 0.0236 (0.0695)\n",
            "TRAIN(086): [380/391] Batch: 0.0483 (0.0503) Data: 0.0206 (0.0197) Loss: 0.1242 (0.0697)\n",
            "TRAIN(086): [390/391] Batch: 0.0446 (0.0503) Data: 0.0259 (0.0197) Loss: 0.0401 (0.0698)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(86)         0:00:19         0:00:07         0:00:11          0.0698\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(087): [ 10/391] Batch: 0.0576 (0.0711) Data: 0.0146 (0.0333) Loss: 0.1214 (0.0834)\n",
            "TRAIN(087): [ 20/391] Batch: 0.0457 (0.0610) Data: 0.0228 (0.0246) Loss: 0.1357 (0.0859)\n",
            "TRAIN(087): [ 30/391] Batch: 0.0473 (0.0583) Data: 0.0189 (0.0219) Loss: 0.0328 (0.0843)\n",
            "TRAIN(087): [ 40/391] Batch: 0.0481 (0.0565) Data: 0.0244 (0.0210) Loss: 0.0456 (0.0859)\n",
            "TRAIN(087): [ 50/391] Batch: 0.0464 (0.0548) Data: 0.0273 (0.0212) Loss: 0.0489 (0.0842)\n",
            "TRAIN(087): [ 60/391] Batch: 0.0459 (0.0537) Data: 0.0261 (0.0212) Loss: 0.0829 (0.0843)\n",
            "TRAIN(087): [ 70/391] Batch: 0.0468 (0.0529) Data: 0.0262 (0.0214) Loss: 0.0253 (0.0834)\n",
            "TRAIN(087): [ 80/391] Batch: 0.0435 (0.0525) Data: 0.0247 (0.0214) Loss: 0.0651 (0.0817)\n",
            "TRAIN(087): [ 90/391] Batch: 0.0424 (0.0521) Data: 0.0251 (0.0214) Loss: 0.0414 (0.0804)\n",
            "TRAIN(087): [100/391] Batch: 0.0577 (0.0520) Data: 0.0211 (0.0213) Loss: 0.1003 (0.0807)\n",
            "TRAIN(087): [110/391] Batch: 0.0563 (0.0518) Data: 0.0165 (0.0210) Loss: 0.1212 (0.0819)\n",
            "TRAIN(087): [120/391] Batch: 0.0547 (0.0516) Data: 0.0239 (0.0210) Loss: 0.1209 (0.0848)\n",
            "TRAIN(087): [130/391] Batch: 0.0434 (0.0514) Data: 0.0267 (0.0209) Loss: 0.1354 (0.0881)\n",
            "TRAIN(087): [140/391] Batch: 0.0531 (0.0514) Data: 0.0228 (0.0209) Loss: 0.1134 (0.0877)\n",
            "TRAIN(087): [150/391] Batch: 0.0459 (0.0511) Data: 0.0272 (0.0210) Loss: 0.0675 (0.0870)\n",
            "TRAIN(087): [160/391] Batch: 0.0457 (0.0509) Data: 0.0254 (0.0211) Loss: 0.0262 (0.0865)\n",
            "TRAIN(087): [170/391] Batch: 0.0421 (0.0508) Data: 0.0203 (0.0210) Loss: 0.1123 (0.0856)\n",
            "TRAIN(087): [180/391] Batch: 0.0496 (0.0507) Data: 0.0218 (0.0210) Loss: 0.0414 (0.0845)\n",
            "TRAIN(087): [190/391] Batch: 0.0419 (0.0507) Data: 0.0253 (0.0208) Loss: 0.0428 (0.0832)\n",
            "TRAIN(087): [200/391] Batch: 0.0466 (0.0505) Data: 0.0254 (0.0208) Loss: 0.0547 (0.0827)\n",
            "TRAIN(087): [210/391] Batch: 0.0508 (0.0505) Data: 0.0166 (0.0208) Loss: 0.0658 (0.0829)\n",
            "TRAIN(087): [220/391] Batch: 0.0469 (0.0504) Data: 0.0241 (0.0208) Loss: 0.0784 (0.0826)\n",
            "TRAIN(087): [230/391] Batch: 0.0560 (0.0505) Data: 0.0125 (0.0207) Loss: 0.0724 (0.0819)\n",
            "TRAIN(087): [240/391] Batch: 0.0573 (0.0505) Data: 0.0130 (0.0205) Loss: 0.1084 (0.0822)\n",
            "TRAIN(087): [250/391] Batch: 0.0463 (0.0506) Data: 0.0200 (0.0203) Loss: 0.0659 (0.0813)\n",
            "TRAIN(087): [260/391] Batch: 0.0612 (0.0507) Data: 0.0133 (0.0202) Loss: 0.1289 (0.0809)\n",
            "TRAIN(087): [270/391] Batch: 0.0632 (0.0508) Data: 0.0125 (0.0199) Loss: 0.0865 (0.0811)\n",
            "TRAIN(087): [280/391] Batch: 0.0532 (0.0509) Data: 0.0111 (0.0197) Loss: 0.0458 (0.0803)\n",
            "TRAIN(087): [290/391] Batch: 0.0473 (0.0509) Data: 0.0170 (0.0195) Loss: 0.0747 (0.0799)\n",
            "TRAIN(087): [300/391] Batch: 0.0388 (0.0510) Data: 0.0218 (0.0193) Loss: 0.0555 (0.0797)\n",
            "TRAIN(087): [310/391] Batch: 0.0448 (0.0510) Data: 0.0243 (0.0192) Loss: 0.0467 (0.0788)\n",
            "TRAIN(087): [320/391] Batch: 0.0454 (0.0509) Data: 0.0267 (0.0193) Loss: 0.0792 (0.0789)\n",
            "TRAIN(087): [330/391] Batch: 0.0453 (0.0507) Data: 0.0268 (0.0194) Loss: 0.0531 (0.0793)\n",
            "TRAIN(087): [340/391] Batch: 0.0468 (0.0507) Data: 0.0264 (0.0195) Loss: 0.0380 (0.0789)\n",
            "TRAIN(087): [350/391] Batch: 0.0538 (0.0506) Data: 0.0239 (0.0196) Loss: 0.0267 (0.0789)\n",
            "TRAIN(087): [360/391] Batch: 0.0464 (0.0506) Data: 0.0203 (0.0196) Loss: 0.0803 (0.0794)\n",
            "TRAIN(087): [370/391] Batch: 0.0452 (0.0505) Data: 0.0262 (0.0196) Loss: 0.0142 (0.0788)\n",
            "TRAIN(087): [380/391] Batch: 0.0605 (0.0505) Data: 0.0149 (0.0196) Loss: 0.1072 (0.0788)\n",
            "TRAIN(087): [390/391] Batch: 0.0443 (0.0504) Data: 0.0280 (0.0196) Loss: 0.0970 (0.0792)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(87)         0:00:19         0:00:07         0:00:12          0.0792\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(088): [ 10/391] Batch: 0.0449 (0.0619) Data: 0.0248 (0.0333) Loss: 0.0832 (0.0670)\n",
            "TRAIN(088): [ 20/391] Batch: 0.0484 (0.0561) Data: 0.0253 (0.0260) Loss: 0.1118 (0.0737)\n",
            "TRAIN(088): [ 30/391] Batch: 0.0493 (0.0547) Data: 0.0160 (0.0236) Loss: 0.0680 (0.0748)\n",
            "TRAIN(088): [ 40/391] Batch: 0.0450 (0.0533) Data: 0.0244 (0.0226) Loss: 0.0401 (0.0724)\n",
            "TRAIN(088): [ 50/391] Batch: 0.0483 (0.0530) Data: 0.0189 (0.0215) Loss: 0.0582 (0.0728)\n",
            "TRAIN(088): [ 60/391] Batch: 0.0444 (0.0523) Data: 0.0258 (0.0211) Loss: 0.0549 (0.0710)\n",
            "TRAIN(088): [ 70/391] Batch: 0.0542 (0.0522) Data: 0.0171 (0.0203) Loss: 0.0467 (0.0706)\n",
            "TRAIN(088): [ 80/391] Batch: 0.0474 (0.0515) Data: 0.0269 (0.0205) Loss: 0.0581 (0.0697)\n",
            "TRAIN(088): [ 90/391] Batch: 0.0490 (0.0512) Data: 0.0253 (0.0204) Loss: 0.0838 (0.0707)\n",
            "TRAIN(088): [100/391] Batch: 0.0558 (0.0512) Data: 0.0209 (0.0202) Loss: 0.0429 (0.0700)\n",
            "TRAIN(088): [110/391] Batch: 0.0448 (0.0511) Data: 0.0253 (0.0200) Loss: 0.0700 (0.0693)\n",
            "TRAIN(088): [120/391] Batch: 0.0519 (0.0512) Data: 0.0134 (0.0195) Loss: 0.0724 (0.0712)\n",
            "TRAIN(088): [130/391] Batch: 0.0489 (0.0512) Data: 0.0189 (0.0193) Loss: 0.1025 (0.0713)\n",
            "TRAIN(088): [140/391] Batch: 0.0492 (0.0511) Data: 0.0202 (0.0192) Loss: 0.0520 (0.0719)\n",
            "TRAIN(088): [150/391] Batch: 0.0485 (0.0511) Data: 0.0193 (0.0191) Loss: 0.0769 (0.0718)\n",
            "TRAIN(088): [160/391] Batch: 0.0442 (0.0511) Data: 0.0185 (0.0188) Loss: 0.0526 (0.0717)\n",
            "TRAIN(088): [170/391] Batch: 0.0502 (0.0510) Data: 0.0183 (0.0188) Loss: 0.1462 (0.0714)\n",
            "TRAIN(088): [180/391] Batch: 0.0643 (0.0511) Data: 0.0128 (0.0186) Loss: 0.1245 (0.0713)\n",
            "TRAIN(088): [190/391] Batch: 0.0534 (0.0511) Data: 0.0133 (0.0184) Loss: 0.1286 (0.0708)\n",
            "TRAIN(088): [200/391] Batch: 0.0466 (0.0511) Data: 0.0240 (0.0185) Loss: 0.0947 (0.0716)\n",
            "TRAIN(088): [210/391] Batch: 0.0455 (0.0509) Data: 0.0267 (0.0187) Loss: 0.1353 (0.0723)\n",
            "TRAIN(088): [220/391] Batch: 0.0484 (0.0507) Data: 0.0257 (0.0188) Loss: 0.0969 (0.0721)\n",
            "TRAIN(088): [230/391] Batch: 0.0481 (0.0507) Data: 0.0151 (0.0188) Loss: 0.0962 (0.0722)\n",
            "TRAIN(088): [240/391] Batch: 0.0586 (0.0508) Data: 0.0152 (0.0187) Loss: 0.1439 (0.0723)\n",
            "TRAIN(088): [250/391] Batch: 0.0385 (0.0507) Data: 0.0255 (0.0188) Loss: 0.0444 (0.0724)\n",
            "TRAIN(088): [260/391] Batch: 0.0465 (0.0507) Data: 0.0221 (0.0188) Loss: 0.1454 (0.0727)\n",
            "TRAIN(088): [270/391] Batch: 0.0477 (0.0506) Data: 0.0247 (0.0189) Loss: 0.0378 (0.0725)\n",
            "TRAIN(088): [280/391] Batch: 0.0453 (0.0505) Data: 0.0257 (0.0189) Loss: 0.0473 (0.0721)\n",
            "TRAIN(088): [290/391] Batch: 0.0533 (0.0505) Data: 0.0242 (0.0189) Loss: 0.0807 (0.0717)\n",
            "TRAIN(088): [300/391] Batch: 0.0462 (0.0504) Data: 0.0253 (0.0190) Loss: 0.0413 (0.0712)\n",
            "TRAIN(088): [310/391] Batch: 0.0477 (0.0503) Data: 0.0248 (0.0191) Loss: 0.0871 (0.0710)\n",
            "TRAIN(088): [320/391] Batch: 0.0430 (0.0502) Data: 0.0252 (0.0192) Loss: 0.1434 (0.0712)\n",
            "TRAIN(088): [330/391] Batch: 0.0459 (0.0501) Data: 0.0257 (0.0193) Loss: 0.1140 (0.0718)\n",
            "TRAIN(088): [340/391] Batch: 0.0445 (0.0501) Data: 0.0272 (0.0192) Loss: 0.0767 (0.0715)\n",
            "TRAIN(088): [350/391] Batch: 0.0460 (0.0500) Data: 0.0262 (0.0193) Loss: 0.1207 (0.0712)\n",
            "TRAIN(088): [360/391] Batch: 0.0469 (0.0500) Data: 0.0239 (0.0194) Loss: 0.0696 (0.0710)\n",
            "TRAIN(088): [370/391] Batch: 0.0472 (0.0499) Data: 0.0267 (0.0195) Loss: 0.0961 (0.0707)\n",
            "TRAIN(088): [380/391] Batch: 0.0559 (0.0499) Data: 0.0216 (0.0195) Loss: 0.0404 (0.0705)\n",
            "TRAIN(088): [390/391] Batch: 0.0482 (0.0499) Data: 0.0229 (0.0195) Loss: 0.0847 (0.0703)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(88)         0:00:19         0:00:07         0:00:11          0.0703\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(089): [ 10/391] Batch: 0.0540 (0.0724) Data: 0.0139 (0.0361) Loss: 0.0639 (0.0784)\n",
            "TRAIN(089): [ 20/391] Batch: 0.0649 (0.0634) Data: 0.0111 (0.0261) Loss: 0.0643 (0.0702)\n",
            "TRAIN(089): [ 30/391] Batch: 0.0582 (0.0594) Data: 0.0132 (0.0218) Loss: 0.1341 (0.0717)\n",
            "TRAIN(089): [ 40/391] Batch: 0.0489 (0.0571) Data: 0.0193 (0.0200) Loss: 0.1036 (0.0765)\n",
            "TRAIN(089): [ 50/391] Batch: 0.0507 (0.0559) Data: 0.0200 (0.0191) Loss: 0.0961 (0.0766)\n",
            "TRAIN(089): [ 60/391] Batch: 0.0426 (0.0551) Data: 0.0257 (0.0188) Loss: 0.0443 (0.0771)\n",
            "TRAIN(089): [ 70/391] Batch: 0.0476 (0.0542) Data: 0.0268 (0.0193) Loss: 0.0405 (0.0741)\n",
            "TRAIN(089): [ 80/391] Batch: 0.0466 (0.0536) Data: 0.0247 (0.0193) Loss: 0.0807 (0.0727)\n",
            "TRAIN(089): [ 90/391] Batch: 0.0426 (0.0530) Data: 0.0240 (0.0195) Loss: 0.0909 (0.0712)\n",
            "TRAIN(089): [100/391] Batch: 0.0458 (0.0526) Data: 0.0256 (0.0197) Loss: 0.0426 (0.0698)\n",
            "TRAIN(089): [110/391] Batch: 0.0500 (0.0523) Data: 0.0186 (0.0196) Loss: 0.0635 (0.0682)\n",
            "TRAIN(089): [120/391] Batch: 0.0489 (0.0523) Data: 0.0154 (0.0193) Loss: 0.0557 (0.0676)\n",
            "TRAIN(089): [130/391] Batch: 0.0424 (0.0521) Data: 0.0236 (0.0191) Loss: 0.0469 (0.0686)\n",
            "TRAIN(089): [140/391] Batch: 0.0510 (0.0519) Data: 0.0225 (0.0192) Loss: 0.0554 (0.0666)\n",
            "TRAIN(089): [150/391] Batch: 0.0458 (0.0516) Data: 0.0260 (0.0194) Loss: 0.0379 (0.0663)\n",
            "TRAIN(089): [160/391] Batch: 0.0483 (0.0514) Data: 0.0248 (0.0195) Loss: 0.0779 (0.0666)\n",
            "TRAIN(089): [170/391] Batch: 0.0436 (0.0514) Data: 0.0177 (0.0194) Loss: 0.1125 (0.0676)\n",
            "TRAIN(089): [180/391] Batch: 0.0463 (0.0511) Data: 0.0277 (0.0195) Loss: 0.0976 (0.0680)\n",
            "TRAIN(089): [190/391] Batch: 0.0527 (0.0511) Data: 0.0170 (0.0195) Loss: 0.1210 (0.0676)\n",
            "TRAIN(089): [200/391] Batch: 0.0550 (0.0511) Data: 0.0245 (0.0194) Loss: 0.0582 (0.0675)\n",
            "TRAIN(089): [210/391] Batch: 0.0482 (0.0510) Data: 0.0274 (0.0194) Loss: 0.0629 (0.0678)\n",
            "TRAIN(089): [220/391] Batch: 0.0370 (0.0509) Data: 0.0249 (0.0194) Loss: 0.0405 (0.0670)\n",
            "TRAIN(089): [230/391] Batch: 0.0473 (0.0508) Data: 0.0273 (0.0195) Loss: 0.0419 (0.0676)\n",
            "TRAIN(089): [240/391] Batch: 0.0503 (0.0508) Data: 0.0240 (0.0195) Loss: 0.0669 (0.0678)\n",
            "TRAIN(089): [250/391] Batch: 0.0384 (0.0507) Data: 0.0234 (0.0195) Loss: 0.0840 (0.0681)\n",
            "TRAIN(089): [260/391] Batch: 0.0528 (0.0507) Data: 0.0195 (0.0195) Loss: 0.0271 (0.0678)\n",
            "TRAIN(089): [270/391] Batch: 0.0444 (0.0507) Data: 0.0152 (0.0194) Loss: 0.1021 (0.0686)\n",
            "TRAIN(089): [280/391] Batch: 0.0515 (0.0507) Data: 0.0175 (0.0192) Loss: 0.0413 (0.0682)\n",
            "TRAIN(089): [290/391] Batch: 0.0518 (0.0508) Data: 0.0192 (0.0191) Loss: 0.0961 (0.0689)\n",
            "TRAIN(089): [300/391] Batch: 0.0558 (0.0508) Data: 0.0121 (0.0190) Loss: 0.1000 (0.0687)\n",
            "TRAIN(089): [310/391] Batch: 0.0476 (0.0508) Data: 0.0158 (0.0188) Loss: 0.0575 (0.0686)\n",
            "TRAIN(089): [320/391] Batch: 0.0520 (0.0508) Data: 0.0165 (0.0187) Loss: 0.1192 (0.0688)\n",
            "TRAIN(089): [330/391] Batch: 0.0559 (0.0508) Data: 0.0232 (0.0187) Loss: 0.0600 (0.0692)\n",
            "TRAIN(089): [340/391] Batch: 0.0522 (0.0508) Data: 0.0161 (0.0187) Loss: 0.0661 (0.0691)\n",
            "TRAIN(089): [350/391] Batch: 0.0523 (0.0507) Data: 0.0241 (0.0188) Loss: 0.0933 (0.0694)\n",
            "TRAIN(089): [360/391] Batch: 0.0492 (0.0507) Data: 0.0182 (0.0187) Loss: 0.0129 (0.0688)\n",
            "TRAIN(089): [370/391] Batch: 0.0483 (0.0506) Data: 0.0249 (0.0188) Loss: 0.1202 (0.0689)\n",
            "TRAIN(089): [380/391] Batch: 0.0468 (0.0506) Data: 0.0267 (0.0189) Loss: 0.2476 (0.0691)\n",
            "TRAIN(089): [390/391] Batch: 0.0462 (0.0505) Data: 0.0261 (0.0190) Loss: 0.0982 (0.0694)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(89)         0:00:19         0:00:07         0:00:12          0.0694\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(090): [ 10/391] Batch: 0.0451 (0.0633) Data: 0.0257 (0.0353) Loss: 0.0473 (0.0618)\n",
            "TRAIN(090): [ 20/391] Batch: 0.0551 (0.0579) Data: 0.0238 (0.0268) Loss: 0.1093 (0.0688)\n",
            "TRAIN(090): [ 30/391] Batch: 0.0449 (0.0547) Data: 0.0266 (0.0252) Loss: 0.0374 (0.0629)\n",
            "TRAIN(090): [ 40/391] Batch: 0.0489 (0.0528) Data: 0.0273 (0.0246) Loss: 0.0606 (0.0653)\n",
            "TRAIN(090): [ 50/391] Batch: 0.0483 (0.0521) Data: 0.0258 (0.0239) Loss: 0.0587 (0.0639)\n",
            "TRAIN(090): [ 60/391] Batch: 0.0601 (0.0516) Data: 0.0203 (0.0232) Loss: 0.0378 (0.0636)\n",
            "TRAIN(090): [ 70/391] Batch: 0.0376 (0.0511) Data: 0.0239 (0.0228) Loss: 0.0612 (0.0631)\n",
            "TRAIN(090): [ 80/391] Batch: 0.0436 (0.0508) Data: 0.0241 (0.0224) Loss: 0.0318 (0.0647)\n",
            "TRAIN(090): [ 90/391] Batch: 0.0409 (0.0508) Data: 0.0209 (0.0219) Loss: 0.0632 (0.0659)\n",
            "TRAIN(090): [100/391] Batch: 0.0560 (0.0510) Data: 0.0162 (0.0212) Loss: 0.0451 (0.0654)\n",
            "TRAIN(090): [110/391] Batch: 0.0552 (0.0507) Data: 0.0213 (0.0213) Loss: 0.1288 (0.0675)\n",
            "TRAIN(090): [120/391] Batch: 0.0541 (0.0506) Data: 0.0215 (0.0211) Loss: 0.0409 (0.0661)\n",
            "TRAIN(090): [130/391] Batch: 0.0710 (0.0507) Data: 0.0120 (0.0208) Loss: 0.0573 (0.0659)\n",
            "TRAIN(090): [140/391] Batch: 0.0435 (0.0508) Data: 0.0222 (0.0205) Loss: 0.0496 (0.0657)\n",
            "TRAIN(090): [150/391] Batch: 0.0510 (0.0509) Data: 0.0209 (0.0202) Loss: 0.0652 (0.0658)\n",
            "TRAIN(090): [160/391] Batch: 0.0507 (0.0509) Data: 0.0201 (0.0201) Loss: 0.0533 (0.0658)\n",
            "TRAIN(090): [170/391] Batch: 0.0561 (0.0510) Data: 0.0170 (0.0199) Loss: 0.0928 (0.0660)\n",
            "TRAIN(090): [180/391] Batch: 0.0568 (0.0511) Data: 0.0176 (0.0196) Loss: 0.1564 (0.0676)\n",
            "TRAIN(090): [190/391] Batch: 0.0567 (0.0511) Data: 0.0163 (0.0194) Loss: 0.0448 (0.0676)\n",
            "TRAIN(090): [200/391] Batch: 0.0447 (0.0512) Data: 0.0250 (0.0193) Loss: 0.0458 (0.0673)\n",
            "TRAIN(090): [210/391] Batch: 0.0559 (0.0510) Data: 0.0234 (0.0194) Loss: 0.0938 (0.0677)\n",
            "TRAIN(090): [220/391] Batch: 0.0460 (0.0509) Data: 0.0252 (0.0196) Loss: 0.1160 (0.0691)\n",
            "TRAIN(090): [230/391] Batch: 0.0578 (0.0509) Data: 0.0152 (0.0195) Loss: 0.0401 (0.0685)\n",
            "TRAIN(090): [240/391] Batch: 0.0428 (0.0507) Data: 0.0254 (0.0196) Loss: 0.0602 (0.0684)\n",
            "TRAIN(090): [250/391] Batch: 0.0468 (0.0505) Data: 0.0262 (0.0197) Loss: 0.0950 (0.0685)\n",
            "TRAIN(090): [260/391] Batch: 0.0465 (0.0504) Data: 0.0267 (0.0199) Loss: 0.1077 (0.0692)\n",
            "TRAIN(090): [270/391] Batch: 0.0571 (0.0504) Data: 0.0156 (0.0199) Loss: 0.1531 (0.0703)\n",
            "TRAIN(090): [280/391] Batch: 0.0458 (0.0503) Data: 0.0254 (0.0199) Loss: 0.0355 (0.0704)\n",
            "TRAIN(090): [290/391] Batch: 0.0560 (0.0504) Data: 0.0209 (0.0199) Loss: 0.1119 (0.0703)\n",
            "TRAIN(090): [300/391] Batch: 0.0378 (0.0503) Data: 0.0243 (0.0199) Loss: 0.1110 (0.0710)\n",
            "TRAIN(090): [310/391] Batch: 0.0481 (0.0502) Data: 0.0259 (0.0200) Loss: 0.0207 (0.0711)\n",
            "TRAIN(090): [320/391] Batch: 0.0456 (0.0502) Data: 0.0261 (0.0200) Loss: 0.1014 (0.0710)\n",
            "TRAIN(090): [330/391] Batch: 0.0462 (0.0501) Data: 0.0268 (0.0200) Loss: 0.0685 (0.0710)\n",
            "TRAIN(090): [340/391] Batch: 0.0552 (0.0501) Data: 0.0220 (0.0201) Loss: 0.0826 (0.0712)\n",
            "TRAIN(090): [350/391] Batch: 0.0457 (0.0501) Data: 0.0271 (0.0201) Loss: 0.0572 (0.0716)\n",
            "TRAIN(090): [360/391] Batch: 0.0464 (0.0500) Data: 0.0269 (0.0201) Loss: 0.0757 (0.0718)\n",
            "TRAIN(090): [370/391] Batch: 0.0464 (0.0500) Data: 0.0264 (0.0202) Loss: 0.0686 (0.0721)\n",
            "TRAIN(090): [380/391] Batch: 0.0431 (0.0500) Data: 0.0189 (0.0202) Loss: 0.0337 (0.0721)\n",
            "TRAIN(090): [390/391] Batch: 0.0477 (0.0499) Data: 0.0260 (0.0202) Loss: 0.1071 (0.0726)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(90)         0:00:19         0:00:07         0:00:11          0.0726\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(091): [ 10/391] Batch: 0.0440 (0.0640) Data: 0.0204 (0.0344) Loss: 0.0689 (0.0723)\n",
            "TRAIN(091): [ 20/391] Batch: 0.0535 (0.0589) Data: 0.0183 (0.0249) Loss: 0.1727 (0.0717)\n",
            "TRAIN(091): [ 30/391] Batch: 0.0465 (0.0565) Data: 0.0149 (0.0216) Loss: 0.1275 (0.0699)\n",
            "TRAIN(091): [ 40/391] Batch: 0.0598 (0.0553) Data: 0.0131 (0.0197) Loss: 0.0556 (0.0694)\n",
            "TRAIN(091): [ 50/391] Batch: 0.0462 (0.0546) Data: 0.0206 (0.0188) Loss: 0.1056 (0.0754)\n",
            "TRAIN(091): [ 60/391] Batch: 0.0497 (0.0539) Data: 0.0208 (0.0183) Loss: 0.1404 (0.0760)\n",
            "TRAIN(091): [ 70/391] Batch: 0.0595 (0.0538) Data: 0.0134 (0.0179) Loss: 0.0743 (0.0767)\n",
            "TRAIN(091): [ 80/391] Batch: 0.0451 (0.0531) Data: 0.0257 (0.0180) Loss: 0.1188 (0.0769)\n",
            "TRAIN(091): [ 90/391] Batch: 0.0514 (0.0527) Data: 0.0157 (0.0183) Loss: 0.0325 (0.0761)\n",
            "TRAIN(091): [100/391] Batch: 0.0646 (0.0522) Data: 0.0141 (0.0185) Loss: 0.0577 (0.0762)\n",
            "TRAIN(091): [110/391] Batch: 0.0428 (0.0520) Data: 0.0224 (0.0185) Loss: 0.0736 (0.0756)\n",
            "TRAIN(091): [120/391] Batch: 0.0520 (0.0519) Data: 0.0235 (0.0187) Loss: 0.0689 (0.0756)\n",
            "TRAIN(091): [130/391] Batch: 0.0488 (0.0518) Data: 0.0269 (0.0189) Loss: 0.1640 (0.0754)\n",
            "TRAIN(091): [140/391] Batch: 0.0521 (0.0517) Data: 0.0240 (0.0192) Loss: 0.0998 (0.0764)\n",
            "TRAIN(091): [150/391] Batch: 0.0493 (0.0515) Data: 0.0244 (0.0193) Loss: 0.0524 (0.0754)\n",
            "TRAIN(091): [160/391] Batch: 0.0524 (0.0516) Data: 0.0219 (0.0193) Loss: 0.0667 (0.0753)\n",
            "TRAIN(091): [170/391] Batch: 0.0449 (0.0515) Data: 0.0258 (0.0195) Loss: 0.1015 (0.0760)\n",
            "TRAIN(091): [180/391] Batch: 0.0516 (0.0513) Data: 0.0176 (0.0195) Loss: 0.0482 (0.0758)\n",
            "TRAIN(091): [190/391] Batch: 0.0455 (0.0512) Data: 0.0270 (0.0196) Loss: 0.0335 (0.0747)\n",
            "TRAIN(091): [200/391] Batch: 0.0453 (0.0511) Data: 0.0236 (0.0197) Loss: 0.0537 (0.0746)\n",
            "TRAIN(091): [210/391] Batch: 0.0488 (0.0509) Data: 0.0233 (0.0198) Loss: 0.1207 (0.0749)\n",
            "TRAIN(091): [220/391] Batch: 0.0532 (0.0508) Data: 0.0200 (0.0199) Loss: 0.0728 (0.0742)\n",
            "TRAIN(091): [230/391] Batch: 0.0460 (0.0507) Data: 0.0212 (0.0199) Loss: 0.0663 (0.0738)\n",
            "TRAIN(091): [240/391] Batch: 0.0469 (0.0506) Data: 0.0271 (0.0200) Loss: 0.0691 (0.0735)\n",
            "TRAIN(091): [250/391] Batch: 0.0450 (0.0504) Data: 0.0270 (0.0202) Loss: 0.0562 (0.0727)\n",
            "TRAIN(091): [260/391] Batch: 0.0492 (0.0504) Data: 0.0254 (0.0202) Loss: 0.0434 (0.0726)\n",
            "TRAIN(091): [270/391] Batch: 0.0463 (0.0504) Data: 0.0186 (0.0202) Loss: 0.0609 (0.0724)\n",
            "TRAIN(091): [280/391] Batch: 0.0535 (0.0504) Data: 0.0198 (0.0201) Loss: 0.0822 (0.0721)\n",
            "TRAIN(091): [290/391] Batch: 0.0341 (0.0504) Data: 0.0259 (0.0199) Loss: 0.0796 (0.0722)\n",
            "TRAIN(091): [300/391] Batch: 0.0485 (0.0504) Data: 0.0204 (0.0198) Loss: 0.0584 (0.0719)\n",
            "TRAIN(091): [310/391] Batch: 0.0523 (0.0505) Data: 0.0157 (0.0198) Loss: 0.0422 (0.0715)\n",
            "TRAIN(091): [320/391] Batch: 0.0513 (0.0505) Data: 0.0188 (0.0196) Loss: 0.0631 (0.0716)\n",
            "TRAIN(091): [330/391] Batch: 0.0355 (0.0505) Data: 0.0256 (0.0196) Loss: 0.0641 (0.0712)\n",
            "TRAIN(091): [340/391] Batch: 0.0552 (0.0506) Data: 0.0135 (0.0195) Loss: 0.0598 (0.0708)\n",
            "TRAIN(091): [350/391] Batch: 0.0467 (0.0505) Data: 0.0278 (0.0195) Loss: 0.1240 (0.0710)\n",
            "TRAIN(091): [360/391] Batch: 0.0403 (0.0505) Data: 0.0270 (0.0195) Loss: 0.0728 (0.0708)\n",
            "TRAIN(091): [370/391] Batch: 0.0438 (0.0505) Data: 0.0237 (0.0195) Loss: 0.1011 (0.0709)\n",
            "TRAIN(091): [380/391] Batch: 0.0465 (0.0504) Data: 0.0261 (0.0196) Loss: 0.1087 (0.0713)\n",
            "TRAIN(091): [390/391] Batch: 0.0464 (0.0503) Data: 0.0280 (0.0197) Loss: 0.1488 (0.0720)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(91)         0:00:19         0:00:07         0:00:11          0.0720\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(092): [ 10/391] Batch: 0.0469 (0.0654) Data: 0.0259 (0.0315) Loss: 0.1409 (0.0889)\n",
            "TRAIN(092): [ 20/391] Batch: 0.0504 (0.0571) Data: 0.0164 (0.0265) Loss: 0.0775 (0.0810)\n",
            "TRAIN(092): [ 30/391] Batch: 0.0456 (0.0541) Data: 0.0263 (0.0244) Loss: 0.0509 (0.0749)\n",
            "TRAIN(092): [ 40/391] Batch: 0.0571 (0.0526) Data: 0.0192 (0.0240) Loss: 0.0515 (0.0707)\n",
            "TRAIN(092): [ 50/391] Batch: 0.0373 (0.0518) Data: 0.0267 (0.0228) Loss: 0.0451 (0.0744)\n",
            "TRAIN(092): [ 60/391] Batch: 0.0542 (0.0514) Data: 0.0170 (0.0223) Loss: 0.0546 (0.0757)\n",
            "TRAIN(092): [ 70/391] Batch: 0.0453 (0.0509) Data: 0.0262 (0.0221) Loss: 0.1095 (0.0740)\n",
            "TRAIN(092): [ 80/391] Batch: 0.0471 (0.0506) Data: 0.0234 (0.0219) Loss: 0.1174 (0.0767)\n",
            "TRAIN(092): [ 90/391] Batch: 0.0461 (0.0503) Data: 0.0274 (0.0220) Loss: 0.0352 (0.0740)\n",
            "TRAIN(092): [100/391] Batch: 0.0521 (0.0501) Data: 0.0202 (0.0221) Loss: 0.0972 (0.0758)\n",
            "TRAIN(092): [110/391] Batch: 0.0471 (0.0498) Data: 0.0268 (0.0221) Loss: 0.0507 (0.0757)\n",
            "TRAIN(092): [120/391] Batch: 0.0534 (0.0497) Data: 0.0249 (0.0222) Loss: 0.0722 (0.0749)\n",
            "TRAIN(092): [130/391] Batch: 0.0461 (0.0495) Data: 0.0269 (0.0223) Loss: 0.0579 (0.0743)\n",
            "TRAIN(092): [140/391] Batch: 0.0431 (0.0495) Data: 0.0261 (0.0221) Loss: 0.0448 (0.0745)\n",
            "TRAIN(092): [150/391] Batch: 0.0499 (0.0494) Data: 0.0250 (0.0220) Loss: 0.1189 (0.0742)\n",
            "TRAIN(092): [160/391] Batch: 0.0556 (0.0496) Data: 0.0152 (0.0216) Loss: 0.0796 (0.0738)\n",
            "TRAIN(092): [170/391] Batch: 0.0582 (0.0497) Data: 0.0147 (0.0213) Loss: 0.0409 (0.0737)\n",
            "TRAIN(092): [180/391] Batch: 0.0421 (0.0496) Data: 0.0212 (0.0211) Loss: 0.1060 (0.0742)\n",
            "TRAIN(092): [190/391] Batch: 0.0458 (0.0497) Data: 0.0208 (0.0209) Loss: 0.1226 (0.0741)\n",
            "TRAIN(092): [200/391] Batch: 0.0575 (0.0498) Data: 0.0190 (0.0207) Loss: 0.0606 (0.0730)\n",
            "TRAIN(092): [210/391] Batch: 0.0483 (0.0497) Data: 0.0221 (0.0205) Loss: 0.0586 (0.0727)\n",
            "TRAIN(092): [220/391] Batch: 0.0429 (0.0497) Data: 0.0280 (0.0204) Loss: 0.0370 (0.0725)\n",
            "TRAIN(092): [230/391] Batch: 0.0480 (0.0497) Data: 0.0231 (0.0204) Loss: 0.0511 (0.0719)\n",
            "TRAIN(092): [240/391] Batch: 0.0454 (0.0496) Data: 0.0271 (0.0205) Loss: 0.0426 (0.0711)\n",
            "TRAIN(092): [250/391] Batch: 0.0439 (0.0495) Data: 0.0197 (0.0205) Loss: 0.0808 (0.0707)\n",
            "TRAIN(092): [260/391] Batch: 0.0449 (0.0495) Data: 0.0271 (0.0206) Loss: 0.1045 (0.0708)\n",
            "TRAIN(092): [270/391] Batch: 0.0476 (0.0495) Data: 0.0233 (0.0205) Loss: 0.0605 (0.0706)\n",
            "TRAIN(092): [280/391] Batch: 0.0498 (0.0495) Data: 0.0262 (0.0205) Loss: 0.1170 (0.0708)\n",
            "TRAIN(092): [290/391] Batch: 0.0613 (0.0495) Data: 0.0174 (0.0205) Loss: 0.0400 (0.0703)\n",
            "TRAIN(092): [300/391] Batch: 0.0455 (0.0494) Data: 0.0269 (0.0206) Loss: 0.0583 (0.0703)\n",
            "TRAIN(092): [310/391] Batch: 0.0487 (0.0494) Data: 0.0260 (0.0207) Loss: 0.0933 (0.0702)\n",
            "TRAIN(092): [320/391] Batch: 0.0554 (0.0494) Data: 0.0226 (0.0206) Loss: 0.0832 (0.0700)\n",
            "TRAIN(092): [330/391] Batch: 0.0569 (0.0494) Data: 0.0246 (0.0207) Loss: 0.0355 (0.0696)\n",
            "TRAIN(092): [340/391] Batch: 0.0502 (0.0494) Data: 0.0265 (0.0208) Loss: 0.0355 (0.0692)\n",
            "TRAIN(092): [350/391] Batch: 0.0488 (0.0494) Data: 0.0266 (0.0208) Loss: 0.1415 (0.0694)\n",
            "TRAIN(092): [360/391] Batch: 0.0525 (0.0494) Data: 0.0189 (0.0208) Loss: 0.1058 (0.0698)\n",
            "TRAIN(092): [370/391] Batch: 0.0479 (0.0493) Data: 0.0254 (0.0209) Loss: 0.0443 (0.0699)\n",
            "TRAIN(092): [380/391] Batch: 0.0525 (0.0492) Data: 0.0250 (0.0209) Loss: 0.0614 (0.0700)\n",
            "TRAIN(092): [390/391] Batch: 0.0458 (0.0492) Data: 0.0286 (0.0210) Loss: 0.0501 (0.0700)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(92)         0:00:19         0:00:08         0:00:11          0.0700\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(093): [ 10/391] Batch: 0.0508 (0.0663) Data: 0.0179 (0.0298) Loss: 0.0724 (0.0645)\n",
            "TRAIN(093): [ 20/391] Batch: 0.0480 (0.0581) Data: 0.0180 (0.0240) Loss: 0.0707 (0.0681)\n",
            "TRAIN(093): [ 30/391] Batch: 0.0500 (0.0549) Data: 0.0170 (0.0232) Loss: 0.1014 (0.0667)\n",
            "TRAIN(093): [ 40/391] Batch: 0.0623 (0.0540) Data: 0.0148 (0.0214) Loss: 0.0651 (0.0688)\n",
            "TRAIN(093): [ 50/391] Batch: 0.0610 (0.0535) Data: 0.0149 (0.0205) Loss: 0.0665 (0.0682)\n",
            "TRAIN(093): [ 60/391] Batch: 0.0417 (0.0529) Data: 0.0245 (0.0198) Loss: 0.0797 (0.0702)\n",
            "TRAIN(093): [ 70/391] Batch: 0.0436 (0.0526) Data: 0.0239 (0.0197) Loss: 0.0636 (0.0723)\n",
            "TRAIN(093): [ 80/391] Batch: 0.0512 (0.0524) Data: 0.0202 (0.0195) Loss: 0.0588 (0.0716)\n",
            "TRAIN(093): [ 90/391] Batch: 0.0450 (0.0522) Data: 0.0213 (0.0193) Loss: 0.1058 (0.0721)\n",
            "TRAIN(093): [100/391] Batch: 0.0426 (0.0520) Data: 0.0271 (0.0195) Loss: 0.0819 (0.0731)\n",
            "TRAIN(093): [110/391] Batch: 0.0463 (0.0517) Data: 0.0279 (0.0195) Loss: 0.0732 (0.0731)\n",
            "TRAIN(093): [120/391] Batch: 0.0495 (0.0516) Data: 0.0242 (0.0197) Loss: 0.0246 (0.0710)\n",
            "TRAIN(093): [130/391] Batch: 0.0586 (0.0514) Data: 0.0178 (0.0196) Loss: 0.0593 (0.0707)\n",
            "TRAIN(093): [140/391] Batch: 0.0485 (0.0513) Data: 0.0198 (0.0197) Loss: 0.1192 (0.0710)\n",
            "TRAIN(093): [150/391] Batch: 0.0516 (0.0510) Data: 0.0245 (0.0199) Loss: 0.0652 (0.0701)\n",
            "TRAIN(093): [160/391] Batch: 0.0472 (0.0508) Data: 0.0266 (0.0200) Loss: 0.0447 (0.0712)\n",
            "TRAIN(093): [170/391] Batch: 0.0458 (0.0507) Data: 0.0201 (0.0201) Loss: 0.0499 (0.0703)\n",
            "TRAIN(093): [180/391] Batch: 0.0545 (0.0506) Data: 0.0246 (0.0200) Loss: 0.0818 (0.0703)\n",
            "TRAIN(093): [190/391] Batch: 0.0519 (0.0505) Data: 0.0257 (0.0201) Loss: 0.0457 (0.0700)\n",
            "TRAIN(093): [200/391] Batch: 0.0519 (0.0505) Data: 0.0170 (0.0200) Loss: 0.0828 (0.0700)\n",
            "TRAIN(093): [210/391] Batch: 0.0424 (0.0504) Data: 0.0269 (0.0199) Loss: 0.0386 (0.0708)\n",
            "TRAIN(093): [220/391] Batch: 0.0491 (0.0505) Data: 0.0189 (0.0198) Loss: 0.1433 (0.0716)\n",
            "TRAIN(093): [230/391] Batch: 0.0522 (0.0504) Data: 0.0258 (0.0198) Loss: 0.0810 (0.0715)\n",
            "TRAIN(093): [240/391] Batch: 0.0538 (0.0503) Data: 0.0225 (0.0199) Loss: 0.0531 (0.0722)\n",
            "TRAIN(093): [250/391] Batch: 0.0441 (0.0503) Data: 0.0174 (0.0199) Loss: 0.0577 (0.0739)\n",
            "TRAIN(093): [260/391] Batch: 0.0470 (0.0502) Data: 0.0270 (0.0199) Loss: 0.1289 (0.0736)\n",
            "TRAIN(093): [270/391] Batch: 0.0470 (0.0501) Data: 0.0265 (0.0201) Loss: 0.0847 (0.0734)\n",
            "TRAIN(093): [280/391] Batch: 0.0438 (0.0501) Data: 0.0236 (0.0201) Loss: 0.0503 (0.0730)\n",
            "TRAIN(093): [290/391] Batch: 0.0553 (0.0500) Data: 0.0235 (0.0201) Loss: 0.1022 (0.0729)\n",
            "TRAIN(093): [300/391] Batch: 0.0514 (0.0500) Data: 0.0188 (0.0201) Loss: 0.1089 (0.0730)\n",
            "TRAIN(093): [310/391] Batch: 0.0645 (0.0501) Data: 0.0152 (0.0201) Loss: 0.0932 (0.0726)\n",
            "TRAIN(093): [320/391] Batch: 0.0609 (0.0501) Data: 0.0142 (0.0200) Loss: 0.0601 (0.0732)\n",
            "TRAIN(093): [330/391] Batch: 0.0471 (0.0501) Data: 0.0214 (0.0199) Loss: 0.0480 (0.0728)\n",
            "TRAIN(093): [340/391] Batch: 0.0480 (0.0501) Data: 0.0219 (0.0198) Loss: 0.0687 (0.0727)\n",
            "TRAIN(093): [350/391] Batch: 0.0534 (0.0501) Data: 0.0202 (0.0197) Loss: 0.0544 (0.0726)\n",
            "TRAIN(093): [360/391] Batch: 0.0420 (0.0502) Data: 0.0196 (0.0196) Loss: 0.0884 (0.0727)\n",
            "TRAIN(093): [370/391] Batch: 0.0532 (0.0502) Data: 0.0241 (0.0197) Loss: 0.1207 (0.0724)\n",
            "TRAIN(093): [380/391] Batch: 0.0477 (0.0501) Data: 0.0261 (0.0197) Loss: 0.0612 (0.0728)\n",
            "TRAIN(093): [390/391] Batch: 0.0478 (0.0501) Data: 0.0270 (0.0198) Loss: 0.1612 (0.0727)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(93)         0:00:19         0:00:07         0:00:11          0.0727\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(094): [ 10/391] Batch: 0.0490 (0.0665) Data: 0.0248 (0.0313) Loss: 0.0786 (0.0676)\n",
            "TRAIN(094): [ 20/391] Batch: 0.0469 (0.0582) Data: 0.0268 (0.0270) Loss: 0.0345 (0.0699)\n",
            "TRAIN(094): [ 30/391] Batch: 0.0464 (0.0550) Data: 0.0274 (0.0253) Loss: 0.0724 (0.0739)\n",
            "TRAIN(094): [ 40/391] Batch: 0.0441 (0.0535) Data: 0.0268 (0.0241) Loss: 0.1478 (0.0757)\n",
            "TRAIN(094): [ 50/391] Batch: 0.0411 (0.0526) Data: 0.0258 (0.0236) Loss: 0.0337 (0.0763)\n",
            "TRAIN(094): [ 60/391] Batch: 0.0412 (0.0519) Data: 0.0272 (0.0233) Loss: 0.0742 (0.0742)\n",
            "TRAIN(094): [ 70/391] Batch: 0.0453 (0.0515) Data: 0.0258 (0.0231) Loss: 0.1493 (0.0774)\n",
            "TRAIN(094): [ 80/391] Batch: 0.0460 (0.0512) Data: 0.0268 (0.0230) Loss: 0.0832 (0.0765)\n",
            "TRAIN(094): [ 90/391] Batch: 0.0447 (0.0510) Data: 0.0258 (0.0228) Loss: 0.0537 (0.0782)\n",
            "TRAIN(094): [100/391] Batch: 0.0560 (0.0509) Data: 0.0188 (0.0227) Loss: 0.1048 (0.0787)\n",
            "TRAIN(094): [110/391] Batch: 0.0443 (0.0508) Data: 0.0276 (0.0223) Loss: 0.0306 (0.0795)\n",
            "TRAIN(094): [120/391] Batch: 0.0491 (0.0507) Data: 0.0187 (0.0221) Loss: 0.0734 (0.0785)\n",
            "TRAIN(094): [130/391] Batch: 0.0464 (0.0506) Data: 0.0212 (0.0220) Loss: 0.0833 (0.0777)\n",
            "TRAIN(094): [140/391] Batch: 0.0436 (0.0504) Data: 0.0299 (0.0218) Loss: 0.0764 (0.0771)\n",
            "TRAIN(094): [150/391] Batch: 0.0450 (0.0502) Data: 0.0279 (0.0218) Loss: 0.0714 (0.0755)\n",
            "TRAIN(094): [160/391] Batch: 0.0379 (0.0501) Data: 0.0251 (0.0218) Loss: 0.0362 (0.0755)\n",
            "TRAIN(094): [170/391] Batch: 0.0410 (0.0500) Data: 0.0270 (0.0216) Loss: 0.0303 (0.0741)\n",
            "TRAIN(094): [180/391] Batch: 0.0590 (0.0502) Data: 0.0140 (0.0213) Loss: 0.0330 (0.0743)\n",
            "TRAIN(094): [190/391] Batch: 0.0487 (0.0503) Data: 0.0139 (0.0208) Loss: 0.0218 (0.0731)\n",
            "TRAIN(094): [200/391] Batch: 0.0398 (0.0504) Data: 0.0253 (0.0206) Loss: 0.1498 (0.0737)\n",
            "TRAIN(094): [210/391] Batch: 0.0519 (0.0505) Data: 0.0158 (0.0204) Loss: 0.0628 (0.0731)\n",
            "TRAIN(094): [220/391] Batch: 0.0476 (0.0505) Data: 0.0219 (0.0202) Loss: 0.0525 (0.0730)\n",
            "TRAIN(094): [230/391] Batch: 0.0545 (0.0506) Data: 0.0153 (0.0201) Loss: 0.0358 (0.0725)\n",
            "TRAIN(094): [240/391] Batch: 0.0467 (0.0504) Data: 0.0288 (0.0201) Loss: 0.0388 (0.0718)\n",
            "TRAIN(094): [250/391] Batch: 0.0473 (0.0504) Data: 0.0250 (0.0202) Loss: 0.0904 (0.0727)\n",
            "TRAIN(094): [260/391] Batch: 0.0380 (0.0503) Data: 0.0280 (0.0202) Loss: 0.1043 (0.0731)\n",
            "TRAIN(094): [270/391] Batch: 0.0456 (0.0502) Data: 0.0275 (0.0203) Loss: 0.0682 (0.0730)\n",
            "TRAIN(094): [280/391] Batch: 0.0414 (0.0502) Data: 0.0249 (0.0203) Loss: 0.0497 (0.0724)\n",
            "TRAIN(094): [290/391] Batch: 0.0458 (0.0501) Data: 0.0282 (0.0204) Loss: 0.0571 (0.0721)\n",
            "TRAIN(094): [300/391] Batch: 0.0438 (0.0500) Data: 0.0272 (0.0205) Loss: 0.0529 (0.0720)\n",
            "TRAIN(094): [310/391] Batch: 0.0474 (0.0499) Data: 0.0269 (0.0205) Loss: 0.1119 (0.0719)\n",
            "TRAIN(094): [320/391] Batch: 0.0461 (0.0499) Data: 0.0281 (0.0206) Loss: 0.0827 (0.0730)\n",
            "TRAIN(094): [330/391] Batch: 0.0492 (0.0499) Data: 0.0180 (0.0206) Loss: 0.1877 (0.0739)\n",
            "TRAIN(094): [340/391] Batch: 0.0459 (0.0499) Data: 0.0221 (0.0206) Loss: 0.0376 (0.0736)\n",
            "TRAIN(094): [350/391] Batch: 0.0501 (0.0498) Data: 0.0233 (0.0206) Loss: 0.0386 (0.0731)\n",
            "TRAIN(094): [360/391] Batch: 0.0428 (0.0498) Data: 0.0267 (0.0206) Loss: 0.1084 (0.0736)\n",
            "TRAIN(094): [370/391] Batch: 0.0458 (0.0498) Data: 0.0217 (0.0205) Loss: 0.1293 (0.0736)\n",
            "TRAIN(094): [380/391] Batch: 0.0456 (0.0498) Data: 0.0271 (0.0205) Loss: 0.1423 (0.0737)\n",
            "TRAIN(094): [390/391] Batch: 0.0453 (0.0497) Data: 0.0284 (0.0206) Loss: 0.1676 (0.0738)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(94)         0:00:19         0:00:08         0:00:11          0.0738\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(095): [ 10/391] Batch: 0.0469 (0.0635) Data: 0.0271 (0.0327) Loss: 0.0443 (0.0736)\n",
            "TRAIN(095): [ 20/391] Batch: 0.0527 (0.0559) Data: 0.0297 (0.0285) Loss: 0.0613 (0.0655)\n",
            "TRAIN(095): [ 30/391] Batch: 0.0459 (0.0528) Data: 0.0272 (0.0269) Loss: 0.0881 (0.0643)\n",
            "TRAIN(095): [ 40/391] Batch: 0.0467 (0.0518) Data: 0.0284 (0.0258) Loss: 0.0830 (0.0681)\n",
            "TRAIN(095): [ 50/391] Batch: 0.0502 (0.0511) Data: 0.0203 (0.0251) Loss: 0.0138 (0.0691)\n",
            "TRAIN(095): [ 60/391] Batch: 0.0853 (0.0517) Data: 0.0046 (0.0236) Loss: 0.0235 (0.0717)\n",
            "TRAIN(095): [ 70/391] Batch: 0.0472 (0.0513) Data: 0.0206 (0.0225) Loss: 0.0606 (0.0718)\n",
            "TRAIN(095): [ 80/391] Batch: 0.0450 (0.0513) Data: 0.0203 (0.0221) Loss: 0.0532 (0.0723)\n",
            "TRAIN(095): [ 90/391] Batch: 0.0454 (0.0511) Data: 0.0213 (0.0215) Loss: 0.0582 (0.0742)\n",
            "TRAIN(095): [100/391] Batch: 0.0422 (0.0511) Data: 0.0244 (0.0212) Loss: 0.0547 (0.0731)\n",
            "TRAIN(095): [110/391] Batch: 0.0490 (0.0512) Data: 0.0188 (0.0209) Loss: 0.0764 (0.0723)\n",
            "TRAIN(095): [120/391] Batch: 0.0475 (0.0511) Data: 0.0215 (0.0207) Loss: 0.0697 (0.0717)\n",
            "TRAIN(095): [130/391] Batch: 0.0451 (0.0511) Data: 0.0241 (0.0204) Loss: 0.0834 (0.0730)\n",
            "TRAIN(095): [140/391] Batch: 0.0476 (0.0509) Data: 0.0254 (0.0206) Loss: 0.0787 (0.0727)\n",
            "TRAIN(095): [150/391] Batch: 0.0480 (0.0508) Data: 0.0269 (0.0207) Loss: 0.0424 (0.0721)\n",
            "TRAIN(095): [160/391] Batch: 0.0457 (0.0507) Data: 0.0258 (0.0208) Loss: 0.0698 (0.0712)\n",
            "TRAIN(095): [170/391] Batch: 0.0439 (0.0505) Data: 0.0265 (0.0210) Loss: 0.0242 (0.0701)\n",
            "TRAIN(095): [180/391] Batch: 0.0461 (0.0503) Data: 0.0265 (0.0210) Loss: 0.0500 (0.0691)\n",
            "TRAIN(095): [190/391] Batch: 0.0441 (0.0503) Data: 0.0261 (0.0209) Loss: 0.2352 (0.0702)\n",
            "TRAIN(095): [200/391] Batch: 0.0474 (0.0501) Data: 0.0272 (0.0211) Loss: 0.0417 (0.0724)\n",
            "TRAIN(095): [210/391] Batch: 0.0468 (0.0500) Data: 0.0269 (0.0212) Loss: 0.0620 (0.0735)\n",
            "TRAIN(095): [220/391] Batch: 0.0461 (0.0500) Data: 0.0274 (0.0211) Loss: 0.0597 (0.0746)\n",
            "TRAIN(095): [230/391] Batch: 0.0477 (0.0499) Data: 0.0267 (0.0212) Loss: 0.0653 (0.0759)\n",
            "TRAIN(095): [240/391] Batch: 0.0525 (0.0499) Data: 0.0216 (0.0212) Loss: 0.1145 (0.0769)\n",
            "TRAIN(095): [250/391] Batch: 0.0451 (0.0498) Data: 0.0282 (0.0213) Loss: 0.0626 (0.0771)\n",
            "TRAIN(095): [260/391] Batch: 0.0468 (0.0497) Data: 0.0276 (0.0214) Loss: 0.1207 (0.0771)\n",
            "TRAIN(095): [270/391] Batch: 0.0433 (0.0497) Data: 0.0262 (0.0214) Loss: 0.1341 (0.0784)\n",
            "TRAIN(095): [280/391] Batch: 0.0466 (0.0497) Data: 0.0267 (0.0214) Loss: 0.0764 (0.0786)\n",
            "TRAIN(095): [290/391] Batch: 0.0480 (0.0496) Data: 0.0270 (0.0215) Loss: 0.0921 (0.0787)\n",
            "TRAIN(095): [300/391] Batch: 0.0474 (0.0495) Data: 0.0276 (0.0216) Loss: 0.0424 (0.0789)\n",
            "TRAIN(095): [310/391] Batch: 0.0466 (0.0495) Data: 0.0215 (0.0216) Loss: 0.0216 (0.0786)\n",
            "TRAIN(095): [320/391] Batch: 0.0509 (0.0495) Data: 0.0217 (0.0215) Loss: 0.0360 (0.0785)\n",
            "TRAIN(095): [330/391] Batch: 0.0548 (0.0496) Data: 0.0181 (0.0214) Loss: 0.0626 (0.0780)\n",
            "TRAIN(095): [340/391] Batch: 0.0571 (0.0496) Data: 0.0146 (0.0213) Loss: 0.0832 (0.0778)\n",
            "TRAIN(095): [350/391] Batch: 0.0499 (0.0497) Data: 0.0152 (0.0212) Loss: 0.0633 (0.0772)\n",
            "TRAIN(095): [360/391] Batch: 0.0546 (0.0497) Data: 0.0190 (0.0211) Loss: 0.0290 (0.0768)\n",
            "TRAIN(095): [370/391] Batch: 0.0679 (0.0498) Data: 0.0174 (0.0209) Loss: 0.0462 (0.0766)\n",
            "TRAIN(095): [380/391] Batch: 0.0544 (0.0498) Data: 0.0149 (0.0208) Loss: 0.0658 (0.0767)\n",
            "TRAIN(095): [390/391] Batch: 0.0473 (0.0498) Data: 0.0275 (0.0207) Loss: 0.0934 (0.0766)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(95)         0:00:19         0:00:08         0:00:11          0.0766\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(096): [ 10/391] Batch: 0.0387 (0.0650) Data: 0.0258 (0.0297) Loss: 0.0716 (0.0828)\n",
            "TRAIN(096): [ 20/391] Batch: 0.0521 (0.0565) Data: 0.0289 (0.0276) Loss: 0.1234 (0.0871)\n",
            "TRAIN(096): [ 30/391] Batch: 0.0455 (0.0533) Data: 0.0260 (0.0261) Loss: 0.0204 (0.0842)\n",
            "TRAIN(096): [ 40/391] Batch: 0.0496 (0.0521) Data: 0.0214 (0.0249) Loss: 0.1236 (0.0829)\n",
            "TRAIN(096): [ 50/391] Batch: 0.0453 (0.0513) Data: 0.0273 (0.0239) Loss: 0.0820 (0.0853)\n",
            "TRAIN(096): [ 60/391] Batch: 0.0559 (0.0510) Data: 0.0197 (0.0232) Loss: 0.0719 (0.0845)\n",
            "TRAIN(096): [ 70/391] Batch: 0.0517 (0.0510) Data: 0.0173 (0.0222) Loss: 0.0686 (0.0848)\n",
            "TRAIN(096): [ 80/391] Batch: 0.0462 (0.0507) Data: 0.0206 (0.0217) Loss: 0.0897 (0.0836)\n",
            "TRAIN(096): [ 90/391] Batch: 0.0469 (0.0505) Data: 0.0267 (0.0215) Loss: 0.2045 (0.0839)\n",
            "TRAIN(096): [100/391] Batch: 0.0418 (0.0503) Data: 0.0282 (0.0215) Loss: 0.0442 (0.0826)\n",
            "TRAIN(096): [110/391] Batch: 0.0514 (0.0502) Data: 0.0264 (0.0216) Loss: 0.0669 (0.0833)\n",
            "TRAIN(096): [120/391] Batch: 0.0507 (0.0499) Data: 0.0246 (0.0217) Loss: 0.0607 (0.0852)\n",
            "TRAIN(096): [130/391] Batch: 0.0454 (0.0497) Data: 0.0277 (0.0218) Loss: 0.0955 (0.0843)\n",
            "TRAIN(096): [140/391] Batch: 0.0355 (0.0497) Data: 0.0269 (0.0217) Loss: 0.0200 (0.0832)\n",
            "TRAIN(096): [150/391] Batch: 0.0449 (0.0495) Data: 0.0273 (0.0218) Loss: 0.0498 (0.0833)\n",
            "TRAIN(096): [160/391] Batch: 0.0592 (0.0495) Data: 0.0183 (0.0218) Loss: 0.0637 (0.0831)\n",
            "TRAIN(096): [170/391] Batch: 0.0468 (0.0493) Data: 0.0270 (0.0219) Loss: 0.0238 (0.0831)\n",
            "TRAIN(096): [180/391] Batch: 0.0549 (0.0494) Data: 0.0175 (0.0218) Loss: 0.0706 (0.0821)\n",
            "TRAIN(096): [190/391] Batch: 0.0533 (0.0494) Data: 0.0180 (0.0215) Loss: 0.0536 (0.0813)\n",
            "TRAIN(096): [200/391] Batch: 0.0571 (0.0494) Data: 0.0210 (0.0215) Loss: 0.0879 (0.0804)\n",
            "TRAIN(096): [210/391] Batch: 0.0523 (0.0494) Data: 0.0156 (0.0213) Loss: 0.0398 (0.0802)\n",
            "TRAIN(096): [220/391] Batch: 0.0584 (0.0495) Data: 0.0143 (0.0211) Loss: 0.0195 (0.0794)\n",
            "TRAIN(096): [230/391] Batch: 0.0469 (0.0496) Data: 0.0207 (0.0209) Loss: 0.0271 (0.0786)\n",
            "TRAIN(096): [240/391] Batch: 0.0549 (0.0497) Data: 0.0116 (0.0205) Loss: 0.0953 (0.0789)\n",
            "TRAIN(096): [250/391] Batch: 0.0592 (0.0498) Data: 0.0135 (0.0203) Loss: 0.0755 (0.0784)\n",
            "TRAIN(096): [260/391] Batch: 0.0645 (0.0499) Data: 0.0139 (0.0201) Loss: 0.0737 (0.0785)\n",
            "TRAIN(096): [270/391] Batch: 0.0459 (0.0498) Data: 0.0258 (0.0201) Loss: 0.0504 (0.0777)\n",
            "TRAIN(096): [280/391] Batch: 0.0560 (0.0499) Data: 0.0225 (0.0202) Loss: 0.0713 (0.0773)\n",
            "TRAIN(096): [290/391] Batch: 0.0476 (0.0498) Data: 0.0274 (0.0203) Loss: 0.0494 (0.0768)\n",
            "TRAIN(096): [300/391] Batch: 0.0469 (0.0498) Data: 0.0164 (0.0203) Loss: 0.0555 (0.0763)\n",
            "TRAIN(096): [310/391] Batch: 0.0455 (0.0497) Data: 0.0277 (0.0203) Loss: 0.1534 (0.0764)\n",
            "TRAIN(096): [320/391] Batch: 0.0513 (0.0497) Data: 0.0234 (0.0202) Loss: 0.1373 (0.0769)\n",
            "TRAIN(096): [330/391] Batch: 0.0454 (0.0496) Data: 0.0274 (0.0203) Loss: 0.2012 (0.0781)\n",
            "TRAIN(096): [340/391] Batch: 0.0464 (0.0496) Data: 0.0276 (0.0205) Loss: 0.0364 (0.0779)\n",
            "TRAIN(096): [350/391] Batch: 0.0447 (0.0496) Data: 0.0265 (0.0205) Loss: 0.0475 (0.0775)\n",
            "TRAIN(096): [360/391] Batch: 0.0458 (0.0495) Data: 0.0281 (0.0205) Loss: 0.0863 (0.0780)\n",
            "TRAIN(096): [370/391] Batch: 0.0417 (0.0495) Data: 0.0258 (0.0206) Loss: 0.1468 (0.0779)\n",
            "TRAIN(096): [380/391] Batch: 0.0474 (0.0494) Data: 0.0265 (0.0206) Loss: 0.1527 (0.0783)\n",
            "TRAIN(096): [390/391] Batch: 0.0451 (0.0494) Data: 0.0279 (0.0207) Loss: 0.0458 (0.0787)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(96)         0:00:19         0:00:08         0:00:11          0.0787\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(097): [ 10/391] Batch: 0.0431 (0.0630) Data: 0.0265 (0.0330) Loss: 0.1211 (0.0729)\n",
            "TRAIN(097): [ 20/391] Batch: 0.0543 (0.0551) Data: 0.0277 (0.0288) Loss: 0.0678 (0.0828)\n",
            "TRAIN(097): [ 30/391] Batch: 0.0481 (0.0530) Data: 0.0257 (0.0259) Loss: 0.0370 (0.0840)\n",
            "TRAIN(097): [ 40/391] Batch: 0.0476 (0.0515) Data: 0.0275 (0.0253) Loss: 0.0512 (0.0851)\n",
            "TRAIN(097): [ 50/391] Batch: 0.0427 (0.0513) Data: 0.0243 (0.0240) Loss: 0.0794 (0.0827)\n",
            "TRAIN(097): [ 60/391] Batch: 0.0488 (0.0512) Data: 0.0238 (0.0233) Loss: 0.0741 (0.0802)\n",
            "TRAIN(097): [ 70/391] Batch: 0.0496 (0.0510) Data: 0.0212 (0.0228) Loss: 0.0334 (0.0785)\n",
            "TRAIN(097): [ 80/391] Batch: 0.0619 (0.0512) Data: 0.0148 (0.0221) Loss: 0.1195 (0.0779)\n",
            "TRAIN(097): [ 90/391] Batch: 0.0492 (0.0512) Data: 0.0140 (0.0214) Loss: 0.0962 (0.0771)\n",
            "TRAIN(097): [100/391] Batch: 0.0526 (0.0512) Data: 0.0200 (0.0211) Loss: 0.0995 (0.0764)\n",
            "TRAIN(097): [110/391] Batch: 0.0563 (0.0511) Data: 0.0207 (0.0206) Loss: 0.0498 (0.0751)\n",
            "TRAIN(097): [120/391] Batch: 0.0515 (0.0510) Data: 0.0135 (0.0202) Loss: 0.0571 (0.0739)\n",
            "TRAIN(097): [130/391] Batch: 0.0523 (0.0510) Data: 0.0154 (0.0200) Loss: 0.0413 (0.0721)\n",
            "TRAIN(097): [140/391] Batch: 0.0503 (0.0509) Data: 0.0232 (0.0198) Loss: 0.0673 (0.0725)\n",
            "TRAIN(097): [150/391] Batch: 0.0513 (0.0508) Data: 0.0259 (0.0200) Loss: 0.0530 (0.0722)\n",
            "TRAIN(097): [160/391] Batch: 0.0364 (0.0506) Data: 0.0275 (0.0202) Loss: 0.0797 (0.0707)\n",
            "TRAIN(097): [170/391] Batch: 0.0466 (0.0505) Data: 0.0213 (0.0203) Loss: 0.0734 (0.0705)\n",
            "TRAIN(097): [180/391] Batch: 0.0515 (0.0504) Data: 0.0181 (0.0203) Loss: 0.0255 (0.0698)\n",
            "TRAIN(097): [190/391] Batch: 0.0455 (0.0503) Data: 0.0284 (0.0204) Loss: 0.0688 (0.0697)\n",
            "TRAIN(097): [200/391] Batch: 0.0452 (0.0501) Data: 0.0277 (0.0205) Loss: 0.0996 (0.0703)\n",
            "TRAIN(097): [210/391] Batch: 0.0547 (0.0501) Data: 0.0156 (0.0205) Loss: 0.0614 (0.0701)\n",
            "TRAIN(097): [220/391] Batch: 0.0393 (0.0501) Data: 0.0248 (0.0204) Loss: 0.0328 (0.0693)\n",
            "TRAIN(097): [230/391] Batch: 0.0499 (0.0500) Data: 0.0264 (0.0204) Loss: 0.1441 (0.0691)\n",
            "TRAIN(097): [240/391] Batch: 0.0468 (0.0499) Data: 0.0258 (0.0206) Loss: 0.0701 (0.0694)\n",
            "TRAIN(097): [250/391] Batch: 0.0465 (0.0499) Data: 0.0251 (0.0206) Loss: 0.0732 (0.0692)\n",
            "TRAIN(097): [260/391] Batch: 0.0375 (0.0499) Data: 0.0263 (0.0206) Loss: 0.1183 (0.0688)\n",
            "TRAIN(097): [270/391] Batch: 0.0465 (0.0498) Data: 0.0273 (0.0207) Loss: 0.0410 (0.0690)\n",
            "TRAIN(097): [280/391] Batch: 0.0457 (0.0497) Data: 0.0275 (0.0207) Loss: 0.0227 (0.0683)\n",
            "TRAIN(097): [290/391] Batch: 0.0457 (0.0497) Data: 0.0282 (0.0208) Loss: 0.0606 (0.0680)\n",
            "TRAIN(097): [300/391] Batch: 0.0401 (0.0496) Data: 0.0266 (0.0208) Loss: 0.0557 (0.0676)\n",
            "TRAIN(097): [310/391] Batch: 0.0520 (0.0496) Data: 0.0161 (0.0208) Loss: 0.0327 (0.0671)\n",
            "TRAIN(097): [320/391] Batch: 0.0450 (0.0495) Data: 0.0263 (0.0209) Loss: 0.0264 (0.0668)\n",
            "TRAIN(097): [330/391] Batch: 0.0476 (0.0495) Data: 0.0244 (0.0209) Loss: 0.0621 (0.0664)\n",
            "TRAIN(097): [340/391] Batch: 0.0458 (0.0494) Data: 0.0274 (0.0210) Loss: 0.0745 (0.0666)\n",
            "TRAIN(097): [350/391] Batch: 0.0482 (0.0494) Data: 0.0191 (0.0210) Loss: 0.0660 (0.0664)\n",
            "TRAIN(097): [360/391] Batch: 0.0506 (0.0494) Data: 0.0205 (0.0209) Loss: 0.0310 (0.0659)\n",
            "TRAIN(097): [370/391] Batch: 0.0472 (0.0494) Data: 0.0203 (0.0208) Loss: 0.0828 (0.0659)\n",
            "TRAIN(097): [380/391] Batch: 0.0524 (0.0495) Data: 0.0190 (0.0207) Loss: 0.0797 (0.0656)\n",
            "TRAIN(097): [390/391] Batch: 0.0555 (0.0495) Data: 0.0201 (0.0207) Loss: 0.0988 (0.0653)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(97)         0:00:19         0:00:08         0:00:11          0.0653\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(098): [ 10/391] Batch: 0.0453 (0.0722) Data: 0.0140 (0.0328) Loss: 0.0639 (0.0724)\n",
            "TRAIN(098): [ 20/391] Batch: 0.0495 (0.0617) Data: 0.0171 (0.0246) Loss: 0.1235 (0.0766)\n",
            "TRAIN(098): [ 30/391] Batch: 0.0469 (0.0563) Data: 0.0282 (0.0246) Loss: 0.0935 (0.0725)\n",
            "TRAIN(098): [ 40/391] Batch: 0.0462 (0.0550) Data: 0.0223 (0.0229) Loss: 0.1193 (0.0691)\n",
            "TRAIN(098): [ 50/391] Batch: 0.0462 (0.0540) Data: 0.0223 (0.0218) Loss: 0.0507 (0.0684)\n",
            "TRAIN(098): [ 60/391] Batch: 0.0459 (0.0533) Data: 0.0230 (0.0212) Loss: 0.0396 (0.0670)\n",
            "TRAIN(098): [ 70/391] Batch: 0.0459 (0.0526) Data: 0.0272 (0.0211) Loss: 0.0598 (0.0675)\n",
            "TRAIN(098): [ 80/391] Batch: 0.0547 (0.0521) Data: 0.0184 (0.0211) Loss: 0.0418 (0.0683)\n",
            "TRAIN(098): [ 90/391] Batch: 0.0487 (0.0518) Data: 0.0223 (0.0207) Loss: 0.0686 (0.0665)\n",
            "TRAIN(098): [100/391] Batch: 0.0525 (0.0517) Data: 0.0156 (0.0204) Loss: 0.0724 (0.0655)\n",
            "TRAIN(098): [110/391] Batch: 0.0461 (0.0512) Data: 0.0277 (0.0207) Loss: 0.0188 (0.0644)\n",
            "TRAIN(098): [120/391] Batch: 0.0547 (0.0510) Data: 0.0224 (0.0209) Loss: 0.0641 (0.0653)\n",
            "TRAIN(098): [130/391] Batch: 0.0471 (0.0508) Data: 0.0274 (0.0209) Loss: 0.0391 (0.0651)\n",
            "TRAIN(098): [140/391] Batch: 0.0502 (0.0506) Data: 0.0249 (0.0209) Loss: 0.0702 (0.0646)\n",
            "TRAIN(098): [150/391] Batch: 0.0461 (0.0504) Data: 0.0258 (0.0211) Loss: 0.0441 (0.0646)\n",
            "TRAIN(098): [160/391] Batch: 0.0445 (0.0503) Data: 0.0225 (0.0209) Loss: 0.0427 (0.0639)\n",
            "TRAIN(098): [170/391] Batch: 0.0447 (0.0502) Data: 0.0280 (0.0211) Loss: 0.0736 (0.0635)\n",
            "TRAIN(098): [180/391] Batch: 0.0524 (0.0501) Data: 0.0253 (0.0212) Loss: 0.0533 (0.0625)\n",
            "TRAIN(098): [190/391] Batch: 0.0461 (0.0499) Data: 0.0273 (0.0213) Loss: 0.0804 (0.0630)\n",
            "TRAIN(098): [200/391] Batch: 0.0393 (0.0499) Data: 0.0245 (0.0213) Loss: 0.0631 (0.0627)\n",
            "TRAIN(098): [210/391] Batch: 0.0503 (0.0498) Data: 0.0261 (0.0213) Loss: 0.0543 (0.0612)\n",
            "TRAIN(098): [220/391] Batch: 0.0555 (0.0498) Data: 0.0143 (0.0213) Loss: 0.0841 (0.0603)\n",
            "TRAIN(098): [230/391] Batch: 0.0612 (0.0500) Data: 0.0153 (0.0211) Loss: 0.0779 (0.0603)\n",
            "TRAIN(098): [240/391] Batch: 0.0541 (0.0500) Data: 0.0157 (0.0208) Loss: 0.1677 (0.0604)\n",
            "TRAIN(098): [250/391] Batch: 0.0642 (0.0500) Data: 0.0154 (0.0206) Loss: 0.0596 (0.0606)\n",
            "TRAIN(098): [260/391] Batch: 0.0470 (0.0500) Data: 0.0200 (0.0205) Loss: 0.0779 (0.0610)\n",
            "TRAIN(098): [270/391] Batch: 0.0461 (0.0500) Data: 0.0162 (0.0203) Loss: 0.0597 (0.0615)\n",
            "TRAIN(098): [280/391] Batch: 0.0599 (0.0501) Data: 0.0155 (0.0202) Loss: 0.0438 (0.0614)\n",
            "TRAIN(098): [290/391] Batch: 0.0535 (0.0501) Data: 0.0167 (0.0201) Loss: 0.0331 (0.0612)\n",
            "TRAIN(098): [300/391] Batch: 0.0524 (0.0501) Data: 0.0228 (0.0200) Loss: 0.0339 (0.0610)\n",
            "TRAIN(098): [310/391] Batch: 0.0435 (0.0501) Data: 0.0270 (0.0200) Loss: 0.0156 (0.0609)\n",
            "TRAIN(098): [320/391] Batch: 0.0507 (0.0500) Data: 0.0198 (0.0201) Loss: 0.0525 (0.0609)\n",
            "TRAIN(098): [330/391] Batch: 0.0465 (0.0499) Data: 0.0257 (0.0202) Loss: 0.0343 (0.0607)\n",
            "TRAIN(098): [340/391] Batch: 0.0533 (0.0499) Data: 0.0173 (0.0202) Loss: 0.0660 (0.0604)\n",
            "TRAIN(098): [350/391] Batch: 0.0502 (0.0499) Data: 0.0245 (0.0201) Loss: 0.0829 (0.0602)\n",
            "TRAIN(098): [360/391] Batch: 0.0465 (0.0498) Data: 0.0279 (0.0201) Loss: 0.0135 (0.0600)\n",
            "TRAIN(098): [370/391] Batch: 0.0447 (0.0498) Data: 0.0269 (0.0202) Loss: 0.0515 (0.0596)\n",
            "TRAIN(098): [380/391] Batch: 0.0475 (0.0497) Data: 0.0253 (0.0202) Loss: 0.0660 (0.0596)\n",
            "TRAIN(098): [390/391] Batch: 0.0458 (0.0496) Data: 0.0267 (0.0203) Loss: 0.0729 (0.0595)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(98)         0:00:19         0:00:07         0:00:11          0.0595\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(099): [ 10/391] Batch: 0.0489 (0.0642) Data: 0.0254 (0.0319) Loss: 0.0616 (0.0428)\n",
            "TRAIN(099): [ 20/391] Batch: 0.0484 (0.0567) Data: 0.0274 (0.0271) Loss: 0.0267 (0.0419)\n",
            "TRAIN(099): [ 30/391] Batch: 0.0484 (0.0534) Data: 0.0245 (0.0259) Loss: 0.0302 (0.0479)\n",
            "TRAIN(099): [ 40/391] Batch: 0.0524 (0.0522) Data: 0.0208 (0.0249) Loss: 0.1093 (0.0512)\n",
            "TRAIN(099): [ 50/391] Batch: 0.0513 (0.0514) Data: 0.0214 (0.0240) Loss: 0.0622 (0.0532)\n",
            "TRAIN(099): [ 60/391] Batch: 0.0465 (0.0508) Data: 0.0192 (0.0236) Loss: 0.0467 (0.0543)\n",
            "TRAIN(099): [ 70/391] Batch: 0.0573 (0.0506) Data: 0.0157 (0.0229) Loss: 0.0426 (0.0580)\n",
            "TRAIN(099): [ 80/391] Batch: 0.0469 (0.0505) Data: 0.0231 (0.0221) Loss: 0.1443 (0.0590)\n",
            "TRAIN(099): [ 90/391] Batch: 0.0504 (0.0503) Data: 0.0265 (0.0218) Loss: 0.0978 (0.0603)\n",
            "TRAIN(099): [100/391] Batch: 0.0473 (0.0504) Data: 0.0184 (0.0215) Loss: 0.1059 (0.0610)\n",
            "TRAIN(099): [110/391] Batch: 0.0595 (0.0505) Data: 0.0160 (0.0213) Loss: 0.0573 (0.0602)\n",
            "TRAIN(099): [120/391] Batch: 0.0443 (0.0503) Data: 0.0166 (0.0207) Loss: 0.0907 (0.0592)\n",
            "TRAIN(099): [130/391] Batch: 0.0475 (0.0501) Data: 0.0212 (0.0205) Loss: 0.0381 (0.0594)\n",
            "TRAIN(099): [140/391] Batch: 0.0580 (0.0502) Data: 0.0186 (0.0204) Loss: 0.0909 (0.0593)\n",
            "TRAIN(099): [150/391] Batch: 0.0517 (0.0501) Data: 0.0219 (0.0203) Loss: 0.0542 (0.0589)\n",
            "TRAIN(099): [160/391] Batch: 0.0466 (0.0501) Data: 0.0207 (0.0202) Loss: 0.0841 (0.0586)\n",
            "TRAIN(099): [170/391] Batch: 0.0463 (0.0499) Data: 0.0278 (0.0204) Loss: 0.0533 (0.0586)\n",
            "TRAIN(099): [180/391] Batch: 0.0494 (0.0499) Data: 0.0184 (0.0203) Loss: 0.0732 (0.0593)\n",
            "TRAIN(099): [190/391] Batch: 0.0455 (0.0498) Data: 0.0263 (0.0204) Loss: 0.0543 (0.0586)\n",
            "TRAIN(099): [200/391] Batch: 0.0495 (0.0498) Data: 0.0185 (0.0203) Loss: 0.0761 (0.0585)\n",
            "TRAIN(099): [210/391] Batch: 0.0482 (0.0497) Data: 0.0265 (0.0204) Loss: 0.0197 (0.0580)\n",
            "TRAIN(099): [220/391] Batch: 0.0491 (0.0496) Data: 0.0261 (0.0205) Loss: 0.0309 (0.0580)\n",
            "TRAIN(099): [230/391] Batch: 0.0552 (0.0496) Data: 0.0220 (0.0205) Loss: 0.0188 (0.0574)\n",
            "TRAIN(099): [240/391] Batch: 0.0405 (0.0495) Data: 0.0241 (0.0205) Loss: 0.0191 (0.0577)\n",
            "TRAIN(099): [250/391] Batch: 0.0550 (0.0495) Data: 0.0238 (0.0205) Loss: 0.0693 (0.0572)\n",
            "TRAIN(099): [260/391] Batch: 0.0516 (0.0496) Data: 0.0157 (0.0203) Loss: 0.0606 (0.0574)\n",
            "TRAIN(099): [270/391] Batch: 0.0473 (0.0496) Data: 0.0270 (0.0203) Loss: 0.0592 (0.0576)\n",
            "TRAIN(099): [280/391] Batch: 0.0575 (0.0496) Data: 0.0190 (0.0203) Loss: 0.0363 (0.0580)\n",
            "TRAIN(099): [290/391] Batch: 0.0571 (0.0496) Data: 0.0225 (0.0203) Loss: 0.0746 (0.0585)\n",
            "TRAIN(099): [300/391] Batch: 0.0472 (0.0495) Data: 0.0277 (0.0203) Loss: 0.0634 (0.0584)\n",
            "TRAIN(099): [310/391] Batch: 0.0416 (0.0495) Data: 0.0264 (0.0204) Loss: 0.0835 (0.0585)\n",
            "TRAIN(099): [320/391] Batch: 0.0453 (0.0495) Data: 0.0282 (0.0204) Loss: 0.0274 (0.0588)\n",
            "TRAIN(099): [330/391] Batch: 0.0477 (0.0494) Data: 0.0269 (0.0204) Loss: 0.0722 (0.0593)\n",
            "TRAIN(099): [340/391] Batch: 0.0457 (0.0494) Data: 0.0176 (0.0204) Loss: 0.0949 (0.0592)\n",
            "TRAIN(099): [350/391] Batch: 0.0441 (0.0493) Data: 0.0266 (0.0205) Loss: 0.0436 (0.0599)\n",
            "TRAIN(099): [360/391] Batch: 0.0407 (0.0493) Data: 0.0284 (0.0206) Loss: 0.0506 (0.0601)\n",
            "TRAIN(099): [370/391] Batch: 0.0534 (0.0493) Data: 0.0126 (0.0206) Loss: 0.0775 (0.0603)\n",
            "TRAIN(099): [380/391] Batch: 0.0441 (0.0493) Data: 0.0201 (0.0204) Loss: 0.0763 (0.0604)\n",
            "TRAIN(099): [390/391] Batch: 0.0464 (0.0493) Data: 0.0279 (0.0204) Loss: 0.1730 (0.0607)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(99)         0:00:19         0:00:07         0:00:11          0.0607\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(100): [ 10/391] Batch: 0.0504 (0.0687) Data: 0.0138 (0.0277) Loss: 0.0827 (0.0633)\n",
            "TRAIN(100): [ 20/391] Batch: 0.0530 (0.0600) Data: 0.0197 (0.0228) Loss: 0.0836 (0.0605)\n",
            "TRAIN(100): [ 30/391] Batch: 0.0453 (0.0571) Data: 0.0194 (0.0206) Loss: 0.0434 (0.0537)\n",
            "TRAIN(100): [ 40/391] Batch: 0.0474 (0.0551) Data: 0.0181 (0.0201) Loss: 0.0919 (0.0580)\n",
            "TRAIN(100): [ 50/391] Batch: 0.0479 (0.0533) Data: 0.0260 (0.0209) Loss: 0.0295 (0.0554)\n",
            "TRAIN(100): [ 60/391] Batch: 0.0472 (0.0523) Data: 0.0254 (0.0212) Loss: 0.0756 (0.0539)\n",
            "TRAIN(100): [ 70/391] Batch: 0.0549 (0.0516) Data: 0.0216 (0.0215) Loss: 0.1278 (0.0542)\n",
            "TRAIN(100): [ 80/391] Batch: 0.0477 (0.0515) Data: 0.0239 (0.0210) Loss: 0.0483 (0.0556)\n",
            "TRAIN(100): [ 90/391] Batch: 0.0474 (0.0510) Data: 0.0278 (0.0213) Loss: 0.0913 (0.0548)\n",
            "TRAIN(100): [100/391] Batch: 0.0489 (0.0509) Data: 0.0185 (0.0212) Loss: 0.0905 (0.0554)\n",
            "TRAIN(100): [110/391] Batch: 0.0503 (0.0508) Data: 0.0154 (0.0208) Loss: 0.0609 (0.0557)\n",
            "TRAIN(100): [120/391] Batch: 0.0588 (0.0506) Data: 0.0216 (0.0207) Loss: 0.0291 (0.0557)\n",
            "TRAIN(100): [130/391] Batch: 0.0458 (0.0504) Data: 0.0272 (0.0207) Loss: 0.1217 (0.0563)\n",
            "TRAIN(100): [140/391] Batch: 0.0473 (0.0501) Data: 0.0268 (0.0210) Loss: 0.0511 (0.0569)\n",
            "TRAIN(100): [150/391] Batch: 0.0581 (0.0502) Data: 0.0198 (0.0209) Loss: 0.0684 (0.0571)\n",
            "TRAIN(100): [160/391] Batch: 0.0504 (0.0500) Data: 0.0268 (0.0209) Loss: 0.0623 (0.0581)\n",
            "TRAIN(100): [170/391] Batch: 0.0428 (0.0499) Data: 0.0272 (0.0210) Loss: 0.0489 (0.0577)\n",
            "TRAIN(100): [180/391] Batch: 0.0538 (0.0499) Data: 0.0164 (0.0209) Loss: 0.0539 (0.0581)\n",
            "TRAIN(100): [190/391] Batch: 0.0428 (0.0499) Data: 0.0192 (0.0207) Loss: 0.0998 (0.0583)\n",
            "TRAIN(100): [200/391] Batch: 0.0541 (0.0499) Data: 0.0170 (0.0206) Loss: 0.0687 (0.0583)\n",
            "TRAIN(100): [210/391] Batch: 0.0488 (0.0499) Data: 0.0227 (0.0204) Loss: 0.0146 (0.0580)\n",
            "TRAIN(100): [220/391] Batch: 0.0550 (0.0499) Data: 0.0187 (0.0203) Loss: 0.0147 (0.0589)\n",
            "TRAIN(100): [230/391] Batch: 0.0466 (0.0498) Data: 0.0276 (0.0203) Loss: 0.0381 (0.0586)\n",
            "TRAIN(100): [240/391] Batch: 0.0496 (0.0498) Data: 0.0151 (0.0202) Loss: 0.0529 (0.0587)\n",
            "TRAIN(100): [250/391] Batch: 0.0482 (0.0499) Data: 0.0164 (0.0200) Loss: 0.0467 (0.0591)\n",
            "TRAIN(100): [260/391] Batch: 0.0425 (0.0499) Data: 0.0215 (0.0197) Loss: 0.0588 (0.0588)\n",
            "TRAIN(100): [270/391] Batch: 0.0392 (0.0500) Data: 0.0316 (0.0197) Loss: 0.0740 (0.0585)\n",
            "TRAIN(100): [280/391] Batch: 0.0491 (0.0500) Data: 0.0187 (0.0195) Loss: 0.0568 (0.0587)\n",
            "TRAIN(100): [290/391] Batch: 0.0478 (0.0500) Data: 0.0215 (0.0195) Loss: 0.0315 (0.0587)\n",
            "TRAIN(100): [300/391] Batch: 0.0605 (0.0501) Data: 0.0160 (0.0194) Loss: 0.0965 (0.0586)\n",
            "TRAIN(100): [310/391] Batch: 0.0455 (0.0500) Data: 0.0271 (0.0194) Loss: 0.0724 (0.0592)\n",
            "TRAIN(100): [320/391] Batch: 0.0467 (0.0499) Data: 0.0270 (0.0195) Loss: 0.0298 (0.0591)\n",
            "TRAIN(100): [330/391] Batch: 0.0411 (0.0499) Data: 0.0255 (0.0196) Loss: 0.0334 (0.0586)\n",
            "TRAIN(100): [340/391] Batch: 0.0569 (0.0498) Data: 0.0228 (0.0197) Loss: 0.0777 (0.0587)\n",
            "TRAIN(100): [350/391] Batch: 0.0453 (0.0498) Data: 0.0180 (0.0197) Loss: 0.0300 (0.0581)\n",
            "TRAIN(100): [360/391] Batch: 0.0510 (0.0498) Data: 0.0173 (0.0197) Loss: 0.0252 (0.0579)\n",
            "TRAIN(100): [370/391] Batch: 0.0425 (0.0497) Data: 0.0189 (0.0197) Loss: 0.0495 (0.0578)\n",
            "TRAIN(100): [380/391] Batch: 0.0529 (0.0497) Data: 0.0253 (0.0197) Loss: 0.0830 (0.0577)\n",
            "TRAIN(100): [390/391] Batch: 0.0490 (0.0496) Data: 0.0252 (0.0198) Loss: 0.0868 (0.0576)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(100)         0:00:19         0:00:07         0:00:11          0.0576\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(101): [ 10/391] Batch: 0.0442 (0.0630) Data: 0.0271 (0.0326) Loss: 0.0680 (0.0507)\n",
            "TRAIN(101): [ 20/391] Batch: 0.0460 (0.0567) Data: 0.0251 (0.0267) Loss: 0.1471 (0.0680)\n",
            "TRAIN(101): [ 30/391] Batch: 0.0463 (0.0545) Data: 0.0229 (0.0237) Loss: 0.0383 (0.0638)\n",
            "TRAIN(101): [ 40/391] Batch: 0.0433 (0.0531) Data: 0.0195 (0.0227) Loss: 0.0921 (0.0614)\n",
            "TRAIN(101): [ 50/391] Batch: 0.0536 (0.0520) Data: 0.0193 (0.0219) Loss: 0.0589 (0.0608)\n",
            "TRAIN(101): [ 60/391] Batch: 0.0517 (0.0512) Data: 0.0246 (0.0221) Loss: 0.0436 (0.0600)\n",
            "TRAIN(101): [ 70/391] Batch: 0.0357 (0.0509) Data: 0.0258 (0.0214) Loss: 0.0594 (0.0596)\n",
            "TRAIN(101): [ 80/391] Batch: 0.0366 (0.0506) Data: 0.0269 (0.0211) Loss: 0.0657 (0.0595)\n",
            "TRAIN(101): [ 90/391] Batch: 0.0528 (0.0505) Data: 0.0169 (0.0211) Loss: 0.0666 (0.0595)\n",
            "TRAIN(101): [100/391] Batch: 0.0434 (0.0504) Data: 0.0218 (0.0209) Loss: 0.1298 (0.0607)\n",
            "TRAIN(101): [110/391] Batch: 0.0474 (0.0501) Data: 0.0278 (0.0210) Loss: 0.0501 (0.0608)\n",
            "TRAIN(101): [120/391] Batch: 0.0498 (0.0503) Data: 0.0170 (0.0205) Loss: 0.0952 (0.0618)\n",
            "TRAIN(101): [130/391] Batch: 0.0454 (0.0502) Data: 0.0162 (0.0202) Loss: 0.0705 (0.0618)\n",
            "TRAIN(101): [140/391] Batch: 0.0470 (0.0502) Data: 0.0200 (0.0200) Loss: 0.0629 (0.0610)\n",
            "TRAIN(101): [150/391] Batch: 0.0385 (0.0502) Data: 0.0244 (0.0199) Loss: 0.0430 (0.0621)\n",
            "TRAIN(101): [160/391] Batch: 0.0418 (0.0503) Data: 0.0198 (0.0197) Loss: 0.0575 (0.0611)\n",
            "TRAIN(101): [170/391] Batch: 0.0508 (0.0503) Data: 0.0201 (0.0195) Loss: 0.0206 (0.0604)\n",
            "TRAIN(101): [180/391] Batch: 0.0485 (0.0504) Data: 0.0186 (0.0194) Loss: 0.0581 (0.0607)\n",
            "TRAIN(101): [190/391] Batch: 0.0465 (0.0503) Data: 0.0272 (0.0195) Loss: 0.0144 (0.0609)\n",
            "TRAIN(101): [200/391] Batch: 0.0494 (0.0502) Data: 0.0255 (0.0196) Loss: 0.0544 (0.0607)\n",
            "TRAIN(101): [210/391] Batch: 0.0466 (0.0501) Data: 0.0273 (0.0197) Loss: 0.0746 (0.0612)\n",
            "TRAIN(101): [220/391] Batch: 0.0497 (0.0500) Data: 0.0255 (0.0199) Loss: 0.0937 (0.0611)\n",
            "TRAIN(101): [230/391] Batch: 0.0542 (0.0499) Data: 0.0149 (0.0199) Loss: 0.0948 (0.0618)\n",
            "TRAIN(101): [240/391] Batch: 0.0525 (0.0499) Data: 0.0253 (0.0199) Loss: 0.0869 (0.0616)\n",
            "TRAIN(101): [250/391] Batch: 0.0593 (0.0499) Data: 0.0181 (0.0199) Loss: 0.0758 (0.0614)\n",
            "TRAIN(101): [260/391] Batch: 0.0455 (0.0497) Data: 0.0273 (0.0201) Loss: 0.0513 (0.0616)\n",
            "TRAIN(101): [270/391] Batch: 0.0465 (0.0496) Data: 0.0268 (0.0202) Loss: 0.0487 (0.0611)\n",
            "TRAIN(101): [280/391] Batch: 0.0464 (0.0496) Data: 0.0256 (0.0203) Loss: 0.0582 (0.0607)\n",
            "TRAIN(101): [290/391] Batch: 0.0485 (0.0496) Data: 0.0178 (0.0203) Loss: 0.0345 (0.0606)\n",
            "TRAIN(101): [300/391] Batch: 0.0526 (0.0495) Data: 0.0252 (0.0204) Loss: 0.1897 (0.0608)\n",
            "TRAIN(101): [310/391] Batch: 0.0371 (0.0495) Data: 0.0276 (0.0204) Loss: 0.0360 (0.0606)\n",
            "TRAIN(101): [320/391] Batch: 0.0433 (0.0494) Data: 0.0262 (0.0204) Loss: 0.0396 (0.0604)\n",
            "TRAIN(101): [330/391] Batch: 0.0574 (0.0494) Data: 0.0184 (0.0204) Loss: 0.0216 (0.0601)\n",
            "TRAIN(101): [340/391] Batch: 0.0380 (0.0494) Data: 0.0223 (0.0204) Loss: 0.1246 (0.0608)\n",
            "TRAIN(101): [350/391] Batch: 0.0457 (0.0494) Data: 0.0216 (0.0204) Loss: 0.0361 (0.0606)\n",
            "TRAIN(101): [360/391] Batch: 0.0459 (0.0493) Data: 0.0275 (0.0204) Loss: 0.0849 (0.0605)\n",
            "TRAIN(101): [370/391] Batch: 0.0458 (0.0493) Data: 0.0279 (0.0205) Loss: 0.0344 (0.0606)\n",
            "TRAIN(101): [380/391] Batch: 0.0374 (0.0492) Data: 0.0278 (0.0206) Loss: 0.1124 (0.0605)\n",
            "TRAIN(101): [390/391] Batch: 0.0476 (0.0492) Data: 0.0256 (0.0206) Loss: 0.0917 (0.0606)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(101)         0:00:19         0:00:08         0:00:11          0.0606\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(102): [ 10/391] Batch: 0.0518 (0.0693) Data: 0.0132 (0.0303) Loss: 0.0685 (0.0707)\n",
            "TRAIN(102): [ 20/391] Batch: 0.0489 (0.0610) Data: 0.0167 (0.0229) Loss: 0.0355 (0.0641)\n",
            "TRAIN(102): [ 30/391] Batch: 0.0510 (0.0578) Data: 0.0154 (0.0205) Loss: 0.0486 (0.0673)\n",
            "TRAIN(102): [ 40/391] Batch: 0.0505 (0.0559) Data: 0.0219 (0.0203) Loss: 0.0729 (0.0692)\n",
            "TRAIN(102): [ 50/391] Batch: 0.0471 (0.0549) Data: 0.0234 (0.0197) Loss: 0.0301 (0.0677)\n",
            "TRAIN(102): [ 60/391] Batch: 0.0368 (0.0539) Data: 0.0266 (0.0195) Loss: 0.0323 (0.0674)\n",
            "TRAIN(102): [ 70/391] Batch: 0.0469 (0.0533) Data: 0.0223 (0.0197) Loss: 0.0718 (0.0651)\n",
            "TRAIN(102): [ 80/391] Batch: 0.0459 (0.0525) Data: 0.0278 (0.0200) Loss: 0.0609 (0.0638)\n",
            "TRAIN(102): [ 90/391] Batch: 0.0479 (0.0521) Data: 0.0272 (0.0201) Loss: 0.0670 (0.0617)\n",
            "TRAIN(102): [100/391] Batch: 0.0463 (0.0517) Data: 0.0249 (0.0203) Loss: 0.0796 (0.0614)\n",
            "TRAIN(102): [110/391] Batch: 0.0474 (0.0513) Data: 0.0185 (0.0205) Loss: 0.0250 (0.0595)\n",
            "TRAIN(102): [120/391] Batch: 0.0549 (0.0510) Data: 0.0186 (0.0207) Loss: 0.0540 (0.0588)\n",
            "TRAIN(102): [130/391] Batch: 0.0454 (0.0506) Data: 0.0266 (0.0209) Loss: 0.0472 (0.0576)\n",
            "TRAIN(102): [140/391] Batch: 0.0454 (0.0503) Data: 0.0274 (0.0212) Loss: 0.0612 (0.0580)\n",
            "TRAIN(102): [150/391] Batch: 0.0475 (0.0503) Data: 0.0272 (0.0210) Loss: 0.0574 (0.0580)\n",
            "TRAIN(102): [160/391] Batch: 0.0438 (0.0501) Data: 0.0281 (0.0210) Loss: 0.0505 (0.0587)\n",
            "TRAIN(102): [170/391] Batch: 0.0443 (0.0500) Data: 0.0274 (0.0211) Loss: 0.0863 (0.0590)\n",
            "TRAIN(102): [180/391] Batch: 0.0556 (0.0499) Data: 0.0180 (0.0211) Loss: 0.0675 (0.0585)\n",
            "TRAIN(102): [190/391] Batch: 0.0430 (0.0497) Data: 0.0272 (0.0212) Loss: 0.0812 (0.0585)\n",
            "TRAIN(102): [200/391] Batch: 0.0520 (0.0497) Data: 0.0203 (0.0213) Loss: 0.0621 (0.0594)\n",
            "TRAIN(102): [210/391] Batch: 0.0452 (0.0496) Data: 0.0196 (0.0212) Loss: 0.0435 (0.0596)\n",
            "TRAIN(102): [220/391] Batch: 0.0466 (0.0495) Data: 0.0264 (0.0212) Loss: 0.0714 (0.0593)\n",
            "TRAIN(102): [230/391] Batch: 0.0460 (0.0495) Data: 0.0257 (0.0213) Loss: 0.0530 (0.0589)\n",
            "TRAIN(102): [240/391] Batch: 0.0463 (0.0494) Data: 0.0268 (0.0213) Loss: 0.1206 (0.0594)\n",
            "TRAIN(102): [250/391] Batch: 0.0448 (0.0494) Data: 0.0274 (0.0214) Loss: 0.0756 (0.0595)\n",
            "TRAIN(102): [260/391] Batch: 0.0465 (0.0493) Data: 0.0270 (0.0215) Loss: 0.0687 (0.0596)\n",
            "TRAIN(102): [270/391] Batch: 0.0570 (0.0495) Data: 0.0138 (0.0213) Loss: 0.1172 (0.0593)\n",
            "TRAIN(102): [280/391] Batch: 0.0490 (0.0494) Data: 0.0209 (0.0212) Loss: 0.0681 (0.0588)\n",
            "TRAIN(102): [290/391] Batch: 0.0475 (0.0494) Data: 0.0216 (0.0211) Loss: 0.0708 (0.0587)\n",
            "TRAIN(102): [300/391] Batch: 0.0407 (0.0494) Data: 0.0240 (0.0210) Loss: 0.0973 (0.0586)\n",
            "TRAIN(102): [310/391] Batch: 0.0614 (0.0496) Data: 0.0145 (0.0209) Loss: 0.1660 (0.0588)\n",
            "TRAIN(102): [320/391] Batch: 0.0540 (0.0496) Data: 0.0206 (0.0207) Loss: 0.0620 (0.0594)\n",
            "TRAIN(102): [330/391] Batch: 0.0430 (0.0497) Data: 0.0255 (0.0206) Loss: 0.0645 (0.0591)\n",
            "TRAIN(102): [340/391] Batch: 0.0462 (0.0496) Data: 0.0270 (0.0206) Loss: 0.0281 (0.0588)\n",
            "TRAIN(102): [350/391] Batch: 0.0473 (0.0496) Data: 0.0266 (0.0206) Loss: 0.1223 (0.0585)\n",
            "TRAIN(102): [360/391] Batch: 0.0434 (0.0495) Data: 0.0263 (0.0207) Loss: 0.0370 (0.0580)\n",
            "TRAIN(102): [370/391] Batch: 0.0556 (0.0495) Data: 0.0219 (0.0207) Loss: 0.0798 (0.0578)\n",
            "TRAIN(102): [380/391] Batch: 0.0502 (0.0495) Data: 0.0182 (0.0207) Loss: 0.0829 (0.0583)\n",
            "TRAIN(102): [390/391] Batch: 0.0470 (0.0495) Data: 0.0283 (0.0207) Loss: 0.0222 (0.0582)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(102)         0:00:19         0:00:08         0:00:11          0.0582\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(103): [ 10/391] Batch: 0.0454 (0.0649) Data: 0.0270 (0.0323) Loss: 0.1022 (0.0715)\n",
            "TRAIN(103): [ 20/391] Batch: 0.0400 (0.0571) Data: 0.0272 (0.0264) Loss: 0.0503 (0.0614)\n",
            "TRAIN(103): [ 30/391] Batch: 0.0482 (0.0548) Data: 0.0185 (0.0243) Loss: 0.0341 (0.0639)\n",
            "TRAIN(103): [ 40/391] Batch: 0.0480 (0.0534) Data: 0.0264 (0.0230) Loss: 0.0374 (0.0620)\n",
            "TRAIN(103): [ 50/391] Batch: 0.0443 (0.0525) Data: 0.0284 (0.0226) Loss: 0.0550 (0.0610)\n",
            "TRAIN(103): [ 60/391] Batch: 0.0441 (0.0518) Data: 0.0270 (0.0226) Loss: 0.0502 (0.0602)\n",
            "TRAIN(103): [ 70/391] Batch: 0.0523 (0.0516) Data: 0.0230 (0.0223) Loss: 0.0152 (0.0580)\n",
            "TRAIN(103): [ 80/391] Batch: 0.0528 (0.0514) Data: 0.0194 (0.0220) Loss: 0.0725 (0.0589)\n",
            "TRAIN(103): [ 90/391] Batch: 0.0499 (0.0510) Data: 0.0261 (0.0219) Loss: 0.0346 (0.0563)\n",
            "TRAIN(103): [100/391] Batch: 0.0493 (0.0507) Data: 0.0268 (0.0219) Loss: 0.0762 (0.0564)\n",
            "TRAIN(103): [110/391] Batch: 0.0494 (0.0506) Data: 0.0280 (0.0218) Loss: 0.0395 (0.0573)\n",
            "TRAIN(103): [120/391] Batch: 0.0454 (0.0503) Data: 0.0273 (0.0220) Loss: 0.0420 (0.0558)\n",
            "TRAIN(103): [130/391] Batch: 0.0442 (0.0501) Data: 0.0274 (0.0220) Loss: 0.0875 (0.0561)\n",
            "TRAIN(103): [140/391] Batch: 0.0457 (0.0500) Data: 0.0208 (0.0220) Loss: 0.0833 (0.0573)\n",
            "TRAIN(103): [150/391] Batch: 0.0574 (0.0501) Data: 0.0168 (0.0216) Loss: 0.0129 (0.0568)\n",
            "TRAIN(103): [160/391] Batch: 0.0469 (0.0501) Data: 0.0221 (0.0214) Loss: 0.0237 (0.0569)\n",
            "TRAIN(103): [170/391] Batch: 0.0468 (0.0501) Data: 0.0171 (0.0211) Loss: 0.0409 (0.0567)\n",
            "TRAIN(103): [180/391] Batch: 0.0464 (0.0501) Data: 0.0220 (0.0208) Loss: 0.0411 (0.0567)\n",
            "TRAIN(103): [190/391] Batch: 0.0512 (0.0501) Data: 0.0221 (0.0207) Loss: 0.1000 (0.0574)\n",
            "TRAIN(103): [200/391] Batch: 0.0567 (0.0501) Data: 0.0153 (0.0205) Loss: 0.0347 (0.0574)\n",
            "TRAIN(103): [210/391] Batch: 0.0488 (0.0500) Data: 0.0268 (0.0207) Loss: 0.0321 (0.0570)\n",
            "TRAIN(103): [220/391] Batch: 0.0581 (0.0500) Data: 0.0184 (0.0207) Loss: 0.0611 (0.0571)\n",
            "TRAIN(103): [230/391] Batch: 0.0462 (0.0499) Data: 0.0270 (0.0206) Loss: 0.0817 (0.0569)\n",
            "TRAIN(103): [240/391] Batch: 0.0463 (0.0499) Data: 0.0270 (0.0206) Loss: 0.0391 (0.0575)\n",
            "TRAIN(103): [250/391] Batch: 0.0505 (0.0498) Data: 0.0176 (0.0207) Loss: 0.0449 (0.0573)\n",
            "TRAIN(103): [260/391] Batch: 0.0459 (0.0497) Data: 0.0275 (0.0207) Loss: 0.1070 (0.0572)\n",
            "TRAIN(103): [270/391] Batch: 0.0445 (0.0496) Data: 0.0267 (0.0209) Loss: 0.0165 (0.0562)\n",
            "TRAIN(103): [280/391] Batch: 0.0443 (0.0496) Data: 0.0246 (0.0208) Loss: 0.0230 (0.0556)\n",
            "TRAIN(103): [290/391] Batch: 0.0502 (0.0497) Data: 0.0235 (0.0208) Loss: 0.0230 (0.0553)\n",
            "TRAIN(103): [300/391] Batch: 0.0440 (0.0497) Data: 0.0261 (0.0208) Loss: 0.0670 (0.0554)\n",
            "TRAIN(103): [310/391] Batch: 0.0475 (0.0497) Data: 0.0199 (0.0208) Loss: 0.0737 (0.0550)\n",
            "TRAIN(103): [320/391] Batch: 0.0435 (0.0496) Data: 0.0271 (0.0208) Loss: 0.0366 (0.0553)\n",
            "TRAIN(103): [330/391] Batch: 0.0463 (0.0497) Data: 0.0224 (0.0207) Loss: 0.0524 (0.0557)\n",
            "TRAIN(103): [340/391] Batch: 0.0450 (0.0496) Data: 0.0280 (0.0208) Loss: 0.0293 (0.0553)\n",
            "TRAIN(103): [350/391] Batch: 0.0507 (0.0496) Data: 0.0271 (0.0208) Loss: 0.0596 (0.0552)\n",
            "TRAIN(103): [360/391] Batch: 0.0469 (0.0496) Data: 0.0276 (0.0209) Loss: 0.0059 (0.0552)\n",
            "TRAIN(103): [370/391] Batch: 0.0516 (0.0495) Data: 0.0188 (0.0209) Loss: 0.0472 (0.0549)\n",
            "TRAIN(103): [380/391] Batch: 0.0447 (0.0495) Data: 0.0206 (0.0209) Loss: 0.0275 (0.0548)\n",
            "TRAIN(103): [390/391] Batch: 0.0456 (0.0494) Data: 0.0276 (0.0210) Loss: 0.0692 (0.0548)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(103)         0:00:19         0:00:08         0:00:11          0.0548\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(104): [ 10/391] Batch: 0.0421 (0.0638) Data: 0.0236 (0.0308) Loss: 0.0646 (0.0602)\n",
            "TRAIN(104): [ 20/391] Batch: 0.0570 (0.0580) Data: 0.0161 (0.0241) Loss: 0.1190 (0.0670)\n",
            "TRAIN(104): [ 30/391] Batch: 0.0447 (0.0554) Data: 0.0176 (0.0217) Loss: 0.0247 (0.0617)\n",
            "TRAIN(104): [ 40/391] Batch: 0.0473 (0.0540) Data: 0.0210 (0.0204) Loss: 0.0763 (0.0651)\n",
            "TRAIN(104): [ 50/391] Batch: 0.0546 (0.0535) Data: 0.0192 (0.0198) Loss: 0.0774 (0.0613)\n",
            "TRAIN(104): [ 60/391] Batch: 0.0462 (0.0531) Data: 0.0176 (0.0192) Loss: 0.0644 (0.0619)\n",
            "TRAIN(104): [ 70/391] Batch: 0.0498 (0.0529) Data: 0.0176 (0.0186) Loss: 0.0976 (0.0609)\n",
            "TRAIN(104): [ 80/391] Batch: 0.0511 (0.0525) Data: 0.0153 (0.0183) Loss: 0.0441 (0.0592)\n",
            "TRAIN(104): [ 90/391] Batch: 0.0467 (0.0519) Data: 0.0269 (0.0186) Loss: 0.0624 (0.0593)\n",
            "TRAIN(104): [100/391] Batch: 0.0453 (0.0514) Data: 0.0272 (0.0191) Loss: 0.0368 (0.0609)\n",
            "TRAIN(104): [110/391] Batch: 0.0445 (0.0511) Data: 0.0190 (0.0192) Loss: 0.0264 (0.0590)\n",
            "TRAIN(104): [120/391] Batch: 0.0468 (0.0508) Data: 0.0262 (0.0193) Loss: 0.0228 (0.0590)\n",
            "TRAIN(104): [130/391] Batch: 0.0437 (0.0506) Data: 0.0270 (0.0193) Loss: 0.0319 (0.0582)\n",
            "TRAIN(104): [140/391] Batch: 0.0505 (0.0505) Data: 0.0187 (0.0195) Loss: 0.0640 (0.0585)\n",
            "TRAIN(104): [150/391] Batch: 0.0531 (0.0505) Data: 0.0147 (0.0193) Loss: 0.1062 (0.0580)\n",
            "TRAIN(104): [160/391] Batch: 0.0526 (0.0503) Data: 0.0257 (0.0195) Loss: 0.0372 (0.0568)\n",
            "TRAIN(104): [170/391] Batch: 0.0506 (0.0504) Data: 0.0190 (0.0195) Loss: 0.0704 (0.0572)\n",
            "TRAIN(104): [180/391] Batch: 0.0461 (0.0502) Data: 0.0284 (0.0197) Loss: 0.0290 (0.0568)\n",
            "TRAIN(104): [190/391] Batch: 0.0418 (0.0502) Data: 0.0276 (0.0197) Loss: 0.0602 (0.0566)\n",
            "TRAIN(104): [200/391] Batch: 0.0496 (0.0502) Data: 0.0183 (0.0198) Loss: 0.0484 (0.0560)\n",
            "TRAIN(104): [210/391] Batch: 0.0516 (0.0502) Data: 0.0249 (0.0198) Loss: 0.0727 (0.0565)\n",
            "TRAIN(104): [220/391] Batch: 0.0443 (0.0500) Data: 0.0279 (0.0199) Loss: 0.0863 (0.0566)\n",
            "TRAIN(104): [230/391] Batch: 0.0442 (0.0499) Data: 0.0275 (0.0201) Loss: 0.0678 (0.0564)\n",
            "TRAIN(104): [240/391] Batch: 0.0470 (0.0498) Data: 0.0274 (0.0202) Loss: 0.0663 (0.0559)\n",
            "TRAIN(104): [250/391] Batch: 0.0574 (0.0497) Data: 0.0164 (0.0203) Loss: 0.0351 (0.0554)\n",
            "TRAIN(104): [260/391] Batch: 0.0471 (0.0496) Data: 0.0276 (0.0205) Loss: 0.0459 (0.0555)\n",
            "TRAIN(104): [270/391] Batch: 0.0547 (0.0496) Data: 0.0174 (0.0203) Loss: 0.0274 (0.0555)\n",
            "TRAIN(104): [280/391] Batch: 0.0432 (0.0495) Data: 0.0280 (0.0204) Loss: 0.0340 (0.0550)\n",
            "TRAIN(104): [290/391] Batch: 0.0503 (0.0495) Data: 0.0202 (0.0204) Loss: 0.0684 (0.0548)\n",
            "TRAIN(104): [300/391] Batch: 0.0471 (0.0495) Data: 0.0216 (0.0203) Loss: 0.0164 (0.0545)\n",
            "TRAIN(104): [310/391] Batch: 0.0501 (0.0495) Data: 0.0160 (0.0202) Loss: 0.0175 (0.0550)\n",
            "TRAIN(104): [320/391] Batch: 0.0480 (0.0495) Data: 0.0208 (0.0202) Loss: 0.0192 (0.0550)\n",
            "TRAIN(104): [330/391] Batch: 0.0453 (0.0495) Data: 0.0195 (0.0201) Loss: 0.0435 (0.0551)\n",
            "TRAIN(104): [340/391] Batch: 0.0412 (0.0496) Data: 0.0239 (0.0200) Loss: 0.0763 (0.0559)\n",
            "TRAIN(104): [350/391] Batch: 0.0450 (0.0497) Data: 0.0171 (0.0198) Loss: 0.0711 (0.0557)\n",
            "TRAIN(104): [360/391] Batch: 0.0445 (0.0496) Data: 0.0274 (0.0199) Loss: 0.0336 (0.0561)\n",
            "TRAIN(104): [370/391] Batch: 0.0458 (0.0495) Data: 0.0276 (0.0199) Loss: 0.0394 (0.0559)\n",
            "TRAIN(104): [380/391] Batch: 0.0456 (0.0495) Data: 0.0236 (0.0199) Loss: 0.0434 (0.0557)\n",
            "TRAIN(104): [390/391] Batch: 0.0456 (0.0494) Data: 0.0270 (0.0200) Loss: 0.0115 (0.0552)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(104)         0:00:19         0:00:07         0:00:11          0.0552\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(105): [ 10/391] Batch: 0.0539 (0.0683) Data: 0.0153 (0.0277) Loss: 0.1265 (0.0491)\n",
            "TRAIN(105): [ 20/391] Batch: 0.0489 (0.0585) Data: 0.0252 (0.0246) Loss: 0.0849 (0.0470)\n",
            "TRAIN(105): [ 30/391] Batch: 0.0431 (0.0557) Data: 0.0258 (0.0227) Loss: 0.0352 (0.0479)\n",
            "TRAIN(105): [ 40/391] Batch: 0.0464 (0.0534) Data: 0.0282 (0.0231) Loss: 0.0947 (0.0492)\n",
            "TRAIN(105): [ 50/391] Batch: 0.0453 (0.0523) Data: 0.0261 (0.0231) Loss: 0.0482 (0.0540)\n",
            "TRAIN(105): [ 60/391] Batch: 0.0462 (0.0513) Data: 0.0284 (0.0233) Loss: 0.1379 (0.0549)\n",
            "TRAIN(105): [ 70/391] Batch: 0.0462 (0.0511) Data: 0.0209 (0.0226) Loss: 0.0235 (0.0536)\n",
            "TRAIN(105): [ 80/391] Batch: 0.0465 (0.0506) Data: 0.0279 (0.0227) Loss: 0.0413 (0.0527)\n",
            "TRAIN(105): [ 90/391] Batch: 0.0464 (0.0504) Data: 0.0284 (0.0224) Loss: 0.0259 (0.0516)\n",
            "TRAIN(105): [100/391] Batch: 0.0481 (0.0501) Data: 0.0280 (0.0225) Loss: 0.0293 (0.0524)\n",
            "TRAIN(105): [110/391] Batch: 0.0365 (0.0500) Data: 0.0235 (0.0223) Loss: 0.0419 (0.0527)\n",
            "TRAIN(105): [120/391] Batch: 0.0511 (0.0501) Data: 0.0176 (0.0218) Loss: 0.0764 (0.0531)\n",
            "TRAIN(105): [130/391] Batch: 0.0464 (0.0499) Data: 0.0196 (0.0219) Loss: 0.0099 (0.0527)\n",
            "TRAIN(105): [140/391] Batch: 0.0483 (0.0497) Data: 0.0254 (0.0220) Loss: 0.0360 (0.0520)\n",
            "TRAIN(105): [150/391] Batch: 0.0451 (0.0495) Data: 0.0269 (0.0220) Loss: 0.0748 (0.0525)\n",
            "TRAIN(105): [160/391] Batch: 0.0496 (0.0494) Data: 0.0255 (0.0220) Loss: 0.0241 (0.0529)\n",
            "TRAIN(105): [170/391] Batch: 0.0525 (0.0495) Data: 0.0142 (0.0217) Loss: 0.0351 (0.0527)\n",
            "TRAIN(105): [180/391] Batch: 0.0446 (0.0495) Data: 0.0176 (0.0213) Loss: 0.0886 (0.0525)\n",
            "TRAIN(105): [190/391] Batch: 0.0482 (0.0496) Data: 0.0194 (0.0210) Loss: 0.0272 (0.0529)\n",
            "TRAIN(105): [200/391] Batch: 0.0508 (0.0496) Data: 0.0195 (0.0209) Loss: 0.0329 (0.0530)\n",
            "TRAIN(105): [210/391] Batch: 0.0430 (0.0497) Data: 0.0232 (0.0206) Loss: 0.0480 (0.0545)\n",
            "TRAIN(105): [220/391] Batch: 0.0446 (0.0497) Data: 0.0238 (0.0205) Loss: 0.0368 (0.0547)\n",
            "TRAIN(105): [230/391] Batch: 0.0440 (0.0498) Data: 0.0232 (0.0203) Loss: 0.0257 (0.0555)\n",
            "TRAIN(105): [240/391] Batch: 0.0465 (0.0497) Data: 0.0278 (0.0204) Loss: 0.0532 (0.0556)\n",
            "TRAIN(105): [250/391] Batch: 0.0518 (0.0497) Data: 0.0174 (0.0205) Loss: 0.0466 (0.0555)\n",
            "TRAIN(105): [260/391] Batch: 0.0406 (0.0496) Data: 0.0269 (0.0204) Loss: 0.0611 (0.0553)\n",
            "TRAIN(105): [270/391] Batch: 0.0455 (0.0495) Data: 0.0274 (0.0205) Loss: 0.0185 (0.0547)\n",
            "TRAIN(105): [280/391] Batch: 0.0469 (0.0494) Data: 0.0266 (0.0206) Loss: 0.0599 (0.0555)\n",
            "TRAIN(105): [290/391] Batch: 0.0472 (0.0493) Data: 0.0275 (0.0207) Loss: 0.0428 (0.0555)\n",
            "TRAIN(105): [300/391] Batch: 0.0493 (0.0493) Data: 0.0168 (0.0207) Loss: 0.0923 (0.0560)\n",
            "TRAIN(105): [310/391] Batch: 0.0524 (0.0493) Data: 0.0187 (0.0206) Loss: 0.0528 (0.0562)\n",
            "TRAIN(105): [320/391] Batch: 0.0417 (0.0493) Data: 0.0266 (0.0207) Loss: 0.0198 (0.0558)\n",
            "TRAIN(105): [330/391] Batch: 0.0486 (0.0492) Data: 0.0254 (0.0207) Loss: 0.0226 (0.0559)\n",
            "TRAIN(105): [340/391] Batch: 0.0593 (0.0492) Data: 0.0169 (0.0206) Loss: 0.0457 (0.0558)\n",
            "TRAIN(105): [350/391] Batch: 0.0450 (0.0491) Data: 0.0265 (0.0207) Loss: 0.1250 (0.0558)\n",
            "TRAIN(105): [360/391] Batch: 0.0458 (0.0491) Data: 0.0260 (0.0208) Loss: 0.0632 (0.0556)\n",
            "TRAIN(105): [370/391] Batch: 0.0462 (0.0491) Data: 0.0269 (0.0208) Loss: 0.0834 (0.0560)\n",
            "TRAIN(105): [380/391] Batch: 0.0542 (0.0490) Data: 0.0244 (0.0209) Loss: 0.0705 (0.0560)\n",
            "TRAIN(105): [390/391] Batch: 0.0459 (0.0490) Data: 0.0283 (0.0209) Loss: 0.0988 (0.0561)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(105)         0:00:19         0:00:08         0:00:10          0.0561\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(106): [ 10/391] Batch: 0.0508 (0.0664) Data: 0.0170 (0.0313) Loss: 0.0578 (0.0541)\n",
            "TRAIN(106): [ 20/391] Batch: 0.0505 (0.0579) Data: 0.0245 (0.0258) Loss: 0.0284 (0.0578)\n",
            "TRAIN(106): [ 30/391] Batch: 0.0484 (0.0542) Data: 0.0250 (0.0249) Loss: 0.0522 (0.0560)\n",
            "TRAIN(106): [ 40/391] Batch: 0.0547 (0.0527) Data: 0.0229 (0.0244) Loss: 0.1092 (0.0593)\n",
            "TRAIN(106): [ 50/391] Batch: 0.0522 (0.0525) Data: 0.0193 (0.0230) Loss: 0.1011 (0.0588)\n",
            "TRAIN(106): [ 60/391] Batch: 0.0465 (0.0520) Data: 0.0219 (0.0221) Loss: 0.0584 (0.0576)\n",
            "TRAIN(106): [ 70/391] Batch: 0.0436 (0.0518) Data: 0.0240 (0.0216) Loss: 0.0651 (0.0619)\n",
            "TRAIN(106): [ 80/391] Batch: 0.0425 (0.0517) Data: 0.0202 (0.0209) Loss: 0.0286 (0.0615)\n",
            "TRAIN(106): [ 90/391] Batch: 0.0484 (0.0515) Data: 0.0212 (0.0205) Loss: 0.0512 (0.0612)\n",
            "TRAIN(106): [100/391] Batch: 0.0466 (0.0511) Data: 0.0220 (0.0204) Loss: 0.0225 (0.0593)\n",
            "TRAIN(106): [110/391] Batch: 0.0456 (0.0510) Data: 0.0267 (0.0203) Loss: 0.1189 (0.0596)\n",
            "TRAIN(106): [120/391] Batch: 0.0505 (0.0507) Data: 0.0260 (0.0206) Loss: 0.0471 (0.0592)\n",
            "TRAIN(106): [130/391] Batch: 0.0459 (0.0505) Data: 0.0269 (0.0208) Loss: 0.0710 (0.0590)\n",
            "TRAIN(106): [140/391] Batch: 0.0524 (0.0503) Data: 0.0248 (0.0210) Loss: 0.0313 (0.0583)\n",
            "TRAIN(106): [150/391] Batch: 0.0456 (0.0501) Data: 0.0188 (0.0210) Loss: 0.0249 (0.0587)\n",
            "TRAIN(106): [160/391] Batch: 0.0509 (0.0501) Data: 0.0246 (0.0209) Loss: 0.0431 (0.0580)\n",
            "TRAIN(106): [170/391] Batch: 0.0456 (0.0501) Data: 0.0259 (0.0209) Loss: 0.0327 (0.0587)\n",
            "TRAIN(106): [180/391] Batch: 0.0490 (0.0501) Data: 0.0204 (0.0208) Loss: 0.1027 (0.0584)\n",
            "TRAIN(106): [190/391] Batch: 0.0463 (0.0500) Data: 0.0271 (0.0208) Loss: 0.1545 (0.0600)\n",
            "TRAIN(106): [200/391] Batch: 0.0457 (0.0499) Data: 0.0280 (0.0209) Loss: 0.0166 (0.0596)\n",
            "TRAIN(106): [210/391] Batch: 0.0480 (0.0497) Data: 0.0259 (0.0210) Loss: 0.1041 (0.0598)\n",
            "TRAIN(106): [220/391] Batch: 0.0458 (0.0497) Data: 0.0275 (0.0211) Loss: 0.0650 (0.0604)\n",
            "TRAIN(106): [230/391] Batch: 0.0462 (0.0496) Data: 0.0268 (0.0212) Loss: 0.0917 (0.0605)\n",
            "TRAIN(106): [240/391] Batch: 0.0471 (0.0496) Data: 0.0274 (0.0212) Loss: 0.0717 (0.0609)\n",
            "TRAIN(106): [250/391] Batch: 0.0451 (0.0495) Data: 0.0283 (0.0213) Loss: 0.0418 (0.0605)\n",
            "TRAIN(106): [260/391] Batch: 0.0536 (0.0496) Data: 0.0169 (0.0212) Loss: 0.0897 (0.0608)\n",
            "TRAIN(106): [270/391] Batch: 0.0453 (0.0495) Data: 0.0265 (0.0211) Loss: 0.0344 (0.0607)\n",
            "TRAIN(106): [280/391] Batch: 0.0347 (0.0495) Data: 0.0253 (0.0211) Loss: 0.0479 (0.0608)\n",
            "TRAIN(106): [290/391] Batch: 0.0402 (0.0494) Data: 0.0281 (0.0211) Loss: 0.0987 (0.0611)\n",
            "TRAIN(106): [300/391] Batch: 0.0490 (0.0493) Data: 0.0246 (0.0212) Loss: 0.0309 (0.0606)\n",
            "TRAIN(106): [310/391] Batch: 0.0464 (0.0492) Data: 0.0287 (0.0213) Loss: 0.0639 (0.0608)\n",
            "TRAIN(106): [320/391] Batch: 0.0461 (0.0493) Data: 0.0146 (0.0211) Loss: 0.0604 (0.0608)\n",
            "TRAIN(106): [330/391] Batch: 0.0473 (0.0493) Data: 0.0167 (0.0210) Loss: 0.0472 (0.0607)\n",
            "TRAIN(106): [340/391] Batch: 0.0414 (0.0494) Data: 0.0248 (0.0209) Loss: 0.0897 (0.0614)\n",
            "TRAIN(106): [350/391] Batch: 0.0477 (0.0494) Data: 0.0210 (0.0208) Loss: 0.0287 (0.0612)\n",
            "TRAIN(106): [360/391] Batch: 0.0527 (0.0495) Data: 0.0177 (0.0206) Loss: 0.0500 (0.0610)\n",
            "TRAIN(106): [370/391] Batch: 0.0475 (0.0494) Data: 0.0218 (0.0206) Loss: 0.0850 (0.0611)\n",
            "TRAIN(106): [380/391] Batch: 0.0543 (0.0495) Data: 0.0150 (0.0204) Loss: 0.0428 (0.0608)\n",
            "TRAIN(106): [390/391] Batch: 0.0462 (0.0494) Data: 0.0288 (0.0205) Loss: 0.0861 (0.0609)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(106)         0:00:19         0:00:08         0:00:11          0.0609\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(107): [ 10/391] Batch: 0.0444 (0.0660) Data: 0.0235 (0.0295) Loss: 0.0482 (0.0676)\n",
            "TRAIN(107): [ 20/391] Batch: 0.0503 (0.0567) Data: 0.0264 (0.0269) Loss: 0.0449 (0.0592)\n",
            "TRAIN(107): [ 30/391] Batch: 0.0474 (0.0542) Data: 0.0185 (0.0248) Loss: 0.1887 (0.0631)\n",
            "TRAIN(107): [ 40/391] Batch: 0.0481 (0.0528) Data: 0.0264 (0.0236) Loss: 0.0444 (0.0608)\n",
            "TRAIN(107): [ 50/391] Batch: 0.0481 (0.0519) Data: 0.0235 (0.0232) Loss: 0.0260 (0.0585)\n",
            "TRAIN(107): [ 60/391] Batch: 0.0480 (0.0513) Data: 0.0275 (0.0226) Loss: 0.0273 (0.0579)\n",
            "TRAIN(107): [ 70/391] Batch: 0.0452 (0.0508) Data: 0.0278 (0.0227) Loss: 0.0697 (0.0598)\n",
            "TRAIN(107): [ 80/391] Batch: 0.0428 (0.0505) Data: 0.0260 (0.0226) Loss: 0.0649 (0.0602)\n",
            "TRAIN(107): [ 90/391] Batch: 0.0536 (0.0506) Data: 0.0179 (0.0220) Loss: 0.0438 (0.0617)\n",
            "TRAIN(107): [100/391] Batch: 0.0461 (0.0506) Data: 0.0229 (0.0217) Loss: 0.0314 (0.0622)\n",
            "TRAIN(107): [110/391] Batch: 0.0398 (0.0504) Data: 0.0286 (0.0215) Loss: 0.0370 (0.0629)\n",
            "TRAIN(107): [120/391] Batch: 0.0375 (0.0503) Data: 0.0271 (0.0212) Loss: 0.0431 (0.0608)\n",
            "TRAIN(107): [130/391] Batch: 0.0448 (0.0503) Data: 0.0269 (0.0212) Loss: 0.0201 (0.0605)\n",
            "TRAIN(107): [140/391] Batch: 0.0461 (0.0501) Data: 0.0280 (0.0214) Loss: 0.0410 (0.0595)\n",
            "TRAIN(107): [150/391] Batch: 0.0524 (0.0499) Data: 0.0214 (0.0214) Loss: 0.0698 (0.0594)\n",
            "TRAIN(107): [160/391] Batch: 0.0485 (0.0498) Data: 0.0268 (0.0215) Loss: 0.1039 (0.0584)\n",
            "TRAIN(107): [170/391] Batch: 0.0460 (0.0497) Data: 0.0269 (0.0215) Loss: 0.0243 (0.0580)\n",
            "TRAIN(107): [180/391] Batch: 0.0457 (0.0496) Data: 0.0268 (0.0215) Loss: 0.0267 (0.0576)\n",
            "TRAIN(107): [190/391] Batch: 0.0506 (0.0495) Data: 0.0179 (0.0215) Loss: 0.0769 (0.0572)\n",
            "TRAIN(107): [200/391] Batch: 0.0413 (0.0496) Data: 0.0237 (0.0213) Loss: 0.0668 (0.0569)\n",
            "TRAIN(107): [210/391] Batch: 0.0412 (0.0496) Data: 0.0227 (0.0212) Loss: 0.0218 (0.0564)\n",
            "TRAIN(107): [220/391] Batch: 0.0455 (0.0497) Data: 0.0221 (0.0209) Loss: 0.0280 (0.0564)\n",
            "TRAIN(107): [230/391] Batch: 0.0544 (0.0498) Data: 0.0195 (0.0208) Loss: 0.0789 (0.0555)\n",
            "TRAIN(107): [240/391] Batch: 0.0497 (0.0499) Data: 0.0205 (0.0206) Loss: 0.0548 (0.0554)\n",
            "TRAIN(107): [250/391] Batch: 0.0526 (0.0498) Data: 0.0200 (0.0206) Loss: 0.0295 (0.0549)\n",
            "TRAIN(107): [260/391] Batch: 0.0560 (0.0499) Data: 0.0233 (0.0205) Loss: 0.0294 (0.0541)\n",
            "TRAIN(107): [270/391] Batch: 0.0470 (0.0497) Data: 0.0278 (0.0206) Loss: 0.0383 (0.0539)\n",
            "TRAIN(107): [280/391] Batch: 0.0468 (0.0496) Data: 0.0272 (0.0207) Loss: 0.0380 (0.0534)\n",
            "TRAIN(107): [290/391] Batch: 0.0564 (0.0496) Data: 0.0231 (0.0208) Loss: 0.0649 (0.0536)\n",
            "TRAIN(107): [300/391] Batch: 0.0543 (0.0496) Data: 0.0226 (0.0208) Loss: 0.0400 (0.0541)\n",
            "TRAIN(107): [310/391] Batch: 0.0459 (0.0495) Data: 0.0238 (0.0208) Loss: 0.0634 (0.0550)\n",
            "TRAIN(107): [320/391] Batch: 0.0493 (0.0495) Data: 0.0238 (0.0208) Loss: 0.0468 (0.0552)\n",
            "TRAIN(107): [330/391] Batch: 0.0480 (0.0495) Data: 0.0190 (0.0207) Loss: 0.0305 (0.0558)\n",
            "TRAIN(107): [340/391] Batch: 0.0469 (0.0495) Data: 0.0275 (0.0207) Loss: 0.0637 (0.0557)\n",
            "TRAIN(107): [350/391] Batch: 0.0460 (0.0494) Data: 0.0270 (0.0207) Loss: 0.0534 (0.0559)\n",
            "TRAIN(107): [360/391] Batch: 0.0547 (0.0494) Data: 0.0237 (0.0208) Loss: 0.0727 (0.0558)\n",
            "TRAIN(107): [370/391] Batch: 0.0455 (0.0494) Data: 0.0236 (0.0208) Loss: 0.0594 (0.0557)\n",
            "TRAIN(107): [380/391] Batch: 0.0466 (0.0494) Data: 0.0171 (0.0208) Loss: 0.1187 (0.0555)\n",
            "TRAIN(107): [390/391] Batch: 0.0472 (0.0493) Data: 0.0273 (0.0208) Loss: 0.0390 (0.0555)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(107)         0:00:19         0:00:08         0:00:11          0.0555\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(108): [ 10/391] Batch: 0.0476 (0.0643) Data: 0.0166 (0.0315) Loss: 0.0902 (0.0679)\n",
            "TRAIN(108): [ 20/391] Batch: 0.0493 (0.0563) Data: 0.0275 (0.0279) Loss: 0.0334 (0.0639)\n",
            "TRAIN(108): [ 30/391] Batch: 0.0619 (0.0538) Data: 0.0176 (0.0261) Loss: 0.0822 (0.0606)\n",
            "TRAIN(108): [ 40/391] Batch: 0.0432 (0.0528) Data: 0.0244 (0.0245) Loss: 0.0188 (0.0610)\n",
            "TRAIN(108): [ 50/391] Batch: 0.0482 (0.0524) Data: 0.0235 (0.0231) Loss: 0.0283 (0.0578)\n",
            "TRAIN(108): [ 60/391] Batch: 0.0578 (0.0520) Data: 0.0226 (0.0226) Loss: 0.0453 (0.0607)\n",
            "TRAIN(108): [ 70/391] Batch: 0.0488 (0.0516) Data: 0.0188 (0.0222) Loss: 0.0613 (0.0589)\n",
            "TRAIN(108): [ 80/391] Batch: 0.0499 (0.0515) Data: 0.0196 (0.0216) Loss: 0.0664 (0.0587)\n",
            "TRAIN(108): [ 90/391] Batch: 0.0452 (0.0516) Data: 0.0208 (0.0212) Loss: 0.0550 (0.0576)\n",
            "TRAIN(108): [100/391] Batch: 0.0386 (0.0513) Data: 0.0276 (0.0210) Loss: 0.0808 (0.0579)\n",
            "TRAIN(108): [110/391] Batch: 0.0397 (0.0514) Data: 0.0208 (0.0208) Loss: 0.0262 (0.0569)\n",
            "TRAIN(108): [120/391] Batch: 0.0468 (0.0512) Data: 0.0167 (0.0204) Loss: 0.0203 (0.0555)\n",
            "TRAIN(108): [130/391] Batch: 0.0495 (0.0512) Data: 0.0197 (0.0201) Loss: 0.0652 (0.0544)\n",
            "TRAIN(108): [140/391] Batch: 0.0455 (0.0512) Data: 0.0251 (0.0201) Loss: 0.0137 (0.0531)\n",
            "TRAIN(108): [150/391] Batch: 0.0518 (0.0510) Data: 0.0185 (0.0202) Loss: 0.1498 (0.0532)\n",
            "TRAIN(108): [160/391] Batch: 0.0458 (0.0508) Data: 0.0260 (0.0202) Loss: 0.0355 (0.0525)\n",
            "TRAIN(108): [170/391] Batch: 0.0551 (0.0508) Data: 0.0166 (0.0200) Loss: 0.0436 (0.0524)\n",
            "TRAIN(108): [180/391] Batch: 0.0353 (0.0506) Data: 0.0268 (0.0200) Loss: 0.0148 (0.0514)\n",
            "TRAIN(108): [190/391] Batch: 0.0493 (0.0505) Data: 0.0243 (0.0202) Loss: 0.0508 (0.0511)\n",
            "TRAIN(108): [200/391] Batch: 0.0512 (0.0503) Data: 0.0252 (0.0203) Loss: 0.0186 (0.0511)\n",
            "TRAIN(108): [210/391] Batch: 0.0454 (0.0502) Data: 0.0272 (0.0205) Loss: 0.0599 (0.0502)\n",
            "TRAIN(108): [220/391] Batch: 0.0436 (0.0501) Data: 0.0259 (0.0206) Loss: 0.0209 (0.0500)\n",
            "TRAIN(108): [230/391] Batch: 0.0415 (0.0501) Data: 0.0238 (0.0204) Loss: 0.0230 (0.0498)\n",
            "TRAIN(108): [240/391] Batch: 0.0453 (0.0500) Data: 0.0283 (0.0205) Loss: 0.0810 (0.0510)\n",
            "TRAIN(108): [250/391] Batch: 0.0519 (0.0500) Data: 0.0258 (0.0204) Loss: 0.1143 (0.0511)\n",
            "TRAIN(108): [260/391] Batch: 0.0466 (0.0499) Data: 0.0277 (0.0205) Loss: 0.0352 (0.0515)\n",
            "TRAIN(108): [270/391] Batch: 0.0523 (0.0499) Data: 0.0177 (0.0204) Loss: 0.0228 (0.0519)\n",
            "TRAIN(108): [280/391] Batch: 0.0388 (0.0499) Data: 0.0251 (0.0203) Loss: 0.1178 (0.0522)\n",
            "TRAIN(108): [290/391] Batch: 0.0518 (0.0499) Data: 0.0190 (0.0203) Loss: 0.0192 (0.0518)\n",
            "TRAIN(108): [300/391] Batch: 0.0522 (0.0498) Data: 0.0258 (0.0204) Loss: 0.0818 (0.0516)\n",
            "TRAIN(108): [310/391] Batch: 0.0465 (0.0498) Data: 0.0196 (0.0203) Loss: 0.0500 (0.0517)\n",
            "TRAIN(108): [320/391] Batch: 0.0442 (0.0498) Data: 0.0258 (0.0203) Loss: 0.0330 (0.0516)\n",
            "TRAIN(108): [330/391] Batch: 0.0468 (0.0498) Data: 0.0287 (0.0203) Loss: 0.0471 (0.0518)\n",
            "TRAIN(108): [340/391] Batch: 0.0452 (0.0497) Data: 0.0287 (0.0204) Loss: 0.0810 (0.0519)\n",
            "TRAIN(108): [350/391] Batch: 0.0468 (0.0497) Data: 0.0154 (0.0203) Loss: 0.0138 (0.0517)\n",
            "TRAIN(108): [360/391] Batch: 0.0623 (0.0498) Data: 0.0155 (0.0202) Loss: 0.0362 (0.0515)\n",
            "TRAIN(108): [370/391] Batch: 0.0467 (0.0497) Data: 0.0162 (0.0200) Loss: 0.1083 (0.0518)\n",
            "TRAIN(108): [380/391] Batch: 0.0477 (0.0497) Data: 0.0175 (0.0199) Loss: 0.0316 (0.0518)\n",
            "TRAIN(108): [390/391] Batch: 0.0461 (0.0497) Data: 0.0289 (0.0199) Loss: 0.0845 (0.0524)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(108)         0:00:19         0:00:07         0:00:11          0.0524\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(109): [ 10/391] Batch: 0.0535 (0.0717) Data: 0.0142 (0.0305) Loss: 0.0281 (0.0572)\n",
            "TRAIN(109): [ 20/391] Batch: 0.0475 (0.0606) Data: 0.0189 (0.0247) Loss: 0.0677 (0.0530)\n",
            "TRAIN(109): [ 30/391] Batch: 0.0437 (0.0566) Data: 0.0238 (0.0232) Loss: 0.0503 (0.0594)\n",
            "TRAIN(109): [ 40/391] Batch: 0.0497 (0.0549) Data: 0.0267 (0.0227) Loss: 0.0333 (0.0586)\n",
            "TRAIN(109): [ 50/391] Batch: 0.0484 (0.0538) Data: 0.0275 (0.0226) Loss: 0.0734 (0.0578)\n",
            "TRAIN(109): [ 60/391] Batch: 0.0556 (0.0531) Data: 0.0235 (0.0224) Loss: 0.0534 (0.0635)\n",
            "TRAIN(109): [ 70/391] Batch: 0.0536 (0.0525) Data: 0.0244 (0.0220) Loss: 0.0630 (0.0632)\n",
            "TRAIN(109): [ 80/391] Batch: 0.0461 (0.0518) Data: 0.0269 (0.0221) Loss: 0.0921 (0.0619)\n",
            "TRAIN(109): [ 90/391] Batch: 0.0478 (0.0514) Data: 0.0274 (0.0221) Loss: 0.0454 (0.0617)\n",
            "TRAIN(109): [100/391] Batch: 0.0408 (0.0514) Data: 0.0195 (0.0217) Loss: 0.0279 (0.0611)\n",
            "TRAIN(109): [110/391] Batch: 0.0435 (0.0511) Data: 0.0252 (0.0217) Loss: 0.0798 (0.0619)\n",
            "TRAIN(109): [120/391] Batch: 0.0453 (0.0509) Data: 0.0278 (0.0217) Loss: 0.0280 (0.0605)\n",
            "TRAIN(109): [130/391] Batch: 0.0526 (0.0507) Data: 0.0244 (0.0218) Loss: 0.0808 (0.0595)\n",
            "TRAIN(109): [140/391] Batch: 0.0456 (0.0504) Data: 0.0272 (0.0219) Loss: 0.0186 (0.0586)\n",
            "TRAIN(109): [150/391] Batch: 0.0477 (0.0504) Data: 0.0204 (0.0217) Loss: 0.0259 (0.0575)\n",
            "TRAIN(109): [160/391] Batch: 0.0466 (0.0502) Data: 0.0277 (0.0217) Loss: 0.0662 (0.0576)\n",
            "TRAIN(109): [170/391] Batch: 0.0465 (0.0500) Data: 0.0279 (0.0219) Loss: 0.0280 (0.0571)\n",
            "TRAIN(109): [180/391] Batch: 0.0522 (0.0500) Data: 0.0149 (0.0217) Loss: 0.0866 (0.0571)\n",
            "TRAIN(109): [190/391] Batch: 0.0461 (0.0499) Data: 0.0278 (0.0217) Loss: 0.0120 (0.0569)\n",
            "TRAIN(109): [200/391] Batch: 0.0453 (0.0498) Data: 0.0277 (0.0217) Loss: 0.0151 (0.0563)\n",
            "TRAIN(109): [210/391] Batch: 0.0418 (0.0498) Data: 0.0197 (0.0216) Loss: 0.0603 (0.0569)\n",
            "TRAIN(109): [220/391] Batch: 0.0433 (0.0497) Data: 0.0238 (0.0216) Loss: 0.0253 (0.0567)\n",
            "TRAIN(109): [230/391] Batch: 0.0400 (0.0498) Data: 0.0258 (0.0215) Loss: 0.0392 (0.0559)\n",
            "TRAIN(109): [240/391] Batch: 0.0505 (0.0498) Data: 0.0167 (0.0212) Loss: 0.0541 (0.0553)\n",
            "TRAIN(109): [250/391] Batch: 0.0567 (0.0498) Data: 0.0201 (0.0211) Loss: 0.0298 (0.0550)\n",
            "TRAIN(109): [260/391] Batch: 0.0437 (0.0499) Data: 0.0183 (0.0209) Loss: 0.0663 (0.0555)\n",
            "TRAIN(109): [270/391] Batch: 0.0511 (0.0499) Data: 0.0194 (0.0208) Loss: 0.1550 (0.0557)\n",
            "TRAIN(109): [280/391] Batch: 0.0543 (0.0500) Data: 0.0180 (0.0207) Loss: 0.0517 (0.0556)\n",
            "TRAIN(109): [290/391] Batch: 0.0452 (0.0499) Data: 0.0186 (0.0207) Loss: 0.0811 (0.0554)\n",
            "TRAIN(109): [300/391] Batch: 0.0458 (0.0499) Data: 0.0229 (0.0207) Loss: 0.0306 (0.0553)\n",
            "TRAIN(109): [310/391] Batch: 0.0503 (0.0498) Data: 0.0267 (0.0208) Loss: 0.0726 (0.0551)\n",
            "TRAIN(109): [320/391] Batch: 0.0475 (0.0497) Data: 0.0257 (0.0209) Loss: 0.0238 (0.0553)\n",
            "TRAIN(109): [330/391] Batch: 0.0471 (0.0497) Data: 0.0280 (0.0209) Loss: 0.0485 (0.0551)\n",
            "TRAIN(109): [340/391] Batch: 0.0447 (0.0497) Data: 0.0191 (0.0209) Loss: 0.0209 (0.0550)\n",
            "TRAIN(109): [350/391] Batch: 0.0577 (0.0498) Data: 0.0159 (0.0207) Loss: 0.0424 (0.0552)\n",
            "TRAIN(109): [360/391] Batch: 0.0561 (0.0498) Data: 0.0231 (0.0207) Loss: 0.0288 (0.0549)\n",
            "TRAIN(109): [370/391] Batch: 0.0547 (0.0498) Data: 0.0137 (0.0206) Loss: 0.0148 (0.0546)\n",
            "TRAIN(109): [380/391] Batch: 0.0432 (0.0498) Data: 0.0274 (0.0206) Loss: 0.0427 (0.0548)\n",
            "TRAIN(109): [390/391] Batch: 0.0467 (0.0497) Data: 0.0282 (0.0207) Loss: 0.0892 (0.0552)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(109)         0:00:19         0:00:08         0:00:11          0.0552\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(110): [ 10/391] Batch: 0.0482 (0.0612) Data: 0.0219 (0.0352) Loss: 0.0848 (0.0579)\n",
            "TRAIN(110): [ 20/391] Batch: 0.0593 (0.0567) Data: 0.0215 (0.0264) Loss: 0.0687 (0.0583)\n",
            "TRAIN(110): [ 30/391] Batch: 0.0441 (0.0537) Data: 0.0276 (0.0248) Loss: 0.0225 (0.0562)\n",
            "TRAIN(110): [ 40/391] Batch: 0.0474 (0.0525) Data: 0.0184 (0.0234) Loss: 0.0510 (0.0553)\n",
            "TRAIN(110): [ 50/391] Batch: 0.0523 (0.0518) Data: 0.0194 (0.0225) Loss: 0.0343 (0.0518)\n",
            "TRAIN(110): [ 60/391] Batch: 0.0470 (0.0512) Data: 0.0280 (0.0225) Loss: 0.0764 (0.0534)\n",
            "TRAIN(110): [ 70/391] Batch: 0.0467 (0.0509) Data: 0.0276 (0.0226) Loss: 0.0817 (0.0530)\n",
            "TRAIN(110): [ 80/391] Batch: 0.0465 (0.0508) Data: 0.0233 (0.0223) Loss: 0.0559 (0.0537)\n",
            "TRAIN(110): [ 90/391] Batch: 0.0652 (0.0511) Data: 0.0105 (0.0215) Loss: 0.0757 (0.0540)\n",
            "TRAIN(110): [100/391] Batch: 0.0419 (0.0509) Data: 0.0249 (0.0213) Loss: 0.0781 (0.0537)\n",
            "TRAIN(110): [110/391] Batch: 0.0591 (0.0511) Data: 0.0157 (0.0208) Loss: 0.1112 (0.0555)\n",
            "TRAIN(110): [120/391] Batch: 0.0565 (0.0512) Data: 0.0135 (0.0204) Loss: 0.0787 (0.0566)\n",
            "TRAIN(110): [130/391] Batch: 0.0476 (0.0513) Data: 0.0150 (0.0201) Loss: 0.0884 (0.0573)\n",
            "TRAIN(110): [140/391] Batch: 0.0465 (0.0512) Data: 0.0215 (0.0200) Loss: 0.0225 (0.0570)\n",
            "TRAIN(110): [150/391] Batch: 0.0399 (0.0511) Data: 0.0227 (0.0197) Loss: 0.0524 (0.0571)\n",
            "TRAIN(110): [160/391] Batch: 0.0475 (0.0510) Data: 0.0258 (0.0198) Loss: 0.0698 (0.0569)\n",
            "TRAIN(110): [170/391] Batch: 0.0541 (0.0509) Data: 0.0228 (0.0197) Loss: 0.0573 (0.0576)\n",
            "TRAIN(110): [180/391] Batch: 0.0484 (0.0508) Data: 0.0194 (0.0198) Loss: 0.0905 (0.0580)\n",
            "TRAIN(110): [190/391] Batch: 0.0473 (0.0505) Data: 0.0267 (0.0199) Loss: 0.0346 (0.0574)\n",
            "TRAIN(110): [200/391] Batch: 0.0533 (0.0504) Data: 0.0211 (0.0200) Loss: 0.0458 (0.0567)\n",
            "TRAIN(110): [210/391] Batch: 0.0471 (0.0504) Data: 0.0198 (0.0199) Loss: 0.0485 (0.0563)\n",
            "TRAIN(110): [220/391] Batch: 0.0457 (0.0503) Data: 0.0259 (0.0199) Loss: 0.0367 (0.0556)\n",
            "TRAIN(110): [230/391] Batch: 0.0518 (0.0502) Data: 0.0256 (0.0200) Loss: 0.0805 (0.0553)\n",
            "TRAIN(110): [240/391] Batch: 0.0532 (0.0502) Data: 0.0245 (0.0202) Loss: 0.0225 (0.0552)\n",
            "TRAIN(110): [250/391] Batch: 0.0434 (0.0501) Data: 0.0275 (0.0202) Loss: 0.0284 (0.0547)\n",
            "TRAIN(110): [260/391] Batch: 0.0465 (0.0500) Data: 0.0267 (0.0203) Loss: 0.0343 (0.0548)\n",
            "TRAIN(110): [270/391] Batch: 0.0482 (0.0499) Data: 0.0200 (0.0203) Loss: 0.1553 (0.0546)\n",
            "TRAIN(110): [280/391] Batch: 0.0433 (0.0499) Data: 0.0199 (0.0202) Loss: 0.0680 (0.0547)\n",
            "TRAIN(110): [290/391] Batch: 0.0457 (0.0498) Data: 0.0285 (0.0203) Loss: 0.0429 (0.0548)\n",
            "TRAIN(110): [300/391] Batch: 0.0484 (0.0498) Data: 0.0259 (0.0204) Loss: 0.0326 (0.0549)\n",
            "TRAIN(110): [310/391] Batch: 0.0469 (0.0498) Data: 0.0191 (0.0204) Loss: 0.0100 (0.0547)\n",
            "TRAIN(110): [320/391] Batch: 0.0414 (0.0498) Data: 0.0263 (0.0203) Loss: 0.0670 (0.0548)\n",
            "TRAIN(110): [330/391] Batch: 0.0496 (0.0498) Data: 0.0201 (0.0203) Loss: 0.0898 (0.0553)\n",
            "TRAIN(110): [340/391] Batch: 0.0514 (0.0497) Data: 0.0254 (0.0204) Loss: 0.0288 (0.0552)\n",
            "TRAIN(110): [350/391] Batch: 0.0463 (0.0497) Data: 0.0265 (0.0204) Loss: 0.0787 (0.0552)\n",
            "TRAIN(110): [360/391] Batch: 0.0384 (0.0496) Data: 0.0278 (0.0204) Loss: 0.0330 (0.0550)\n",
            "TRAIN(110): [370/391] Batch: 0.0464 (0.0497) Data: 0.0171 (0.0202) Loss: 0.0231 (0.0545)\n",
            "TRAIN(110): [380/391] Batch: 0.0596 (0.0498) Data: 0.0177 (0.0201) Loss: 0.0439 (0.0546)\n",
            "TRAIN(110): [390/391] Batch: 0.0449 (0.0498) Data: 0.0284 (0.0201) Loss: 0.0669 (0.0550)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(110)         0:00:19         0:00:07         0:00:11          0.0550\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(111): [ 10/391] Batch: 0.0513 (0.0714) Data: 0.0144 (0.0308) Loss: 0.0424 (0.0504)\n",
            "TRAIN(111): [ 20/391] Batch: 0.0609 (0.0615) Data: 0.0145 (0.0239) Loss: 0.0728 (0.0578)\n",
            "TRAIN(111): [ 30/391] Batch: 0.0500 (0.0576) Data: 0.0176 (0.0217) Loss: 0.0215 (0.0578)\n",
            "TRAIN(111): [ 40/391] Batch: 0.0529 (0.0556) Data: 0.0174 (0.0211) Loss: 0.0833 (0.0610)\n",
            "TRAIN(111): [ 50/391] Batch: 0.0404 (0.0544) Data: 0.0259 (0.0205) Loss: 0.0792 (0.0626)\n",
            "TRAIN(111): [ 60/391] Batch: 0.0557 (0.0534) Data: 0.0244 (0.0208) Loss: 0.0627 (0.0624)\n",
            "TRAIN(111): [ 70/391] Batch: 0.0414 (0.0527) Data: 0.0260 (0.0210) Loss: 0.0179 (0.0628)\n",
            "TRAIN(111): [ 80/391] Batch: 0.0476 (0.0521) Data: 0.0288 (0.0210) Loss: 0.0508 (0.0610)\n",
            "TRAIN(111): [ 90/391] Batch: 0.0458 (0.0516) Data: 0.0271 (0.0213) Loss: 0.0253 (0.0601)\n",
            "TRAIN(111): [100/391] Batch: 0.0479 (0.0515) Data: 0.0233 (0.0213) Loss: 0.0869 (0.0601)\n",
            "TRAIN(111): [110/391] Batch: 0.0383 (0.0513) Data: 0.0275 (0.0211) Loss: 0.0434 (0.0611)\n",
            "TRAIN(111): [120/391] Batch: 0.0462 (0.0510) Data: 0.0280 (0.0213) Loss: 0.0422 (0.0604)\n",
            "TRAIN(111): [130/391] Batch: 0.0449 (0.0508) Data: 0.0222 (0.0212) Loss: 0.0232 (0.0596)\n",
            "TRAIN(111): [140/391] Batch: 0.0454 (0.0506) Data: 0.0270 (0.0211) Loss: 0.0387 (0.0588)\n",
            "TRAIN(111): [150/391] Batch: 0.0506 (0.0504) Data: 0.0236 (0.0213) Loss: 0.1191 (0.0589)\n",
            "TRAIN(111): [160/391] Batch: 0.0456 (0.0502) Data: 0.0275 (0.0213) Loss: 0.0587 (0.0578)\n",
            "TRAIN(111): [170/391] Batch: 0.0463 (0.0501) Data: 0.0276 (0.0214) Loss: 0.0702 (0.0583)\n",
            "TRAIN(111): [180/391] Batch: 0.0466 (0.0499) Data: 0.0279 (0.0215) Loss: 0.1122 (0.0600)\n",
            "TRAIN(111): [190/391] Batch: 0.0515 (0.0500) Data: 0.0184 (0.0213) Loss: 0.2580 (0.0616)\n",
            "TRAIN(111): [200/391] Batch: 0.0526 (0.0499) Data: 0.0178 (0.0213) Loss: 0.0584 (0.0634)\n",
            "TRAIN(111): [210/391] Batch: 0.0473 (0.0499) Data: 0.0230 (0.0211) Loss: 0.0378 (0.0640)\n",
            "TRAIN(111): [220/391] Batch: 0.0504 (0.0498) Data: 0.0261 (0.0212) Loss: 0.1845 (0.0653)\n",
            "TRAIN(111): [230/391] Batch: 0.0602 (0.0499) Data: 0.0200 (0.0212) Loss: 0.0384 (0.0653)\n",
            "TRAIN(111): [240/391] Batch: 0.0440 (0.0499) Data: 0.0245 (0.0210) Loss: 0.1740 (0.0657)\n",
            "TRAIN(111): [250/391] Batch: 0.0515 (0.0499) Data: 0.0179 (0.0208) Loss: 0.0948 (0.0665)\n",
            "TRAIN(111): [260/391] Batch: 0.0575 (0.0500) Data: 0.0125 (0.0207) Loss: 0.0496 (0.0663)\n",
            "TRAIN(111): [270/391] Batch: 0.0396 (0.0500) Data: 0.0255 (0.0205) Loss: 0.0378 (0.0666)\n",
            "TRAIN(111): [280/391] Batch: 0.0433 (0.0501) Data: 0.0198 (0.0204) Loss: 0.0144 (0.0663)\n",
            "TRAIN(111): [290/391] Batch: 0.0557 (0.0501) Data: 0.0138 (0.0203) Loss: 0.0806 (0.0664)\n",
            "TRAIN(111): [300/391] Batch: 0.0475 (0.0502) Data: 0.0236 (0.0202) Loss: 0.0717 (0.0660)\n",
            "TRAIN(111): [310/391] Batch: 0.0474 (0.0500) Data: 0.0275 (0.0203) Loss: 0.0396 (0.0653)\n",
            "TRAIN(111): [320/391] Batch: 0.0444 (0.0500) Data: 0.0271 (0.0204) Loss: 0.0218 (0.0654)\n",
            "TRAIN(111): [330/391] Batch: 0.0465 (0.0499) Data: 0.0284 (0.0205) Loss: 0.0358 (0.0654)\n",
            "TRAIN(111): [340/391] Batch: 0.0443 (0.0499) Data: 0.0203 (0.0204) Loss: 0.0218 (0.0652)\n",
            "TRAIN(111): [350/391] Batch: 0.0518 (0.0499) Data: 0.0263 (0.0204) Loss: 0.0173 (0.0653)\n",
            "TRAIN(111): [360/391] Batch: 0.0524 (0.0498) Data: 0.0247 (0.0205) Loss: 0.0296 (0.0647)\n",
            "TRAIN(111): [370/391] Batch: 0.0451 (0.0498) Data: 0.0277 (0.0205) Loss: 0.0951 (0.0647)\n",
            "TRAIN(111): [380/391] Batch: 0.0426 (0.0498) Data: 0.0248 (0.0206) Loss: 0.0329 (0.0645)\n",
            "TRAIN(111): [390/391] Batch: 0.0459 (0.0497) Data: 0.0277 (0.0206) Loss: 0.1555 (0.0642)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(111)         0:00:19         0:00:08         0:00:11          0.0642\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(112): [ 10/391] Batch: 0.0438 (0.0638) Data: 0.0221 (0.0319) Loss: 0.0505 (0.0569)\n",
            "TRAIN(112): [ 20/391] Batch: 0.0441 (0.0574) Data: 0.0200 (0.0247) Loss: 0.0448 (0.0598)\n",
            "TRAIN(112): [ 30/391] Batch: 0.0468 (0.0550) Data: 0.0238 (0.0231) Loss: 0.0444 (0.0553)\n",
            "TRAIN(112): [ 40/391] Batch: 0.0462 (0.0533) Data: 0.0278 (0.0230) Loss: 0.0494 (0.0557)\n",
            "TRAIN(112): [ 50/391] Batch: 0.0526 (0.0528) Data: 0.0161 (0.0223) Loss: 0.0631 (0.0574)\n",
            "TRAIN(112): [ 60/391] Batch: 0.0499 (0.0523) Data: 0.0253 (0.0217) Loss: 0.0281 (0.0556)\n",
            "TRAIN(112): [ 70/391] Batch: 0.0516 (0.0518) Data: 0.0262 (0.0218) Loss: 0.0369 (0.0559)\n",
            "TRAIN(112): [ 80/391] Batch: 0.0457 (0.0512) Data: 0.0269 (0.0220) Loss: 0.0562 (0.0548)\n",
            "TRAIN(112): [ 90/391] Batch: 0.0400 (0.0508) Data: 0.0280 (0.0218) Loss: 0.0637 (0.0538)\n",
            "TRAIN(112): [100/391] Batch: 0.0511 (0.0506) Data: 0.0258 (0.0218) Loss: 0.0513 (0.0549)\n",
            "TRAIN(112): [110/391] Batch: 0.0502 (0.0506) Data: 0.0212 (0.0216) Loss: 0.0159 (0.0536)\n",
            "TRAIN(112): [120/391] Batch: 0.0581 (0.0506) Data: 0.0210 (0.0214) Loss: 0.0734 (0.0543)\n",
            "TRAIN(112): [130/391] Batch: 0.0539 (0.0506) Data: 0.0237 (0.0212) Loss: 0.0335 (0.0539)\n",
            "TRAIN(112): [140/391] Batch: 0.0451 (0.0506) Data: 0.0171 (0.0209) Loss: 0.0353 (0.0532)\n",
            "TRAIN(112): [150/391] Batch: 0.0475 (0.0506) Data: 0.0146 (0.0207) Loss: 0.0364 (0.0532)\n",
            "TRAIN(112): [160/391] Batch: 0.0502 (0.0505) Data: 0.0167 (0.0204) Loss: 0.0271 (0.0533)\n",
            "TRAIN(112): [170/391] Batch: 0.0521 (0.0505) Data: 0.0189 (0.0201) Loss: 0.0936 (0.0548)\n",
            "TRAIN(112): [180/391] Batch: 0.0505 (0.0505) Data: 0.0164 (0.0200) Loss: 0.1182 (0.0551)\n",
            "TRAIN(112): [190/391] Batch: 0.0475 (0.0504) Data: 0.0201 (0.0200) Loss: 0.0465 (0.0545)\n",
            "TRAIN(112): [200/391] Batch: 0.0467 (0.0503) Data: 0.0274 (0.0201) Loss: 0.0472 (0.0540)\n",
            "TRAIN(112): [210/391] Batch: 0.0461 (0.0502) Data: 0.0263 (0.0202) Loss: 0.0409 (0.0541)\n",
            "TRAIN(112): [220/391] Batch: 0.0366 (0.0501) Data: 0.0269 (0.0203) Loss: 0.0239 (0.0537)\n",
            "TRAIN(112): [230/391] Batch: 0.0464 (0.0500) Data: 0.0278 (0.0203) Loss: 0.0760 (0.0541)\n",
            "TRAIN(112): [240/391] Batch: 0.0488 (0.0500) Data: 0.0270 (0.0204) Loss: 0.0872 (0.0539)\n",
            "TRAIN(112): [250/391] Batch: 0.0462 (0.0499) Data: 0.0260 (0.0204) Loss: 0.0176 (0.0538)\n",
            "TRAIN(112): [260/391] Batch: 0.0464 (0.0499) Data: 0.0276 (0.0204) Loss: 0.0538 (0.0537)\n",
            "TRAIN(112): [270/391] Batch: 0.0540 (0.0498) Data: 0.0198 (0.0205) Loss: 0.0546 (0.0537)\n",
            "TRAIN(112): [280/391] Batch: 0.0436 (0.0498) Data: 0.0257 (0.0205) Loss: 0.0491 (0.0533)\n",
            "TRAIN(112): [290/391] Batch: 0.0463 (0.0497) Data: 0.0291 (0.0205) Loss: 0.0774 (0.0531)\n",
            "TRAIN(112): [300/391] Batch: 0.0456 (0.0496) Data: 0.0273 (0.0206) Loss: 0.0355 (0.0529)\n",
            "TRAIN(112): [310/391] Batch: 0.0514 (0.0496) Data: 0.0252 (0.0206) Loss: 0.0151 (0.0530)\n",
            "TRAIN(112): [320/391] Batch: 0.0455 (0.0496) Data: 0.0281 (0.0207) Loss: 0.0461 (0.0527)\n",
            "TRAIN(112): [330/391] Batch: 0.0453 (0.0495) Data: 0.0270 (0.0207) Loss: 0.1516 (0.0528)\n",
            "TRAIN(112): [340/391] Batch: 0.0499 (0.0495) Data: 0.0257 (0.0207) Loss: 0.0347 (0.0529)\n",
            "TRAIN(112): [350/391] Batch: 0.0456 (0.0495) Data: 0.0190 (0.0207) Loss: 0.0410 (0.0529)\n",
            "TRAIN(112): [360/391] Batch: 0.0547 (0.0495) Data: 0.0256 (0.0207) Loss: 0.1054 (0.0533)\n",
            "TRAIN(112): [370/391] Batch: 0.0464 (0.0494) Data: 0.0277 (0.0208) Loss: 0.0758 (0.0537)\n",
            "TRAIN(112): [380/391] Batch: 0.0540 (0.0495) Data: 0.0160 (0.0207) Loss: 0.0454 (0.0539)\n",
            "TRAIN(112): [390/391] Batch: 0.0499 (0.0494) Data: 0.0265 (0.0207) Loss: 0.0751 (0.0538)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(112)         0:00:19         0:00:08         0:00:11          0.0538\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(113): [ 10/391] Batch: 0.0373 (0.0715) Data: 0.0190 (0.0325) Loss: 0.0203 (0.0571)\n",
            "TRAIN(113): [ 20/391] Batch: 0.0489 (0.0604) Data: 0.0221 (0.0265) Loss: 0.1161 (0.0643)\n",
            "TRAIN(113): [ 30/391] Batch: 0.0536 (0.0566) Data: 0.0189 (0.0239) Loss: 0.0323 (0.0612)\n",
            "TRAIN(113): [ 40/391] Batch: 0.0502 (0.0551) Data: 0.0165 (0.0221) Loss: 0.0417 (0.0590)\n",
            "TRAIN(113): [ 50/391] Batch: 0.0461 (0.0539) Data: 0.0288 (0.0217) Loss: 0.0374 (0.0594)\n",
            "TRAIN(113): [ 60/391] Batch: 0.0459 (0.0528) Data: 0.0287 (0.0221) Loss: 0.0475 (0.0614)\n",
            "TRAIN(113): [ 70/391] Batch: 0.0425 (0.0524) Data: 0.0205 (0.0217) Loss: 0.0477 (0.0612)\n",
            "TRAIN(113): [ 80/391] Batch: 0.0563 (0.0519) Data: 0.0244 (0.0215) Loss: 0.0670 (0.0609)\n",
            "TRAIN(113): [ 90/391] Batch: 0.0490 (0.0518) Data: 0.0183 (0.0210) Loss: 0.0458 (0.0607)\n",
            "TRAIN(113): [100/391] Batch: 0.0478 (0.0515) Data: 0.0187 (0.0207) Loss: 0.0834 (0.0603)\n",
            "TRAIN(113): [110/391] Batch: 0.0457 (0.0513) Data: 0.0224 (0.0204) Loss: 0.0968 (0.0603)\n",
            "TRAIN(113): [120/391] Batch: 0.0498 (0.0513) Data: 0.0179 (0.0201) Loss: 0.0354 (0.0597)\n",
            "TRAIN(113): [130/391] Batch: 0.0460 (0.0510) Data: 0.0278 (0.0201) Loss: 0.0361 (0.0598)\n",
            "TRAIN(113): [140/391] Batch: 0.0445 (0.0508) Data: 0.0283 (0.0202) Loss: 0.0728 (0.0592)\n",
            "TRAIN(113): [150/391] Batch: 0.0517 (0.0506) Data: 0.0171 (0.0203) Loss: 0.0421 (0.0591)\n",
            "TRAIN(113): [160/391] Batch: 0.0498 (0.0505) Data: 0.0244 (0.0203) Loss: 0.0345 (0.0586)\n",
            "TRAIN(113): [170/391] Batch: 0.0570 (0.0505) Data: 0.0218 (0.0204) Loss: 0.0201 (0.0578)\n",
            "TRAIN(113): [180/391] Batch: 0.0501 (0.0504) Data: 0.0187 (0.0204) Loss: 0.0461 (0.0573)\n",
            "TRAIN(113): [190/391] Batch: 0.0498 (0.0503) Data: 0.0175 (0.0203) Loss: 0.0971 (0.0580)\n",
            "TRAIN(113): [200/391] Batch: 0.0456 (0.0502) Data: 0.0279 (0.0203) Loss: 0.0730 (0.0585)\n",
            "TRAIN(113): [210/391] Batch: 0.0456 (0.0501) Data: 0.0288 (0.0205) Loss: 0.0428 (0.0584)\n",
            "TRAIN(113): [220/391] Batch: 0.0409 (0.0500) Data: 0.0205 (0.0205) Loss: 0.0738 (0.0583)\n",
            "TRAIN(113): [230/391] Batch: 0.0519 (0.0500) Data: 0.0183 (0.0203) Loss: 0.0328 (0.0575)\n",
            "TRAIN(113): [240/391] Batch: 0.0495 (0.0500) Data: 0.0269 (0.0203) Loss: 0.0568 (0.0575)\n",
            "TRAIN(113): [250/391] Batch: 0.0488 (0.0499) Data: 0.0238 (0.0204) Loss: 0.0566 (0.0570)\n",
            "TRAIN(113): [260/391] Batch: 0.0609 (0.0500) Data: 0.0170 (0.0202) Loss: 0.0648 (0.0571)\n",
            "TRAIN(113): [270/391] Batch: 0.0442 (0.0500) Data: 0.0189 (0.0201) Loss: 0.0316 (0.0568)\n",
            "TRAIN(113): [280/391] Batch: 0.0463 (0.0501) Data: 0.0200 (0.0200) Loss: 0.1136 (0.0567)\n",
            "TRAIN(113): [290/391] Batch: 0.0432 (0.0501) Data: 0.0229 (0.0198) Loss: 0.1068 (0.0571)\n",
            "TRAIN(113): [300/391] Batch: 0.0517 (0.0502) Data: 0.0168 (0.0196) Loss: 0.0107 (0.0567)\n",
            "TRAIN(113): [310/391] Batch: 0.0464 (0.0502) Data: 0.0204 (0.0195) Loss: 0.0244 (0.0565)\n",
            "TRAIN(113): [320/391] Batch: 0.0560 (0.0502) Data: 0.0155 (0.0194) Loss: 0.0256 (0.0564)\n",
            "TRAIN(113): [330/391] Batch: 0.0520 (0.0501) Data: 0.0262 (0.0195) Loss: 0.0478 (0.0561)\n",
            "TRAIN(113): [340/391] Batch: 0.0458 (0.0501) Data: 0.0272 (0.0196) Loss: 0.0171 (0.0559)\n",
            "TRAIN(113): [350/391] Batch: 0.0460 (0.0500) Data: 0.0294 (0.0197) Loss: 0.0303 (0.0560)\n",
            "TRAIN(113): [360/391] Batch: 0.0520 (0.0500) Data: 0.0245 (0.0197) Loss: 0.1281 (0.0563)\n",
            "TRAIN(113): [370/391] Batch: 0.0458 (0.0499) Data: 0.0269 (0.0198) Loss: 0.0693 (0.0562)\n",
            "TRAIN(113): [380/391] Batch: 0.0466 (0.0499) Data: 0.0265 (0.0199) Loss: 0.0360 (0.0559)\n",
            "TRAIN(113): [390/391] Batch: 0.0471 (0.0498) Data: 0.0280 (0.0199) Loss: 0.0289 (0.0556)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(113)         0:00:19         0:00:07         0:00:11          0.0556\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(114): [ 10/391] Batch: 0.0463 (0.0638) Data: 0.0268 (0.0314) Loss: 0.0213 (0.0512)\n",
            "TRAIN(114): [ 20/391] Batch: 0.0546 (0.0556) Data: 0.0271 (0.0281) Loss: 0.0580 (0.0523)\n",
            "TRAIN(114): [ 30/391] Batch: 0.0496 (0.0533) Data: 0.0244 (0.0254) Loss: 0.0492 (0.0520)\n",
            "TRAIN(114): [ 40/391] Batch: 0.0593 (0.0525) Data: 0.0191 (0.0247) Loss: 0.0313 (0.0500)\n",
            "TRAIN(114): [ 50/391] Batch: 0.0517 (0.0518) Data: 0.0186 (0.0238) Loss: 0.0463 (0.0480)\n",
            "TRAIN(114): [ 60/391] Batch: 0.0497 (0.0512) Data: 0.0189 (0.0233) Loss: 0.0482 (0.0488)\n",
            "TRAIN(114): [ 70/391] Batch: 0.0441 (0.0507) Data: 0.0279 (0.0229) Loss: 0.0927 (0.0502)\n",
            "TRAIN(114): [ 80/391] Batch: 0.0362 (0.0504) Data: 0.0282 (0.0225) Loss: 0.1578 (0.0510)\n",
            "TRAIN(114): [ 90/391] Batch: 0.0448 (0.0502) Data: 0.0277 (0.0226) Loss: 0.1209 (0.0531)\n",
            "TRAIN(114): [100/391] Batch: 0.0455 (0.0499) Data: 0.0287 (0.0227) Loss: 0.0465 (0.0525)\n",
            "TRAIN(114): [110/391] Batch: 0.0467 (0.0496) Data: 0.0269 (0.0229) Loss: 0.0499 (0.0520)\n",
            "TRAIN(114): [120/391] Batch: 0.0467 (0.0494) Data: 0.0270 (0.0229) Loss: 0.0535 (0.0520)\n",
            "TRAIN(114): [130/391] Batch: 0.0504 (0.0496) Data: 0.0193 (0.0225) Loss: 0.0854 (0.0528)\n",
            "TRAIN(114): [140/391] Batch: 0.0432 (0.0497) Data: 0.0219 (0.0221) Loss: 0.0436 (0.0530)\n",
            "TRAIN(114): [150/391] Batch: 0.0504 (0.0499) Data: 0.0196 (0.0218) Loss: 0.0210 (0.0533)\n",
            "TRAIN(114): [160/391] Batch: 0.0524 (0.0499) Data: 0.0206 (0.0216) Loss: 0.0670 (0.0533)\n",
            "TRAIN(114): [170/391] Batch: 0.0550 (0.0500) Data: 0.0162 (0.0213) Loss: 0.0909 (0.0534)\n",
            "TRAIN(114): [180/391] Batch: 0.0607 (0.0502) Data: 0.0161 (0.0211) Loss: 0.0831 (0.0534)\n",
            "TRAIN(114): [190/391] Batch: 0.0541 (0.0502) Data: 0.0163 (0.0208) Loss: 0.0447 (0.0530)\n",
            "TRAIN(114): [200/391] Batch: 0.0482 (0.0502) Data: 0.0175 (0.0206) Loss: 0.0087 (0.0537)\n",
            "TRAIN(114): [210/391] Batch: 0.0386 (0.0502) Data: 0.0248 (0.0205) Loss: 0.0361 (0.0539)\n",
            "TRAIN(114): [220/391] Batch: 0.0473 (0.0501) Data: 0.0274 (0.0207) Loss: 0.0585 (0.0546)\n",
            "TRAIN(114): [230/391] Batch: 0.0452 (0.0500) Data: 0.0260 (0.0207) Loss: 0.0707 (0.0546)\n",
            "TRAIN(114): [240/391] Batch: 0.0434 (0.0499) Data: 0.0183 (0.0207) Loss: 0.0468 (0.0546)\n",
            "TRAIN(114): [250/391] Batch: 0.0446 (0.0498) Data: 0.0283 (0.0208) Loss: 0.0662 (0.0545)\n",
            "TRAIN(114): [260/391] Batch: 0.0453 (0.0497) Data: 0.0268 (0.0209) Loss: 0.0574 (0.0550)\n",
            "TRAIN(114): [270/391] Batch: 0.0535 (0.0497) Data: 0.0172 (0.0209) Loss: 0.0692 (0.0548)\n",
            "TRAIN(114): [280/391] Batch: 0.0481 (0.0497) Data: 0.0178 (0.0208) Loss: 0.0343 (0.0549)\n",
            "TRAIN(114): [290/391] Batch: 0.0481 (0.0497) Data: 0.0213 (0.0209) Loss: 0.0201 (0.0548)\n",
            "TRAIN(114): [300/391] Batch: 0.0521 (0.0497) Data: 0.0184 (0.0208) Loss: 0.0547 (0.0549)\n",
            "TRAIN(114): [310/391] Batch: 0.0500 (0.0496) Data: 0.0195 (0.0207) Loss: 0.0469 (0.0552)\n",
            "TRAIN(114): [320/391] Batch: 0.0429 (0.0496) Data: 0.0250 (0.0206) Loss: 0.0211 (0.0549)\n",
            "TRAIN(114): [330/391] Batch: 0.0552 (0.0496) Data: 0.0173 (0.0206) Loss: 0.0320 (0.0549)\n",
            "TRAIN(114): [340/391] Batch: 0.0505 (0.0497) Data: 0.0174 (0.0205) Loss: 0.0822 (0.0550)\n",
            "TRAIN(114): [350/391] Batch: 0.0554 (0.0497) Data: 0.0175 (0.0204) Loss: 0.0212 (0.0548)\n",
            "TRAIN(114): [360/391] Batch: 0.0451 (0.0497) Data: 0.0208 (0.0204) Loss: 0.1010 (0.0547)\n",
            "TRAIN(114): [370/391] Batch: 0.0515 (0.0496) Data: 0.0222 (0.0204) Loss: 0.1158 (0.0552)\n",
            "TRAIN(114): [380/391] Batch: 0.0462 (0.0496) Data: 0.0267 (0.0205) Loss: 0.0190 (0.0554)\n",
            "TRAIN(114): [390/391] Batch: 0.0456 (0.0495) Data: 0.0279 (0.0205) Loss: 0.0278 (0.0552)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(114)         0:00:19         0:00:08         0:00:11          0.0552\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(115): [ 10/391] Batch: 0.0519 (0.0696) Data: 0.0151 (0.0297) Loss: 0.0647 (0.0480)\n",
            "TRAIN(115): [ 20/391] Batch: 0.0489 (0.0599) Data: 0.0198 (0.0237) Loss: 0.0796 (0.0527)\n",
            "TRAIN(115): [ 30/391] Batch: 0.0590 (0.0575) Data: 0.0142 (0.0217) Loss: 0.0555 (0.0490)\n",
            "TRAIN(115): [ 40/391] Batch: 0.0454 (0.0558) Data: 0.0152 (0.0204) Loss: 0.0268 (0.0475)\n",
            "TRAIN(115): [ 50/391] Batch: 0.0375 (0.0546) Data: 0.0248 (0.0196) Loss: 0.0433 (0.0488)\n",
            "TRAIN(115): [ 60/391] Batch: 0.0488 (0.0540) Data: 0.0226 (0.0190) Loss: 0.0462 (0.0486)\n",
            "TRAIN(115): [ 70/391] Batch: 0.0450 (0.0533) Data: 0.0280 (0.0190) Loss: 0.0413 (0.0512)\n",
            "TRAIN(115): [ 80/391] Batch: 0.0569 (0.0528) Data: 0.0158 (0.0192) Loss: 0.0392 (0.0527)\n",
            "TRAIN(115): [ 90/391] Batch: 0.0456 (0.0523) Data: 0.0262 (0.0195) Loss: 0.0145 (0.0517)\n",
            "TRAIN(115): [100/391] Batch: 0.0436 (0.0519) Data: 0.0267 (0.0199) Loss: 0.0272 (0.0506)\n",
            "TRAIN(115): [110/391] Batch: 0.0509 (0.0516) Data: 0.0252 (0.0202) Loss: 0.0503 (0.0516)\n",
            "TRAIN(115): [120/391] Batch: 0.0566 (0.0514) Data: 0.0230 (0.0203) Loss: 0.0409 (0.0502)\n",
            "TRAIN(115): [130/391] Batch: 0.0436 (0.0512) Data: 0.0244 (0.0203) Loss: 0.0530 (0.0496)\n",
            "TRAIN(115): [140/391] Batch: 0.0505 (0.0511) Data: 0.0185 (0.0201) Loss: 0.0320 (0.0498)\n",
            "TRAIN(115): [150/391] Batch: 0.0554 (0.0510) Data: 0.0230 (0.0200) Loss: 0.0251 (0.0494)\n",
            "TRAIN(115): [160/391] Batch: 0.0593 (0.0509) Data: 0.0176 (0.0201) Loss: 0.0509 (0.0497)\n",
            "TRAIN(115): [170/391] Batch: 0.0451 (0.0508) Data: 0.0275 (0.0200) Loss: 0.0492 (0.0499)\n",
            "TRAIN(115): [180/391] Batch: 0.0584 (0.0506) Data: 0.0181 (0.0202) Loss: 0.0378 (0.0498)\n",
            "TRAIN(115): [190/391] Batch: 0.0544 (0.0505) Data: 0.0168 (0.0202) Loss: 0.0380 (0.0492)\n",
            "TRAIN(115): [200/391] Batch: 0.0438 (0.0505) Data: 0.0205 (0.0200) Loss: 0.0430 (0.0494)\n",
            "TRAIN(115): [210/391] Batch: 0.0369 (0.0504) Data: 0.0260 (0.0199) Loss: 0.0534 (0.0494)\n",
            "TRAIN(115): [220/391] Batch: 0.0519 (0.0504) Data: 0.0264 (0.0200) Loss: 0.0812 (0.0489)\n",
            "TRAIN(115): [230/391] Batch: 0.0476 (0.0504) Data: 0.0273 (0.0201) Loss: 0.0238 (0.0487)\n",
            "TRAIN(115): [240/391] Batch: 0.0424 (0.0503) Data: 0.0268 (0.0200) Loss: 0.1055 (0.0491)\n",
            "TRAIN(115): [250/391] Batch: 0.0478 (0.0503) Data: 0.0206 (0.0201) Loss: 0.0947 (0.0490)\n",
            "TRAIN(115): [260/391] Batch: 0.0458 (0.0502) Data: 0.0274 (0.0201) Loss: 0.0232 (0.0486)\n",
            "TRAIN(115): [270/391] Batch: 0.0497 (0.0502) Data: 0.0197 (0.0200) Loss: 0.0362 (0.0480)\n",
            "TRAIN(115): [280/391] Batch: 0.0382 (0.0502) Data: 0.0263 (0.0200) Loss: 0.0288 (0.0478)\n",
            "TRAIN(115): [290/391] Batch: 0.0410 (0.0503) Data: 0.0244 (0.0199) Loss: 0.0544 (0.0473)\n",
            "TRAIN(115): [300/391] Batch: 0.0403 (0.0503) Data: 0.0248 (0.0199) Loss: 0.0196 (0.0470)\n",
            "TRAIN(115): [310/391] Batch: 0.0589 (0.0504) Data: 0.0122 (0.0198) Loss: 0.0096 (0.0466)\n",
            "TRAIN(115): [320/391] Batch: 0.0456 (0.0504) Data: 0.0147 (0.0196) Loss: 0.0234 (0.0465)\n",
            "TRAIN(115): [330/391] Batch: 0.0584 (0.0504) Data: 0.0140 (0.0195) Loss: 0.0286 (0.0465)\n",
            "TRAIN(115): [340/391] Batch: 0.0573 (0.0504) Data: 0.0213 (0.0195) Loss: 0.0893 (0.0463)\n",
            "TRAIN(115): [350/391] Batch: 0.0605 (0.0504) Data: 0.0177 (0.0196) Loss: 0.0208 (0.0458)\n",
            "TRAIN(115): [360/391] Batch: 0.0468 (0.0503) Data: 0.0273 (0.0196) Loss: 0.0379 (0.0457)\n",
            "TRAIN(115): [370/391] Batch: 0.0385 (0.0502) Data: 0.0272 (0.0197) Loss: 0.0185 (0.0453)\n",
            "TRAIN(115): [380/391] Batch: 0.0469 (0.0502) Data: 0.0265 (0.0198) Loss: 0.0306 (0.0451)\n",
            "TRAIN(115): [390/391] Batch: 0.0466 (0.0501) Data: 0.0288 (0.0199) Loss: 0.0299 (0.0448)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(115)         0:00:19         0:00:07         0:00:11          0.0448\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(116): [ 10/391] Batch: 0.0426 (0.0637) Data: 0.0191 (0.0307) Loss: 0.0165 (0.0391)\n",
            "TRAIN(116): [ 20/391] Batch: 0.0483 (0.0564) Data: 0.0276 (0.0262) Loss: 0.0412 (0.0368)\n",
            "TRAIN(116): [ 30/391] Batch: 0.0455 (0.0533) Data: 0.0245 (0.0247) Loss: 0.0106 (0.0359)\n",
            "TRAIN(116): [ 40/391] Batch: 0.0581 (0.0523) Data: 0.0177 (0.0240) Loss: 0.0559 (0.0343)\n",
            "TRAIN(116): [ 50/391] Batch: 0.0452 (0.0513) Data: 0.0288 (0.0231) Loss: 0.0600 (0.0370)\n",
            "TRAIN(116): [ 60/391] Batch: 0.0490 (0.0509) Data: 0.0221 (0.0229) Loss: 0.0183 (0.0372)\n",
            "TRAIN(116): [ 70/391] Batch: 0.0555 (0.0507) Data: 0.0151 (0.0223) Loss: 0.0462 (0.0381)\n",
            "TRAIN(116): [ 80/391] Batch: 0.0445 (0.0506) Data: 0.0257 (0.0221) Loss: 0.0448 (0.0383)\n",
            "TRAIN(116): [ 90/391] Batch: 0.0521 (0.0504) Data: 0.0251 (0.0221) Loss: 0.0663 (0.0397)\n",
            "TRAIN(116): [100/391] Batch: 0.0521 (0.0504) Data: 0.0184 (0.0220) Loss: 0.0560 (0.0416)\n",
            "TRAIN(116): [110/391] Batch: 0.0532 (0.0504) Data: 0.0175 (0.0216) Loss: 0.0811 (0.0424)\n",
            "TRAIN(116): [120/391] Batch: 0.0522 (0.0504) Data: 0.0155 (0.0212) Loss: 0.0759 (0.0426)\n",
            "TRAIN(116): [130/391] Batch: 0.0459 (0.0503) Data: 0.0182 (0.0209) Loss: 0.0642 (0.0424)\n",
            "TRAIN(116): [140/391] Batch: 0.0583 (0.0502) Data: 0.0183 (0.0209) Loss: 0.0164 (0.0418)\n",
            "TRAIN(116): [150/391] Batch: 0.0466 (0.0503) Data: 0.0176 (0.0205) Loss: 0.0408 (0.0412)\n",
            "TRAIN(116): [160/391] Batch: 0.0530 (0.0503) Data: 0.0199 (0.0202) Loss: 0.0313 (0.0416)\n",
            "TRAIN(116): [170/391] Batch: 0.0599 (0.0504) Data: 0.0122 (0.0201) Loss: 0.0261 (0.0412)\n",
            "TRAIN(116): [180/391] Batch: 0.0401 (0.0503) Data: 0.0252 (0.0200) Loss: 0.0519 (0.0408)\n",
            "TRAIN(116): [190/391] Batch: 0.0532 (0.0504) Data: 0.0200 (0.0199) Loss: 0.1458 (0.0408)\n",
            "TRAIN(116): [200/391] Batch: 0.0507 (0.0504) Data: 0.0203 (0.0197) Loss: 0.0295 (0.0407)\n",
            "TRAIN(116): [210/391] Batch: 0.0451 (0.0504) Data: 0.0257 (0.0196) Loss: 0.0520 (0.0415)\n",
            "TRAIN(116): [220/391] Batch: 0.0478 (0.0504) Data: 0.0251 (0.0197) Loss: 0.0052 (0.0417)\n",
            "TRAIN(116): [230/391] Batch: 0.0465 (0.0504) Data: 0.0180 (0.0197) Loss: 0.0133 (0.0414)\n",
            "TRAIN(116): [240/391] Batch: 0.0526 (0.0503) Data: 0.0162 (0.0197) Loss: 0.0087 (0.0417)\n",
            "TRAIN(116): [250/391] Batch: 0.0467 (0.0502) Data: 0.0278 (0.0198) Loss: 0.0133 (0.0421)\n",
            "TRAIN(116): [260/391] Batch: 0.0535 (0.0502) Data: 0.0232 (0.0199) Loss: 0.0595 (0.0422)\n",
            "TRAIN(116): [270/391] Batch: 0.0453 (0.0501) Data: 0.0273 (0.0200) Loss: 0.0793 (0.0431)\n",
            "TRAIN(116): [280/391] Batch: 0.0492 (0.0500) Data: 0.0248 (0.0201) Loss: 0.0321 (0.0435)\n",
            "TRAIN(116): [290/391] Batch: 0.0376 (0.0500) Data: 0.0266 (0.0200) Loss: 0.0237 (0.0438)\n",
            "TRAIN(116): [300/391] Batch: 0.0479 (0.0499) Data: 0.0277 (0.0202) Loss: 0.0417 (0.0440)\n",
            "TRAIN(116): [310/391] Batch: 0.0456 (0.0498) Data: 0.0272 (0.0202) Loss: 0.0250 (0.0440)\n",
            "TRAIN(116): [320/391] Batch: 0.0459 (0.0498) Data: 0.0283 (0.0202) Loss: 0.0447 (0.0445)\n",
            "TRAIN(116): [330/391] Batch: 0.0536 (0.0498) Data: 0.0241 (0.0203) Loss: 0.0333 (0.0444)\n",
            "TRAIN(116): [340/391] Batch: 0.0452 (0.0497) Data: 0.0233 (0.0203) Loss: 0.0798 (0.0451)\n",
            "TRAIN(116): [350/391] Batch: 0.0422 (0.0498) Data: 0.0264 (0.0202) Loss: 0.0332 (0.0451)\n",
            "TRAIN(116): [360/391] Batch: 0.0436 (0.0497) Data: 0.0279 (0.0203) Loss: 0.0070 (0.0451)\n",
            "TRAIN(116): [370/391] Batch: 0.0445 (0.0497) Data: 0.0268 (0.0204) Loss: 0.0871 (0.0450)\n",
            "TRAIN(116): [380/391] Batch: 0.0517 (0.0497) Data: 0.0187 (0.0204) Loss: 0.0354 (0.0453)\n",
            "TRAIN(116): [390/391] Batch: 0.0479 (0.0497) Data: 0.0287 (0.0204) Loss: 0.0372 (0.0453)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(116)         0:00:19         0:00:07         0:00:11          0.0453\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(117): [ 10/391] Batch: 0.0431 (0.0627) Data: 0.0271 (0.0343) Loss: 0.0696 (0.0690)\n",
            "TRAIN(117): [ 20/391] Batch: 0.0529 (0.0570) Data: 0.0200 (0.0271) Loss: 0.0611 (0.0619)\n",
            "TRAIN(117): [ 30/391] Batch: 0.0405 (0.0547) Data: 0.0227 (0.0239) Loss: 0.0606 (0.0598)\n",
            "TRAIN(117): [ 40/391] Batch: 0.0410 (0.0536) Data: 0.0231 (0.0221) Loss: 0.0400 (0.0532)\n",
            "TRAIN(117): [ 50/391] Batch: 0.0545 (0.0530) Data: 0.0198 (0.0211) Loss: 0.0348 (0.0559)\n",
            "TRAIN(117): [ 60/391] Batch: 0.0448 (0.0526) Data: 0.0208 (0.0205) Loss: 0.0756 (0.0579)\n",
            "TRAIN(117): [ 70/391] Batch: 0.0397 (0.0523) Data: 0.0232 (0.0199) Loss: 0.0326 (0.0562)\n",
            "TRAIN(117): [ 80/391] Batch: 0.0435 (0.0523) Data: 0.0178 (0.0195) Loss: 0.0447 (0.0550)\n",
            "TRAIN(117): [ 90/391] Batch: 0.0423 (0.0518) Data: 0.0281 (0.0197) Loss: 0.0999 (0.0552)\n",
            "TRAIN(117): [100/391] Batch: 0.0427 (0.0515) Data: 0.0253 (0.0199) Loss: 0.0615 (0.0547)\n",
            "TRAIN(117): [110/391] Batch: 0.0533 (0.0512) Data: 0.0219 (0.0201) Loss: 0.0933 (0.0549)\n",
            "TRAIN(117): [120/391] Batch: 0.0431 (0.0509) Data: 0.0259 (0.0201) Loss: 0.0361 (0.0559)\n",
            "TRAIN(117): [130/391] Batch: 0.0457 (0.0508) Data: 0.0270 (0.0202) Loss: 0.0229 (0.0571)\n",
            "TRAIN(117): [140/391] Batch: 0.0513 (0.0506) Data: 0.0253 (0.0204) Loss: 0.0596 (0.0576)\n",
            "TRAIN(117): [150/391] Batch: 0.0537 (0.0506) Data: 0.0172 (0.0202) Loss: 0.0979 (0.0577)\n",
            "TRAIN(117): [160/391] Batch: 0.0491 (0.0505) Data: 0.0268 (0.0203) Loss: 0.0292 (0.0576)\n",
            "TRAIN(117): [170/391] Batch: 0.0448 (0.0505) Data: 0.0193 (0.0202) Loss: 0.0575 (0.0582)\n",
            "TRAIN(117): [180/391] Batch: 0.0473 (0.0503) Data: 0.0270 (0.0202) Loss: 0.0134 (0.0582)\n",
            "TRAIN(117): [190/391] Batch: 0.0458 (0.0501) Data: 0.0282 (0.0204) Loss: 0.0802 (0.0581)\n",
            "TRAIN(117): [200/391] Batch: 0.0469 (0.0500) Data: 0.0278 (0.0206) Loss: 0.0297 (0.0576)\n",
            "TRAIN(117): [210/391] Batch: 0.0469 (0.0500) Data: 0.0266 (0.0205) Loss: 0.0600 (0.0577)\n",
            "TRAIN(117): [220/391] Batch: 0.0450 (0.0498) Data: 0.0281 (0.0207) Loss: 0.0274 (0.0576)\n",
            "TRAIN(117): [230/391] Batch: 0.0465 (0.0498) Data: 0.0180 (0.0206) Loss: 0.0406 (0.0577)\n",
            "TRAIN(117): [240/391] Batch: 0.0478 (0.0498) Data: 0.0167 (0.0205) Loss: 0.0970 (0.0572)\n",
            "TRAIN(117): [250/391] Batch: 0.0428 (0.0498) Data: 0.0258 (0.0204) Loss: 0.0697 (0.0568)\n",
            "TRAIN(117): [260/391] Batch: 0.0449 (0.0496) Data: 0.0290 (0.0205) Loss: 0.0736 (0.0563)\n",
            "TRAIN(117): [270/391] Batch: 0.0411 (0.0496) Data: 0.0251 (0.0205) Loss: 0.0547 (0.0563)\n",
            "TRAIN(117): [280/391] Batch: 0.0443 (0.0496) Data: 0.0257 (0.0205) Loss: 0.0932 (0.0564)\n",
            "TRAIN(117): [290/391] Batch: 0.0522 (0.0497) Data: 0.0189 (0.0206) Loss: 0.0224 (0.0562)\n",
            "TRAIN(117): [300/391] Batch: 0.0558 (0.0498) Data: 0.0142 (0.0204) Loss: 0.0654 (0.0563)\n",
            "TRAIN(117): [310/391] Batch: 0.0515 (0.0498) Data: 0.0196 (0.0203) Loss: 0.0681 (0.0560)\n",
            "TRAIN(117): [320/391] Batch: 0.0538 (0.0499) Data: 0.0145 (0.0201) Loss: 0.0274 (0.0560)\n",
            "TRAIN(117): [330/391] Batch: 0.0459 (0.0498) Data: 0.0204 (0.0200) Loss: 0.0754 (0.0558)\n",
            "TRAIN(117): [340/391] Batch: 0.0349 (0.0498) Data: 0.0273 (0.0199) Loss: 0.0538 (0.0556)\n",
            "TRAIN(117): [350/391] Batch: 0.0487 (0.0499) Data: 0.0217 (0.0199) Loss: 0.0640 (0.0555)\n",
            "TRAIN(117): [360/391] Batch: 0.0440 (0.0498) Data: 0.0254 (0.0199) Loss: 0.0393 (0.0553)\n",
            "TRAIN(117): [370/391] Batch: 0.0461 (0.0498) Data: 0.0268 (0.0200) Loss: 0.1089 (0.0555)\n",
            "TRAIN(117): [380/391] Batch: 0.0454 (0.0498) Data: 0.0207 (0.0200) Loss: 0.0238 (0.0553)\n",
            "TRAIN(117): [390/391] Batch: 0.0499 (0.0497) Data: 0.0261 (0.0199) Loss: 0.0193 (0.0548)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(117)         0:00:19         0:00:07         0:00:11          0.0548\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(118): [ 10/391] Batch: 0.0453 (0.0649) Data: 0.0191 (0.0313) Loss: 0.0484 (0.0494)\n",
            "TRAIN(118): [ 20/391] Batch: 0.0513 (0.0557) Data: 0.0289 (0.0277) Loss: 0.0333 (0.0445)\n",
            "TRAIN(118): [ 30/391] Batch: 0.0387 (0.0529) Data: 0.0254 (0.0259) Loss: 0.0525 (0.0514)\n",
            "TRAIN(118): [ 40/391] Batch: 0.0462 (0.0516) Data: 0.0278 (0.0250) Loss: 0.0918 (0.0571)\n",
            "TRAIN(118): [ 50/391] Batch: 0.0353 (0.0509) Data: 0.0261 (0.0243) Loss: 0.0494 (0.0610)\n",
            "TRAIN(118): [ 60/391] Batch: 0.0473 (0.0504) Data: 0.0275 (0.0240) Loss: 0.0267 (0.0603)\n",
            "TRAIN(118): [ 70/391] Batch: 0.0399 (0.0500) Data: 0.0276 (0.0239) Loss: 0.0226 (0.0598)\n",
            "TRAIN(118): [ 80/391] Batch: 0.0492 (0.0502) Data: 0.0181 (0.0233) Loss: 0.0959 (0.0589)\n",
            "TRAIN(118): [ 90/391] Batch: 0.0466 (0.0499) Data: 0.0287 (0.0234) Loss: 0.0380 (0.0590)\n",
            "TRAIN(118): [100/391] Batch: 0.0460 (0.0496) Data: 0.0272 (0.0234) Loss: 0.0383 (0.0579)\n",
            "TRAIN(118): [110/391] Batch: 0.0445 (0.0497) Data: 0.0236 (0.0230) Loss: 0.0556 (0.0572)\n",
            "TRAIN(118): [120/391] Batch: 0.0461 (0.0496) Data: 0.0266 (0.0229) Loss: 0.0547 (0.0585)\n",
            "TRAIN(118): [130/391] Batch: 0.0514 (0.0498) Data: 0.0162 (0.0224) Loss: 0.0767 (0.0591)\n",
            "TRAIN(118): [140/391] Batch: 0.0454 (0.0496) Data: 0.0178 (0.0223) Loss: 0.1003 (0.0598)\n",
            "TRAIN(118): [150/391] Batch: 0.0443 (0.0494) Data: 0.0272 (0.0224) Loss: 0.0074 (0.0596)\n",
            "TRAIN(118): [160/391] Batch: 0.0461 (0.0494) Data: 0.0283 (0.0222) Loss: 0.1636 (0.0601)\n",
            "TRAIN(118): [170/391] Batch: 0.0464 (0.0497) Data: 0.0208 (0.0219) Loss: 0.0143 (0.0603)\n",
            "TRAIN(118): [180/391] Batch: 0.0481 (0.0496) Data: 0.0228 (0.0218) Loss: 0.0710 (0.0602)\n",
            "TRAIN(118): [190/391] Batch: 0.0545 (0.0496) Data: 0.0190 (0.0216) Loss: 0.0780 (0.0601)\n",
            "TRAIN(118): [200/391] Batch: 0.0441 (0.0498) Data: 0.0236 (0.0214) Loss: 0.0642 (0.0601)\n",
            "TRAIN(118): [210/391] Batch: 0.0531 (0.0499) Data: 0.0172 (0.0212) Loss: 0.0491 (0.0604)\n",
            "TRAIN(118): [220/391] Batch: 0.0486 (0.0498) Data: 0.0205 (0.0211) Loss: 0.0387 (0.0594)\n",
            "TRAIN(118): [230/391] Batch: 0.0441 (0.0499) Data: 0.0231 (0.0209) Loss: 0.0405 (0.0599)\n",
            "TRAIN(118): [240/391] Batch: 0.0461 (0.0498) Data: 0.0272 (0.0209) Loss: 0.0632 (0.0592)\n",
            "TRAIN(118): [250/391] Batch: 0.0438 (0.0498) Data: 0.0184 (0.0208) Loss: 0.0654 (0.0589)\n",
            "TRAIN(118): [260/391] Batch: 0.0438 (0.0498) Data: 0.0265 (0.0208) Loss: 0.0269 (0.0587)\n",
            "TRAIN(118): [270/391] Batch: 0.0461 (0.0497) Data: 0.0280 (0.0208) Loss: 0.1391 (0.0591)\n",
            "TRAIN(118): [280/391] Batch: 0.0472 (0.0497) Data: 0.0284 (0.0209) Loss: 0.0412 (0.0593)\n",
            "TRAIN(118): [290/391] Batch: 0.0411 (0.0496) Data: 0.0271 (0.0210) Loss: 0.0741 (0.0588)\n",
            "TRAIN(118): [300/391] Batch: 0.0580 (0.0495) Data: 0.0222 (0.0210) Loss: 0.0831 (0.0588)\n",
            "TRAIN(118): [310/391] Batch: 0.0492 (0.0495) Data: 0.0266 (0.0210) Loss: 0.0668 (0.0584)\n",
            "TRAIN(118): [320/391] Batch: 0.0538 (0.0495) Data: 0.0248 (0.0210) Loss: 0.0371 (0.0577)\n",
            "TRAIN(118): [330/391] Batch: 0.0353 (0.0494) Data: 0.0271 (0.0210) Loss: 0.0383 (0.0574)\n",
            "TRAIN(118): [340/391] Batch: 0.0441 (0.0494) Data: 0.0229 (0.0210) Loss: 0.0767 (0.0569)\n",
            "TRAIN(118): [350/391] Batch: 0.0428 (0.0494) Data: 0.0233 (0.0210) Loss: 0.0474 (0.0564)\n",
            "TRAIN(118): [360/391] Batch: 0.0492 (0.0494) Data: 0.0263 (0.0211) Loss: 0.0462 (0.0562)\n",
            "TRAIN(118): [370/391] Batch: 0.0546 (0.0494) Data: 0.0238 (0.0211) Loss: 0.0176 (0.0560)\n",
            "TRAIN(118): [380/391] Batch: 0.0474 (0.0493) Data: 0.0266 (0.0212) Loss: 0.0701 (0.0554)\n",
            "TRAIN(118): [390/391] Batch: 0.0446 (0.0493) Data: 0.0278 (0.0212) Loss: 0.0275 (0.0549)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(118)         0:00:19         0:00:08         0:00:10          0.0549\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(119): [ 10/391] Batch: 0.0573 (0.0654) Data: 0.0167 (0.0307) Loss: 0.0544 (0.0543)\n",
            "TRAIN(119): [ 20/391] Batch: 0.0458 (0.0567) Data: 0.0192 (0.0257) Loss: 0.0661 (0.0530)\n",
            "TRAIN(119): [ 30/391] Batch: 0.0517 (0.0535) Data: 0.0210 (0.0248) Loss: 0.0701 (0.0535)\n",
            "TRAIN(119): [ 40/391] Batch: 0.0473 (0.0522) Data: 0.0247 (0.0238) Loss: 0.0144 (0.0524)\n",
            "TRAIN(119): [ 50/391] Batch: 0.0475 (0.0517) Data: 0.0215 (0.0222) Loss: 0.0906 (0.0517)\n",
            "TRAIN(119): [ 60/391] Batch: 0.0512 (0.0517) Data: 0.0160 (0.0215) Loss: 0.0837 (0.0535)\n",
            "TRAIN(119): [ 70/391] Batch: 0.0482 (0.0516) Data: 0.0150 (0.0202) Loss: 0.1251 (0.0559)\n",
            "TRAIN(119): [ 80/391] Batch: 0.0550 (0.0516) Data: 0.0163 (0.0199) Loss: 0.0873 (0.0546)\n",
            "TRAIN(119): [ 90/391] Batch: 0.0432 (0.0514) Data: 0.0243 (0.0196) Loss: 0.0738 (0.0540)\n",
            "TRAIN(119): [100/391] Batch: 0.0535 (0.0512) Data: 0.0171 (0.0191) Loss: 0.0289 (0.0528)\n",
            "TRAIN(119): [110/391] Batch: 0.0570 (0.0512) Data: 0.0168 (0.0190) Loss: 0.0208 (0.0526)\n",
            "TRAIN(119): [120/391] Batch: 0.0528 (0.0511) Data: 0.0219 (0.0189) Loss: 0.0467 (0.0517)\n",
            "TRAIN(119): [130/391] Batch: 0.0417 (0.0510) Data: 0.0253 (0.0190) Loss: 0.0469 (0.0516)\n",
            "TRAIN(119): [140/391] Batch: 0.0489 (0.0507) Data: 0.0256 (0.0194) Loss: 0.0718 (0.0524)\n",
            "TRAIN(119): [150/391] Batch: 0.0459 (0.0505) Data: 0.0260 (0.0196) Loss: 0.0288 (0.0527)\n",
            "TRAIN(119): [160/391] Batch: 0.0467 (0.0503) Data: 0.0282 (0.0198) Loss: 0.0478 (0.0526)\n",
            "TRAIN(119): [170/391] Batch: 0.0433 (0.0502) Data: 0.0273 (0.0199) Loss: 0.0241 (0.0518)\n",
            "TRAIN(119): [180/391] Batch: 0.0464 (0.0501) Data: 0.0274 (0.0201) Loss: 0.0398 (0.0520)\n",
            "TRAIN(119): [190/391] Batch: 0.0424 (0.0500) Data: 0.0254 (0.0201) Loss: 0.0355 (0.0515)\n",
            "TRAIN(119): [200/391] Batch: 0.0451 (0.0499) Data: 0.0277 (0.0202) Loss: 0.0387 (0.0513)\n",
            "TRAIN(119): [210/391] Batch: 0.0363 (0.0498) Data: 0.0256 (0.0202) Loss: 0.0537 (0.0511)\n",
            "TRAIN(119): [220/391] Batch: 0.0481 (0.0498) Data: 0.0244 (0.0203) Loss: 0.0414 (0.0507)\n",
            "TRAIN(119): [230/391] Batch: 0.0468 (0.0497) Data: 0.0263 (0.0204) Loss: 0.0276 (0.0503)\n",
            "TRAIN(119): [240/391] Batch: 0.0460 (0.0496) Data: 0.0280 (0.0205) Loss: 0.0365 (0.0497)\n",
            "TRAIN(119): [250/391] Batch: 0.0466 (0.0495) Data: 0.0274 (0.0206) Loss: 0.0251 (0.0495)\n",
            "TRAIN(119): [260/391] Batch: 0.0435 (0.0495) Data: 0.0180 (0.0206) Loss: 0.0369 (0.0493)\n",
            "TRAIN(119): [270/391] Batch: 0.0446 (0.0493) Data: 0.0253 (0.0207) Loss: 0.0503 (0.0494)\n",
            "TRAIN(119): [280/391] Batch: 0.0538 (0.0494) Data: 0.0159 (0.0207) Loss: 0.0759 (0.0497)\n",
            "TRAIN(119): [290/391] Batch: 0.0476 (0.0494) Data: 0.0170 (0.0207) Loss: 0.0559 (0.0497)\n",
            "TRAIN(119): [300/391] Batch: 0.0501 (0.0494) Data: 0.0183 (0.0206) Loss: 0.0487 (0.0494)\n",
            "TRAIN(119): [310/391] Batch: 0.0396 (0.0494) Data: 0.0250 (0.0206) Loss: 0.0616 (0.0497)\n",
            "TRAIN(119): [320/391] Batch: 0.0528 (0.0494) Data: 0.0163 (0.0205) Loss: 0.0522 (0.0495)\n",
            "TRAIN(119): [330/391] Batch: 0.0440 (0.0495) Data: 0.0222 (0.0205) Loss: 0.0272 (0.0492)\n",
            "TRAIN(119): [340/391] Batch: 0.0619 (0.0495) Data: 0.0153 (0.0204) Loss: 0.0947 (0.0495)\n",
            "TRAIN(119): [350/391] Batch: 0.0683 (0.0496) Data: 0.0124 (0.0203) Loss: 0.0846 (0.0497)\n",
            "TRAIN(119): [360/391] Batch: 0.0494 (0.0496) Data: 0.0186 (0.0201) Loss: 0.0228 (0.0494)\n",
            "TRAIN(119): [370/391] Batch: 0.0534 (0.0497) Data: 0.0182 (0.0200) Loss: 0.0269 (0.0493)\n",
            "TRAIN(119): [380/391] Batch: 0.0497 (0.0498) Data: 0.0233 (0.0199) Loss: 0.0436 (0.0493)\n",
            "TRAIN(119): [390/391] Batch: 0.0438 (0.0497) Data: 0.0281 (0.0200) Loss: 0.0233 (0.0493)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(119)         0:00:19         0:00:07         0:00:11          0.0493\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(120): [ 10/391] Batch: 0.0410 (0.0642) Data: 0.0260 (0.0312) Loss: 0.0899 (0.0523)\n",
            "TRAIN(120): [ 20/391] Batch: 0.0510 (0.0561) Data: 0.0280 (0.0275) Loss: 0.0520 (0.0503)\n",
            "TRAIN(120): [ 30/391] Batch: 0.0496 (0.0532) Data: 0.0260 (0.0261) Loss: 0.0470 (0.0492)\n",
            "TRAIN(120): [ 40/391] Batch: 0.0464 (0.0520) Data: 0.0280 (0.0249) Loss: 0.0408 (0.0474)\n",
            "TRAIN(120): [ 50/391] Batch: 0.0455 (0.0513) Data: 0.0269 (0.0247) Loss: 0.0316 (0.0453)\n",
            "TRAIN(120): [ 60/391] Batch: 0.0487 (0.0510) Data: 0.0227 (0.0236) Loss: 0.0269 (0.0451)\n",
            "TRAIN(120): [ 70/391] Batch: 0.0466 (0.0507) Data: 0.0259 (0.0234) Loss: 0.0281 (0.0439)\n",
            "TRAIN(120): [ 80/391] Batch: 0.0430 (0.0505) Data: 0.0268 (0.0231) Loss: 0.0628 (0.0449)\n",
            "TRAIN(120): [ 90/391] Batch: 0.0467 (0.0503) Data: 0.0289 (0.0229) Loss: 0.0730 (0.0452)\n",
            "TRAIN(120): [100/391] Batch: 0.0460 (0.0500) Data: 0.0279 (0.0231) Loss: 0.0919 (0.0455)\n",
            "TRAIN(120): [110/391] Batch: 0.0478 (0.0497) Data: 0.0263 (0.0231) Loss: 0.0188 (0.0451)\n",
            "TRAIN(120): [120/391] Batch: 0.0458 (0.0495) Data: 0.0270 (0.0231) Loss: 0.0415 (0.0442)\n",
            "TRAIN(120): [130/391] Batch: 0.0519 (0.0494) Data: 0.0163 (0.0229) Loss: 0.0572 (0.0445)\n",
            "TRAIN(120): [140/391] Batch: 0.0511 (0.0494) Data: 0.0191 (0.0227) Loss: 0.0261 (0.0442)\n",
            "TRAIN(120): [150/391] Batch: 0.0497 (0.0495) Data: 0.0187 (0.0223) Loss: 0.0349 (0.0438)\n",
            "TRAIN(120): [160/391] Batch: 0.0429 (0.0493) Data: 0.0245 (0.0223) Loss: 0.0346 (0.0440)\n",
            "TRAIN(120): [170/391] Batch: 0.0522 (0.0494) Data: 0.0166 (0.0221) Loss: 0.0454 (0.0446)\n",
            "TRAIN(120): [180/391] Batch: 0.0434 (0.0492) Data: 0.0208 (0.0220) Loss: 0.0814 (0.0442)\n",
            "TRAIN(120): [190/391] Batch: 0.0561 (0.0492) Data: 0.0229 (0.0221) Loss: 0.0542 (0.0445)\n",
            "TRAIN(120): [200/391] Batch: 0.0525 (0.0493) Data: 0.0198 (0.0218) Loss: 0.0098 (0.0441)\n",
            "TRAIN(120): [210/391] Batch: 0.0461 (0.0493) Data: 0.0212 (0.0216) Loss: 0.0447 (0.0439)\n",
            "TRAIN(120): [220/391] Batch: 0.0475 (0.0494) Data: 0.0191 (0.0214) Loss: 0.0192 (0.0437)\n",
            "TRAIN(120): [230/391] Batch: 0.0524 (0.0495) Data: 0.0193 (0.0212) Loss: 0.0293 (0.0434)\n",
            "TRAIN(120): [240/391] Batch: 0.0486 (0.0495) Data: 0.0205 (0.0211) Loss: 0.0388 (0.0432)\n",
            "TRAIN(120): [250/391] Batch: 0.0585 (0.0496) Data: 0.0128 (0.0210) Loss: 0.0411 (0.0426)\n",
            "TRAIN(120): [260/391] Batch: 0.0477 (0.0496) Data: 0.0210 (0.0208) Loss: 0.0158 (0.0426)\n",
            "TRAIN(120): [270/391] Batch: 0.0417 (0.0495) Data: 0.0192 (0.0207) Loss: 0.0171 (0.0427)\n",
            "TRAIN(120): [280/391] Batch: 0.0524 (0.0495) Data: 0.0227 (0.0208) Loss: 0.0153 (0.0426)\n",
            "TRAIN(120): [290/391] Batch: 0.0552 (0.0495) Data: 0.0241 (0.0209) Loss: 0.0630 (0.0431)\n",
            "TRAIN(120): [300/391] Batch: 0.0449 (0.0495) Data: 0.0185 (0.0209) Loss: 0.0287 (0.0429)\n",
            "TRAIN(120): [310/391] Batch: 0.0423 (0.0494) Data: 0.0269 (0.0210) Loss: 0.0242 (0.0433)\n",
            "TRAIN(120): [320/391] Batch: 0.0517 (0.0494) Data: 0.0256 (0.0210) Loss: 0.0926 (0.0432)\n",
            "TRAIN(120): [330/391] Batch: 0.0516 (0.0493) Data: 0.0220 (0.0210) Loss: 0.0799 (0.0433)\n",
            "TRAIN(120): [340/391] Batch: 0.0403 (0.0493) Data: 0.0252 (0.0210) Loss: 0.0587 (0.0431)\n",
            "TRAIN(120): [350/391] Batch: 0.0414 (0.0493) Data: 0.0258 (0.0210) Loss: 0.0486 (0.0431)\n",
            "TRAIN(120): [360/391] Batch: 0.0465 (0.0493) Data: 0.0259 (0.0210) Loss: 0.0450 (0.0432)\n",
            "TRAIN(120): [370/391] Batch: 0.0503 (0.0493) Data: 0.0170 (0.0209) Loss: 0.1088 (0.0433)\n",
            "TRAIN(120): [380/391] Batch: 0.0537 (0.0493) Data: 0.0234 (0.0209) Loss: 0.0220 (0.0436)\n",
            "TRAIN(120): [390/391] Batch: 0.0462 (0.0493) Data: 0.0283 (0.0210) Loss: 0.0356 (0.0433)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(120)         0:00:19         0:00:08         0:00:11          0.0433\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(121): [ 10/391] Batch: 0.0519 (0.0653) Data: 0.0252 (0.0318) Loss: 0.0249 (0.0405)\n",
            "TRAIN(121): [ 20/391] Batch: 0.0496 (0.0564) Data: 0.0291 (0.0282) Loss: 0.0479 (0.0347)\n",
            "TRAIN(121): [ 30/391] Batch: 0.0459 (0.0536) Data: 0.0268 (0.0261) Loss: 0.0333 (0.0318)\n",
            "TRAIN(121): [ 40/391] Batch: 0.0494 (0.0522) Data: 0.0232 (0.0250) Loss: 0.0539 (0.0332)\n",
            "TRAIN(121): [ 50/391] Batch: 0.0487 (0.0516) Data: 0.0253 (0.0241) Loss: 0.0739 (0.0321)\n",
            "TRAIN(121): [ 60/391] Batch: 0.0477 (0.0508) Data: 0.0288 (0.0241) Loss: 0.0414 (0.0334)\n",
            "TRAIN(121): [ 70/391] Batch: 0.0541 (0.0508) Data: 0.0155 (0.0234) Loss: 0.0625 (0.0356)\n",
            "TRAIN(121): [ 80/391] Batch: 0.0513 (0.0507) Data: 0.0177 (0.0227) Loss: 0.0148 (0.0355)\n",
            "TRAIN(121): [ 90/391] Batch: 0.0485 (0.0505) Data: 0.0212 (0.0223) Loss: 0.0249 (0.0384)\n",
            "TRAIN(121): [100/391] Batch: 0.0586 (0.0506) Data: 0.0182 (0.0218) Loss: 0.0260 (0.0383)\n",
            "TRAIN(121): [110/391] Batch: 0.0518 (0.0507) Data: 0.0186 (0.0214) Loss: 0.0492 (0.0383)\n",
            "TRAIN(121): [120/391] Batch: 0.0478 (0.0506) Data: 0.0160 (0.0210) Loss: 0.0148 (0.0384)\n",
            "TRAIN(121): [130/391] Batch: 0.0534 (0.0505) Data: 0.0157 (0.0206) Loss: 0.0281 (0.0386)\n",
            "TRAIN(121): [140/391] Batch: 0.0433 (0.0505) Data: 0.0256 (0.0204) Loss: 0.0292 (0.0390)\n",
            "TRAIN(121): [150/391] Batch: 0.0477 (0.0504) Data: 0.0255 (0.0205) Loss: 0.0130 (0.0402)\n",
            "TRAIN(121): [160/391] Batch: 0.0531 (0.0503) Data: 0.0243 (0.0205) Loss: 0.0386 (0.0413)\n",
            "TRAIN(121): [170/391] Batch: 0.0494 (0.0502) Data: 0.0245 (0.0206) Loss: 0.0229 (0.0413)\n",
            "TRAIN(121): [180/391] Batch: 0.0433 (0.0500) Data: 0.0253 (0.0207) Loss: 0.0266 (0.0418)\n",
            "TRAIN(121): [190/391] Batch: 0.0468 (0.0499) Data: 0.0275 (0.0208) Loss: 0.0161 (0.0420)\n",
            "TRAIN(121): [200/391] Batch: 0.0497 (0.0499) Data: 0.0178 (0.0208) Loss: 0.0688 (0.0424)\n",
            "TRAIN(121): [210/391] Batch: 0.0476 (0.0499) Data: 0.0152 (0.0206) Loss: 0.0574 (0.0429)\n",
            "TRAIN(121): [220/391] Batch: 0.0508 (0.0498) Data: 0.0238 (0.0207) Loss: 0.0414 (0.0426)\n",
            "TRAIN(121): [230/391] Batch: 0.0577 (0.0498) Data: 0.0181 (0.0208) Loss: 0.0176 (0.0427)\n",
            "TRAIN(121): [240/391] Batch: 0.0470 (0.0497) Data: 0.0231 (0.0207) Loss: 0.1138 (0.0427)\n",
            "TRAIN(121): [250/391] Batch: 0.0433 (0.0497) Data: 0.0264 (0.0208) Loss: 0.0680 (0.0428)\n",
            "TRAIN(121): [260/391] Batch: 0.0488 (0.0496) Data: 0.0198 (0.0208) Loss: 0.0478 (0.0428)\n",
            "TRAIN(121): [270/391] Batch: 0.0538 (0.0496) Data: 0.0239 (0.0209) Loss: 0.0833 (0.0429)\n",
            "TRAIN(121): [280/391] Batch: 0.0462 (0.0495) Data: 0.0280 (0.0209) Loss: 0.0292 (0.0433)\n",
            "TRAIN(121): [290/391] Batch: 0.0479 (0.0496) Data: 0.0169 (0.0210) Loss: 0.0416 (0.0433)\n",
            "TRAIN(121): [300/391] Batch: 0.0503 (0.0495) Data: 0.0181 (0.0210) Loss: 0.0897 (0.0435)\n",
            "TRAIN(121): [310/391] Batch: 0.0427 (0.0495) Data: 0.0256 (0.0209) Loss: 0.0405 (0.0434)\n",
            "TRAIN(121): [320/391] Batch: 0.0459 (0.0494) Data: 0.0264 (0.0210) Loss: 0.0547 (0.0437)\n",
            "TRAIN(121): [330/391] Batch: 0.0467 (0.0494) Data: 0.0262 (0.0210) Loss: 0.0266 (0.0441)\n",
            "TRAIN(121): [340/391] Batch: 0.0457 (0.0495) Data: 0.0185 (0.0209) Loss: 0.0273 (0.0441)\n",
            "TRAIN(121): [350/391] Batch: 0.0558 (0.0495) Data: 0.0193 (0.0208) Loss: 0.0725 (0.0443)\n",
            "TRAIN(121): [360/391] Batch: 0.0462 (0.0495) Data: 0.0193 (0.0207) Loss: 0.1544 (0.0443)\n",
            "TRAIN(121): [370/391] Batch: 0.0510 (0.0495) Data: 0.0218 (0.0206) Loss: 0.0925 (0.0447)\n",
            "TRAIN(121): [380/391] Batch: 0.0524 (0.0495) Data: 0.0164 (0.0204) Loss: 0.0367 (0.0450)\n",
            "TRAIN(121): [390/391] Batch: 0.0473 (0.0495) Data: 0.0274 (0.0205) Loss: 0.0160 (0.0450)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(121)         0:00:19         0:00:07         0:00:11          0.0450\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(122): [ 10/391] Batch: 0.0367 (0.0685) Data: 0.0257 (0.0320) Loss: 0.0983 (0.0456)\n",
            "TRAIN(122): [ 20/391] Batch: 0.0417 (0.0592) Data: 0.0266 (0.0272) Loss: 0.0600 (0.0457)\n",
            "TRAIN(122): [ 30/391] Batch: 0.0492 (0.0557) Data: 0.0193 (0.0256) Loss: 0.0148 (0.0435)\n",
            "TRAIN(122): [ 40/391] Batch: 0.0493 (0.0542) Data: 0.0221 (0.0239) Loss: 0.0452 (0.0423)\n",
            "TRAIN(122): [ 50/391] Batch: 0.0570 (0.0531) Data: 0.0188 (0.0233) Loss: 0.0248 (0.0423)\n",
            "TRAIN(122): [ 60/391] Batch: 0.0420 (0.0521) Data: 0.0261 (0.0232) Loss: 0.0678 (0.0429)\n",
            "TRAIN(122): [ 70/391] Batch: 0.0448 (0.0517) Data: 0.0267 (0.0232) Loss: 0.0788 (0.0446)\n",
            "TRAIN(122): [ 80/391] Batch: 0.0449 (0.0513) Data: 0.0263 (0.0232) Loss: 0.0532 (0.0444)\n",
            "TRAIN(122): [ 90/391] Batch: 0.0535 (0.0511) Data: 0.0170 (0.0230) Loss: 0.0225 (0.0442)\n",
            "TRAIN(122): [100/391] Batch: 0.0515 (0.0511) Data: 0.0235 (0.0226) Loss: 0.0378 (0.0448)\n",
            "TRAIN(122): [110/391] Batch: 0.0497 (0.0510) Data: 0.0186 (0.0224) Loss: 0.0675 (0.0449)\n",
            "TRAIN(122): [120/391] Batch: 0.0472 (0.0508) Data: 0.0274 (0.0222) Loss: 0.0509 (0.0444)\n",
            "TRAIN(122): [130/391] Batch: 0.0463 (0.0506) Data: 0.0228 (0.0222) Loss: 0.0351 (0.0445)\n",
            "TRAIN(122): [140/391] Batch: 0.0467 (0.0506) Data: 0.0183 (0.0218) Loss: 0.0268 (0.0449)\n",
            "TRAIN(122): [150/391] Batch: 0.0473 (0.0505) Data: 0.0271 (0.0217) Loss: 0.0719 (0.0444)\n",
            "TRAIN(122): [160/391] Batch: 0.0465 (0.0503) Data: 0.0273 (0.0219) Loss: 0.0209 (0.0443)\n",
            "TRAIN(122): [170/391] Batch: 0.0461 (0.0502) Data: 0.0279 (0.0219) Loss: 0.0449 (0.0434)\n",
            "TRAIN(122): [180/391] Batch: 0.0481 (0.0500) Data: 0.0256 (0.0220) Loss: 0.0243 (0.0428)\n",
            "TRAIN(122): [190/391] Batch: 0.0482 (0.0499) Data: 0.0248 (0.0221) Loss: 0.0156 (0.0425)\n",
            "TRAIN(122): [200/391] Batch: 0.0470 (0.0497) Data: 0.0277 (0.0221) Loss: 0.0221 (0.0424)\n",
            "TRAIN(122): [210/391] Batch: 0.0465 (0.0496) Data: 0.0267 (0.0222) Loss: 0.0610 (0.0424)\n",
            "TRAIN(122): [220/391] Batch: 0.0507 (0.0497) Data: 0.0209 (0.0220) Loss: 0.0167 (0.0420)\n",
            "TRAIN(122): [230/391] Batch: 0.0505 (0.0497) Data: 0.0222 (0.0218) Loss: 0.0533 (0.0423)\n",
            "TRAIN(122): [240/391] Batch: 0.0379 (0.0497) Data: 0.0242 (0.0217) Loss: 0.0137 (0.0420)\n",
            "TRAIN(122): [250/391] Batch: 0.0526 (0.0498) Data: 0.0191 (0.0216) Loss: 0.0505 (0.0421)\n",
            "TRAIN(122): [260/391] Batch: 0.0476 (0.0498) Data: 0.0191 (0.0215) Loss: 0.0265 (0.0419)\n",
            "TRAIN(122): [270/391] Batch: 0.0446 (0.0498) Data: 0.0227 (0.0213) Loss: 0.0150 (0.0415)\n",
            "TRAIN(122): [280/391] Batch: 0.0474 (0.0498) Data: 0.0269 (0.0213) Loss: 0.0138 (0.0414)\n",
            "TRAIN(122): [290/391] Batch: 0.0582 (0.0498) Data: 0.0222 (0.0214) Loss: 0.1014 (0.0415)\n",
            "TRAIN(122): [300/391] Batch: 0.0542 (0.0498) Data: 0.0155 (0.0213) Loss: 0.0077 (0.0415)\n",
            "TRAIN(122): [310/391] Batch: 0.0490 (0.0498) Data: 0.0168 (0.0212) Loss: 0.0580 (0.0416)\n",
            "TRAIN(122): [320/391] Batch: 0.0456 (0.0497) Data: 0.0285 (0.0212) Loss: 0.0545 (0.0414)\n",
            "TRAIN(122): [330/391] Batch: 0.0534 (0.0496) Data: 0.0159 (0.0212) Loss: 0.0089 (0.0414)\n",
            "TRAIN(122): [340/391] Batch: 0.0486 (0.0496) Data: 0.0268 (0.0212) Loss: 0.0698 (0.0420)\n",
            "TRAIN(122): [350/391] Batch: 0.0580 (0.0496) Data: 0.0225 (0.0212) Loss: 0.0508 (0.0420)\n",
            "TRAIN(122): [360/391] Batch: 0.0531 (0.0496) Data: 0.0224 (0.0212) Loss: 0.0388 (0.0419)\n",
            "TRAIN(122): [370/391] Batch: 0.0478 (0.0495) Data: 0.0277 (0.0212) Loss: 0.0878 (0.0421)\n",
            "TRAIN(122): [380/391] Batch: 0.0467 (0.0495) Data: 0.0270 (0.0213) Loss: 0.0333 (0.0420)\n",
            "TRAIN(122): [390/391] Batch: 0.0481 (0.0495) Data: 0.0285 (0.0212) Loss: 0.1169 (0.0422)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(122)         0:00:19         0:00:08         0:00:11          0.0422\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(123): [ 10/391] Batch: 0.0454 (0.0609) Data: 0.0270 (0.0386) Loss: 0.0527 (0.0364)\n",
            "TRAIN(123): [ 20/391] Batch: 0.0504 (0.0539) Data: 0.0334 (0.0315) Loss: 0.0435 (0.0456)\n",
            "TRAIN(123): [ 30/391] Batch: 0.0563 (0.0527) Data: 0.0190 (0.0280) Loss: 0.0557 (0.0474)\n",
            "TRAIN(123): [ 40/391] Batch: 0.0477 (0.0517) Data: 0.0220 (0.0259) Loss: 0.0596 (0.0468)\n",
            "TRAIN(123): [ 50/391] Batch: 0.0367 (0.0510) Data: 0.0274 (0.0245) Loss: 0.0289 (0.0455)\n",
            "TRAIN(123): [ 60/391] Batch: 0.0483 (0.0504) Data: 0.0256 (0.0243) Loss: 0.0540 (0.0452)\n",
            "TRAIN(123): [ 70/391] Batch: 0.0394 (0.0499) Data: 0.0270 (0.0240) Loss: 0.0826 (0.0452)\n",
            "TRAIN(123): [ 80/391] Batch: 0.0464 (0.0495) Data: 0.0279 (0.0240) Loss: 0.0196 (0.0441)\n",
            "TRAIN(123): [ 90/391] Batch: 0.0555 (0.0496) Data: 0.0145 (0.0237) Loss: 0.0770 (0.0432)\n",
            "TRAIN(123): [100/391] Batch: 0.0460 (0.0495) Data: 0.0211 (0.0231) Loss: 0.1125 (0.0429)\n",
            "TRAIN(123): [110/391] Batch: 0.0474 (0.0496) Data: 0.0221 (0.0228) Loss: 0.0713 (0.0423)\n",
            "TRAIN(123): [120/391] Batch: 0.0524 (0.0498) Data: 0.0193 (0.0223) Loss: 0.0986 (0.0420)\n",
            "TRAIN(123): [130/391] Batch: 0.0530 (0.0499) Data: 0.0199 (0.0219) Loss: 0.0460 (0.0415)\n",
            "TRAIN(123): [140/391] Batch: 0.0480 (0.0499) Data: 0.0207 (0.0216) Loss: 0.0348 (0.0410)\n",
            "TRAIN(123): [150/391] Batch: 0.0447 (0.0501) Data: 0.0204 (0.0213) Loss: 0.0086 (0.0402)\n",
            "TRAIN(123): [160/391] Batch: 0.0551 (0.0500) Data: 0.0247 (0.0213) Loss: 0.0798 (0.0415)\n",
            "TRAIN(123): [170/391] Batch: 0.0457 (0.0500) Data: 0.0279 (0.0213) Loss: 0.0334 (0.0412)\n",
            "TRAIN(123): [180/391] Batch: 0.0501 (0.0499) Data: 0.0171 (0.0212) Loss: 0.0215 (0.0422)\n",
            "TRAIN(123): [190/391] Batch: 0.0469 (0.0498) Data: 0.0286 (0.0212) Loss: 0.0612 (0.0424)\n",
            "TRAIN(123): [200/391] Batch: 0.0475 (0.0497) Data: 0.0279 (0.0214) Loss: 0.0227 (0.0423)\n",
            "TRAIN(123): [210/391] Batch: 0.0457 (0.0495) Data: 0.0266 (0.0215) Loss: 0.0438 (0.0429)\n",
            "TRAIN(123): [220/391] Batch: 0.0472 (0.0494) Data: 0.0278 (0.0216) Loss: 0.0989 (0.0433)\n",
            "TRAIN(123): [230/391] Batch: 0.0496 (0.0493) Data: 0.0237 (0.0216) Loss: 0.1531 (0.0453)\n",
            "TRAIN(123): [240/391] Batch: 0.0495 (0.0493) Data: 0.0251 (0.0217) Loss: 0.0508 (0.0460)\n",
            "TRAIN(123): [250/391] Batch: 0.0363 (0.0492) Data: 0.0263 (0.0216) Loss: 0.1052 (0.0461)\n",
            "TRAIN(123): [260/391] Batch: 0.0387 (0.0493) Data: 0.0254 (0.0215) Loss: 0.0711 (0.0464)\n",
            "TRAIN(123): [270/391] Batch: 0.0459 (0.0492) Data: 0.0282 (0.0216) Loss: 0.0463 (0.0468)\n",
            "TRAIN(123): [280/391] Batch: 0.0465 (0.0491) Data: 0.0270 (0.0216) Loss: 0.0477 (0.0475)\n",
            "TRAIN(123): [290/391] Batch: 0.0489 (0.0491) Data: 0.0268 (0.0217) Loss: 0.0849 (0.0478)\n",
            "TRAIN(123): [300/391] Batch: 0.0504 (0.0491) Data: 0.0238 (0.0216) Loss: 0.0129 (0.0477)\n",
            "TRAIN(123): [310/391] Batch: 0.0475 (0.0491) Data: 0.0244 (0.0216) Loss: 0.0736 (0.0479)\n",
            "TRAIN(123): [320/391] Batch: 0.0450 (0.0490) Data: 0.0270 (0.0217) Loss: 0.0404 (0.0478)\n",
            "TRAIN(123): [330/391] Batch: 0.0487 (0.0490) Data: 0.0266 (0.0217) Loss: 0.0619 (0.0480)\n",
            "TRAIN(123): [340/391] Batch: 0.0460 (0.0490) Data: 0.0269 (0.0218) Loss: 0.0741 (0.0481)\n",
            "TRAIN(123): [350/391] Batch: 0.0624 (0.0490) Data: 0.0141 (0.0217) Loss: 0.0497 (0.0480)\n",
            "TRAIN(123): [360/391] Batch: 0.0561 (0.0490) Data: 0.0206 (0.0217) Loss: 0.1049 (0.0485)\n",
            "TRAIN(123): [370/391] Batch: 0.0487 (0.0491) Data: 0.0169 (0.0215) Loss: 0.0497 (0.0488)\n",
            "TRAIN(123): [380/391] Batch: 0.0478 (0.0490) Data: 0.0211 (0.0214) Loss: 0.0389 (0.0487)\n",
            "TRAIN(123): [390/391] Batch: 0.0443 (0.0490) Data: 0.0241 (0.0214) Loss: 0.0981 (0.0489)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(123)         0:00:19         0:00:08         0:00:10          0.0489\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(124): [ 10/391] Batch: 0.0451 (0.0680) Data: 0.0154 (0.0291) Loss: 0.0348 (0.0403)\n",
            "TRAIN(124): [ 20/391] Batch: 0.0504 (0.0597) Data: 0.0187 (0.0227) Loss: 0.0555 (0.0454)\n",
            "TRAIN(124): [ 30/391] Batch: 0.0436 (0.0563) Data: 0.0253 (0.0209) Loss: 0.0304 (0.0502)\n",
            "TRAIN(124): [ 40/391] Batch: 0.0448 (0.0545) Data: 0.0223 (0.0210) Loss: 0.0281 (0.0493)\n",
            "TRAIN(124): [ 50/391] Batch: 0.0361 (0.0533) Data: 0.0266 (0.0204) Loss: 0.0270 (0.0515)\n",
            "TRAIN(124): [ 60/391] Batch: 0.0542 (0.0528) Data: 0.0155 (0.0199) Loss: 0.0851 (0.0524)\n",
            "TRAIN(124): [ 70/391] Batch: 0.0458 (0.0521) Data: 0.0264 (0.0201) Loss: 0.0250 (0.0527)\n",
            "TRAIN(124): [ 80/391] Batch: 0.0444 (0.0516) Data: 0.0271 (0.0204) Loss: 0.0315 (0.0522)\n",
            "TRAIN(124): [ 90/391] Batch: 0.0432 (0.0513) Data: 0.0256 (0.0205) Loss: 0.0572 (0.0519)\n",
            "TRAIN(124): [100/391] Batch: 0.0487 (0.0511) Data: 0.0264 (0.0207) Loss: 0.0483 (0.0510)\n",
            "TRAIN(124): [110/391] Batch: 0.0452 (0.0507) Data: 0.0282 (0.0209) Loss: 0.0297 (0.0492)\n",
            "TRAIN(124): [120/391] Batch: 0.0501 (0.0506) Data: 0.0265 (0.0209) Loss: 0.0179 (0.0483)\n",
            "TRAIN(124): [130/391] Batch: 0.0465 (0.0506) Data: 0.0180 (0.0206) Loss: 0.0492 (0.0476)\n",
            "TRAIN(124): [140/391] Batch: 0.0355 (0.0505) Data: 0.0276 (0.0204) Loss: 0.0560 (0.0469)\n",
            "TRAIN(124): [150/391] Batch: 0.0581 (0.0504) Data: 0.0158 (0.0206) Loss: 0.0283 (0.0457)\n",
            "TRAIN(124): [160/391] Batch: 0.0384 (0.0503) Data: 0.0267 (0.0205) Loss: 0.0610 (0.0460)\n",
            "TRAIN(124): [170/391] Batch: 0.0464 (0.0503) Data: 0.0224 (0.0204) Loss: 0.0122 (0.0457)\n",
            "TRAIN(124): [180/391] Batch: 0.0423 (0.0501) Data: 0.0254 (0.0205) Loss: 0.0462 (0.0454)\n",
            "TRAIN(124): [190/391] Batch: 0.0512 (0.0500) Data: 0.0247 (0.0206) Loss: 0.0332 (0.0452)\n",
            "TRAIN(124): [200/391] Batch: 0.0493 (0.0499) Data: 0.0245 (0.0207) Loss: 0.0665 (0.0467)\n",
            "TRAIN(124): [210/391] Batch: 0.0482 (0.0497) Data: 0.0279 (0.0209) Loss: 0.0217 (0.0463)\n",
            "TRAIN(124): [220/391] Batch: 0.0521 (0.0498) Data: 0.0180 (0.0208) Loss: 0.0526 (0.0465)\n",
            "TRAIN(124): [230/391] Batch: 0.0584 (0.0498) Data: 0.0216 (0.0208) Loss: 0.0708 (0.0466)\n",
            "TRAIN(124): [240/391] Batch: 0.0481 (0.0498) Data: 0.0157 (0.0205) Loss: 0.0510 (0.0466)\n",
            "TRAIN(124): [250/391] Batch: 0.0790 (0.0499) Data: 0.0076 (0.0202) Loss: 0.0403 (0.0467)\n",
            "TRAIN(124): [260/391] Batch: 0.0535 (0.0499) Data: 0.0187 (0.0201) Loss: 0.0418 (0.0468)\n",
            "TRAIN(124): [270/391] Batch: 0.0395 (0.0499) Data: 0.0267 (0.0201) Loss: 0.0158 (0.0472)\n",
            "TRAIN(124): [280/391] Batch: 0.0477 (0.0500) Data: 0.0149 (0.0200) Loss: 0.0463 (0.0473)\n",
            "TRAIN(124): [290/391] Batch: 0.0457 (0.0500) Data: 0.0198 (0.0198) Loss: 0.1545 (0.0474)\n",
            "TRAIN(124): [300/391] Batch: 0.0473 (0.0499) Data: 0.0269 (0.0198) Loss: 0.0071 (0.0471)\n",
            "TRAIN(124): [310/391] Batch: 0.0467 (0.0499) Data: 0.0267 (0.0199) Loss: 0.0864 (0.0469)\n",
            "TRAIN(124): [320/391] Batch: 0.0446 (0.0498) Data: 0.0273 (0.0200) Loss: 0.0454 (0.0468)\n",
            "TRAIN(124): [330/391] Batch: 0.0566 (0.0498) Data: 0.0236 (0.0201) Loss: 0.0527 (0.0471)\n",
            "TRAIN(124): [340/391] Batch: 0.0519 (0.0498) Data: 0.0189 (0.0201) Loss: 0.0510 (0.0470)\n",
            "TRAIN(124): [350/391] Batch: 0.0532 (0.0497) Data: 0.0198 (0.0201) Loss: 0.0531 (0.0473)\n",
            "TRAIN(124): [360/391] Batch: 0.0551 (0.0497) Data: 0.0169 (0.0201) Loss: 0.0539 (0.0471)\n",
            "TRAIN(124): [370/391] Batch: 0.0466 (0.0497) Data: 0.0178 (0.0200) Loss: 0.1037 (0.0475)\n",
            "TRAIN(124): [380/391] Batch: 0.0456 (0.0497) Data: 0.0172 (0.0199) Loss: 0.0448 (0.0479)\n",
            "TRAIN(124): [390/391] Batch: 0.0457 (0.0496) Data: 0.0292 (0.0199) Loss: 0.1048 (0.0486)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(124)         0:00:19         0:00:07         0:00:11          0.0486\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(125): [ 10/391] Batch: 0.0543 (0.0649) Data: 0.0168 (0.0289) Loss: 0.0289 (0.0538)\n",
            "TRAIN(125): [ 20/391] Batch: 0.0541 (0.0574) Data: 0.0178 (0.0242) Loss: 0.0549 (0.0491)\n",
            "TRAIN(125): [ 30/391] Batch: 0.0448 (0.0541) Data: 0.0278 (0.0232) Loss: 0.0464 (0.0523)\n",
            "TRAIN(125): [ 40/391] Batch: 0.0506 (0.0527) Data: 0.0265 (0.0232) Loss: 0.0258 (0.0500)\n",
            "TRAIN(125): [ 50/391] Batch: 0.0461 (0.0517) Data: 0.0265 (0.0232) Loss: 0.0443 (0.0493)\n",
            "TRAIN(125): [ 60/391] Batch: 0.0514 (0.0516) Data: 0.0178 (0.0221) Loss: 0.0543 (0.0500)\n",
            "TRAIN(125): [ 70/391] Batch: 0.0434 (0.0512) Data: 0.0267 (0.0218) Loss: 0.0380 (0.0516)\n",
            "TRAIN(125): [ 80/391] Batch: 0.0507 (0.0510) Data: 0.0259 (0.0217) Loss: 0.0530 (0.0508)\n",
            "TRAIN(125): [ 90/391] Batch: 0.0454 (0.0506) Data: 0.0253 (0.0219) Loss: 0.0308 (0.0505)\n",
            "TRAIN(125): [100/391] Batch: 0.0453 (0.0505) Data: 0.0269 (0.0219) Loss: 0.1108 (0.0491)\n",
            "TRAIN(125): [110/391] Batch: 0.0512 (0.0504) Data: 0.0202 (0.0217) Loss: 0.0526 (0.0482)\n",
            "TRAIN(125): [120/391] Batch: 0.0483 (0.0505) Data: 0.0160 (0.0211) Loss: 0.0308 (0.0487)\n",
            "TRAIN(125): [130/391] Batch: 0.0499 (0.0505) Data: 0.0147 (0.0205) Loss: 0.0550 (0.0483)\n",
            "TRAIN(125): [140/391] Batch: 0.0472 (0.0506) Data: 0.0201 (0.0203) Loss: 0.0626 (0.0487)\n",
            "TRAIN(125): [150/391] Batch: 0.0562 (0.0507) Data: 0.0189 (0.0202) Loss: 0.0327 (0.0480)\n",
            "TRAIN(125): [160/391] Batch: 0.0514 (0.0507) Data: 0.0197 (0.0200) Loss: 0.0315 (0.0475)\n",
            "TRAIN(125): [170/391] Batch: 0.0537 (0.0508) Data: 0.0181 (0.0199) Loss: 0.0189 (0.0476)\n",
            "TRAIN(125): [180/391] Batch: 0.0456 (0.0506) Data: 0.0279 (0.0201) Loss: 0.0427 (0.0469)\n",
            "TRAIN(125): [190/391] Batch: 0.0407 (0.0506) Data: 0.0240 (0.0201) Loss: 0.1078 (0.0471)\n",
            "TRAIN(125): [200/391] Batch: 0.0516 (0.0505) Data: 0.0248 (0.0200) Loss: 0.0351 (0.0468)\n",
            "TRAIN(125): [210/391] Batch: 0.0539 (0.0504) Data: 0.0247 (0.0202) Loss: 0.0333 (0.0465)\n",
            "TRAIN(125): [220/391] Batch: 0.0468 (0.0502) Data: 0.0271 (0.0203) Loss: 0.0303 (0.0461)\n",
            "TRAIN(125): [230/391] Batch: 0.0565 (0.0501) Data: 0.0246 (0.0205) Loss: 0.0304 (0.0459)\n",
            "TRAIN(125): [240/391] Batch: 0.0480 (0.0501) Data: 0.0174 (0.0204) Loss: 0.0276 (0.0460)\n",
            "TRAIN(125): [250/391] Batch: 0.0470 (0.0500) Data: 0.0278 (0.0205) Loss: 0.0620 (0.0461)\n",
            "TRAIN(125): [260/391] Batch: 0.0450 (0.0499) Data: 0.0273 (0.0205) Loss: 0.0339 (0.0465)\n",
            "TRAIN(125): [270/391] Batch: 0.0499 (0.0498) Data: 0.0251 (0.0206) Loss: 0.0619 (0.0460)\n",
            "TRAIN(125): [280/391] Batch: 0.0469 (0.0497) Data: 0.0275 (0.0207) Loss: 0.0275 (0.0455)\n",
            "TRAIN(125): [290/391] Batch: 0.0457 (0.0496) Data: 0.0271 (0.0208) Loss: 0.0477 (0.0456)\n",
            "TRAIN(125): [300/391] Batch: 0.0463 (0.0496) Data: 0.0270 (0.0209) Loss: 0.0465 (0.0457)\n",
            "TRAIN(125): [310/391] Batch: 0.0468 (0.0495) Data: 0.0249 (0.0209) Loss: 0.0183 (0.0456)\n",
            "TRAIN(125): [320/391] Batch: 0.0534 (0.0496) Data: 0.0234 (0.0209) Loss: 0.0248 (0.0454)\n",
            "TRAIN(125): [330/391] Batch: 0.0556 (0.0495) Data: 0.0157 (0.0209) Loss: 0.0201 (0.0450)\n",
            "TRAIN(125): [340/391] Batch: 0.0504 (0.0496) Data: 0.0220 (0.0208) Loss: 0.0618 (0.0451)\n",
            "TRAIN(125): [350/391] Batch: 0.0571 (0.0496) Data: 0.0165 (0.0208) Loss: 0.0222 (0.0449)\n",
            "TRAIN(125): [360/391] Batch: 0.0605 (0.0495) Data: 0.0155 (0.0208) Loss: 0.0877 (0.0449)\n",
            "TRAIN(125): [370/391] Batch: 0.0597 (0.0495) Data: 0.0181 (0.0208) Loss: 0.0201 (0.0447)\n",
            "TRAIN(125): [380/391] Batch: 0.0475 (0.0496) Data: 0.0191 (0.0207) Loss: 0.0827 (0.0452)\n",
            "TRAIN(125): [390/391] Batch: 0.0467 (0.0495) Data: 0.0279 (0.0206) Loss: 0.1210 (0.0453)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(125)         0:00:19         0:00:08         0:00:11          0.0453\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(126): [ 10/391] Batch: 0.0498 (0.0697) Data: 0.0140 (0.0278) Loss: 0.0585 (0.0493)\n",
            "TRAIN(126): [ 20/391] Batch: 0.0513 (0.0611) Data: 0.0149 (0.0208) Loss: 0.0217 (0.0488)\n",
            "TRAIN(126): [ 30/391] Batch: 0.0432 (0.0573) Data: 0.0210 (0.0192) Loss: 0.0232 (0.0465)\n",
            "TRAIN(126): [ 40/391] Batch: 0.0540 (0.0557) Data: 0.0201 (0.0182) Loss: 0.0192 (0.0481)\n",
            "TRAIN(126): [ 50/391] Batch: 0.0534 (0.0544) Data: 0.0183 (0.0185) Loss: 0.0181 (0.0486)\n",
            "TRAIN(126): [ 60/391] Batch: 0.0515 (0.0533) Data: 0.0253 (0.0189) Loss: 0.0520 (0.0483)\n",
            "TRAIN(126): [ 70/391] Batch: 0.0474 (0.0530) Data: 0.0231 (0.0190) Loss: 0.1321 (0.0499)\n",
            "TRAIN(126): [ 80/391] Batch: 0.0404 (0.0525) Data: 0.0260 (0.0189) Loss: 0.0444 (0.0507)\n",
            "TRAIN(126): [ 90/391] Batch: 0.0469 (0.0521) Data: 0.0278 (0.0195) Loss: 0.0139 (0.0502)\n",
            "TRAIN(126): [100/391] Batch: 0.0456 (0.0516) Data: 0.0272 (0.0198) Loss: 0.0396 (0.0493)\n",
            "TRAIN(126): [110/391] Batch: 0.0493 (0.0514) Data: 0.0192 (0.0200) Loss: 0.0405 (0.0496)\n",
            "TRAIN(126): [120/391] Batch: 0.0494 (0.0510) Data: 0.0246 (0.0202) Loss: 0.0188 (0.0485)\n",
            "TRAIN(126): [130/391] Batch: 0.0467 (0.0507) Data: 0.0271 (0.0204) Loss: 0.0637 (0.0495)\n",
            "TRAIN(126): [140/391] Batch: 0.0456 (0.0504) Data: 0.0273 (0.0206) Loss: 0.0520 (0.0490)\n",
            "TRAIN(126): [150/391] Batch: 0.0448 (0.0504) Data: 0.0228 (0.0207) Loss: 0.0295 (0.0491)\n",
            "TRAIN(126): [160/391] Batch: 0.0410 (0.0504) Data: 0.0182 (0.0204) Loss: 0.0656 (0.0489)\n",
            "TRAIN(126): [170/391] Batch: 0.0523 (0.0502) Data: 0.0246 (0.0206) Loss: 0.0371 (0.0482)\n",
            "TRAIN(126): [180/391] Batch: 0.0547 (0.0501) Data: 0.0243 (0.0207) Loss: 0.0233 (0.0477)\n",
            "TRAIN(126): [190/391] Batch: 0.0473 (0.0500) Data: 0.0278 (0.0207) Loss: 0.0153 (0.0474)\n",
            "TRAIN(126): [200/391] Batch: 0.0388 (0.0499) Data: 0.0279 (0.0208) Loss: 0.0613 (0.0479)\n",
            "TRAIN(126): [210/391] Batch: 0.0410 (0.0498) Data: 0.0241 (0.0208) Loss: 0.0443 (0.0483)\n",
            "TRAIN(126): [220/391] Batch: 0.0426 (0.0498) Data: 0.0266 (0.0208) Loss: 0.0336 (0.0484)\n",
            "TRAIN(126): [230/391] Batch: 0.0494 (0.0498) Data: 0.0231 (0.0207) Loss: 0.0303 (0.0482)\n",
            "TRAIN(126): [240/391] Batch: 0.0547 (0.0497) Data: 0.0161 (0.0207) Loss: 0.0279 (0.0482)\n",
            "TRAIN(126): [250/391] Batch: 0.0597 (0.0497) Data: 0.0140 (0.0207) Loss: 0.0392 (0.0486)\n",
            "TRAIN(126): [260/391] Batch: 0.0493 (0.0498) Data: 0.0169 (0.0205) Loss: 0.0376 (0.0487)\n",
            "TRAIN(126): [270/391] Batch: 0.0470 (0.0498) Data: 0.0216 (0.0204) Loss: 0.0265 (0.0483)\n",
            "TRAIN(126): [280/391] Batch: 0.0519 (0.0499) Data: 0.0147 (0.0202) Loss: 0.0305 (0.0484)\n",
            "TRAIN(126): [290/391] Batch: 0.0442 (0.0498) Data: 0.0176 (0.0200) Loss: 0.0403 (0.0484)\n",
            "TRAIN(126): [300/391] Batch: 0.0452 (0.0498) Data: 0.0233 (0.0199) Loss: 0.0193 (0.0482)\n",
            "TRAIN(126): [310/391] Batch: 0.0446 (0.0499) Data: 0.0168 (0.0198) Loss: 0.0717 (0.0483)\n",
            "TRAIN(126): [320/391] Batch: 0.0488 (0.0498) Data: 0.0257 (0.0199) Loss: 0.1053 (0.0498)\n",
            "TRAIN(126): [330/391] Batch: 0.0461 (0.0497) Data: 0.0270 (0.0200) Loss: 0.0756 (0.0506)\n",
            "TRAIN(126): [340/391] Batch: 0.0543 (0.0497) Data: 0.0164 (0.0201) Loss: 0.0528 (0.0509)\n",
            "TRAIN(126): [350/391] Batch: 0.0465 (0.0497) Data: 0.0263 (0.0201) Loss: 0.1378 (0.0510)\n",
            "TRAIN(126): [360/391] Batch: 0.0473 (0.0497) Data: 0.0243 (0.0201) Loss: 0.0445 (0.0509)\n",
            "TRAIN(126): [370/391] Batch: 0.0396 (0.0496) Data: 0.0270 (0.0201) Loss: 0.0620 (0.0507)\n",
            "TRAIN(126): [380/391] Batch: 0.0489 (0.0496) Data: 0.0256 (0.0202) Loss: 0.0341 (0.0507)\n",
            "TRAIN(126): [390/391] Batch: 0.0465 (0.0495) Data: 0.0267 (0.0203) Loss: 0.0256 (0.0510)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(126)         0:00:19         0:00:07         0:00:11          0.0510\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(127): [ 10/391] Batch: 0.0446 (0.0664) Data: 0.0224 (0.0289) Loss: 0.0683 (0.0628)\n",
            "TRAIN(127): [ 20/391] Batch: 0.0509 (0.0585) Data: 0.0170 (0.0242) Loss: 0.0268 (0.0596)\n",
            "TRAIN(127): [ 30/391] Batch: 0.0488 (0.0550) Data: 0.0255 (0.0231) Loss: 0.0398 (0.0526)\n",
            "TRAIN(127): [ 40/391] Batch: 0.0469 (0.0529) Data: 0.0286 (0.0232) Loss: 0.1192 (0.0525)\n",
            "TRAIN(127): [ 50/391] Batch: 0.0398 (0.0521) Data: 0.0239 (0.0228) Loss: 0.0524 (0.0507)\n",
            "TRAIN(127): [ 60/391] Batch: 0.0475 (0.0512) Data: 0.0278 (0.0230) Loss: 0.0673 (0.0512)\n",
            "TRAIN(127): [ 70/391] Batch: 0.0505 (0.0511) Data: 0.0164 (0.0224) Loss: 0.0439 (0.0503)\n",
            "TRAIN(127): [ 80/391] Batch: 0.0479 (0.0507) Data: 0.0267 (0.0225) Loss: 0.0342 (0.0500)\n",
            "TRAIN(127): [ 90/391] Batch: 0.0444 (0.0503) Data: 0.0277 (0.0226) Loss: 0.0252 (0.0509)\n",
            "TRAIN(127): [100/391] Batch: 0.0516 (0.0503) Data: 0.0161 (0.0222) Loss: 0.0799 (0.0520)\n",
            "TRAIN(127): [110/391] Batch: 0.0411 (0.0502) Data: 0.0267 (0.0218) Loss: 0.0884 (0.0519)\n",
            "TRAIN(127): [120/391] Batch: 0.0463 (0.0500) Data: 0.0283 (0.0219) Loss: 0.0510 (0.0528)\n",
            "TRAIN(127): [130/391] Batch: 0.0407 (0.0500) Data: 0.0207 (0.0217) Loss: 0.1136 (0.0531)\n",
            "TRAIN(127): [140/391] Batch: 0.0549 (0.0502) Data: 0.0156 (0.0214) Loss: 0.0499 (0.0529)\n",
            "TRAIN(127): [150/391] Batch: 0.0587 (0.0501) Data: 0.0088 (0.0209) Loss: 0.0146 (0.0533)\n",
            "TRAIN(127): [160/391] Batch: 0.0390 (0.0501) Data: 0.0250 (0.0206) Loss: 0.0159 (0.0533)\n",
            "TRAIN(127): [170/391] Batch: 0.0568 (0.0503) Data: 0.0145 (0.0204) Loss: 0.0253 (0.0536)\n",
            "TRAIN(127): [180/391] Batch: 0.0495 (0.0503) Data: 0.0190 (0.0202) Loss: 0.0745 (0.0536)\n",
            "TRAIN(127): [190/391] Batch: 0.0538 (0.0504) Data: 0.0147 (0.0200) Loss: 0.0675 (0.0541)\n",
            "TRAIN(127): [200/391] Batch: 0.0557 (0.0502) Data: 0.0187 (0.0201) Loss: 0.1124 (0.0542)\n",
            "TRAIN(127): [210/391] Batch: 0.0453 (0.0501) Data: 0.0272 (0.0202) Loss: 0.0823 (0.0537)\n",
            "TRAIN(127): [220/391] Batch: 0.0479 (0.0500) Data: 0.0281 (0.0204) Loss: 0.0313 (0.0532)\n",
            "TRAIN(127): [230/391] Batch: 0.0495 (0.0500) Data: 0.0164 (0.0204) Loss: 0.0536 (0.0538)\n",
            "TRAIN(127): [240/391] Batch: 0.0597 (0.0500) Data: 0.0173 (0.0202) Loss: 0.0302 (0.0538)\n",
            "TRAIN(127): [250/391] Batch: 0.0430 (0.0499) Data: 0.0205 (0.0202) Loss: 0.1464 (0.0541)\n",
            "TRAIN(127): [260/391] Batch: 0.0499 (0.0499) Data: 0.0171 (0.0201) Loss: 0.0212 (0.0534)\n",
            "TRAIN(127): [270/391] Batch: 0.0462 (0.0498) Data: 0.0237 (0.0202) Loss: 0.0312 (0.0534)\n",
            "TRAIN(127): [280/391] Batch: 0.0364 (0.0498) Data: 0.0269 (0.0201) Loss: 0.0506 (0.0542)\n",
            "TRAIN(127): [290/391] Batch: 0.0578 (0.0498) Data: 0.0180 (0.0202) Loss: 0.0268 (0.0541)\n",
            "TRAIN(127): [300/391] Batch: 0.0470 (0.0497) Data: 0.0250 (0.0203) Loss: 0.0681 (0.0541)\n",
            "TRAIN(127): [310/391] Batch: 0.0520 (0.0497) Data: 0.0252 (0.0203) Loss: 0.0747 (0.0548)\n",
            "TRAIN(127): [320/391] Batch: 0.0418 (0.0497) Data: 0.0263 (0.0204) Loss: 0.0648 (0.0550)\n",
            "TRAIN(127): [330/391] Batch: 0.0467 (0.0496) Data: 0.0289 (0.0205) Loss: 0.0257 (0.0552)\n",
            "TRAIN(127): [340/391] Batch: 0.0470 (0.0496) Data: 0.0282 (0.0206) Loss: 0.0464 (0.0549)\n",
            "TRAIN(127): [350/391] Batch: 0.0470 (0.0495) Data: 0.0281 (0.0207) Loss: 0.0098 (0.0550)\n",
            "TRAIN(127): [360/391] Batch: 0.0502 (0.0495) Data: 0.0259 (0.0207) Loss: 0.0997 (0.0556)\n",
            "TRAIN(127): [370/391] Batch: 0.0458 (0.0495) Data: 0.0257 (0.0208) Loss: 0.0296 (0.0553)\n",
            "TRAIN(127): [380/391] Batch: 0.0423 (0.0495) Data: 0.0258 (0.0207) Loss: 0.0332 (0.0551)\n",
            "TRAIN(127): [390/391] Batch: 0.0436 (0.0494) Data: 0.0292 (0.0208) Loss: 0.0605 (0.0548)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(127)         0:00:19         0:00:08         0:00:11          0.0548\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(128): [ 10/391] Batch: 0.0514 (0.0723) Data: 0.0143 (0.0301) Loss: 0.0536 (0.0682)\n",
            "TRAIN(128): [ 20/391] Batch: 0.0465 (0.0613) Data: 0.0209 (0.0248) Loss: 0.0510 (0.0567)\n",
            "TRAIN(128): [ 30/391] Batch: 0.0462 (0.0572) Data: 0.0204 (0.0226) Loss: 0.0277 (0.0571)\n",
            "TRAIN(128): [ 40/391] Batch: 0.0484 (0.0555) Data: 0.0152 (0.0214) Loss: 0.0425 (0.0538)\n",
            "TRAIN(128): [ 50/391] Batch: 0.0598 (0.0545) Data: 0.0162 (0.0206) Loss: 0.0258 (0.0552)\n",
            "TRAIN(128): [ 60/391] Batch: 0.0599 (0.0540) Data: 0.0152 (0.0202) Loss: 0.0392 (0.0534)\n",
            "TRAIN(128): [ 70/391] Batch: 0.0512 (0.0534) Data: 0.0239 (0.0200) Loss: 0.0215 (0.0527)\n",
            "TRAIN(128): [ 80/391] Batch: 0.0450 (0.0527) Data: 0.0247 (0.0204) Loss: 0.0473 (0.0541)\n",
            "TRAIN(128): [ 90/391] Batch: 0.0510 (0.0521) Data: 0.0268 (0.0208) Loss: 0.0489 (0.0529)\n",
            "TRAIN(128): [100/391] Batch: 0.0459 (0.0516) Data: 0.0270 (0.0211) Loss: 0.0984 (0.0533)\n",
            "TRAIN(128): [110/391] Batch: 0.0491 (0.0514) Data: 0.0233 (0.0211) Loss: 0.0933 (0.0535)\n",
            "TRAIN(128): [120/391] Batch: 0.0461 (0.0510) Data: 0.0269 (0.0214) Loss: 0.0206 (0.0519)\n",
            "TRAIN(128): [130/391] Batch: 0.0474 (0.0507) Data: 0.0252 (0.0215) Loss: 0.0236 (0.0521)\n",
            "TRAIN(128): [140/391] Batch: 0.0492 (0.0506) Data: 0.0173 (0.0216) Loss: 0.0723 (0.0519)\n",
            "TRAIN(128): [150/391] Batch: 0.0450 (0.0505) Data: 0.0177 (0.0214) Loss: 0.0265 (0.0518)\n",
            "TRAIN(128): [160/391] Batch: 0.0523 (0.0505) Data: 0.0232 (0.0212) Loss: 0.0276 (0.0525)\n",
            "TRAIN(128): [170/391] Batch: 0.0519 (0.0505) Data: 0.0150 (0.0210) Loss: 0.1012 (0.0528)\n",
            "TRAIN(128): [180/391] Batch: 0.0472 (0.0505) Data: 0.0238 (0.0208) Loss: 0.0248 (0.0527)\n",
            "TRAIN(128): [190/391] Batch: 0.0470 (0.0503) Data: 0.0275 (0.0209) Loss: 0.0095 (0.0527)\n",
            "TRAIN(128): [200/391] Batch: 0.0414 (0.0503) Data: 0.0262 (0.0209) Loss: 0.0772 (0.0526)\n",
            "TRAIN(128): [210/391] Batch: 0.0539 (0.0503) Data: 0.0182 (0.0209) Loss: 0.0318 (0.0525)\n",
            "TRAIN(128): [220/391] Batch: 0.0498 (0.0502) Data: 0.0164 (0.0208) Loss: 0.0750 (0.0526)\n",
            "TRAIN(128): [230/391] Batch: 0.0394 (0.0501) Data: 0.0269 (0.0209) Loss: 0.0203 (0.0519)\n",
            "TRAIN(128): [240/391] Batch: 0.0465 (0.0500) Data: 0.0281 (0.0210) Loss: 0.0600 (0.0516)\n",
            "TRAIN(128): [250/391] Batch: 0.0468 (0.0499) Data: 0.0278 (0.0211) Loss: 0.0171 (0.0513)\n",
            "TRAIN(128): [260/391] Batch: 0.0475 (0.0498) Data: 0.0277 (0.0212) Loss: 0.0219 (0.0511)\n",
            "TRAIN(128): [270/391] Batch: 0.0355 (0.0497) Data: 0.0274 (0.0212) Loss: 0.0432 (0.0507)\n",
            "TRAIN(128): [280/391] Batch: 0.0585 (0.0497) Data: 0.0146 (0.0211) Loss: 0.0105 (0.0502)\n",
            "TRAIN(128): [290/391] Batch: 0.0478 (0.0498) Data: 0.0174 (0.0209) Loss: 0.0372 (0.0499)\n",
            "TRAIN(128): [300/391] Batch: 0.0565 (0.0498) Data: 0.0148 (0.0207) Loss: 0.0846 (0.0497)\n",
            "TRAIN(128): [310/391] Batch: 0.0495 (0.0498) Data: 0.0157 (0.0205) Loss: 0.0146 (0.0495)\n",
            "TRAIN(128): [320/391] Batch: 0.0479 (0.0499) Data: 0.0185 (0.0204) Loss: 0.0611 (0.0495)\n",
            "TRAIN(128): [330/391] Batch: 0.0627 (0.0500) Data: 0.0141 (0.0202) Loss: 0.0372 (0.0493)\n",
            "TRAIN(128): [340/391] Batch: 0.0447 (0.0500) Data: 0.0277 (0.0201) Loss: 0.0541 (0.0491)\n",
            "TRAIN(128): [350/391] Batch: 0.0458 (0.0500) Data: 0.0180 (0.0200) Loss: 0.0121 (0.0488)\n",
            "TRAIN(128): [360/391] Batch: 0.0523 (0.0500) Data: 0.0237 (0.0200) Loss: 0.0372 (0.0486)\n",
            "TRAIN(128): [370/391] Batch: 0.0500 (0.0499) Data: 0.0259 (0.0201) Loss: 0.0391 (0.0484)\n",
            "TRAIN(128): [380/391] Batch: 0.0534 (0.0499) Data: 0.0160 (0.0201) Loss: 0.0312 (0.0486)\n",
            "TRAIN(128): [390/391] Batch: 0.0461 (0.0499) Data: 0.0277 (0.0201) Loss: 0.0356 (0.0483)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(128)         0:00:19         0:00:07         0:00:11          0.0483\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(129): [ 10/391] Batch: 0.0464 (0.0645) Data: 0.0257 (0.0323) Loss: 0.0107 (0.0371)\n",
            "TRAIN(129): [ 20/391] Batch: 0.0511 (0.0579) Data: 0.0256 (0.0272) Loss: 0.0219 (0.0347)\n",
            "TRAIN(129): [ 30/391] Batch: 0.0474 (0.0550) Data: 0.0247 (0.0252) Loss: 0.0263 (0.0356)\n",
            "TRAIN(129): [ 40/391] Batch: 0.0497 (0.0537) Data: 0.0227 (0.0236) Loss: 0.0522 (0.0391)\n",
            "TRAIN(129): [ 50/391] Batch: 0.0381 (0.0526) Data: 0.0272 (0.0230) Loss: 0.0345 (0.0402)\n",
            "TRAIN(129): [ 60/391] Batch: 0.0425 (0.0518) Data: 0.0262 (0.0228) Loss: 0.0366 (0.0408)\n",
            "TRAIN(129): [ 70/391] Batch: 0.0568 (0.0513) Data: 0.0190 (0.0228) Loss: 0.0865 (0.0416)\n",
            "TRAIN(129): [ 80/391] Batch: 0.0511 (0.0508) Data: 0.0258 (0.0227) Loss: 0.0238 (0.0428)\n",
            "TRAIN(129): [ 90/391] Batch: 0.0503 (0.0507) Data: 0.0242 (0.0224) Loss: 0.0684 (0.0435)\n",
            "TRAIN(129): [100/391] Batch: 0.0454 (0.0505) Data: 0.0276 (0.0220) Loss: 0.0281 (0.0438)\n",
            "TRAIN(129): [110/391] Batch: 0.0485 (0.0502) Data: 0.0238 (0.0221) Loss: 0.0651 (0.0443)\n",
            "TRAIN(129): [120/391] Batch: 0.0503 (0.0503) Data: 0.0191 (0.0219) Loss: 0.0410 (0.0453)\n",
            "TRAIN(129): [130/391] Batch: 0.0436 (0.0501) Data: 0.0249 (0.0218) Loss: 0.0711 (0.0451)\n",
            "TRAIN(129): [140/391] Batch: 0.0519 (0.0501) Data: 0.0168 (0.0216) Loss: 0.0393 (0.0447)\n",
            "TRAIN(129): [150/391] Batch: 0.0564 (0.0502) Data: 0.0143 (0.0213) Loss: 0.0971 (0.0451)\n",
            "TRAIN(129): [160/391] Batch: 0.0479 (0.0501) Data: 0.0161 (0.0209) Loss: 0.0679 (0.0453)\n",
            "TRAIN(129): [170/391] Batch: 0.0695 (0.0503) Data: 0.0138 (0.0206) Loss: 0.0570 (0.0451)\n",
            "TRAIN(129): [180/391] Batch: 0.0481 (0.0502) Data: 0.0195 (0.0204) Loss: 0.0468 (0.0450)\n",
            "TRAIN(129): [190/391] Batch: 0.0480 (0.0502) Data: 0.0190 (0.0203) Loss: 0.0111 (0.0448)\n",
            "TRAIN(129): [200/391] Batch: 0.0469 (0.0502) Data: 0.0174 (0.0200) Loss: 0.0146 (0.0443)\n",
            "TRAIN(129): [210/391] Batch: 0.0540 (0.0501) Data: 0.0214 (0.0199) Loss: 0.0473 (0.0440)\n",
            "TRAIN(129): [220/391] Batch: 0.0491 (0.0501) Data: 0.0251 (0.0199) Loss: 0.0333 (0.0444)\n",
            "TRAIN(129): [230/391] Batch: 0.0468 (0.0500) Data: 0.0274 (0.0201) Loss: 0.0225 (0.0450)\n",
            "TRAIN(129): [240/391] Batch: 0.0450 (0.0499) Data: 0.0274 (0.0201) Loss: 0.0904 (0.0445)\n",
            "TRAIN(129): [250/391] Batch: 0.0457 (0.0498) Data: 0.0278 (0.0202) Loss: 0.0150 (0.0445)\n",
            "TRAIN(129): [260/391] Batch: 0.0549 (0.0498) Data: 0.0222 (0.0202) Loss: 0.0374 (0.0444)\n",
            "TRAIN(129): [270/391] Batch: 0.0511 (0.0499) Data: 0.0170 (0.0201) Loss: 0.0641 (0.0442)\n",
            "TRAIN(129): [280/391] Batch: 0.0589 (0.0498) Data: 0.0172 (0.0201) Loss: 0.0546 (0.0444)\n",
            "TRAIN(129): [290/391] Batch: 0.0538 (0.0498) Data: 0.0164 (0.0201) Loss: 0.0932 (0.0449)\n",
            "TRAIN(129): [300/391] Batch: 0.0496 (0.0497) Data: 0.0238 (0.0201) Loss: 0.0581 (0.0447)\n",
            "TRAIN(129): [310/391] Batch: 0.0501 (0.0497) Data: 0.0264 (0.0202) Loss: 0.0421 (0.0446)\n",
            "TRAIN(129): [320/391] Batch: 0.0415 (0.0496) Data: 0.0259 (0.0202) Loss: 0.0233 (0.0443)\n",
            "TRAIN(129): [330/391] Batch: 0.0452 (0.0496) Data: 0.0267 (0.0203) Loss: 0.0820 (0.0446)\n",
            "TRAIN(129): [340/391] Batch: 0.0509 (0.0496) Data: 0.0256 (0.0203) Loss: 0.0261 (0.0445)\n",
            "TRAIN(129): [350/391] Batch: 0.0462 (0.0496) Data: 0.0282 (0.0203) Loss: 0.0726 (0.0447)\n",
            "TRAIN(129): [360/391] Batch: 0.0510 (0.0495) Data: 0.0246 (0.0204) Loss: 0.0235 (0.0445)\n",
            "TRAIN(129): [370/391] Batch: 0.0460 (0.0495) Data: 0.0277 (0.0204) Loss: 0.0207 (0.0443)\n",
            "TRAIN(129): [380/391] Batch: 0.0487 (0.0494) Data: 0.0191 (0.0205) Loss: 0.0880 (0.0444)\n",
            "TRAIN(129): [390/391] Batch: 0.0461 (0.0494) Data: 0.0272 (0.0205) Loss: 0.0617 (0.0445)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(129)         0:00:19         0:00:08         0:00:11          0.0445\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(130): [ 10/391] Batch: 0.0558 (0.0651) Data: 0.0228 (0.0323) Loss: 0.0210 (0.0367)\n",
            "TRAIN(130): [ 20/391] Batch: 0.0509 (0.0566) Data: 0.0277 (0.0276) Loss: 0.0218 (0.0379)\n",
            "TRAIN(130): [ 30/391] Batch: 0.0435 (0.0547) Data: 0.0215 (0.0243) Loss: 0.0430 (0.0421)\n",
            "TRAIN(130): [ 40/391] Batch: 0.0462 (0.0538) Data: 0.0252 (0.0230) Loss: 0.0436 (0.0443)\n",
            "TRAIN(130): [ 50/391] Batch: 0.0500 (0.0533) Data: 0.0203 (0.0216) Loss: 0.0666 (0.0451)\n",
            "TRAIN(130): [ 60/391] Batch: 0.0602 (0.0532) Data: 0.0142 (0.0206) Loss: 0.0297 (0.0433)\n",
            "TRAIN(130): [ 70/391] Batch: 0.0581 (0.0532) Data: 0.0122 (0.0198) Loss: 0.1167 (0.0445)\n",
            "TRAIN(130): [ 80/391] Batch: 0.0511 (0.0526) Data: 0.0207 (0.0198) Loss: 0.0410 (0.0447)\n",
            "TRAIN(130): [ 90/391] Batch: 0.0418 (0.0523) Data: 0.0202 (0.0195) Loss: 0.0677 (0.0445)\n",
            "TRAIN(130): [100/391] Batch: 0.0528 (0.0518) Data: 0.0255 (0.0199) Loss: 0.0282 (0.0440)\n",
            "TRAIN(130): [110/391] Batch: 0.0444 (0.0515) Data: 0.0275 (0.0201) Loss: 0.0390 (0.0440)\n",
            "TRAIN(130): [120/391] Batch: 0.0454 (0.0512) Data: 0.0280 (0.0202) Loss: 0.0388 (0.0434)\n",
            "TRAIN(130): [130/391] Batch: 0.0454 (0.0510) Data: 0.0276 (0.0203) Loss: 0.0287 (0.0431)\n",
            "TRAIN(130): [140/391] Batch: 0.0462 (0.0508) Data: 0.0252 (0.0206) Loss: 0.0515 (0.0429)\n",
            "TRAIN(130): [150/391] Batch: 0.0439 (0.0506) Data: 0.0274 (0.0207) Loss: 0.0205 (0.0422)\n",
            "TRAIN(130): [160/391] Batch: 0.0466 (0.0506) Data: 0.0226 (0.0204) Loss: 0.0626 (0.0424)\n",
            "TRAIN(130): [170/391] Batch: 0.0541 (0.0506) Data: 0.0168 (0.0203) Loss: 0.1018 (0.0433)\n",
            "TRAIN(130): [180/391] Batch: 0.0511 (0.0506) Data: 0.0171 (0.0201) Loss: 0.0631 (0.0433)\n",
            "TRAIN(130): [190/391] Batch: 0.0588 (0.0507) Data: 0.0146 (0.0198) Loss: 0.0471 (0.0429)\n",
            "TRAIN(130): [200/391] Batch: 0.0478 (0.0506) Data: 0.0258 (0.0198) Loss: 0.0564 (0.0425)\n",
            "TRAIN(130): [210/391] Batch: 0.0515 (0.0506) Data: 0.0224 (0.0199) Loss: 0.0515 (0.0423)\n",
            "TRAIN(130): [220/391] Batch: 0.0462 (0.0505) Data: 0.0280 (0.0199) Loss: 0.0269 (0.0419)\n",
            "TRAIN(130): [230/391] Batch: 0.0472 (0.0504) Data: 0.0272 (0.0201) Loss: 0.0718 (0.0419)\n",
            "TRAIN(130): [240/391] Batch: 0.0490 (0.0504) Data: 0.0194 (0.0201) Loss: 0.0294 (0.0421)\n",
            "TRAIN(130): [250/391] Batch: 0.0546 (0.0502) Data: 0.0247 (0.0202) Loss: 0.0462 (0.0423)\n",
            "TRAIN(130): [260/391] Batch: 0.0464 (0.0501) Data: 0.0263 (0.0203) Loss: 0.0404 (0.0420)\n",
            "TRAIN(130): [270/391] Batch: 0.0562 (0.0500) Data: 0.0225 (0.0205) Loss: 0.0285 (0.0420)\n",
            "TRAIN(130): [280/391] Batch: 0.0459 (0.0500) Data: 0.0179 (0.0203) Loss: 0.0164 (0.0422)\n",
            "TRAIN(130): [290/391] Batch: 0.0468 (0.0499) Data: 0.0284 (0.0204) Loss: 0.0326 (0.0422)\n",
            "TRAIN(130): [300/391] Batch: 0.0604 (0.0500) Data: 0.0174 (0.0203) Loss: 0.0606 (0.0426)\n",
            "TRAIN(130): [310/391] Batch: 0.0621 (0.0501) Data: 0.0161 (0.0202) Loss: 0.0557 (0.0425)\n",
            "TRAIN(130): [320/391] Batch: 0.0616 (0.0501) Data: 0.0142 (0.0202) Loss: 0.0165 (0.0422)\n",
            "TRAIN(130): [330/391] Batch: 0.0497 (0.0502) Data: 0.0199 (0.0200) Loss: 0.0475 (0.0418)\n",
            "TRAIN(130): [340/391] Batch: 0.0604 (0.0502) Data: 0.0151 (0.0200) Loss: 0.0188 (0.0416)\n",
            "TRAIN(130): [350/391] Batch: 0.0475 (0.0502) Data: 0.0209 (0.0198) Loss: 0.0158 (0.0414)\n",
            "TRAIN(130): [360/391] Batch: 0.0358 (0.0502) Data: 0.0254 (0.0197) Loss: 0.0207 (0.0411)\n",
            "TRAIN(130): [370/391] Batch: 0.0470 (0.0501) Data: 0.0275 (0.0199) Loss: 0.0201 (0.0407)\n",
            "TRAIN(130): [380/391] Batch: 0.0493 (0.0500) Data: 0.0256 (0.0199) Loss: 0.0387 (0.0407)\n",
            "TRAIN(130): [390/391] Batch: 0.0466 (0.0500) Data: 0.0280 (0.0200) Loss: 0.0569 (0.0408)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(130)         0:00:19         0:00:07         0:00:11          0.0408\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(131): [ 10/391] Batch: 0.0455 (0.0641) Data: 0.0268 (0.0325) Loss: 0.0307 (0.0429)\n",
            "TRAIN(131): [ 20/391] Batch: 0.0460 (0.0566) Data: 0.0254 (0.0264) Loss: 0.1195 (0.0423)\n",
            "TRAIN(131): [ 30/391] Batch: 0.0481 (0.0537) Data: 0.0269 (0.0254) Loss: 0.0847 (0.0443)\n",
            "TRAIN(131): [ 40/391] Batch: 0.0491 (0.0525) Data: 0.0239 (0.0241) Loss: 0.0738 (0.0479)\n",
            "TRAIN(131): [ 50/391] Batch: 0.0508 (0.0519) Data: 0.0254 (0.0237) Loss: 0.0184 (0.0468)\n",
            "TRAIN(131): [ 60/391] Batch: 0.0431 (0.0513) Data: 0.0269 (0.0235) Loss: 0.0593 (0.0469)\n",
            "TRAIN(131): [ 70/391] Batch: 0.0494 (0.0510) Data: 0.0182 (0.0229) Loss: 0.0849 (0.0471)\n",
            "TRAIN(131): [ 80/391] Batch: 0.0527 (0.0509) Data: 0.0185 (0.0222) Loss: 0.0931 (0.0469)\n",
            "TRAIN(131): [ 90/391] Batch: 0.0466 (0.0505) Data: 0.0266 (0.0221) Loss: 0.0403 (0.0464)\n",
            "TRAIN(131): [100/391] Batch: 0.0447 (0.0502) Data: 0.0259 (0.0221) Loss: 0.0108 (0.0450)\n",
            "TRAIN(131): [110/391] Batch: 0.0466 (0.0501) Data: 0.0278 (0.0222) Loss: 0.0286 (0.0444)\n",
            "TRAIN(131): [120/391] Batch: 0.0486 (0.0500) Data: 0.0160 (0.0220) Loss: 0.0340 (0.0436)\n",
            "TRAIN(131): [130/391] Batch: 0.0573 (0.0500) Data: 0.0165 (0.0217) Loss: 0.0427 (0.0432)\n",
            "TRAIN(131): [140/391] Batch: 0.0466 (0.0497) Data: 0.0285 (0.0219) Loss: 0.0082 (0.0428)\n",
            "TRAIN(131): [150/391] Batch: 0.0380 (0.0496) Data: 0.0245 (0.0219) Loss: 0.0147 (0.0427)\n",
            "TRAIN(131): [160/391] Batch: 0.0478 (0.0495) Data: 0.0258 (0.0219) Loss: 0.0172 (0.0434)\n",
            "TRAIN(131): [170/391] Batch: 0.0580 (0.0496) Data: 0.0162 (0.0218) Loss: 0.0326 (0.0436)\n",
            "TRAIN(131): [180/391] Batch: 0.0468 (0.0496) Data: 0.0209 (0.0216) Loss: 0.0129 (0.0438)\n",
            "TRAIN(131): [190/391] Batch: 0.0563 (0.0498) Data: 0.0141 (0.0213) Loss: 0.0573 (0.0435)\n",
            "TRAIN(131): [200/391] Batch: 0.0430 (0.0499) Data: 0.0188 (0.0210) Loss: 0.0334 (0.0435)\n",
            "TRAIN(131): [210/391] Batch: 0.0450 (0.0500) Data: 0.0163 (0.0208) Loss: 0.0174 (0.0437)\n",
            "TRAIN(131): [220/391] Batch: 0.0537 (0.0500) Data: 0.0190 (0.0205) Loss: 0.0310 (0.0440)\n",
            "TRAIN(131): [230/391] Batch: 0.0493 (0.0500) Data: 0.0188 (0.0205) Loss: 0.0672 (0.0442)\n",
            "TRAIN(131): [240/391] Batch: 0.0474 (0.0500) Data: 0.0190 (0.0204) Loss: 0.0303 (0.0440)\n",
            "TRAIN(131): [250/391] Batch: 0.0500 (0.0500) Data: 0.0231 (0.0202) Loss: 0.0723 (0.0446)\n",
            "TRAIN(131): [260/391] Batch: 0.0462 (0.0499) Data: 0.0280 (0.0203) Loss: 0.0558 (0.0450)\n",
            "TRAIN(131): [270/391] Batch: 0.0626 (0.0498) Data: 0.0169 (0.0204) Loss: 0.0180 (0.0453)\n",
            "TRAIN(131): [280/391] Batch: 0.0502 (0.0498) Data: 0.0171 (0.0204) Loss: 0.0367 (0.0453)\n",
            "TRAIN(131): [290/391] Batch: 0.0476 (0.0497) Data: 0.0269 (0.0205) Loss: 0.0538 (0.0452)\n",
            "TRAIN(131): [300/391] Batch: 0.0425 (0.0497) Data: 0.0252 (0.0205) Loss: 0.0277 (0.0449)\n",
            "TRAIN(131): [310/391] Batch: 0.0530 (0.0496) Data: 0.0181 (0.0205) Loss: 0.0673 (0.0452)\n",
            "TRAIN(131): [320/391] Batch: 0.0551 (0.0496) Data: 0.0219 (0.0206) Loss: 0.0733 (0.0450)\n",
            "TRAIN(131): [330/391] Batch: 0.0476 (0.0496) Data: 0.0269 (0.0206) Loss: 0.0106 (0.0451)\n",
            "TRAIN(131): [340/391] Batch: 0.0554 (0.0496) Data: 0.0232 (0.0206) Loss: 0.0391 (0.0458)\n",
            "TRAIN(131): [350/391] Batch: 0.0450 (0.0496) Data: 0.0176 (0.0206) Loss: 0.0361 (0.0456)\n",
            "TRAIN(131): [360/391] Batch: 0.0449 (0.0495) Data: 0.0258 (0.0206) Loss: 0.0190 (0.0458)\n",
            "TRAIN(131): [370/391] Batch: 0.0489 (0.0495) Data: 0.0260 (0.0206) Loss: 0.0425 (0.0456)\n",
            "TRAIN(131): [380/391] Batch: 0.0575 (0.0495) Data: 0.0210 (0.0207) Loss: 0.0488 (0.0455)\n",
            "TRAIN(131): [390/391] Batch: 0.0464 (0.0495) Data: 0.0277 (0.0206) Loss: 0.0288 (0.0456)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(131)         0:00:19         0:00:08         0:00:11          0.0456\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(132): [ 10/391] Batch: 0.0411 (0.0647) Data: 0.0251 (0.0286) Loss: 0.0552 (0.0380)\n",
            "TRAIN(132): [ 20/391] Batch: 0.0539 (0.0572) Data: 0.0240 (0.0255) Loss: 0.0104 (0.0390)\n",
            "TRAIN(132): [ 30/391] Batch: 0.0455 (0.0536) Data: 0.0277 (0.0249) Loss: 0.0690 (0.0380)\n",
            "TRAIN(132): [ 40/391] Batch: 0.0400 (0.0523) Data: 0.0274 (0.0242) Loss: 0.0083 (0.0399)\n",
            "TRAIN(132): [ 50/391] Batch: 0.0604 (0.0525) Data: 0.0123 (0.0229) Loss: 0.0434 (0.0394)\n",
            "TRAIN(132): [ 60/391] Batch: 0.0548 (0.0523) Data: 0.0154 (0.0216) Loss: 0.0202 (0.0372)\n",
            "TRAIN(132): [ 70/391] Batch: 0.0467 (0.0525) Data: 0.0162 (0.0209) Loss: 0.0411 (0.0380)\n",
            "TRAIN(132): [ 80/391] Batch: 0.0456 (0.0523) Data: 0.0210 (0.0202) Loss: 0.0145 (0.0367)\n",
            "TRAIN(132): [ 90/391] Batch: 0.0566 (0.0522) Data: 0.0188 (0.0200) Loss: 0.0479 (0.0369)\n",
            "TRAIN(132): [100/391] Batch: 0.0522 (0.0521) Data: 0.0198 (0.0197) Loss: 0.0211 (0.0377)\n",
            "TRAIN(132): [110/391] Batch: 0.0507 (0.0521) Data: 0.0194 (0.0194) Loss: 0.0439 (0.0366)\n",
            "TRAIN(132): [120/391] Batch: 0.0549 (0.0519) Data: 0.0160 (0.0194) Loss: 0.0191 (0.0364)\n",
            "TRAIN(132): [130/391] Batch: 0.0458 (0.0514) Data: 0.0269 (0.0197) Loss: 0.0386 (0.0367)\n",
            "TRAIN(132): [140/391] Batch: 0.0526 (0.0515) Data: 0.0157 (0.0195) Loss: 0.0478 (0.0373)\n",
            "TRAIN(132): [150/391] Batch: 0.0442 (0.0512) Data: 0.0275 (0.0195) Loss: 0.0182 (0.0369)\n",
            "TRAIN(132): [160/391] Batch: 0.0482 (0.0510) Data: 0.0263 (0.0197) Loss: 0.0363 (0.0373)\n",
            "TRAIN(132): [170/391] Batch: 0.0495 (0.0510) Data: 0.0177 (0.0198) Loss: 0.0408 (0.0375)\n",
            "TRAIN(132): [180/391] Batch: 0.0452 (0.0508) Data: 0.0238 (0.0198) Loss: 0.0516 (0.0378)\n",
            "TRAIN(132): [190/391] Batch: 0.0433 (0.0507) Data: 0.0261 (0.0200) Loss: 0.0213 (0.0375)\n",
            "TRAIN(132): [200/391] Batch: 0.0452 (0.0506) Data: 0.0259 (0.0201) Loss: 0.0872 (0.0380)\n",
            "TRAIN(132): [210/391] Batch: 0.0415 (0.0505) Data: 0.0264 (0.0201) Loss: 0.0627 (0.0384)\n",
            "TRAIN(132): [220/391] Batch: 0.0562 (0.0504) Data: 0.0163 (0.0202) Loss: 0.0456 (0.0382)\n",
            "TRAIN(132): [230/391] Batch: 0.0494 (0.0504) Data: 0.0169 (0.0200) Loss: 0.0157 (0.0380)\n",
            "TRAIN(132): [240/391] Batch: 0.0441 (0.0503) Data: 0.0177 (0.0200) Loss: 0.0252 (0.0381)\n",
            "TRAIN(132): [250/391] Batch: 0.0440 (0.0503) Data: 0.0162 (0.0199) Loss: 0.0505 (0.0383)\n",
            "TRAIN(132): [260/391] Batch: 0.0486 (0.0503) Data: 0.0228 (0.0199) Loss: 0.0489 (0.0386)\n",
            "TRAIN(132): [270/391] Batch: 0.0436 (0.0503) Data: 0.0236 (0.0198) Loss: 0.0914 (0.0391)\n",
            "TRAIN(132): [280/391] Batch: 0.0519 (0.0503) Data: 0.0172 (0.0197) Loss: 0.0509 (0.0391)\n",
            "TRAIN(132): [290/391] Batch: 0.0518 (0.0503) Data: 0.0227 (0.0198) Loss: 0.0758 (0.0396)\n",
            "TRAIN(132): [300/391] Batch: 0.0459 (0.0502) Data: 0.0271 (0.0198) Loss: 0.1051 (0.0398)\n",
            "TRAIN(132): [310/391] Batch: 0.0520 (0.0502) Data: 0.0246 (0.0199) Loss: 0.0453 (0.0401)\n",
            "TRAIN(132): [320/391] Batch: 0.0562 (0.0502) Data: 0.0107 (0.0198) Loss: 0.0348 (0.0404)\n",
            "TRAIN(132): [330/391] Batch: 0.0472 (0.0502) Data: 0.0201 (0.0197) Loss: 0.0547 (0.0405)\n",
            "TRAIN(132): [340/391] Batch: 0.0496 (0.0502) Data: 0.0162 (0.0196) Loss: 0.0440 (0.0406)\n",
            "TRAIN(132): [350/391] Batch: 0.0495 (0.0503) Data: 0.0190 (0.0193) Loss: 0.0307 (0.0403)\n",
            "TRAIN(132): [360/391] Batch: 0.0602 (0.0503) Data: 0.0156 (0.0193) Loss: 0.0246 (0.0404)\n",
            "TRAIN(132): [370/391] Batch: 0.0476 (0.0503) Data: 0.0198 (0.0193) Loss: 0.0314 (0.0404)\n",
            "TRAIN(132): [380/391] Batch: 0.0470 (0.0503) Data: 0.0212 (0.0192) Loss: 0.0695 (0.0405)\n",
            "TRAIN(132): [390/391] Batch: 0.0450 (0.0503) Data: 0.0276 (0.0193) Loss: 0.0565 (0.0405)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(132)         0:00:19         0:00:07         0:00:12          0.0405\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(133): [ 10/391] Batch: 0.0467 (0.0650) Data: 0.0245 (0.0327) Loss: 0.0202 (0.0433)\n",
            "TRAIN(133): [ 20/391] Batch: 0.0528 (0.0573) Data: 0.0166 (0.0265) Loss: 0.0884 (0.0448)\n",
            "TRAIN(133): [ 30/391] Batch: 0.0486 (0.0547) Data: 0.0247 (0.0248) Loss: 0.0538 (0.0504)\n",
            "TRAIN(133): [ 40/391] Batch: 0.0455 (0.0535) Data: 0.0266 (0.0236) Loss: 0.0244 (0.0492)\n",
            "TRAIN(133): [ 50/391] Batch: 0.0407 (0.0525) Data: 0.0261 (0.0231) Loss: 0.0605 (0.0469)\n",
            "TRAIN(133): [ 60/391] Batch: 0.0538 (0.0520) Data: 0.0156 (0.0226) Loss: 0.0331 (0.0446)\n",
            "TRAIN(133): [ 70/391] Batch: 0.0432 (0.0518) Data: 0.0202 (0.0216) Loss: 0.0527 (0.0441)\n",
            "TRAIN(133): [ 80/391] Batch: 0.0486 (0.0513) Data: 0.0268 (0.0216) Loss: 0.0992 (0.0441)\n",
            "TRAIN(133): [ 90/391] Batch: 0.0572 (0.0512) Data: 0.0151 (0.0213) Loss: 0.0188 (0.0435)\n",
            "TRAIN(133): [100/391] Batch: 0.0545 (0.0510) Data: 0.0188 (0.0212) Loss: 0.0489 (0.0421)\n",
            "TRAIN(133): [110/391] Batch: 0.0517 (0.0510) Data: 0.0231 (0.0209) Loss: 0.0164 (0.0428)\n",
            "TRAIN(133): [120/391] Batch: 0.0501 (0.0509) Data: 0.0236 (0.0209) Loss: 0.0245 (0.0428)\n",
            "TRAIN(133): [130/391] Batch: 0.0462 (0.0508) Data: 0.0261 (0.0208) Loss: 0.0331 (0.0426)\n",
            "TRAIN(133): [140/391] Batch: 0.0541 (0.0507) Data: 0.0158 (0.0208) Loss: 0.0756 (0.0425)\n",
            "TRAIN(133): [150/391] Batch: 0.0447 (0.0505) Data: 0.0268 (0.0208) Loss: 0.0372 (0.0431)\n",
            "TRAIN(133): [160/391] Batch: 0.0457 (0.0504) Data: 0.0269 (0.0208) Loss: 0.0214 (0.0424)\n",
            "TRAIN(133): [170/391] Batch: 0.0429 (0.0504) Data: 0.0253 (0.0207) Loss: 0.0587 (0.0425)\n",
            "TRAIN(133): [180/391] Batch: 0.0579 (0.0505) Data: 0.0154 (0.0207) Loss: 0.1167 (0.0427)\n",
            "TRAIN(133): [190/391] Batch: 0.0625 (0.0505) Data: 0.0176 (0.0206) Loss: 0.0493 (0.0429)\n",
            "TRAIN(133): [200/391] Batch: 0.0484 (0.0506) Data: 0.0202 (0.0204) Loss: 0.0249 (0.0423)\n",
            "TRAIN(133): [210/391] Batch: 0.0484 (0.0506) Data: 0.0192 (0.0202) Loss: 0.0484 (0.0416)\n",
            "TRAIN(133): [220/391] Batch: 0.0534 (0.0505) Data: 0.0148 (0.0201) Loss: 0.0429 (0.0416)\n",
            "TRAIN(133): [230/391] Batch: 0.0468 (0.0505) Data: 0.0204 (0.0200) Loss: 0.0364 (0.0419)\n",
            "TRAIN(133): [240/391] Batch: 0.0486 (0.0504) Data: 0.0210 (0.0199) Loss: 0.0236 (0.0420)\n",
            "TRAIN(133): [250/391] Batch: 0.0538 (0.0505) Data: 0.0137 (0.0198) Loss: 0.0680 (0.0420)\n",
            "TRAIN(133): [260/391] Batch: 0.0461 (0.0505) Data: 0.0276 (0.0198) Loss: 0.0611 (0.0420)\n",
            "TRAIN(133): [270/391] Batch: 0.0399 (0.0505) Data: 0.0241 (0.0198) Loss: 0.0612 (0.0419)\n",
            "TRAIN(133): [280/391] Batch: 0.0465 (0.0504) Data: 0.0251 (0.0199) Loss: 0.0349 (0.0418)\n",
            "TRAIN(133): [290/391] Batch: 0.0460 (0.0503) Data: 0.0266 (0.0200) Loss: 0.0260 (0.0416)\n",
            "TRAIN(133): [300/391] Batch: 0.0460 (0.0503) Data: 0.0252 (0.0200) Loss: 0.0271 (0.0419)\n",
            "TRAIN(133): [310/391] Batch: 0.0374 (0.0502) Data: 0.0248 (0.0200) Loss: 0.0352 (0.0420)\n",
            "TRAIN(133): [320/391] Batch: 0.0441 (0.0501) Data: 0.0252 (0.0200) Loss: 0.0128 (0.0418)\n",
            "TRAIN(133): [330/391] Batch: 0.0458 (0.0501) Data: 0.0262 (0.0200) Loss: 0.0614 (0.0419)\n",
            "TRAIN(133): [340/391] Batch: 0.0461 (0.0501) Data: 0.0244 (0.0200) Loss: 0.0364 (0.0414)\n",
            "TRAIN(133): [350/391] Batch: 0.0560 (0.0501) Data: 0.0133 (0.0199) Loss: 0.0262 (0.0413)\n",
            "TRAIN(133): [360/391] Batch: 0.0475 (0.0501) Data: 0.0152 (0.0199) Loss: 0.0127 (0.0412)\n",
            "TRAIN(133): [370/391] Batch: 0.0609 (0.0501) Data: 0.0164 (0.0198) Loss: 0.0389 (0.0414)\n",
            "TRAIN(133): [380/391] Batch: 0.0449 (0.0501) Data: 0.0186 (0.0197) Loss: 0.0163 (0.0413)\n",
            "TRAIN(133): [390/391] Batch: 0.0468 (0.0500) Data: 0.0268 (0.0198) Loss: 0.0227 (0.0411)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(133)         0:00:19         0:00:07         0:00:11          0.0411\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(134): [ 10/391] Batch: 0.0551 (0.0668) Data: 0.0160 (0.0288) Loss: 0.0451 (0.0292)\n",
            "TRAIN(134): [ 20/391] Batch: 0.0449 (0.0583) Data: 0.0251 (0.0235) Loss: 0.0425 (0.0302)\n",
            "TRAIN(134): [ 30/391] Batch: 0.0520 (0.0553) Data: 0.0236 (0.0226) Loss: 0.0156 (0.0279)\n",
            "TRAIN(134): [ 40/391] Batch: 0.0533 (0.0538) Data: 0.0223 (0.0222) Loss: 0.0638 (0.0290)\n",
            "TRAIN(134): [ 50/391] Batch: 0.0575 (0.0528) Data: 0.0214 (0.0220) Loss: 0.0148 (0.0271)\n",
            "TRAIN(134): [ 60/391] Batch: 0.0490 (0.0520) Data: 0.0245 (0.0219) Loss: 0.0245 (0.0304)\n",
            "TRAIN(134): [ 70/391] Batch: 0.0477 (0.0517) Data: 0.0234 (0.0217) Loss: 0.0164 (0.0322)\n",
            "TRAIN(134): [ 80/391] Batch: 0.0461 (0.0519) Data: 0.0190 (0.0210) Loss: 0.0507 (0.0334)\n",
            "TRAIN(134): [ 90/391] Batch: 0.0536 (0.0518) Data: 0.0131 (0.0202) Loss: 0.0228 (0.0335)\n",
            "TRAIN(134): [100/391] Batch: 0.0579 (0.0519) Data: 0.0138 (0.0198) Loss: 0.0308 (0.0344)\n",
            "TRAIN(134): [110/391] Batch: 0.0534 (0.0518) Data: 0.0187 (0.0194) Loss: 0.0360 (0.0343)\n",
            "TRAIN(134): [120/391] Batch: 0.0480 (0.0515) Data: 0.0196 (0.0193) Loss: 0.0659 (0.0345)\n",
            "TRAIN(134): [130/391] Batch: 0.0461 (0.0514) Data: 0.0175 (0.0189) Loss: 0.0332 (0.0342)\n",
            "TRAIN(134): [140/391] Batch: 0.0462 (0.0511) Data: 0.0274 (0.0191) Loss: 0.0813 (0.0353)\n",
            "TRAIN(134): [150/391] Batch: 0.0449 (0.0510) Data: 0.0238 (0.0192) Loss: 0.0408 (0.0360)\n",
            "TRAIN(134): [160/391] Batch: 0.0619 (0.0510) Data: 0.0150 (0.0192) Loss: 0.0293 (0.0363)\n",
            "TRAIN(134): [170/391] Batch: 0.0524 (0.0510) Data: 0.0152 (0.0189) Loss: 0.0288 (0.0369)\n",
            "TRAIN(134): [180/391] Batch: 0.0450 (0.0509) Data: 0.0190 (0.0190) Loss: 0.0361 (0.0380)\n",
            "TRAIN(134): [190/391] Batch: 0.0478 (0.0507) Data: 0.0258 (0.0191) Loss: 0.0502 (0.0381)\n",
            "TRAIN(134): [200/391] Batch: 0.0456 (0.0506) Data: 0.0242 (0.0191) Loss: 0.0411 (0.0381)\n",
            "TRAIN(134): [210/391] Batch: 0.0506 (0.0506) Data: 0.0220 (0.0190) Loss: 0.0431 (0.0377)\n",
            "TRAIN(134): [220/391] Batch: 0.0502 (0.0507) Data: 0.0126 (0.0189) Loss: 0.0483 (0.0378)\n",
            "TRAIN(134): [230/391] Batch: 0.0470 (0.0506) Data: 0.0259 (0.0190) Loss: 0.0246 (0.0383)\n",
            "TRAIN(134): [240/391] Batch: 0.0581 (0.0506) Data: 0.0157 (0.0191) Loss: 0.0749 (0.0386)\n",
            "TRAIN(134): [250/391] Batch: 0.0467 (0.0504) Data: 0.0259 (0.0192) Loss: 0.0518 (0.0388)\n",
            "TRAIN(134): [260/391] Batch: 0.0511 (0.0504) Data: 0.0253 (0.0193) Loss: 0.0190 (0.0389)\n",
            "TRAIN(134): [270/391] Batch: 0.0512 (0.0504) Data: 0.0159 (0.0193) Loss: 0.0897 (0.0395)\n",
            "TRAIN(134): [280/391] Batch: 0.0506 (0.0504) Data: 0.0163 (0.0193) Loss: 0.0363 (0.0397)\n",
            "TRAIN(134): [290/391] Batch: 0.0465 (0.0503) Data: 0.0271 (0.0194) Loss: 0.1476 (0.0401)\n",
            "TRAIN(134): [300/391] Batch: 0.0452 (0.0503) Data: 0.0173 (0.0193) Loss: 0.0574 (0.0404)\n",
            "TRAIN(134): [310/391] Batch: 0.0468 (0.0503) Data: 0.0224 (0.0192) Loss: 0.0189 (0.0404)\n",
            "TRAIN(134): [320/391] Batch: 0.0534 (0.0503) Data: 0.0250 (0.0191) Loss: 0.0485 (0.0411)\n",
            "TRAIN(134): [330/391] Batch: 0.0401 (0.0503) Data: 0.0266 (0.0191) Loss: 0.0956 (0.0412)\n",
            "TRAIN(134): [340/391] Batch: 0.0459 (0.0503) Data: 0.0204 (0.0190) Loss: 0.0377 (0.0412)\n",
            "TRAIN(134): [350/391] Batch: 0.0529 (0.0504) Data: 0.0153 (0.0190) Loss: 0.0135 (0.0416)\n",
            "TRAIN(134): [360/391] Batch: 0.0628 (0.0505) Data: 0.0149 (0.0189) Loss: 0.0664 (0.0419)\n",
            "TRAIN(134): [370/391] Batch: 0.0485 (0.0505) Data: 0.0194 (0.0188) Loss: 0.0662 (0.0422)\n",
            "TRAIN(134): [380/391] Batch: 0.0538 (0.0505) Data: 0.0146 (0.0188) Loss: 0.0398 (0.0422)\n",
            "TRAIN(134): [390/391] Batch: 0.0457 (0.0505) Data: 0.0271 (0.0188) Loss: 0.0648 (0.0423)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(134)         0:00:19         0:00:07         0:00:12          0.0423\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(135): [ 10/391] Batch: 0.0444 (0.0689) Data: 0.0260 (0.0332) Loss: 0.0432 (0.0449)\n",
            "TRAIN(135): [ 20/391] Batch: 0.0538 (0.0596) Data: 0.0155 (0.0264) Loss: 0.0295 (0.0481)\n",
            "TRAIN(135): [ 30/391] Batch: 0.0403 (0.0559) Data: 0.0231 (0.0241) Loss: 0.0578 (0.0499)\n",
            "TRAIN(135): [ 40/391] Batch: 0.0470 (0.0541) Data: 0.0266 (0.0235) Loss: 0.0909 (0.0512)\n",
            "TRAIN(135): [ 50/391] Batch: 0.0505 (0.0534) Data: 0.0161 (0.0224) Loss: 0.0402 (0.0502)\n",
            "TRAIN(135): [ 60/391] Batch: 0.0465 (0.0528) Data: 0.0248 (0.0219) Loss: 0.0307 (0.0511)\n",
            "TRAIN(135): [ 70/391] Batch: 0.0508 (0.0521) Data: 0.0213 (0.0219) Loss: 0.0556 (0.0515)\n",
            "TRAIN(135): [ 80/391] Batch: 0.0427 (0.0519) Data: 0.0178 (0.0215) Loss: 0.0394 (0.0521)\n",
            "TRAIN(135): [ 90/391] Batch: 0.0543 (0.0517) Data: 0.0239 (0.0214) Loss: 0.0434 (0.0505)\n",
            "TRAIN(135): [100/391] Batch: 0.0384 (0.0514) Data: 0.0281 (0.0211) Loss: 0.0337 (0.0495)\n",
            "TRAIN(135): [110/391] Batch: 0.0658 (0.0512) Data: 0.0153 (0.0212) Loss: 0.0479 (0.0485)\n",
            "TRAIN(135): [120/391] Batch: 0.0549 (0.0509) Data: 0.0139 (0.0211) Loss: 0.0278 (0.0481)\n",
            "TRAIN(135): [130/391] Batch: 0.0475 (0.0505) Data: 0.0271 (0.0213) Loss: 0.0221 (0.0479)\n",
            "TRAIN(135): [140/391] Batch: 0.0457 (0.0504) Data: 0.0257 (0.0213) Loss: 0.0359 (0.0476)\n",
            "TRAIN(135): [150/391] Batch: 0.0494 (0.0503) Data: 0.0245 (0.0214) Loss: 0.0170 (0.0478)\n",
            "TRAIN(135): [160/391] Batch: 0.0454 (0.0502) Data: 0.0236 (0.0214) Loss: 0.0375 (0.0484)\n",
            "TRAIN(135): [170/391] Batch: 0.0571 (0.0503) Data: 0.0197 (0.0212) Loss: 0.0541 (0.0478)\n",
            "TRAIN(135): [180/391] Batch: 0.0540 (0.0503) Data: 0.0151 (0.0210) Loss: 0.0941 (0.0478)\n",
            "TRAIN(135): [190/391] Batch: 0.0468 (0.0501) Data: 0.0260 (0.0211) Loss: 0.0538 (0.0480)\n",
            "TRAIN(135): [200/391] Batch: 0.0444 (0.0500) Data: 0.0254 (0.0211) Loss: 0.0123 (0.0480)\n",
            "TRAIN(135): [210/391] Batch: 0.0596 (0.0500) Data: 0.0138 (0.0210) Loss: 0.0256 (0.0480)\n",
            "TRAIN(135): [220/391] Batch: 0.0587 (0.0501) Data: 0.0188 (0.0209) Loss: 0.0526 (0.0479)\n",
            "TRAIN(135): [230/391] Batch: 0.0638 (0.0503) Data: 0.0138 (0.0207) Loss: 0.0595 (0.0481)\n",
            "TRAIN(135): [240/391] Batch: 0.0539 (0.0503) Data: 0.0192 (0.0205) Loss: 0.0432 (0.0483)\n",
            "TRAIN(135): [250/391] Batch: 0.0466 (0.0504) Data: 0.0211 (0.0204) Loss: 0.0262 (0.0482)\n",
            "TRAIN(135): [260/391] Batch: 0.0553 (0.0505) Data: 0.0139 (0.0202) Loss: 0.0703 (0.0483)\n",
            "TRAIN(135): [270/391] Batch: 0.0613 (0.0506) Data: 0.0135 (0.0200) Loss: 0.0379 (0.0483)\n",
            "TRAIN(135): [280/391] Batch: 0.0360 (0.0506) Data: 0.0252 (0.0199) Loss: 0.0400 (0.0482)\n",
            "TRAIN(135): [290/391] Batch: 0.0478 (0.0506) Data: 0.0263 (0.0199) Loss: 0.0976 (0.0477)\n",
            "TRAIN(135): [300/391] Batch: 0.0466 (0.0505) Data: 0.0258 (0.0199) Loss: 0.0342 (0.0472)\n",
            "TRAIN(135): [310/391] Batch: 0.0369 (0.0504) Data: 0.0272 (0.0199) Loss: 0.0543 (0.0469)\n",
            "TRAIN(135): [320/391] Batch: 0.0465 (0.0503) Data: 0.0245 (0.0200) Loss: 0.0506 (0.0467)\n",
            "TRAIN(135): [330/391] Batch: 0.0452 (0.0502) Data: 0.0255 (0.0201) Loss: 0.0543 (0.0464)\n",
            "TRAIN(135): [340/391] Batch: 0.0445 (0.0501) Data: 0.0252 (0.0201) Loss: 0.0252 (0.0463)\n",
            "TRAIN(135): [350/391] Batch: 0.0466 (0.0501) Data: 0.0257 (0.0202) Loss: 0.0165 (0.0460)\n",
            "TRAIN(135): [360/391] Batch: 0.0583 (0.0501) Data: 0.0147 (0.0202) Loss: 0.0259 (0.0459)\n",
            "TRAIN(135): [370/391] Batch: 0.0630 (0.0501) Data: 0.0146 (0.0201) Loss: 0.0327 (0.0459)\n",
            "TRAIN(135): [380/391] Batch: 0.0557 (0.0501) Data: 0.0230 (0.0201) Loss: 0.0281 (0.0457)\n",
            "TRAIN(135): [390/391] Batch: 0.0465 (0.0501) Data: 0.0259 (0.0201) Loss: 0.0283 (0.0456)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(135)         0:00:19         0:00:07         0:00:11          0.0456\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(136): [ 10/391] Batch: 0.0567 (0.0660) Data: 0.0156 (0.0299) Loss: 0.0385 (0.0393)\n",
            "TRAIN(136): [ 20/391] Batch: 0.0516 (0.0577) Data: 0.0244 (0.0252) Loss: 0.0933 (0.0385)\n",
            "TRAIN(136): [ 30/391] Batch: 0.0446 (0.0552) Data: 0.0239 (0.0232) Loss: 0.0439 (0.0417)\n",
            "TRAIN(136): [ 40/391] Batch: 0.0483 (0.0545) Data: 0.0195 (0.0215) Loss: 0.0576 (0.0435)\n",
            "TRAIN(136): [ 50/391] Batch: 0.0491 (0.0540) Data: 0.0237 (0.0211) Loss: 0.0678 (0.0463)\n",
            "TRAIN(136): [ 60/391] Batch: 0.0492 (0.0533) Data: 0.0247 (0.0206) Loss: 0.0177 (0.0459)\n",
            "TRAIN(136): [ 70/391] Batch: 0.0551 (0.0528) Data: 0.0157 (0.0201) Loss: 0.0265 (0.0457)\n",
            "TRAIN(136): [ 80/391] Batch: 0.0523 (0.0526) Data: 0.0198 (0.0200) Loss: 0.0329 (0.0458)\n",
            "TRAIN(136): [ 90/391] Batch: 0.0453 (0.0525) Data: 0.0196 (0.0197) Loss: 0.0175 (0.0456)\n",
            "TRAIN(136): [100/391] Batch: 0.0616 (0.0525) Data: 0.0227 (0.0194) Loss: 0.0487 (0.0463)\n",
            "TRAIN(136): [110/391] Batch: 0.0558 (0.0526) Data: 0.0134 (0.0187) Loss: 0.0809 (0.0467)\n",
            "TRAIN(136): [120/391] Batch: 0.0535 (0.0524) Data: 0.0194 (0.0184) Loss: 0.0217 (0.0468)\n",
            "TRAIN(136): [130/391] Batch: 0.0441 (0.0525) Data: 0.0226 (0.0182) Loss: 0.0155 (0.0473)\n",
            "TRAIN(136): [140/391] Batch: 0.0469 (0.0524) Data: 0.0209 (0.0180) Loss: 0.0158 (0.0473)\n",
            "TRAIN(136): [150/391] Batch: 0.0461 (0.0523) Data: 0.0275 (0.0181) Loss: 0.0184 (0.0465)\n",
            "TRAIN(136): [160/391] Batch: 0.0473 (0.0519) Data: 0.0268 (0.0184) Loss: 0.0545 (0.0462)\n",
            "TRAIN(136): [170/391] Batch: 0.0473 (0.0518) Data: 0.0249 (0.0185) Loss: 0.0465 (0.0458)\n",
            "TRAIN(136): [180/391] Batch: 0.0428 (0.0517) Data: 0.0186 (0.0186) Loss: 0.0700 (0.0453)\n",
            "TRAIN(136): [190/391] Batch: 0.0491 (0.0516) Data: 0.0146 (0.0186) Loss: 0.0125 (0.0446)\n",
            "TRAIN(136): [200/391] Batch: 0.0465 (0.0513) Data: 0.0263 (0.0188) Loss: 0.0639 (0.0445)\n",
            "TRAIN(136): [210/391] Batch: 0.0553 (0.0513) Data: 0.0217 (0.0189) Loss: 0.0326 (0.0447)\n",
            "TRAIN(136): [220/391] Batch: 0.0462 (0.0511) Data: 0.0273 (0.0189) Loss: 0.0893 (0.0444)\n",
            "TRAIN(136): [230/391] Batch: 0.0431 (0.0511) Data: 0.0247 (0.0189) Loss: 0.0584 (0.0442)\n",
            "TRAIN(136): [240/391] Batch: 0.0480 (0.0510) Data: 0.0256 (0.0190) Loss: 0.0687 (0.0440)\n",
            "TRAIN(136): [250/391] Batch: 0.0340 (0.0510) Data: 0.0257 (0.0190) Loss: 0.0730 (0.0439)\n",
            "TRAIN(136): [260/391] Batch: 0.0439 (0.0509) Data: 0.0253 (0.0190) Loss: 0.0379 (0.0438)\n",
            "TRAIN(136): [270/391] Batch: 0.0475 (0.0509) Data: 0.0272 (0.0191) Loss: 0.0451 (0.0440)\n",
            "TRAIN(136): [280/391] Batch: 0.0548 (0.0509) Data: 0.0163 (0.0190) Loss: 0.0448 (0.0436)\n",
            "TRAIN(136): [290/391] Batch: 0.0386 (0.0508) Data: 0.0260 (0.0190) Loss: 0.0467 (0.0434)\n",
            "TRAIN(136): [300/391] Batch: 0.0461 (0.0508) Data: 0.0263 (0.0191) Loss: 0.0463 (0.0431)\n",
            "TRAIN(136): [310/391] Batch: 0.0442 (0.0507) Data: 0.0258 (0.0191) Loss: 0.0083 (0.0428)\n",
            "TRAIN(136): [320/391] Batch: 0.0365 (0.0507) Data: 0.0260 (0.0191) Loss: 0.0572 (0.0428)\n",
            "TRAIN(136): [330/391] Batch: 0.0496 (0.0506) Data: 0.0260 (0.0192) Loss: 0.0519 (0.0425)\n",
            "TRAIN(136): [340/391] Batch: 0.0470 (0.0506) Data: 0.0251 (0.0192) Loss: 0.0351 (0.0422)\n",
            "TRAIN(136): [350/391] Batch: 0.0561 (0.0506) Data: 0.0127 (0.0192) Loss: 0.0208 (0.0420)\n",
            "TRAIN(136): [360/391] Batch: 0.0609 (0.0506) Data: 0.0142 (0.0191) Loss: 0.0581 (0.0420)\n",
            "TRAIN(136): [370/391] Batch: 0.0492 (0.0506) Data: 0.0198 (0.0190) Loss: 0.0167 (0.0419)\n",
            "TRAIN(136): [380/391] Batch: 0.0491 (0.0507) Data: 0.0178 (0.0189) Loss: 0.0280 (0.0417)\n",
            "TRAIN(136): [390/391] Batch: 0.0448 (0.0506) Data: 0.0249 (0.0188) Loss: 0.0184 (0.0418)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(136)         0:00:19         0:00:07         0:00:12          0.0418\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(137): [ 10/391] Batch: 0.0479 (0.0699) Data: 0.0197 (0.0296) Loss: 0.0167 (0.0369)\n",
            "TRAIN(137): [ 20/391] Batch: 0.0587 (0.0615) Data: 0.0151 (0.0222) Loss: 0.0440 (0.0389)\n",
            "TRAIN(137): [ 30/391] Batch: 0.0460 (0.0571) Data: 0.0253 (0.0213) Loss: 0.0469 (0.0431)\n",
            "TRAIN(137): [ 40/391] Batch: 0.0517 (0.0554) Data: 0.0234 (0.0213) Loss: 0.0615 (0.0441)\n",
            "TRAIN(137): [ 50/391] Batch: 0.0445 (0.0538) Data: 0.0269 (0.0213) Loss: 0.0242 (0.0447)\n",
            "TRAIN(137): [ 60/391] Batch: 0.0442 (0.0533) Data: 0.0244 (0.0209) Loss: 0.0596 (0.0431)\n",
            "TRAIN(137): [ 70/391] Batch: 0.0470 (0.0525) Data: 0.0274 (0.0212) Loss: 0.0223 (0.0433)\n",
            "TRAIN(137): [ 80/391] Batch: 0.0530 (0.0524) Data: 0.0157 (0.0208) Loss: 0.0536 (0.0432)\n",
            "TRAIN(137): [ 90/391] Batch: 0.0470 (0.0520) Data: 0.0171 (0.0206) Loss: 0.0185 (0.0428)\n",
            "TRAIN(137): [100/391] Batch: 0.0461 (0.0516) Data: 0.0261 (0.0207) Loss: 0.1337 (0.0433)\n",
            "TRAIN(137): [110/391] Batch: 0.0563 (0.0512) Data: 0.0236 (0.0209) Loss: 0.0337 (0.0429)\n",
            "TRAIN(137): [120/391] Batch: 0.0582 (0.0512) Data: 0.0146 (0.0207) Loss: 0.0246 (0.0422)\n",
            "TRAIN(137): [130/391] Batch: 0.0469 (0.0509) Data: 0.0260 (0.0207) Loss: 0.0546 (0.0429)\n",
            "TRAIN(137): [140/391] Batch: 0.0573 (0.0507) Data: 0.0224 (0.0208) Loss: 0.0538 (0.0429)\n",
            "TRAIN(137): [150/391] Batch: 0.0513 (0.0509) Data: 0.0154 (0.0204) Loss: 0.0612 (0.0424)\n",
            "TRAIN(137): [160/391] Batch: 0.0562 (0.0507) Data: 0.0180 (0.0205) Loss: 0.0093 (0.0431)\n",
            "TRAIN(137): [170/391] Batch: 0.0463 (0.0507) Data: 0.0250 (0.0204) Loss: 0.0824 (0.0437)\n",
            "TRAIN(137): [180/391] Batch: 0.0456 (0.0506) Data: 0.0233 (0.0204) Loss: 0.0404 (0.0440)\n",
            "TRAIN(137): [190/391] Batch: 0.0463 (0.0504) Data: 0.0267 (0.0205) Loss: 0.0175 (0.0441)\n",
            "TRAIN(137): [200/391] Batch: 0.0471 (0.0502) Data: 0.0271 (0.0207) Loss: 0.0810 (0.0447)\n",
            "TRAIN(137): [210/391] Batch: 0.0418 (0.0503) Data: 0.0250 (0.0206) Loss: 0.0534 (0.0448)\n",
            "TRAIN(137): [220/391] Batch: 0.0490 (0.0502) Data: 0.0262 (0.0206) Loss: 0.0446 (0.0450)\n",
            "TRAIN(137): [230/391] Batch: 0.0397 (0.0503) Data: 0.0216 (0.0204) Loss: 0.0702 (0.0448)\n",
            "TRAIN(137): [240/391] Batch: 0.0480 (0.0503) Data: 0.0186 (0.0202) Loss: 0.0489 (0.0445)\n",
            "TRAIN(137): [250/391] Batch: 0.0472 (0.0504) Data: 0.0189 (0.0199) Loss: 0.0291 (0.0445)\n",
            "TRAIN(137): [260/391] Batch: 0.0474 (0.0504) Data: 0.0156 (0.0198) Loss: 0.0165 (0.0449)\n",
            "TRAIN(137): [270/391] Batch: 0.0553 (0.0504) Data: 0.0193 (0.0196) Loss: 0.0193 (0.0446)\n",
            "TRAIN(137): [280/391] Batch: 0.0620 (0.0505) Data: 0.0130 (0.0195) Loss: 0.0291 (0.0440)\n",
            "TRAIN(137): [290/391] Batch: 0.0428 (0.0506) Data: 0.0256 (0.0194) Loss: 0.1077 (0.0447)\n",
            "TRAIN(137): [300/391] Batch: 0.0474 (0.0505) Data: 0.0271 (0.0194) Loss: 0.0568 (0.0445)\n",
            "TRAIN(137): [310/391] Batch: 0.0451 (0.0504) Data: 0.0257 (0.0195) Loss: 0.0401 (0.0442)\n",
            "TRAIN(137): [320/391] Batch: 0.0516 (0.0504) Data: 0.0159 (0.0195) Loss: 0.0614 (0.0440)\n",
            "TRAIN(137): [330/391] Batch: 0.0404 (0.0504) Data: 0.0238 (0.0195) Loss: 0.0865 (0.0440)\n",
            "TRAIN(137): [340/391] Batch: 0.0483 (0.0503) Data: 0.0262 (0.0196) Loss: 0.0432 (0.0438)\n",
            "TRAIN(137): [350/391] Batch: 0.0456 (0.0502) Data: 0.0263 (0.0196) Loss: 0.0148 (0.0436)\n",
            "TRAIN(137): [360/391] Batch: 0.0481 (0.0502) Data: 0.0157 (0.0196) Loss: 0.1130 (0.0435)\n",
            "TRAIN(137): [370/391] Batch: 0.0425 (0.0502) Data: 0.0233 (0.0196) Loss: 0.0221 (0.0433)\n",
            "TRAIN(137): [380/391] Batch: 0.0438 (0.0502) Data: 0.0203 (0.0196) Loss: 0.0143 (0.0430)\n",
            "TRAIN(137): [390/391] Batch: 0.0451 (0.0502) Data: 0.0252 (0.0196) Loss: 0.0398 (0.0428)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(137)         0:00:19         0:00:07         0:00:11          0.0428\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(138): [ 10/391] Batch: 0.0500 (0.0682) Data: 0.0208 (0.0287) Loss: 0.0458 (0.0433)\n",
            "TRAIN(138): [ 20/391] Batch: 0.0529 (0.0585) Data: 0.0157 (0.0255) Loss: 0.0281 (0.0383)\n",
            "TRAIN(138): [ 30/391] Batch: 0.0499 (0.0554) Data: 0.0238 (0.0240) Loss: 0.0186 (0.0367)\n",
            "TRAIN(138): [ 40/391] Batch: 0.0458 (0.0540) Data: 0.0258 (0.0232) Loss: 0.0215 (0.0372)\n",
            "TRAIN(138): [ 50/391] Batch: 0.0458 (0.0530) Data: 0.0251 (0.0227) Loss: 0.0171 (0.0358)\n",
            "TRAIN(138): [ 60/391] Batch: 0.0484 (0.0523) Data: 0.0261 (0.0222) Loss: 0.0542 (0.0367)\n",
            "TRAIN(138): [ 70/391] Batch: 0.0444 (0.0520) Data: 0.0253 (0.0220) Loss: 0.0392 (0.0363)\n",
            "TRAIN(138): [ 80/391] Batch: 0.0484 (0.0520) Data: 0.0160 (0.0213) Loss: 0.0340 (0.0383)\n",
            "TRAIN(138): [ 90/391] Batch: 0.0414 (0.0519) Data: 0.0232 (0.0208) Loss: 0.0364 (0.0391)\n",
            "TRAIN(138): [100/391] Batch: 0.0390 (0.0518) Data: 0.0215 (0.0203) Loss: 0.0644 (0.0400)\n",
            "TRAIN(138): [110/391] Batch: 0.0469 (0.0518) Data: 0.0165 (0.0198) Loss: 0.0182 (0.0391)\n",
            "TRAIN(138): [120/391] Batch: 0.0477 (0.0519) Data: 0.0148 (0.0194) Loss: 0.0883 (0.0403)\n",
            "TRAIN(138): [130/391] Batch: 0.0561 (0.0519) Data: 0.0128 (0.0190) Loss: 0.0138 (0.0394)\n",
            "TRAIN(138): [140/391] Batch: 0.0463 (0.0518) Data: 0.0199 (0.0189) Loss: 0.0438 (0.0389)\n",
            "TRAIN(138): [150/391] Batch: 0.0542 (0.0518) Data: 0.0179 (0.0187) Loss: 0.0161 (0.0388)\n",
            "TRAIN(138): [160/391] Batch: 0.0555 (0.0519) Data: 0.0167 (0.0185) Loss: 0.0106 (0.0392)\n",
            "TRAIN(138): [170/391] Batch: 0.0464 (0.0517) Data: 0.0267 (0.0186) Loss: 0.0261 (0.0398)\n",
            "TRAIN(138): [180/391] Batch: 0.0458 (0.0516) Data: 0.0245 (0.0187) Loss: 0.0169 (0.0399)\n",
            "TRAIN(138): [190/391] Batch: 0.0450 (0.0516) Data: 0.0211 (0.0185) Loss: 0.0216 (0.0395)\n",
            "TRAIN(138): [200/391] Batch: 0.0438 (0.0515) Data: 0.0259 (0.0185) Loss: 0.0321 (0.0392)\n",
            "TRAIN(138): [210/391] Batch: 0.0452 (0.0513) Data: 0.0255 (0.0187) Loss: 0.0076 (0.0391)\n",
            "TRAIN(138): [220/391] Batch: 0.0372 (0.0512) Data: 0.0252 (0.0187) Loss: 0.0356 (0.0389)\n",
            "TRAIN(138): [230/391] Batch: 0.0452 (0.0511) Data: 0.0275 (0.0188) Loss: 0.0585 (0.0386)\n",
            "TRAIN(138): [240/391] Batch: 0.0517 (0.0511) Data: 0.0181 (0.0189) Loss: 0.0173 (0.0383)\n",
            "TRAIN(138): [250/391] Batch: 0.0467 (0.0509) Data: 0.0269 (0.0190) Loss: 0.0471 (0.0383)\n",
            "TRAIN(138): [260/391] Batch: 0.0530 (0.0510) Data: 0.0211 (0.0190) Loss: 0.0743 (0.0382)\n",
            "TRAIN(138): [270/391] Batch: 0.0500 (0.0509) Data: 0.0255 (0.0191) Loss: 0.0458 (0.0383)\n",
            "TRAIN(138): [280/391] Batch: 0.0524 (0.0508) Data: 0.0247 (0.0192) Loss: 0.0371 (0.0385)\n",
            "TRAIN(138): [290/391] Batch: 0.0538 (0.0508) Data: 0.0173 (0.0192) Loss: 0.0907 (0.0387)\n",
            "TRAIN(138): [300/391] Batch: 0.0494 (0.0508) Data: 0.0244 (0.0192) Loss: 0.0571 (0.0386)\n",
            "TRAIN(138): [310/391] Batch: 0.0486 (0.0506) Data: 0.0244 (0.0193) Loss: 0.0312 (0.0389)\n",
            "TRAIN(138): [320/391] Batch: 0.0532 (0.0506) Data: 0.0230 (0.0194) Loss: 0.0953 (0.0393)\n",
            "TRAIN(138): [330/391] Batch: 0.0474 (0.0505) Data: 0.0274 (0.0195) Loss: 0.0436 (0.0392)\n",
            "TRAIN(138): [340/391] Batch: 0.0516 (0.0505) Data: 0.0243 (0.0195) Loss: 0.0407 (0.0392)\n",
            "TRAIN(138): [350/391] Batch: 0.0465 (0.0504) Data: 0.0275 (0.0195) Loss: 0.0232 (0.0401)\n",
            "TRAIN(138): [360/391] Batch: 0.0475 (0.0504) Data: 0.0245 (0.0196) Loss: 0.0057 (0.0400)\n",
            "TRAIN(138): [370/391] Batch: 0.0473 (0.0503) Data: 0.0200 (0.0196) Loss: 0.0297 (0.0399)\n",
            "TRAIN(138): [380/391] Batch: 0.0479 (0.0503) Data: 0.0154 (0.0195) Loss: 0.0165 (0.0402)\n",
            "TRAIN(138): [390/391] Batch: 0.0506 (0.0503) Data: 0.0232 (0.0194) Loss: 0.0833 (0.0407)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(138)         0:00:19         0:00:07         0:00:12          0.0407\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(139): [ 10/391] Batch: 0.0527 (0.0735) Data: 0.0132 (0.0293) Loss: 0.0581 (0.0436)\n",
            "TRAIN(139): [ 20/391] Batch: 0.0628 (0.0635) Data: 0.0124 (0.0220) Loss: 0.0266 (0.0471)\n",
            "TRAIN(139): [ 30/391] Batch: 0.0487 (0.0593) Data: 0.0183 (0.0195) Loss: 0.0141 (0.0471)\n",
            "TRAIN(139): [ 40/391] Batch: 0.0499 (0.0575) Data: 0.0247 (0.0191) Loss: 0.0404 (0.0485)\n",
            "TRAIN(139): [ 50/391] Batch: 0.0467 (0.0557) Data: 0.0250 (0.0194) Loss: 0.0328 (0.0464)\n",
            "TRAIN(139): [ 60/391] Batch: 0.0579 (0.0548) Data: 0.0147 (0.0195) Loss: 0.0316 (0.0468)\n",
            "TRAIN(139): [ 70/391] Batch: 0.0474 (0.0542) Data: 0.0217 (0.0190) Loss: 0.0652 (0.0462)\n",
            "TRAIN(139): [ 80/391] Batch: 0.0580 (0.0538) Data: 0.0159 (0.0190) Loss: 0.0377 (0.0455)\n",
            "TRAIN(139): [ 90/391] Batch: 0.0457 (0.0533) Data: 0.0177 (0.0190) Loss: 0.0182 (0.0444)\n",
            "TRAIN(139): [100/391] Batch: 0.0497 (0.0529) Data: 0.0233 (0.0190) Loss: 0.0531 (0.0445)\n",
            "TRAIN(139): [110/391] Batch: 0.0452 (0.0524) Data: 0.0266 (0.0192) Loss: 0.0517 (0.0437)\n",
            "TRAIN(139): [120/391] Batch: 0.0469 (0.0520) Data: 0.0265 (0.0195) Loss: 0.0296 (0.0427)\n",
            "TRAIN(139): [130/391] Batch: 0.0541 (0.0519) Data: 0.0136 (0.0195) Loss: 0.0502 (0.0421)\n",
            "TRAIN(139): [140/391] Batch: 0.0457 (0.0517) Data: 0.0260 (0.0194) Loss: 0.0230 (0.0426)\n",
            "TRAIN(139): [150/391] Batch: 0.0503 (0.0516) Data: 0.0153 (0.0194) Loss: 0.0233 (0.0420)\n",
            "TRAIN(139): [160/391] Batch: 0.0457 (0.0513) Data: 0.0262 (0.0196) Loss: 0.0130 (0.0416)\n",
            "TRAIN(139): [170/391] Batch: 0.0366 (0.0512) Data: 0.0250 (0.0196) Loss: 0.0578 (0.0421)\n",
            "TRAIN(139): [180/391] Batch: 0.0435 (0.0510) Data: 0.0252 (0.0197) Loss: 0.0466 (0.0427)\n",
            "TRAIN(139): [190/391] Batch: 0.0387 (0.0509) Data: 0.0249 (0.0197) Loss: 0.0483 (0.0428)\n",
            "TRAIN(139): [200/391] Batch: 0.0444 (0.0508) Data: 0.0259 (0.0198) Loss: 0.0249 (0.0429)\n",
            "TRAIN(139): [210/391] Batch: 0.0447 (0.0506) Data: 0.0248 (0.0199) Loss: 0.0349 (0.0425)\n",
            "TRAIN(139): [220/391] Batch: 0.0467 (0.0505) Data: 0.0259 (0.0199) Loss: 0.0125 (0.0420)\n",
            "TRAIN(139): [230/391] Batch: 0.0553 (0.0505) Data: 0.0214 (0.0200) Loss: 0.0091 (0.0412)\n",
            "TRAIN(139): [240/391] Batch: 0.0498 (0.0506) Data: 0.0171 (0.0199) Loss: 0.0549 (0.0407)\n",
            "TRAIN(139): [250/391] Batch: 0.0478 (0.0506) Data: 0.0160 (0.0197) Loss: 0.0368 (0.0407)\n",
            "TRAIN(139): [260/391] Batch: 0.0479 (0.0505) Data: 0.0208 (0.0196) Loss: 0.0352 (0.0405)\n",
            "TRAIN(139): [270/391] Batch: 0.0488 (0.0505) Data: 0.0200 (0.0195) Loss: 0.0211 (0.0404)\n",
            "TRAIN(139): [280/391] Batch: 0.0441 (0.0506) Data: 0.0221 (0.0194) Loss: 0.0484 (0.0409)\n",
            "TRAIN(139): [290/391] Batch: 0.0482 (0.0506) Data: 0.0203 (0.0193) Loss: 0.0276 (0.0410)\n",
            "TRAIN(139): [300/391] Batch: 0.0454 (0.0507) Data: 0.0213 (0.0192) Loss: 0.0170 (0.0409)\n",
            "TRAIN(139): [310/391] Batch: 0.0361 (0.0507) Data: 0.0244 (0.0191) Loss: 0.0220 (0.0410)\n",
            "TRAIN(139): [320/391] Batch: 0.0642 (0.0507) Data: 0.0158 (0.0190) Loss: 0.0134 (0.0410)\n",
            "TRAIN(139): [330/391] Batch: 0.0552 (0.0506) Data: 0.0139 (0.0190) Loss: 0.0517 (0.0410)\n",
            "TRAIN(139): [340/391] Batch: 0.0418 (0.0506) Data: 0.0235 (0.0190) Loss: 0.0551 (0.0413)\n",
            "TRAIN(139): [350/391] Batch: 0.0436 (0.0506) Data: 0.0254 (0.0190) Loss: 0.0249 (0.0412)\n",
            "TRAIN(139): [360/391] Batch: 0.0384 (0.0505) Data: 0.0260 (0.0190) Loss: 0.1066 (0.0413)\n",
            "TRAIN(139): [370/391] Batch: 0.0466 (0.0505) Data: 0.0169 (0.0189) Loss: 0.0156 (0.0412)\n",
            "TRAIN(139): [380/391] Batch: 0.0429 (0.0505) Data: 0.0263 (0.0190) Loss: 0.0130 (0.0410)\n",
            "TRAIN(139): [390/391] Batch: 0.0472 (0.0505) Data: 0.0272 (0.0190) Loss: 0.0298 (0.0411)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(139)         0:00:19         0:00:07         0:00:12          0.0411\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(140): [ 10/391] Batch: 0.0467 (0.0669) Data: 0.0261 (0.0327) Loss: 0.0166 (0.0380)\n",
            "TRAIN(140): [ 20/391] Batch: 0.0488 (0.0577) Data: 0.0252 (0.0275) Loss: 0.0713 (0.0380)\n",
            "TRAIN(140): [ 30/391] Batch: 0.0479 (0.0549) Data: 0.0224 (0.0250) Loss: 0.0447 (0.0369)\n",
            "TRAIN(140): [ 40/391] Batch: 0.0509 (0.0533) Data: 0.0245 (0.0239) Loss: 0.0145 (0.0374)\n",
            "TRAIN(140): [ 50/391] Batch: 0.0414 (0.0526) Data: 0.0251 (0.0230) Loss: 0.0286 (0.0410)\n",
            "TRAIN(140): [ 60/391] Batch: 0.0478 (0.0519) Data: 0.0223 (0.0224) Loss: 0.0316 (0.0403)\n",
            "TRAIN(140): [ 70/391] Batch: 0.0446 (0.0514) Data: 0.0255 (0.0222) Loss: 0.0600 (0.0400)\n",
            "TRAIN(140): [ 80/391] Batch: 0.0444 (0.0511) Data: 0.0261 (0.0220) Loss: 0.0150 (0.0404)\n",
            "TRAIN(140): [ 90/391] Batch: 0.0574 (0.0510) Data: 0.0220 (0.0220) Loss: 0.0146 (0.0398)\n",
            "TRAIN(140): [100/391] Batch: 0.0446 (0.0506) Data: 0.0259 (0.0220) Loss: 0.0198 (0.0392)\n",
            "TRAIN(140): [110/391] Batch: 0.0476 (0.0503) Data: 0.0248 (0.0221) Loss: 0.0688 (0.0392)\n",
            "TRAIN(140): [120/391] Batch: 0.0526 (0.0506) Data: 0.0181 (0.0216) Loss: 0.0717 (0.0389)\n",
            "TRAIN(140): [130/391] Batch: 0.0472 (0.0507) Data: 0.0226 (0.0212) Loss: 0.0274 (0.0391)\n",
            "TRAIN(140): [140/391] Batch: 0.0650 (0.0510) Data: 0.0114 (0.0208) Loss: 0.0722 (0.0398)\n",
            "TRAIN(140): [150/391] Batch: 0.0592 (0.0511) Data: 0.0161 (0.0204) Loss: 0.0707 (0.0420)\n",
            "TRAIN(140): [160/391] Batch: 0.0477 (0.0511) Data: 0.0196 (0.0201) Loss: 0.0147 (0.0420)\n",
            "TRAIN(140): [170/391] Batch: 0.0481 (0.0509) Data: 0.0209 (0.0200) Loss: 0.0332 (0.0414)\n",
            "TRAIN(140): [180/391] Batch: 0.0565 (0.0509) Data: 0.0166 (0.0197) Loss: 0.0139 (0.0418)\n",
            "TRAIN(140): [190/391] Batch: 0.0443 (0.0509) Data: 0.0174 (0.0195) Loss: 0.0456 (0.0413)\n",
            "TRAIN(140): [200/391] Batch: 0.0523 (0.0509) Data: 0.0234 (0.0196) Loss: 0.0517 (0.0415)\n",
            "TRAIN(140): [210/391] Batch: 0.0445 (0.0508) Data: 0.0261 (0.0197) Loss: 0.0233 (0.0414)\n",
            "TRAIN(140): [220/391] Batch: 0.0456 (0.0507) Data: 0.0230 (0.0197) Loss: 0.0309 (0.0414)\n",
            "TRAIN(140): [230/391] Batch: 0.0462 (0.0506) Data: 0.0271 (0.0198) Loss: 0.0517 (0.0413)\n",
            "TRAIN(140): [240/391] Batch: 0.0524 (0.0505) Data: 0.0220 (0.0198) Loss: 0.0348 (0.0409)\n",
            "TRAIN(140): [250/391] Batch: 0.0471 (0.0504) Data: 0.0258 (0.0199) Loss: 0.0226 (0.0405)\n",
            "TRAIN(140): [260/391] Batch: 0.0468 (0.0503) Data: 0.0260 (0.0200) Loss: 0.0288 (0.0401)\n",
            "TRAIN(140): [270/391] Batch: 0.0551 (0.0503) Data: 0.0141 (0.0200) Loss: 0.0157 (0.0398)\n",
            "TRAIN(140): [280/391] Batch: 0.0456 (0.0502) Data: 0.0264 (0.0200) Loss: 0.0148 (0.0394)\n",
            "TRAIN(140): [290/391] Batch: 0.0449 (0.0501) Data: 0.0256 (0.0201) Loss: 0.1018 (0.0397)\n",
            "TRAIN(140): [300/391] Batch: 0.0528 (0.0501) Data: 0.0162 (0.0200) Loss: 0.0200 (0.0393)\n",
            "TRAIN(140): [310/391] Batch: 0.0565 (0.0501) Data: 0.0217 (0.0200) Loss: 0.0666 (0.0392)\n",
            "TRAIN(140): [320/391] Batch: 0.0484 (0.0501) Data: 0.0248 (0.0201) Loss: 0.0335 (0.0391)\n",
            "TRAIN(140): [330/391] Batch: 0.0477 (0.0501) Data: 0.0228 (0.0201) Loss: 0.0103 (0.0391)\n",
            "TRAIN(140): [340/391] Batch: 0.0458 (0.0501) Data: 0.0272 (0.0201) Loss: 0.0252 (0.0391)\n",
            "TRAIN(140): [350/391] Batch: 0.0519 (0.0501) Data: 0.0227 (0.0200) Loss: 0.0197 (0.0390)\n",
            "TRAIN(140): [360/391] Batch: 0.0617 (0.0501) Data: 0.0144 (0.0200) Loss: 0.0898 (0.0388)\n",
            "TRAIN(140): [370/391] Batch: 0.0391 (0.0501) Data: 0.0225 (0.0199) Loss: 0.0538 (0.0388)\n",
            "TRAIN(140): [380/391] Batch: 0.0575 (0.0501) Data: 0.0160 (0.0199) Loss: 0.0483 (0.0388)\n",
            "TRAIN(140): [390/391] Batch: 0.0476 (0.0501) Data: 0.0243 (0.0198) Loss: 0.0483 (0.0388)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(140)         0:00:19         0:00:07         0:00:11          0.0388\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(141): [ 10/391] Batch: 0.0445 (0.0722) Data: 0.0143 (0.0309) Loss: 0.0253 (0.0401)\n",
            "TRAIN(141): [ 20/391] Batch: 0.0600 (0.0627) Data: 0.0142 (0.0229) Loss: 0.0413 (0.0444)\n",
            "TRAIN(141): [ 30/391] Batch: 0.0583 (0.0594) Data: 0.0148 (0.0205) Loss: 0.0140 (0.0414)\n",
            "TRAIN(141): [ 40/391] Batch: 0.0644 (0.0577) Data: 0.0128 (0.0186) Loss: 0.0259 (0.0430)\n",
            "TRAIN(141): [ 50/391] Batch: 0.0567 (0.0567) Data: 0.0211 (0.0180) Loss: 0.0317 (0.0449)\n",
            "TRAIN(141): [ 60/391] Batch: 0.0448 (0.0555) Data: 0.0259 (0.0185) Loss: 0.0279 (0.0451)\n",
            "TRAIN(141): [ 70/391] Batch: 0.0438 (0.0547) Data: 0.0261 (0.0187) Loss: 0.0294 (0.0465)\n",
            "TRAIN(141): [ 80/391] Batch: 0.0653 (0.0540) Data: 0.0144 (0.0190) Loss: 0.0499 (0.0486)\n",
            "TRAIN(141): [ 90/391] Batch: 0.0501 (0.0536) Data: 0.0144 (0.0186) Loss: 0.0670 (0.0505)\n",
            "TRAIN(141): [100/391] Batch: 0.0462 (0.0532) Data: 0.0250 (0.0185) Loss: 0.0063 (0.0503)\n",
            "TRAIN(141): [110/391] Batch: 0.0466 (0.0527) Data: 0.0264 (0.0189) Loss: 0.0805 (0.0507)\n",
            "TRAIN(141): [120/391] Batch: 0.0590 (0.0525) Data: 0.0142 (0.0189) Loss: 0.0646 (0.0509)\n",
            "TRAIN(141): [130/391] Batch: 0.0517 (0.0522) Data: 0.0218 (0.0190) Loss: 0.0239 (0.0506)\n",
            "TRAIN(141): [140/391] Batch: 0.0438 (0.0519) Data: 0.0253 (0.0190) Loss: 0.0714 (0.0510)\n",
            "TRAIN(141): [150/391] Batch: 0.0573 (0.0517) Data: 0.0169 (0.0192) Loss: 0.0503 (0.0499)\n",
            "TRAIN(141): [160/391] Batch: 0.0527 (0.0515) Data: 0.0244 (0.0193) Loss: 0.0165 (0.0490)\n",
            "TRAIN(141): [170/391] Batch: 0.0484 (0.0513) Data: 0.0157 (0.0193) Loss: 0.0717 (0.0489)\n",
            "TRAIN(141): [180/391] Batch: 0.0582 (0.0513) Data: 0.0125 (0.0191) Loss: 0.0461 (0.0490)\n",
            "TRAIN(141): [190/391] Batch: 0.0493 (0.0512) Data: 0.0233 (0.0191) Loss: 0.0093 (0.0481)\n",
            "TRAIN(141): [200/391] Batch: 0.0452 (0.0512) Data: 0.0253 (0.0192) Loss: 0.0497 (0.0485)\n",
            "TRAIN(141): [210/391] Batch: 0.0487 (0.0511) Data: 0.0223 (0.0192) Loss: 0.0193 (0.0480)\n",
            "TRAIN(141): [220/391] Batch: 0.0439 (0.0510) Data: 0.0236 (0.0193) Loss: 0.0311 (0.0473)\n",
            "TRAIN(141): [230/391] Batch: 0.0454 (0.0509) Data: 0.0258 (0.0192) Loss: 0.0390 (0.0468)\n",
            "TRAIN(141): [240/391] Batch: 0.0545 (0.0509) Data: 0.0233 (0.0192) Loss: 0.0806 (0.0469)\n",
            "TRAIN(141): [250/391] Batch: 0.0446 (0.0508) Data: 0.0266 (0.0193) Loss: 0.0518 (0.0469)\n",
            "TRAIN(141): [260/391] Batch: 0.0632 (0.0510) Data: 0.0166 (0.0191) Loss: 0.0340 (0.0469)\n",
            "TRAIN(141): [270/391] Batch: 0.0505 (0.0510) Data: 0.0184 (0.0191) Loss: 0.0557 (0.0471)\n",
            "TRAIN(141): [280/391] Batch: 0.0547 (0.0510) Data: 0.0169 (0.0189) Loss: 0.0237 (0.0471)\n",
            "TRAIN(141): [290/391] Batch: 0.0472 (0.0510) Data: 0.0155 (0.0188) Loss: 0.0232 (0.0466)\n",
            "TRAIN(141): [300/391] Batch: 0.0521 (0.0511) Data: 0.0185 (0.0186) Loss: 0.0402 (0.0462)\n",
            "TRAIN(141): [310/391] Batch: 0.0493 (0.0510) Data: 0.0191 (0.0186) Loss: 0.0420 (0.0458)\n",
            "TRAIN(141): [320/391] Batch: 0.0544 (0.0511) Data: 0.0217 (0.0185) Loss: 0.0322 (0.0462)\n",
            "TRAIN(141): [330/391] Batch: 0.0460 (0.0509) Data: 0.0270 (0.0186) Loss: 0.0130 (0.0459)\n",
            "TRAIN(141): [340/391] Batch: 0.0518 (0.0509) Data: 0.0150 (0.0187) Loss: 0.0552 (0.0462)\n",
            "TRAIN(141): [350/391] Batch: 0.0385 (0.0508) Data: 0.0260 (0.0187) Loss: 0.0153 (0.0461)\n",
            "TRAIN(141): [360/391] Batch: 0.0511 (0.0508) Data: 0.0246 (0.0187) Loss: 0.0117 (0.0458)\n",
            "TRAIN(141): [370/391] Batch: 0.0455 (0.0507) Data: 0.0260 (0.0188) Loss: 0.0315 (0.0454)\n",
            "TRAIN(141): [380/391] Batch: 0.0611 (0.0507) Data: 0.0216 (0.0188) Loss: 0.0366 (0.0452)\n",
            "TRAIN(141): [390/391] Batch: 0.0471 (0.0507) Data: 0.0262 (0.0188) Loss: 0.0129 (0.0449)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(141)         0:00:19         0:00:07         0:00:12          0.0449\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(142): [ 10/391] Batch: 0.0581 (0.0702) Data: 0.0159 (0.0297) Loss: 0.0285 (0.0364)\n",
            "TRAIN(142): [ 20/391] Batch: 0.0521 (0.0601) Data: 0.0188 (0.0248) Loss: 0.0158 (0.0355)\n",
            "TRAIN(142): [ 30/391] Batch: 0.0451 (0.0555) Data: 0.0256 (0.0240) Loss: 0.0284 (0.0436)\n",
            "TRAIN(142): [ 40/391] Batch: 0.0470 (0.0538) Data: 0.0259 (0.0236) Loss: 0.0335 (0.0413)\n",
            "TRAIN(142): [ 50/391] Batch: 0.0526 (0.0528) Data: 0.0192 (0.0227) Loss: 0.0591 (0.0421)\n",
            "TRAIN(142): [ 60/391] Batch: 0.0507 (0.0527) Data: 0.0218 (0.0220) Loss: 0.0592 (0.0414)\n",
            "TRAIN(142): [ 70/391] Batch: 0.0443 (0.0523) Data: 0.0265 (0.0215) Loss: 0.0408 (0.0414)\n",
            "TRAIN(142): [ 80/391] Batch: 0.0475 (0.0518) Data: 0.0234 (0.0214) Loss: 0.0387 (0.0420)\n",
            "TRAIN(142): [ 90/391] Batch: 0.0482 (0.0514) Data: 0.0209 (0.0213) Loss: 0.0424 (0.0419)\n",
            "TRAIN(142): [100/391] Batch: 0.0449 (0.0513) Data: 0.0265 (0.0213) Loss: 0.0243 (0.0414)\n",
            "TRAIN(142): [110/391] Batch: 0.0542 (0.0513) Data: 0.0213 (0.0213) Loss: 0.0234 (0.0408)\n",
            "TRAIN(142): [120/391] Batch: 0.0501 (0.0511) Data: 0.0259 (0.0213) Loss: 0.0434 (0.0402)\n",
            "TRAIN(142): [130/391] Batch: 0.0568 (0.0513) Data: 0.0143 (0.0208) Loss: 0.0228 (0.0402)\n",
            "TRAIN(142): [140/391] Batch: 0.0489 (0.0512) Data: 0.0153 (0.0204) Loss: 0.0283 (0.0406)\n",
            "TRAIN(142): [150/391] Batch: 0.0511 (0.0511) Data: 0.0141 (0.0200) Loss: 0.0061 (0.0406)\n",
            "TRAIN(142): [160/391] Batch: 0.0582 (0.0512) Data: 0.0140 (0.0197) Loss: 0.0669 (0.0415)\n",
            "TRAIN(142): [170/391] Batch: 0.0537 (0.0514) Data: 0.0130 (0.0194) Loss: 0.1306 (0.0433)\n",
            "TRAIN(142): [180/391] Batch: 0.0489 (0.0514) Data: 0.0205 (0.0192) Loss: 0.0214 (0.0446)\n",
            "TRAIN(142): [190/391] Batch: 0.0435 (0.0513) Data: 0.0235 (0.0191) Loss: 0.0116 (0.0447)\n",
            "TRAIN(142): [200/391] Batch: 0.0459 (0.0512) Data: 0.0267 (0.0192) Loss: 0.0481 (0.0448)\n",
            "TRAIN(142): [210/391] Batch: 0.0411 (0.0511) Data: 0.0197 (0.0191) Loss: 0.0310 (0.0445)\n",
            "TRAIN(142): [220/391] Batch: 0.0447 (0.0510) Data: 0.0251 (0.0192) Loss: 0.0569 (0.0448)\n",
            "TRAIN(142): [230/391] Batch: 0.0367 (0.0509) Data: 0.0291 (0.0193) Loss: 0.0686 (0.0452)\n",
            "TRAIN(142): [240/391] Batch: 0.0459 (0.0508) Data: 0.0260 (0.0193) Loss: 0.0373 (0.0455)\n",
            "TRAIN(142): [250/391] Batch: 0.0463 (0.0507) Data: 0.0278 (0.0195) Loss: 0.0344 (0.0458)\n",
            "TRAIN(142): [260/391] Batch: 0.0469 (0.0505) Data: 0.0256 (0.0196) Loss: 0.0624 (0.0460)\n",
            "TRAIN(142): [270/391] Batch: 0.0552 (0.0505) Data: 0.0245 (0.0196) Loss: 0.0668 (0.0462)\n",
            "TRAIN(142): [280/391] Batch: 0.0468 (0.0504) Data: 0.0262 (0.0197) Loss: 0.0475 (0.0465)\n",
            "TRAIN(142): [290/391] Batch: 0.0444 (0.0504) Data: 0.0239 (0.0198) Loss: 0.0913 (0.0467)\n",
            "TRAIN(142): [300/391] Batch: 0.0464 (0.0503) Data: 0.0265 (0.0199) Loss: 0.0427 (0.0468)\n",
            "TRAIN(142): [310/391] Batch: 0.0475 (0.0502) Data: 0.0259 (0.0200) Loss: 0.0161 (0.0465)\n",
            "TRAIN(142): [320/391] Batch: 0.0397 (0.0501) Data: 0.0232 (0.0200) Loss: 0.0099 (0.0462)\n",
            "TRAIN(142): [330/391] Batch: 0.0463 (0.0500) Data: 0.0270 (0.0201) Loss: 0.0134 (0.0463)\n",
            "TRAIN(142): [340/391] Batch: 0.0548 (0.0501) Data: 0.0163 (0.0199) Loss: 0.0296 (0.0459)\n",
            "TRAIN(142): [350/391] Batch: 0.0481 (0.0500) Data: 0.0204 (0.0199) Loss: 0.0160 (0.0457)\n",
            "TRAIN(142): [360/391] Batch: 0.0426 (0.0500) Data: 0.0257 (0.0199) Loss: 0.0411 (0.0457)\n",
            "TRAIN(142): [370/391] Batch: 0.0488 (0.0499) Data: 0.0258 (0.0199) Loss: 0.0283 (0.0456)\n",
            "TRAIN(142): [380/391] Batch: 0.0566 (0.0499) Data: 0.0218 (0.0200) Loss: 0.1115 (0.0457)\n",
            "TRAIN(142): [390/391] Batch: 0.0468 (0.0499) Data: 0.0290 (0.0200) Loss: 0.1571 (0.0458)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(142)         0:00:19         0:00:07         0:00:11          0.0458\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(143): [ 10/391] Batch: 0.0478 (0.0683) Data: 0.0194 (0.0329) Loss: 0.0124 (0.0360)\n",
            "TRAIN(143): [ 20/391] Batch: 0.0500 (0.0617) Data: 0.0139 (0.0230) Loss: 0.0182 (0.0403)\n",
            "TRAIN(143): [ 30/391] Batch: 0.0488 (0.0574) Data: 0.0195 (0.0204) Loss: 0.0231 (0.0382)\n",
            "TRAIN(143): [ 40/391] Batch: 0.0478 (0.0561) Data: 0.0151 (0.0185) Loss: 0.0650 (0.0404)\n",
            "TRAIN(143): [ 50/391] Batch: 0.0553 (0.0553) Data: 0.0177 (0.0182) Loss: 0.0273 (0.0394)\n",
            "TRAIN(143): [ 60/391] Batch: 0.0653 (0.0551) Data: 0.0115 (0.0177) Loss: 0.0228 (0.0394)\n",
            "TRAIN(143): [ 70/391] Batch: 0.0453 (0.0544) Data: 0.0260 (0.0174) Loss: 0.0234 (0.0389)\n",
            "TRAIN(143): [ 80/391] Batch: 0.0469 (0.0538) Data: 0.0259 (0.0178) Loss: 0.0504 (0.0385)\n",
            "TRAIN(143): [ 90/391] Batch: 0.0463 (0.0534) Data: 0.0254 (0.0180) Loss: 0.0081 (0.0377)\n",
            "TRAIN(143): [100/391] Batch: 0.0507 (0.0530) Data: 0.0214 (0.0182) Loss: 0.0338 (0.0391)\n",
            "TRAIN(143): [110/391] Batch: 0.0473 (0.0525) Data: 0.0242 (0.0186) Loss: 0.0443 (0.0391)\n",
            "TRAIN(143): [120/391] Batch: 0.0458 (0.0523) Data: 0.0271 (0.0187) Loss: 0.0229 (0.0393)\n",
            "TRAIN(143): [130/391] Batch: 0.0469 (0.0520) Data: 0.0274 (0.0189) Loss: 0.0521 (0.0393)\n",
            "TRAIN(143): [140/391] Batch: 0.0492 (0.0519) Data: 0.0217 (0.0190) Loss: 0.0249 (0.0390)\n",
            "TRAIN(143): [150/391] Batch: 0.0463 (0.0516) Data: 0.0269 (0.0191) Loss: 0.0823 (0.0398)\n",
            "TRAIN(143): [160/391] Batch: 0.0391 (0.0514) Data: 0.0260 (0.0193) Loss: 0.0688 (0.0407)\n",
            "TRAIN(143): [170/391] Batch: 0.0459 (0.0512) Data: 0.0264 (0.0195) Loss: 0.0526 (0.0408)\n",
            "TRAIN(143): [180/391] Batch: 0.0512 (0.0511) Data: 0.0226 (0.0196) Loss: 0.0833 (0.0420)\n",
            "TRAIN(143): [190/391] Batch: 0.0451 (0.0511) Data: 0.0229 (0.0195) Loss: 0.0373 (0.0429)\n",
            "TRAIN(143): [200/391] Batch: 0.0460 (0.0510) Data: 0.0257 (0.0196) Loss: 0.0226 (0.0432)\n",
            "TRAIN(143): [210/391] Batch: 0.0494 (0.0509) Data: 0.0251 (0.0196) Loss: 0.0427 (0.0437)\n",
            "TRAIN(143): [220/391] Batch: 0.0546 (0.0509) Data: 0.0225 (0.0196) Loss: 0.0466 (0.0437)\n",
            "TRAIN(143): [230/391] Batch: 0.0468 (0.0507) Data: 0.0254 (0.0197) Loss: 0.0323 (0.0433)\n",
            "TRAIN(143): [240/391] Batch: 0.0476 (0.0507) Data: 0.0273 (0.0197) Loss: 0.0461 (0.0433)\n",
            "TRAIN(143): [250/391] Batch: 0.0491 (0.0506) Data: 0.0240 (0.0198) Loss: 0.0427 (0.0434)\n",
            "TRAIN(143): [260/391] Batch: 0.0457 (0.0506) Data: 0.0268 (0.0198) Loss: 0.0210 (0.0430)\n",
            "TRAIN(143): [270/391] Batch: 0.0465 (0.0505) Data: 0.0255 (0.0199) Loss: 0.0105 (0.0425)\n",
            "TRAIN(143): [280/391] Batch: 0.0552 (0.0506) Data: 0.0140 (0.0197) Loss: 0.0160 (0.0427)\n",
            "TRAIN(143): [290/391] Batch: 0.0579 (0.0506) Data: 0.0186 (0.0196) Loss: 0.0224 (0.0424)\n",
            "TRAIN(143): [300/391] Batch: 0.0527 (0.0507) Data: 0.0192 (0.0194) Loss: 0.0161 (0.0426)\n",
            "TRAIN(143): [310/391] Batch: 0.0614 (0.0508) Data: 0.0152 (0.0194) Loss: 0.0613 (0.0425)\n",
            "TRAIN(143): [320/391] Batch: 0.0621 (0.0508) Data: 0.0137 (0.0192) Loss: 0.0304 (0.0424)\n",
            "TRAIN(143): [330/391] Batch: 0.0534 (0.0508) Data: 0.0123 (0.0191) Loss: 0.0413 (0.0423)\n",
            "TRAIN(143): [340/391] Batch: 0.0472 (0.0509) Data: 0.0213 (0.0189) Loss: 0.0585 (0.0422)\n",
            "TRAIN(143): [350/391] Batch: 0.0469 (0.0508) Data: 0.0261 (0.0190) Loss: 0.0414 (0.0421)\n",
            "TRAIN(143): [360/391] Batch: 0.0460 (0.0508) Data: 0.0161 (0.0191) Loss: 0.0499 (0.0422)\n",
            "TRAIN(143): [370/391] Batch: 0.0500 (0.0508) Data: 0.0212 (0.0190) Loss: 0.0091 (0.0423)\n",
            "TRAIN(143): [380/391] Batch: 0.0536 (0.0507) Data: 0.0225 (0.0190) Loss: 0.0197 (0.0422)\n",
            "TRAIN(143): [390/391] Batch: 0.0474 (0.0507) Data: 0.0264 (0.0191) Loss: 0.0186 (0.0419)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(143)         0:00:19         0:00:07         0:00:12          0.0419\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(144): [ 10/391] Batch: 0.0434 (0.0661) Data: 0.0249 (0.0322) Loss: 0.0742 (0.0390)\n",
            "TRAIN(144): [ 20/391] Batch: 0.0522 (0.0594) Data: 0.0222 (0.0250) Loss: 0.0269 (0.0352)\n",
            "TRAIN(144): [ 30/391] Batch: 0.0569 (0.0566) Data: 0.0148 (0.0229) Loss: 0.0710 (0.0356)\n",
            "TRAIN(144): [ 40/391] Batch: 0.0465 (0.0548) Data: 0.0272 (0.0220) Loss: 0.0347 (0.0355)\n",
            "TRAIN(144): [ 50/391] Batch: 0.0547 (0.0542) Data: 0.0214 (0.0211) Loss: 0.0339 (0.0342)\n",
            "TRAIN(144): [ 60/391] Batch: 0.0456 (0.0530) Data: 0.0269 (0.0213) Loss: 0.0438 (0.0331)\n",
            "TRAIN(144): [ 70/391] Batch: 0.0465 (0.0522) Data: 0.0271 (0.0217) Loss: 0.0236 (0.0324)\n",
            "TRAIN(144): [ 80/391] Batch: 0.0458 (0.0516) Data: 0.0247 (0.0215) Loss: 0.0369 (0.0330)\n",
            "TRAIN(144): [ 90/391] Batch: 0.0529 (0.0512) Data: 0.0251 (0.0216) Loss: 0.0388 (0.0323)\n",
            "TRAIN(144): [100/391] Batch: 0.0459 (0.0509) Data: 0.0266 (0.0216) Loss: 0.0575 (0.0328)\n",
            "TRAIN(144): [110/391] Batch: 0.0482 (0.0506) Data: 0.0247 (0.0217) Loss: 0.0319 (0.0317)\n",
            "TRAIN(144): [120/391] Batch: 0.0473 (0.0503) Data: 0.0271 (0.0218) Loss: 0.0130 (0.0318)\n",
            "TRAIN(144): [130/391] Batch: 0.0456 (0.0501) Data: 0.0287 (0.0218) Loss: 0.0232 (0.0330)\n",
            "TRAIN(144): [140/391] Batch: 0.0465 (0.0499) Data: 0.0258 (0.0219) Loss: 0.0236 (0.0325)\n",
            "TRAIN(144): [150/391] Batch: 0.0466 (0.0499) Data: 0.0150 (0.0217) Loss: 0.0241 (0.0327)\n",
            "TRAIN(144): [160/391] Batch: 0.0440 (0.0500) Data: 0.0221 (0.0212) Loss: 0.0304 (0.0329)\n",
            "TRAIN(144): [170/391] Batch: 0.0613 (0.0502) Data: 0.0139 (0.0210) Loss: 0.0774 (0.0336)\n",
            "TRAIN(144): [180/391] Batch: 0.0388 (0.0503) Data: 0.0222 (0.0207) Loss: 0.0510 (0.0340)\n",
            "TRAIN(144): [190/391] Batch: 0.0582 (0.0505) Data: 0.0145 (0.0204) Loss: 0.0256 (0.0346)\n",
            "TRAIN(144): [200/391] Batch: 0.0403 (0.0506) Data: 0.0232 (0.0202) Loss: 0.0443 (0.0346)\n",
            "TRAIN(144): [210/391] Batch: 0.0477 (0.0507) Data: 0.0173 (0.0199) Loss: 0.0123 (0.0344)\n",
            "TRAIN(144): [220/391] Batch: 0.0460 (0.0506) Data: 0.0267 (0.0199) Loss: 0.0396 (0.0349)\n",
            "TRAIN(144): [230/391] Batch: 0.0531 (0.0506) Data: 0.0253 (0.0200) Loss: 0.0250 (0.0355)\n",
            "TRAIN(144): [240/391] Batch: 0.0523 (0.0507) Data: 0.0222 (0.0199) Loss: 0.0171 (0.0355)\n",
            "TRAIN(144): [250/391] Batch: 0.0424 (0.0506) Data: 0.0219 (0.0198) Loss: 0.0616 (0.0355)\n",
            "TRAIN(144): [260/391] Batch: 0.0532 (0.0506) Data: 0.0227 (0.0198) Loss: 0.0324 (0.0354)\n",
            "TRAIN(144): [270/391] Batch: 0.0467 (0.0505) Data: 0.0273 (0.0199) Loss: 0.0481 (0.0360)\n",
            "TRAIN(144): [280/391] Batch: 0.0447 (0.0504) Data: 0.0255 (0.0199) Loss: 0.1021 (0.0359)\n",
            "TRAIN(144): [290/391] Batch: 0.0498 (0.0503) Data: 0.0218 (0.0200) Loss: 0.0610 (0.0362)\n",
            "TRAIN(144): [300/391] Batch: 0.0516 (0.0503) Data: 0.0219 (0.0200) Loss: 0.0287 (0.0360)\n",
            "TRAIN(144): [310/391] Batch: 0.0499 (0.0502) Data: 0.0261 (0.0201) Loss: 0.0363 (0.0359)\n",
            "TRAIN(144): [320/391] Batch: 0.0465 (0.0501) Data: 0.0269 (0.0201) Loss: 0.0271 (0.0357)\n",
            "TRAIN(144): [330/391] Batch: 0.0452 (0.0500) Data: 0.0255 (0.0202) Loss: 0.0689 (0.0358)\n",
            "TRAIN(144): [340/391] Batch: 0.0554 (0.0500) Data: 0.0237 (0.0202) Loss: 0.0319 (0.0357)\n",
            "TRAIN(144): [350/391] Batch: 0.0498 (0.0500) Data: 0.0241 (0.0202) Loss: 0.0480 (0.0354)\n",
            "TRAIN(144): [360/391] Batch: 0.0456 (0.0500) Data: 0.0196 (0.0202) Loss: 0.0437 (0.0351)\n",
            "TRAIN(144): [370/391] Batch: 0.0500 (0.0499) Data: 0.0243 (0.0202) Loss: 0.0084 (0.0351)\n",
            "TRAIN(144): [380/391] Batch: 0.0468 (0.0499) Data: 0.0240 (0.0202) Loss: 0.0203 (0.0350)\n",
            "TRAIN(144): [390/391] Batch: 0.0485 (0.0498) Data: 0.0270 (0.0202) Loss: 0.0035 (0.0349)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(144)         0:00:19         0:00:07         0:00:11          0.0349\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(145): [ 10/391] Batch: 0.0442 (0.0641) Data: 0.0267 (0.0320) Loss: 0.0181 (0.0171)\n",
            "TRAIN(145): [ 20/391] Batch: 0.0477 (0.0562) Data: 0.0248 (0.0267) Loss: 0.0301 (0.0265)\n",
            "TRAIN(145): [ 30/391] Batch: 0.0488 (0.0552) Data: 0.0142 (0.0226) Loss: 0.0214 (0.0304)\n",
            "TRAIN(145): [ 40/391] Batch: 0.0483 (0.0541) Data: 0.0152 (0.0204) Loss: 0.0687 (0.0293)\n",
            "TRAIN(145): [ 50/391] Batch: 0.0470 (0.0533) Data: 0.0153 (0.0191) Loss: 0.0687 (0.0287)\n",
            "TRAIN(145): [ 60/391] Batch: 0.0593 (0.0534) Data: 0.0138 (0.0181) Loss: 0.0306 (0.0296)\n",
            "TRAIN(145): [ 70/391] Batch: 0.0491 (0.0529) Data: 0.0164 (0.0175) Loss: 0.0393 (0.0294)\n",
            "TRAIN(145): [ 80/391] Batch: 0.0533 (0.0532) Data: 0.0164 (0.0172) Loss: 0.0213 (0.0302)\n",
            "TRAIN(145): [ 90/391] Batch: 0.0489 (0.0531) Data: 0.0206 (0.0172) Loss: 0.0354 (0.0306)\n",
            "TRAIN(145): [100/391] Batch: 0.0462 (0.0529) Data: 0.0156 (0.0171) Loss: 0.0336 (0.0311)\n",
            "TRAIN(145): [110/391] Batch: 0.0454 (0.0524) Data: 0.0271 (0.0172) Loss: 0.0374 (0.0315)\n",
            "TRAIN(145): [120/391] Batch: 0.0448 (0.0523) Data: 0.0165 (0.0173) Loss: 0.0730 (0.0316)\n",
            "TRAIN(145): [130/391] Batch: 0.0503 (0.0522) Data: 0.0171 (0.0172) Loss: 0.0608 (0.0323)\n",
            "TRAIN(145): [140/391] Batch: 0.0494 (0.0521) Data: 0.0235 (0.0172) Loss: 0.0178 (0.0318)\n",
            "TRAIN(145): [150/391] Batch: 0.0483 (0.0519) Data: 0.0257 (0.0175) Loss: 0.0126 (0.0322)\n",
            "TRAIN(145): [160/391] Batch: 0.0555 (0.0518) Data: 0.0162 (0.0177) Loss: 0.0600 (0.0325)\n",
            "TRAIN(145): [170/391] Batch: 0.0561 (0.0517) Data: 0.0223 (0.0177) Loss: 0.0240 (0.0324)\n",
            "TRAIN(145): [180/391] Batch: 0.0506 (0.0516) Data: 0.0235 (0.0179) Loss: 0.0574 (0.0331)\n",
            "TRAIN(145): [190/391] Batch: 0.0452 (0.0515) Data: 0.0271 (0.0181) Loss: 0.0114 (0.0334)\n",
            "TRAIN(145): [200/391] Batch: 0.0443 (0.0514) Data: 0.0245 (0.0183) Loss: 0.0362 (0.0330)\n",
            "TRAIN(145): [210/391] Batch: 0.0527 (0.0514) Data: 0.0233 (0.0184) Loss: 0.0473 (0.0335)\n",
            "TRAIN(145): [220/391] Batch: 0.0562 (0.0512) Data: 0.0170 (0.0185) Loss: 0.0553 (0.0338)\n",
            "TRAIN(145): [230/391] Batch: 0.0459 (0.0511) Data: 0.0261 (0.0187) Loss: 0.0095 (0.0342)\n",
            "TRAIN(145): [240/391] Batch: 0.0592 (0.0511) Data: 0.0148 (0.0186) Loss: 0.0331 (0.0342)\n",
            "TRAIN(145): [250/391] Batch: 0.0466 (0.0510) Data: 0.0272 (0.0187) Loss: 0.0181 (0.0343)\n",
            "TRAIN(145): [260/391] Batch: 0.0452 (0.0509) Data: 0.0300 (0.0188) Loss: 0.1017 (0.0347)\n",
            "TRAIN(145): [270/391] Batch: 0.0484 (0.0508) Data: 0.0251 (0.0189) Loss: 0.0245 (0.0351)\n",
            "TRAIN(145): [280/391] Batch: 0.0575 (0.0508) Data: 0.0152 (0.0190) Loss: 0.0217 (0.0351)\n",
            "TRAIN(145): [290/391] Batch: 0.0438 (0.0507) Data: 0.0252 (0.0190) Loss: 0.0246 (0.0354)\n",
            "TRAIN(145): [300/391] Batch: 0.0443 (0.0507) Data: 0.0222 (0.0188) Loss: 0.0641 (0.0359)\n",
            "TRAIN(145): [310/391] Batch: 0.0481 (0.0507) Data: 0.0187 (0.0188) Loss: 0.0477 (0.0363)\n",
            "TRAIN(145): [320/391] Batch: 0.0579 (0.0508) Data: 0.0131 (0.0186) Loss: 0.0425 (0.0366)\n",
            "TRAIN(145): [330/391] Batch: 0.0493 (0.0508) Data: 0.0212 (0.0185) Loss: 0.0178 (0.0368)\n",
            "TRAIN(145): [340/391] Batch: 0.0442 (0.0508) Data: 0.0234 (0.0185) Loss: 0.0252 (0.0370)\n",
            "TRAIN(145): [350/391] Batch: 0.0447 (0.0509) Data: 0.0192 (0.0184) Loss: 0.0793 (0.0373)\n",
            "TRAIN(145): [360/391] Batch: 0.0454 (0.0509) Data: 0.0225 (0.0184) Loss: 0.0718 (0.0378)\n",
            "TRAIN(145): [370/391] Batch: 0.0475 (0.0509) Data: 0.0264 (0.0184) Loss: 0.1118 (0.0383)\n",
            "TRAIN(145): [380/391] Batch: 0.0442 (0.0508) Data: 0.0260 (0.0185) Loss: 0.0338 (0.0383)\n",
            "TRAIN(145): [390/391] Batch: 0.0451 (0.0507) Data: 0.0261 (0.0186) Loss: 0.1302 (0.0391)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(145)         0:00:19         0:00:07         0:00:12          0.0391\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(146): [ 10/391] Batch: 0.0427 (0.0654) Data: 0.0253 (0.0309) Loss: 0.0474 (0.0607)\n",
            "TRAIN(146): [ 20/391] Batch: 0.0493 (0.0571) Data: 0.0246 (0.0260) Loss: 0.0748 (0.0450)\n",
            "TRAIN(146): [ 30/391] Batch: 0.0440 (0.0547) Data: 0.0256 (0.0238) Loss: 0.0603 (0.0473)\n",
            "TRAIN(146): [ 40/391] Batch: 0.0497 (0.0528) Data: 0.0254 (0.0234) Loss: 0.1226 (0.0522)\n",
            "TRAIN(146): [ 50/391] Batch: 0.0573 (0.0520) Data: 0.0166 (0.0231) Loss: 0.0369 (0.0518)\n",
            "TRAIN(146): [ 60/391] Batch: 0.0474 (0.0515) Data: 0.0222 (0.0224) Loss: 0.0707 (0.0513)\n",
            "TRAIN(146): [ 70/391] Batch: 0.0485 (0.0516) Data: 0.0158 (0.0213) Loss: 0.0779 (0.0510)\n",
            "TRAIN(146): [ 80/391] Batch: 0.0584 (0.0514) Data: 0.0189 (0.0212) Loss: 0.0777 (0.0498)\n",
            "TRAIN(146): [ 90/391] Batch: 0.0577 (0.0514) Data: 0.0154 (0.0211) Loss: 0.0427 (0.0499)\n",
            "TRAIN(146): [100/391] Batch: 0.0582 (0.0511) Data: 0.0192 (0.0211) Loss: 0.0316 (0.0493)\n",
            "TRAIN(146): [110/391] Batch: 0.0583 (0.0512) Data: 0.0134 (0.0208) Loss: 0.0649 (0.0492)\n",
            "TRAIN(146): [120/391] Batch: 0.0516 (0.0512) Data: 0.0145 (0.0204) Loss: 0.0738 (0.0489)\n",
            "TRAIN(146): [130/391] Batch: 0.0481 (0.0509) Data: 0.0269 (0.0205) Loss: 0.0556 (0.0483)\n",
            "TRAIN(146): [140/391] Batch: 0.0472 (0.0506) Data: 0.0272 (0.0207) Loss: 0.0733 (0.0482)\n",
            "TRAIN(146): [150/391] Batch: 0.0443 (0.0506) Data: 0.0241 (0.0206) Loss: 0.0310 (0.0486)\n",
            "TRAIN(146): [160/391] Batch: 0.0495 (0.0506) Data: 0.0241 (0.0205) Loss: 0.0202 (0.0484)\n",
            "TRAIN(146): [170/391] Batch: 0.0531 (0.0506) Data: 0.0185 (0.0203) Loss: 0.0230 (0.0485)\n",
            "TRAIN(146): [180/391] Batch: 0.0526 (0.0508) Data: 0.0183 (0.0201) Loss: 0.0361 (0.0487)\n",
            "TRAIN(146): [190/391] Batch: 0.0404 (0.0507) Data: 0.0241 (0.0200) Loss: 0.0386 (0.0478)\n",
            "TRAIN(146): [200/391] Batch: 0.0453 (0.0507) Data: 0.0199 (0.0198) Loss: 0.0351 (0.0476)\n",
            "TRAIN(146): [210/391] Batch: 0.0490 (0.0506) Data: 0.0199 (0.0197) Loss: 0.0086 (0.0480)\n",
            "TRAIN(146): [220/391] Batch: 0.0408 (0.0507) Data: 0.0228 (0.0195) Loss: 0.0367 (0.0475)\n",
            "TRAIN(146): [230/391] Batch: 0.0553 (0.0508) Data: 0.0178 (0.0194) Loss: 0.0257 (0.0471)\n",
            "TRAIN(146): [240/391] Batch: 0.0457 (0.0508) Data: 0.0251 (0.0192) Loss: 0.0727 (0.0466)\n",
            "TRAIN(146): [250/391] Batch: 0.0465 (0.0507) Data: 0.0236 (0.0192) Loss: 0.0555 (0.0468)\n",
            "TRAIN(146): [260/391] Batch: 0.0456 (0.0506) Data: 0.0251 (0.0193) Loss: 0.0634 (0.0471)\n",
            "TRAIN(146): [270/391] Batch: 0.0461 (0.0505) Data: 0.0263 (0.0194) Loss: 0.0590 (0.0467)\n",
            "TRAIN(146): [280/391] Batch: 0.0462 (0.0505) Data: 0.0264 (0.0194) Loss: 0.0248 (0.0465)\n",
            "TRAIN(146): [290/391] Batch: 0.0623 (0.0504) Data: 0.0127 (0.0194) Loss: 0.0291 (0.0463)\n",
            "TRAIN(146): [300/391] Batch: 0.0445 (0.0506) Data: 0.0247 (0.0192) Loss: 0.0181 (0.0460)\n",
            "TRAIN(146): [310/391] Batch: 0.0409 (0.0506) Data: 0.0246 (0.0192) Loss: 0.0764 (0.0462)\n",
            "TRAIN(146): [320/391] Batch: 0.0456 (0.0505) Data: 0.0248 (0.0193) Loss: 0.0521 (0.0461)\n",
            "TRAIN(146): [330/391] Batch: 0.0474 (0.0505) Data: 0.0241 (0.0193) Loss: 0.0348 (0.0462)\n",
            "TRAIN(146): [340/391] Batch: 0.0455 (0.0505) Data: 0.0268 (0.0194) Loss: 0.0337 (0.0461)\n",
            "TRAIN(146): [350/391] Batch: 0.0536 (0.0505) Data: 0.0219 (0.0194) Loss: 0.0117 (0.0457)\n",
            "TRAIN(146): [360/391] Batch: 0.0613 (0.0506) Data: 0.0150 (0.0193) Loss: 0.0299 (0.0458)\n",
            "TRAIN(146): [370/391] Batch: 0.0578 (0.0505) Data: 0.0131 (0.0193) Loss: 0.0673 (0.0459)\n",
            "TRAIN(146): [380/391] Batch: 0.0544 (0.0505) Data: 0.0231 (0.0193) Loss: 0.0510 (0.0458)\n",
            "TRAIN(146): [390/391] Batch: 0.0472 (0.0504) Data: 0.0266 (0.0194) Loss: 0.0610 (0.0457)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(146)         0:00:19         0:00:07         0:00:12          0.0457\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(147): [ 10/391] Batch: 0.0341 (0.0676) Data: 0.0234 (0.0289) Loss: 0.0106 (0.0404)\n",
            "TRAIN(147): [ 20/391] Batch: 0.0494 (0.0573) Data: 0.0270 (0.0263) Loss: 0.0283 (0.0382)\n",
            "TRAIN(147): [ 30/391] Batch: 0.0473 (0.0545) Data: 0.0250 (0.0238) Loss: 0.0544 (0.0385)\n",
            "TRAIN(147): [ 40/391] Batch: 0.0542 (0.0534) Data: 0.0221 (0.0230) Loss: 0.0432 (0.0385)\n",
            "TRAIN(147): [ 50/391] Batch: 0.0526 (0.0533) Data: 0.0172 (0.0217) Loss: 0.0677 (0.0375)\n",
            "TRAIN(147): [ 60/391] Batch: 0.0456 (0.0526) Data: 0.0201 (0.0210) Loss: 0.0752 (0.0398)\n",
            "TRAIN(147): [ 70/391] Batch: 0.0496 (0.0523) Data: 0.0180 (0.0206) Loss: 0.0529 (0.0387)\n",
            "TRAIN(147): [ 80/391] Batch: 0.0472 (0.0523) Data: 0.0147 (0.0197) Loss: 0.0853 (0.0409)\n",
            "TRAIN(147): [ 90/391] Batch: 0.0420 (0.0523) Data: 0.0181 (0.0193) Loss: 0.0564 (0.0421)\n",
            "TRAIN(147): [100/391] Batch: 0.0454 (0.0524) Data: 0.0210 (0.0189) Loss: 0.0511 (0.0416)\n",
            "TRAIN(147): [110/391] Batch: 0.0588 (0.0528) Data: 0.0116 (0.0184) Loss: 0.0347 (0.0444)\n",
            "TRAIN(147): [120/391] Batch: 0.0465 (0.0524) Data: 0.0267 (0.0184) Loss: 0.0645 (0.0444)\n",
            "TRAIN(147): [130/391] Batch: 0.0477 (0.0522) Data: 0.0251 (0.0186) Loss: 0.0253 (0.0443)\n",
            "TRAIN(147): [140/391] Batch: 0.0416 (0.0519) Data: 0.0251 (0.0187) Loss: 0.0891 (0.0459)\n",
            "TRAIN(147): [150/391] Batch: 0.0469 (0.0517) Data: 0.0258 (0.0189) Loss: 0.0359 (0.0456)\n",
            "TRAIN(147): [160/391] Batch: 0.0553 (0.0517) Data: 0.0225 (0.0188) Loss: 0.0584 (0.0453)\n",
            "TRAIN(147): [170/391] Batch: 0.0578 (0.0514) Data: 0.0180 (0.0189) Loss: 0.0235 (0.0447)\n",
            "TRAIN(147): [180/391] Batch: 0.0362 (0.0513) Data: 0.0246 (0.0188) Loss: 0.0823 (0.0444)\n",
            "TRAIN(147): [190/391] Batch: 0.0495 (0.0512) Data: 0.0238 (0.0188) Loss: 0.0313 (0.0443)\n",
            "TRAIN(147): [200/391] Batch: 0.0482 (0.0512) Data: 0.0264 (0.0189) Loss: 0.0439 (0.0442)\n",
            "TRAIN(147): [210/391] Batch: 0.0494 (0.0512) Data: 0.0237 (0.0188) Loss: 0.0157 (0.0442)\n",
            "TRAIN(147): [220/391] Batch: 0.0521 (0.0512) Data: 0.0170 (0.0187) Loss: 0.0657 (0.0445)\n",
            "TRAIN(147): [230/391] Batch: 0.0505 (0.0512) Data: 0.0238 (0.0187) Loss: 0.0645 (0.0450)\n",
            "TRAIN(147): [240/391] Batch: 0.0470 (0.0511) Data: 0.0258 (0.0188) Loss: 0.0381 (0.0451)\n",
            "TRAIN(147): [250/391] Batch: 0.0461 (0.0510) Data: 0.0252 (0.0189) Loss: 0.0565 (0.0454)\n",
            "TRAIN(147): [260/391] Batch: 0.0343 (0.0510) Data: 0.0241 (0.0188) Loss: 0.0333 (0.0452)\n",
            "TRAIN(147): [270/391] Batch: 0.0423 (0.0510) Data: 0.0226 (0.0188) Loss: 0.0558 (0.0457)\n",
            "TRAIN(147): [280/391] Batch: 0.0446 (0.0509) Data: 0.0239 (0.0189) Loss: 0.0228 (0.0456)\n",
            "TRAIN(147): [290/391] Batch: 0.0462 (0.0508) Data: 0.0267 (0.0189) Loss: 0.0298 (0.0455)\n",
            "TRAIN(147): [300/391] Batch: 0.0504 (0.0508) Data: 0.0183 (0.0189) Loss: 0.0274 (0.0451)\n",
            "TRAIN(147): [310/391] Batch: 0.0458 (0.0508) Data: 0.0280 (0.0190) Loss: 0.0321 (0.0447)\n",
            "TRAIN(147): [320/391] Batch: 0.0642 (0.0509) Data: 0.0162 (0.0189) Loss: 0.0214 (0.0449)\n",
            "TRAIN(147): [330/391] Batch: 0.0499 (0.0509) Data: 0.0179 (0.0188) Loss: 0.0682 (0.0452)\n",
            "TRAIN(147): [340/391] Batch: 0.0532 (0.0509) Data: 0.0168 (0.0187) Loss: 0.0350 (0.0451)\n",
            "TRAIN(147): [350/391] Batch: 0.0548 (0.0510) Data: 0.0168 (0.0186) Loss: 0.0540 (0.0452)\n",
            "TRAIN(147): [360/391] Batch: 0.0591 (0.0510) Data: 0.0134 (0.0185) Loss: 0.0541 (0.0447)\n",
            "TRAIN(147): [370/391] Batch: 0.0480 (0.0510) Data: 0.0191 (0.0184) Loss: 0.0516 (0.0449)\n",
            "TRAIN(147): [380/391] Batch: 0.0542 (0.0511) Data: 0.0143 (0.0183) Loss: 0.0210 (0.0447)\n",
            "TRAIN(147): [390/391] Batch: 0.0453 (0.0510) Data: 0.0265 (0.0183) Loss: 0.0472 (0.0447)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(147)         0:00:19         0:00:07         0:00:12          0.0447\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(148): [ 10/391] Batch: 0.0582 (0.0686) Data: 0.0107 (0.0293) Loss: 0.0369 (0.0335)\n",
            "TRAIN(148): [ 20/391] Batch: 0.0472 (0.0579) Data: 0.0250 (0.0257) Loss: 0.0770 (0.0422)\n",
            "TRAIN(148): [ 30/391] Batch: 0.0483 (0.0551) Data: 0.0199 (0.0240) Loss: 0.0299 (0.0418)\n",
            "TRAIN(148): [ 40/391] Batch: 0.0539 (0.0542) Data: 0.0143 (0.0226) Loss: 0.0517 (0.0441)\n",
            "TRAIN(148): [ 50/391] Batch: 0.0528 (0.0536) Data: 0.0227 (0.0215) Loss: 0.0165 (0.0476)\n",
            "TRAIN(148): [ 60/391] Batch: 0.0476 (0.0533) Data: 0.0226 (0.0213) Loss: 0.0218 (0.0490)\n",
            "TRAIN(148): [ 70/391] Batch: 0.0500 (0.0531) Data: 0.0229 (0.0211) Loss: 0.0234 (0.0486)\n",
            "TRAIN(148): [ 80/391] Batch: 0.0518 (0.0529) Data: 0.0229 (0.0210) Loss: 0.0435 (0.0482)\n",
            "TRAIN(148): [ 90/391] Batch: 0.0573 (0.0526) Data: 0.0213 (0.0208) Loss: 0.0284 (0.0481)\n",
            "TRAIN(148): [100/391] Batch: 0.0468 (0.0522) Data: 0.0257 (0.0209) Loss: 0.0572 (0.0469)\n",
            "TRAIN(148): [110/391] Batch: 0.0525 (0.0522) Data: 0.0215 (0.0206) Loss: 0.0323 (0.0471)\n",
            "TRAIN(148): [120/391] Batch: 0.0520 (0.0523) Data: 0.0214 (0.0202) Loss: 0.0131 (0.0463)\n",
            "TRAIN(148): [130/391] Batch: 0.0549 (0.0521) Data: 0.0159 (0.0201) Loss: 0.0140 (0.0451)\n",
            "TRAIN(148): [140/391] Batch: 0.0448 (0.0519) Data: 0.0242 (0.0202) Loss: 0.0547 (0.0453)\n",
            "TRAIN(148): [150/391] Batch: 0.0478 (0.0518) Data: 0.0228 (0.0203) Loss: 0.0328 (0.0451)\n",
            "TRAIN(148): [160/391] Batch: 0.0480 (0.0519) Data: 0.0145 (0.0199) Loss: 0.0189 (0.0446)\n",
            "TRAIN(148): [170/391] Batch: 0.0439 (0.0517) Data: 0.0242 (0.0199) Loss: 0.0185 (0.0439)\n",
            "TRAIN(148): [180/391] Batch: 0.0467 (0.0515) Data: 0.0251 (0.0201) Loss: 0.0733 (0.0439)\n",
            "TRAIN(148): [190/391] Batch: 0.0588 (0.0516) Data: 0.0126 (0.0197) Loss: 0.0313 (0.0436)\n",
            "TRAIN(148): [200/391] Batch: 0.0489 (0.0516) Data: 0.0196 (0.0196) Loss: 0.0537 (0.0437)\n",
            "TRAIN(148): [210/391] Batch: 0.0485 (0.0515) Data: 0.0158 (0.0193) Loss: 0.0188 (0.0438)\n",
            "TRAIN(148): [220/391] Batch: 0.0590 (0.0516) Data: 0.0131 (0.0190) Loss: 0.0413 (0.0435)\n",
            "TRAIN(148): [230/391] Batch: 0.0475 (0.0516) Data: 0.0142 (0.0188) Loss: 0.0188 (0.0436)\n",
            "TRAIN(148): [240/391] Batch: 0.0455 (0.0516) Data: 0.0161 (0.0186) Loss: 0.0334 (0.0439)\n",
            "TRAIN(148): [250/391] Batch: 0.0557 (0.0516) Data: 0.0151 (0.0185) Loss: 0.0584 (0.0439)\n",
            "TRAIN(148): [260/391] Batch: 0.0503 (0.0516) Data: 0.0221 (0.0184) Loss: 0.0704 (0.0440)\n",
            "TRAIN(148): [270/391] Batch: 0.0629 (0.0515) Data: 0.0169 (0.0185) Loss: 0.0212 (0.0444)\n",
            "TRAIN(148): [280/391] Batch: 0.0559 (0.0514) Data: 0.0218 (0.0186) Loss: 0.0190 (0.0437)\n",
            "TRAIN(148): [290/391] Batch: 0.0466 (0.0513) Data: 0.0261 (0.0187) Loss: 0.1643 (0.0440)\n",
            "TRAIN(148): [300/391] Batch: 0.0418 (0.0512) Data: 0.0264 (0.0187) Loss: 0.0330 (0.0439)\n",
            "TRAIN(148): [310/391] Batch: 0.0511 (0.0512) Data: 0.0238 (0.0188) Loss: 0.0321 (0.0437)\n",
            "TRAIN(148): [320/391] Batch: 0.0449 (0.0511) Data: 0.0268 (0.0189) Loss: 0.0557 (0.0438)\n",
            "TRAIN(148): [330/391] Batch: 0.0407 (0.0510) Data: 0.0243 (0.0190) Loss: 0.0392 (0.0436)\n",
            "TRAIN(148): [340/391] Batch: 0.0458 (0.0509) Data: 0.0265 (0.0190) Loss: 0.0427 (0.0437)\n",
            "TRAIN(148): [350/391] Batch: 0.0452 (0.0508) Data: 0.0273 (0.0191) Loss: 0.0206 (0.0435)\n",
            "TRAIN(148): [360/391] Batch: 0.0460 (0.0507) Data: 0.0273 (0.0191) Loss: 0.0384 (0.0433)\n",
            "TRAIN(148): [370/391] Batch: 0.0465 (0.0507) Data: 0.0266 (0.0192) Loss: 0.0191 (0.0429)\n",
            "TRAIN(148): [380/391] Batch: 0.0472 (0.0506) Data: 0.0272 (0.0193) Loss: 0.0500 (0.0429)\n",
            "TRAIN(148): [390/391] Batch: 0.0460 (0.0505) Data: 0.0257 (0.0193) Loss: 0.0482 (0.0429)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(148)         0:00:19         0:00:07         0:00:12          0.0429\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(149): [ 10/391] Batch: 0.0440 (0.0660) Data: 0.0234 (0.0329) Loss: 0.0193 (0.0239)\n",
            "TRAIN(149): [ 20/391] Batch: 0.0479 (0.0573) Data: 0.0261 (0.0278) Loss: 0.0794 (0.0291)\n",
            "TRAIN(149): [ 30/391] Batch: 0.0519 (0.0558) Data: 0.0151 (0.0239) Loss: 0.0250 (0.0302)\n",
            "TRAIN(149): [ 40/391] Batch: 0.0458 (0.0539) Data: 0.0274 (0.0232) Loss: 0.0921 (0.0330)\n",
            "TRAIN(149): [ 50/391] Batch: 0.0443 (0.0528) Data: 0.0253 (0.0229) Loss: 0.0107 (0.0335)\n",
            "TRAIN(149): [ 60/391] Batch: 0.0485 (0.0523) Data: 0.0205 (0.0225) Loss: 0.0125 (0.0342)\n",
            "TRAIN(149): [ 70/391] Batch: 0.0465 (0.0527) Data: 0.0150 (0.0211) Loss: 0.0559 (0.0369)\n",
            "TRAIN(149): [ 80/391] Batch: 0.0510 (0.0526) Data: 0.0187 (0.0203) Loss: 0.0353 (0.0371)\n",
            "TRAIN(149): [ 90/391] Batch: 0.0531 (0.0527) Data: 0.0177 (0.0196) Loss: 0.0438 (0.0374)\n",
            "TRAIN(149): [100/391] Batch: 0.0490 (0.0528) Data: 0.0135 (0.0193) Loss: 0.0265 (0.0381)\n",
            "TRAIN(149): [110/391] Batch: 0.0608 (0.0529) Data: 0.0120 (0.0187) Loss: 0.0252 (0.0375)\n",
            "TRAIN(149): [120/391] Batch: 0.0542 (0.0529) Data: 0.0121 (0.0183) Loss: 0.0432 (0.0380)\n",
            "TRAIN(149): [130/391] Batch: 0.0482 (0.0528) Data: 0.0167 (0.0180) Loss: 0.0239 (0.0383)\n",
            "TRAIN(149): [140/391] Batch: 0.0511 (0.0527) Data: 0.0164 (0.0179) Loss: 0.0376 (0.0381)\n",
            "TRAIN(149): [150/391] Batch: 0.0413 (0.0524) Data: 0.0255 (0.0179) Loss: 0.0289 (0.0380)\n",
            "TRAIN(149): [160/391] Batch: 0.0540 (0.0524) Data: 0.0152 (0.0178) Loss: 0.0221 (0.0379)\n",
            "TRAIN(149): [170/391] Batch: 0.0466 (0.0521) Data: 0.0270 (0.0180) Loss: 0.0223 (0.0382)\n",
            "TRAIN(149): [180/391] Batch: 0.0443 (0.0519) Data: 0.0252 (0.0182) Loss: 0.0246 (0.0386)\n",
            "TRAIN(149): [190/391] Batch: 0.0530 (0.0518) Data: 0.0222 (0.0183) Loss: 0.0224 (0.0387)\n",
            "TRAIN(149): [200/391] Batch: 0.0496 (0.0518) Data: 0.0238 (0.0183) Loss: 0.0684 (0.0387)\n",
            "TRAIN(149): [210/391] Batch: 0.0484 (0.0517) Data: 0.0247 (0.0184) Loss: 0.0371 (0.0385)\n",
            "TRAIN(149): [220/391] Batch: 0.0495 (0.0516) Data: 0.0236 (0.0186) Loss: 0.0482 (0.0396)\n",
            "TRAIN(149): [230/391] Batch: 0.0379 (0.0515) Data: 0.0255 (0.0186) Loss: 0.0872 (0.0397)\n",
            "TRAIN(149): [240/391] Batch: 0.0429 (0.0514) Data: 0.0168 (0.0186) Loss: 0.0784 (0.0401)\n",
            "TRAIN(149): [250/391] Batch: 0.0551 (0.0514) Data: 0.0213 (0.0187) Loss: 0.1270 (0.0408)\n",
            "TRAIN(149): [260/391] Batch: 0.0501 (0.0513) Data: 0.0249 (0.0188) Loss: 0.0581 (0.0412)\n",
            "TRAIN(149): [270/391] Batch: 0.0474 (0.0512) Data: 0.0245 (0.0189) Loss: 0.0435 (0.0417)\n",
            "TRAIN(149): [280/391] Batch: 0.0481 (0.0511) Data: 0.0229 (0.0190) Loss: 0.0574 (0.0423)\n",
            "TRAIN(149): [290/391] Batch: 0.0366 (0.0511) Data: 0.0236 (0.0190) Loss: 0.0580 (0.0422)\n",
            "TRAIN(149): [300/391] Batch: 0.0453 (0.0509) Data: 0.0262 (0.0191) Loss: 0.0393 (0.0425)\n",
            "TRAIN(149): [310/391] Batch: 0.0452 (0.0508) Data: 0.0262 (0.0192) Loss: 0.0210 (0.0426)\n",
            "TRAIN(149): [320/391] Batch: 0.0457 (0.0508) Data: 0.0215 (0.0192) Loss: 0.0545 (0.0428)\n",
            "TRAIN(149): [330/391] Batch: 0.0503 (0.0508) Data: 0.0176 (0.0192) Loss: 0.0339 (0.0430)\n",
            "TRAIN(149): [340/391] Batch: 0.0467 (0.0508) Data: 0.0218 (0.0192) Loss: 0.0225 (0.0430)\n",
            "TRAIN(149): [350/391] Batch: 0.0494 (0.0508) Data: 0.0150 (0.0191) Loss: 0.0428 (0.0429)\n",
            "TRAIN(149): [360/391] Batch: 0.0428 (0.0508) Data: 0.0163 (0.0190) Loss: 0.0065 (0.0425)\n",
            "TRAIN(149): [370/391] Batch: 0.0562 (0.0508) Data: 0.0223 (0.0190) Loss: 0.0310 (0.0424)\n",
            "TRAIN(149): [380/391] Batch: 0.0488 (0.0508) Data: 0.0140 (0.0189) Loss: 0.0282 (0.0424)\n",
            "TRAIN(149): [390/391] Batch: 0.0457 (0.0508) Data: 0.0241 (0.0188) Loss: 0.1323 (0.0423)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(149)         0:00:19         0:00:07         0:00:12          0.0423\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(150): [ 10/391] Batch: 0.0540 (0.0673) Data: 0.0206 (0.0302) Loss: 0.0239 (0.0354)\n",
            "TRAIN(150): [ 20/391] Batch: 0.0517 (0.0572) Data: 0.0254 (0.0262) Loss: 0.0206 (0.0449)\n",
            "TRAIN(150): [ 30/391] Batch: 0.0486 (0.0549) Data: 0.0164 (0.0232) Loss: 0.0587 (0.0477)\n",
            "TRAIN(150): [ 40/391] Batch: 0.0486 (0.0536) Data: 0.0250 (0.0225) Loss: 0.0176 (0.0476)\n",
            "TRAIN(150): [ 50/391] Batch: 0.0611 (0.0530) Data: 0.0147 (0.0218) Loss: 0.0524 (0.0489)\n",
            "TRAIN(150): [ 60/391] Batch: 0.0424 (0.0521) Data: 0.0270 (0.0216) Loss: 0.0234 (0.0469)\n",
            "TRAIN(150): [ 70/391] Batch: 0.0504 (0.0516) Data: 0.0253 (0.0218) Loss: 0.0319 (0.0454)\n",
            "TRAIN(150): [ 80/391] Batch: 0.0367 (0.0513) Data: 0.0249 (0.0216) Loss: 0.0569 (0.0440)\n",
            "TRAIN(150): [ 90/391] Batch: 0.0466 (0.0510) Data: 0.0256 (0.0216) Loss: 0.0233 (0.0427)\n",
            "TRAIN(150): [100/391] Batch: 0.0460 (0.0507) Data: 0.0253 (0.0217) Loss: 0.0181 (0.0415)\n",
            "TRAIN(150): [110/391] Batch: 0.0441 (0.0504) Data: 0.0271 (0.0218) Loss: 0.0556 (0.0414)\n",
            "TRAIN(150): [120/391] Batch: 0.0392 (0.0502) Data: 0.0249 (0.0219) Loss: 0.0261 (0.0413)\n",
            "TRAIN(150): [130/391] Batch: 0.0479 (0.0501) Data: 0.0155 (0.0217) Loss: 0.0334 (0.0408)\n",
            "TRAIN(150): [140/391] Batch: 0.0578 (0.0501) Data: 0.0222 (0.0216) Loss: 0.0203 (0.0408)\n",
            "TRAIN(150): [150/391] Batch: 0.0473 (0.0499) Data: 0.0249 (0.0215) Loss: 0.0258 (0.0409)\n",
            "TRAIN(150): [160/391] Batch: 0.0520 (0.0499) Data: 0.0232 (0.0215) Loss: 0.0250 (0.0413)\n",
            "TRAIN(150): [170/391] Batch: 0.0476 (0.0499) Data: 0.0229 (0.0214) Loss: 0.0624 (0.0411)\n",
            "TRAIN(150): [180/391] Batch: 0.0519 (0.0499) Data: 0.0159 (0.0213) Loss: 0.0298 (0.0408)\n",
            "TRAIN(150): [190/391] Batch: 0.0572 (0.0500) Data: 0.0144 (0.0212) Loss: 0.0497 (0.0415)\n",
            "TRAIN(150): [200/391] Batch: 0.0478 (0.0499) Data: 0.0261 (0.0212) Loss: 0.0628 (0.0417)\n",
            "TRAIN(150): [210/391] Batch: 0.0611 (0.0500) Data: 0.0130 (0.0208) Loss: 0.0184 (0.0418)\n",
            "TRAIN(150): [220/391] Batch: 0.0527 (0.0501) Data: 0.0142 (0.0205) Loss: 0.0797 (0.0420)\n",
            "TRAIN(150): [230/391] Batch: 0.0554 (0.0501) Data: 0.0179 (0.0202) Loss: 0.0172 (0.0425)\n",
            "TRAIN(150): [240/391] Batch: 0.0604 (0.0502) Data: 0.0132 (0.0200) Loss: 0.0225 (0.0433)\n",
            "TRAIN(150): [250/391] Batch: 0.0502 (0.0503) Data: 0.0178 (0.0197) Loss: 0.0684 (0.0434)\n",
            "TRAIN(150): [260/391] Batch: 0.0492 (0.0503) Data: 0.0183 (0.0196) Loss: 0.0275 (0.0437)\n",
            "TRAIN(150): [270/391] Batch: 0.0523 (0.0505) Data: 0.0135 (0.0194) Loss: 0.0595 (0.0434)\n",
            "TRAIN(150): [280/391] Batch: 0.0353 (0.0504) Data: 0.0265 (0.0194) Loss: 0.0368 (0.0437)\n",
            "TRAIN(150): [290/391] Batch: 0.0476 (0.0504) Data: 0.0254 (0.0194) Loss: 0.0315 (0.0439)\n",
            "TRAIN(150): [300/391] Batch: 0.0453 (0.0503) Data: 0.0249 (0.0195) Loss: 0.0462 (0.0448)\n",
            "TRAIN(150): [310/391] Batch: 0.0462 (0.0503) Data: 0.0266 (0.0195) Loss: 0.1485 (0.0453)\n",
            "TRAIN(150): [320/391] Batch: 0.0467 (0.0502) Data: 0.0267 (0.0197) Loss: 0.0320 (0.0458)\n",
            "TRAIN(150): [330/391] Batch: 0.0506 (0.0501) Data: 0.0251 (0.0197) Loss: 0.0883 (0.0461)\n",
            "TRAIN(150): [340/391] Batch: 0.0472 (0.0501) Data: 0.0262 (0.0197) Loss: 0.0404 (0.0464)\n",
            "TRAIN(150): [350/391] Batch: 0.0416 (0.0500) Data: 0.0247 (0.0198) Loss: 0.0826 (0.0462)\n",
            "TRAIN(150): [360/391] Batch: 0.0430 (0.0500) Data: 0.0246 (0.0198) Loss: 0.1154 (0.0462)\n",
            "TRAIN(150): [370/391] Batch: 0.0482 (0.0500) Data: 0.0244 (0.0198) Loss: 0.0173 (0.0458)\n",
            "TRAIN(150): [380/391] Batch: 0.0453 (0.0500) Data: 0.0254 (0.0199) Loss: 0.1080 (0.0461)\n",
            "TRAIN(150): [390/391] Batch: 0.0459 (0.0499) Data: 0.0264 (0.0199) Loss: 0.0335 (0.0460)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(150)         0:00:19         0:00:07         0:00:11          0.0460\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(151): [ 10/391] Batch: 0.0521 (0.0647) Data: 0.0127 (0.0362) Loss: 0.0314 (0.0514)\n",
            "TRAIN(151): [ 20/391] Batch: 0.0550 (0.0583) Data: 0.0232 (0.0270) Loss: 0.0230 (0.0413)\n",
            "TRAIN(151): [ 30/391] Batch: 0.0453 (0.0556) Data: 0.0234 (0.0247) Loss: 0.0213 (0.0416)\n",
            "TRAIN(151): [ 40/391] Batch: 0.0563 (0.0543) Data: 0.0224 (0.0238) Loss: 0.0430 (0.0399)\n",
            "TRAIN(151): [ 50/391] Batch: 0.0465 (0.0537) Data: 0.0206 (0.0223) Loss: 0.0298 (0.0391)\n",
            "TRAIN(151): [ 60/391] Batch: 0.0349 (0.0532) Data: 0.0248 (0.0213) Loss: 0.0453 (0.0387)\n",
            "TRAIN(151): [ 70/391] Batch: 0.0469 (0.0524) Data: 0.0263 (0.0214) Loss: 0.0251 (0.0398)\n",
            "TRAIN(151): [ 80/391] Batch: 0.0581 (0.0522) Data: 0.0122 (0.0210) Loss: 0.0449 (0.0405)\n",
            "TRAIN(151): [ 90/391] Batch: 0.0470 (0.0521) Data: 0.0195 (0.0205) Loss: 0.0333 (0.0394)\n",
            "TRAIN(151): [100/391] Batch: 0.0598 (0.0523) Data: 0.0076 (0.0199) Loss: 0.0387 (0.0395)\n",
            "TRAIN(151): [110/391] Batch: 0.0472 (0.0522) Data: 0.0147 (0.0192) Loss: 0.0394 (0.0389)\n",
            "TRAIN(151): [120/391] Batch: 0.0494 (0.0523) Data: 0.0196 (0.0190) Loss: 0.0256 (0.0387)\n",
            "TRAIN(151): [130/391] Batch: 0.0483 (0.0522) Data: 0.0194 (0.0189) Loss: 0.0571 (0.0390)\n",
            "TRAIN(151): [140/391] Batch: 0.0550 (0.0521) Data: 0.0181 (0.0187) Loss: 0.0363 (0.0392)\n",
            "TRAIN(151): [150/391] Batch: 0.0512 (0.0521) Data: 0.0212 (0.0186) Loss: 0.0306 (0.0392)\n",
            "TRAIN(151): [160/391] Batch: 0.0473 (0.0518) Data: 0.0253 (0.0187) Loss: 0.0672 (0.0391)\n",
            "TRAIN(151): [170/391] Batch: 0.0529 (0.0519) Data: 0.0218 (0.0187) Loss: 0.0574 (0.0392)\n",
            "TRAIN(151): [180/391] Batch: 0.0473 (0.0516) Data: 0.0271 (0.0189) Loss: 0.0229 (0.0390)\n",
            "TRAIN(151): [190/391] Batch: 0.0443 (0.0516) Data: 0.0237 (0.0190) Loss: 0.0114 (0.0392)\n",
            "TRAIN(151): [200/391] Batch: 0.0508 (0.0515) Data: 0.0239 (0.0191) Loss: 0.0146 (0.0391)\n",
            "TRAIN(151): [210/391] Batch: 0.0633 (0.0516) Data: 0.0136 (0.0190) Loss: 0.0438 (0.0393)\n",
            "TRAIN(151): [220/391] Batch: 0.0453 (0.0514) Data: 0.0175 (0.0190) Loss: 0.0727 (0.0398)\n",
            "TRAIN(151): [230/391] Batch: 0.0557 (0.0515) Data: 0.0161 (0.0189) Loss: 0.0701 (0.0406)\n",
            "TRAIN(151): [240/391] Batch: 0.0468 (0.0513) Data: 0.0263 (0.0190) Loss: 0.0338 (0.0411)\n",
            "TRAIN(151): [250/391] Batch: 0.0589 (0.0513) Data: 0.0154 (0.0189) Loss: 0.0241 (0.0416)\n",
            "TRAIN(151): [260/391] Batch: 0.0504 (0.0512) Data: 0.0235 (0.0188) Loss: 0.0104 (0.0415)\n",
            "TRAIN(151): [270/391] Batch: 0.0472 (0.0512) Data: 0.0264 (0.0188) Loss: 0.0274 (0.0417)\n",
            "TRAIN(151): [280/391] Batch: 0.0509 (0.0512) Data: 0.0164 (0.0188) Loss: 0.0555 (0.0420)\n",
            "TRAIN(151): [290/391] Batch: 0.0461 (0.0511) Data: 0.0270 (0.0189) Loss: 0.0485 (0.0419)\n",
            "TRAIN(151): [300/391] Batch: 0.0475 (0.0510) Data: 0.0259 (0.0190) Loss: 0.0439 (0.0418)\n",
            "TRAIN(151): [310/391] Batch: 0.0496 (0.0509) Data: 0.0231 (0.0191) Loss: 0.0391 (0.0422)\n",
            "TRAIN(151): [320/391] Batch: 0.0445 (0.0509) Data: 0.0172 (0.0190) Loss: 0.0448 (0.0421)\n",
            "TRAIN(151): [330/391] Batch: 0.0361 (0.0508) Data: 0.0267 (0.0190) Loss: 0.0192 (0.0425)\n",
            "TRAIN(151): [340/391] Batch: 0.0553 (0.0508) Data: 0.0219 (0.0191) Loss: 0.0529 (0.0426)\n",
            "TRAIN(151): [350/391] Batch: 0.0429 (0.0507) Data: 0.0204 (0.0190) Loss: 0.0183 (0.0426)\n",
            "TRAIN(151): [360/391] Batch: 0.0588 (0.0508) Data: 0.0136 (0.0189) Loss: 0.0371 (0.0428)\n",
            "TRAIN(151): [370/391] Batch: 0.0462 (0.0508) Data: 0.0214 (0.0189) Loss: 0.0107 (0.0427)\n",
            "TRAIN(151): [380/391] Batch: 0.0481 (0.0508) Data: 0.0185 (0.0188) Loss: 0.0241 (0.0428)\n",
            "TRAIN(151): [390/391] Batch: 0.0451 (0.0507) Data: 0.0245 (0.0188) Loss: 0.1461 (0.0430)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(151)         0:00:19         0:00:07         0:00:12          0.0430\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(152): [ 10/391] Batch: 0.0476 (0.0738) Data: 0.0140 (0.0300) Loss: 0.0273 (0.0384)\n",
            "TRAIN(152): [ 20/391] Batch: 0.0477 (0.0626) Data: 0.0215 (0.0232) Loss: 0.0427 (0.0399)\n",
            "TRAIN(152): [ 30/391] Batch: 0.0468 (0.0577) Data: 0.0275 (0.0224) Loss: 0.0299 (0.0405)\n",
            "TRAIN(152): [ 40/391] Batch: 0.0451 (0.0551) Data: 0.0269 (0.0226) Loss: 0.0464 (0.0403)\n",
            "TRAIN(152): [ 50/391] Batch: 0.0477 (0.0537) Data: 0.0254 (0.0223) Loss: 0.0711 (0.0442)\n",
            "TRAIN(152): [ 60/391] Batch: 0.0459 (0.0528) Data: 0.0273 (0.0222) Loss: 0.0268 (0.0440)\n",
            "TRAIN(152): [ 70/391] Batch: 0.0501 (0.0528) Data: 0.0234 (0.0217) Loss: 0.0554 (0.0439)\n",
            "TRAIN(152): [ 80/391] Batch: 0.0554 (0.0525) Data: 0.0217 (0.0214) Loss: 0.0145 (0.0444)\n",
            "TRAIN(152): [ 90/391] Batch: 0.0460 (0.0525) Data: 0.0137 (0.0208) Loss: 0.0895 (0.0444)\n",
            "TRAIN(152): [100/391] Batch: 0.0486 (0.0522) Data: 0.0240 (0.0206) Loss: 0.0866 (0.0452)\n",
            "TRAIN(152): [110/391] Batch: 0.0508 (0.0519) Data: 0.0230 (0.0207) Loss: 0.0322 (0.0454)\n",
            "TRAIN(152): [120/391] Batch: 0.0437 (0.0516) Data: 0.0257 (0.0207) Loss: 0.0171 (0.0445)\n",
            "TRAIN(152): [130/391] Batch: 0.0457 (0.0513) Data: 0.0269 (0.0208) Loss: 0.0484 (0.0439)\n",
            "TRAIN(152): [140/391] Batch: 0.0351 (0.0513) Data: 0.0258 (0.0206) Loss: 0.0689 (0.0449)\n",
            "TRAIN(152): [150/391] Batch: 0.0503 (0.0512) Data: 0.0240 (0.0206) Loss: 0.0642 (0.0453)\n",
            "TRAIN(152): [160/391] Batch: 0.0539 (0.0511) Data: 0.0219 (0.0206) Loss: 0.0197 (0.0450)\n",
            "TRAIN(152): [170/391] Batch: 0.0554 (0.0511) Data: 0.0143 (0.0204) Loss: 0.0266 (0.0444)\n",
            "TRAIN(152): [180/391] Batch: 0.0474 (0.0508) Data: 0.0259 (0.0205) Loss: 0.0324 (0.0436)\n",
            "TRAIN(152): [190/391] Batch: 0.0479 (0.0507) Data: 0.0248 (0.0206) Loss: 0.0265 (0.0430)\n",
            "TRAIN(152): [200/391] Batch: 0.0567 (0.0507) Data: 0.0151 (0.0205) Loss: 0.0777 (0.0425)\n",
            "TRAIN(152): [210/391] Batch: 0.0449 (0.0504) Data: 0.0263 (0.0206) Loss: 0.0714 (0.0425)\n",
            "TRAIN(152): [220/391] Batch: 0.0548 (0.0505) Data: 0.0175 (0.0205) Loss: 0.0371 (0.0427)\n",
            "TRAIN(152): [230/391] Batch: 0.0605 (0.0506) Data: 0.0125 (0.0203) Loss: 0.0570 (0.0429)\n",
            "TRAIN(152): [240/391] Batch: 0.0466 (0.0506) Data: 0.0147 (0.0201) Loss: 0.0813 (0.0427)\n",
            "TRAIN(152): [250/391] Batch: 0.0610 (0.0506) Data: 0.0114 (0.0199) Loss: 0.0062 (0.0422)\n",
            "TRAIN(152): [260/391] Batch: 0.0470 (0.0505) Data: 0.0191 (0.0198) Loss: 0.0497 (0.0426)\n",
            "TRAIN(152): [270/391] Batch: 0.0438 (0.0506) Data: 0.0216 (0.0196) Loss: 0.0451 (0.0427)\n",
            "TRAIN(152): [280/391] Batch: 0.0532 (0.0507) Data: 0.0154 (0.0195) Loss: 0.0405 (0.0428)\n",
            "TRAIN(152): [290/391] Batch: 0.0455 (0.0508) Data: 0.0245 (0.0194) Loss: 0.0106 (0.0426)\n",
            "TRAIN(152): [300/391] Batch: 0.0453 (0.0508) Data: 0.0219 (0.0194) Loss: 0.0244 (0.0428)\n",
            "TRAIN(152): [310/391] Batch: 0.0462 (0.0507) Data: 0.0255 (0.0193) Loss: 0.0441 (0.0426)\n",
            "TRAIN(152): [320/391] Batch: 0.0488 (0.0506) Data: 0.0218 (0.0193) Loss: 0.0205 (0.0425)\n",
            "TRAIN(152): [330/391] Batch: 0.0450 (0.0505) Data: 0.0180 (0.0193) Loss: 0.0399 (0.0426)\n",
            "TRAIN(152): [340/391] Batch: 0.0537 (0.0506) Data: 0.0220 (0.0193) Loss: 0.0132 (0.0428)\n",
            "TRAIN(152): [350/391] Batch: 0.0467 (0.0506) Data: 0.0215 (0.0193) Loss: 0.0515 (0.0428)\n",
            "TRAIN(152): [360/391] Batch: 0.0486 (0.0506) Data: 0.0253 (0.0193) Loss: 0.0418 (0.0430)\n",
            "TRAIN(152): [370/391] Batch: 0.0487 (0.0505) Data: 0.0250 (0.0193) Loss: 0.0935 (0.0433)\n",
            "TRAIN(152): [380/391] Batch: 0.0478 (0.0504) Data: 0.0265 (0.0193) Loss: 0.0410 (0.0434)\n",
            "TRAIN(152): [390/391] Batch: 0.0463 (0.0504) Data: 0.0280 (0.0193) Loss: 0.0755 (0.0437)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(152)         0:00:19         0:00:07         0:00:12          0.0437\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(153): [ 10/391] Batch: 0.0446 (0.0645) Data: 0.0261 (0.0309) Loss: 0.0491 (0.0403)\n",
            "TRAIN(153): [ 20/391] Batch: 0.0539 (0.0574) Data: 0.0238 (0.0266) Loss: 0.0310 (0.0424)\n",
            "TRAIN(153): [ 30/391] Batch: 0.0506 (0.0546) Data: 0.0236 (0.0241) Loss: 0.0282 (0.0388)\n",
            "TRAIN(153): [ 40/391] Batch: 0.0470 (0.0532) Data: 0.0265 (0.0232) Loss: 0.0593 (0.0382)\n",
            "TRAIN(153): [ 50/391] Batch: 0.0476 (0.0519) Data: 0.0249 (0.0229) Loss: 0.0135 (0.0408)\n",
            "TRAIN(153): [ 60/391] Batch: 0.0490 (0.0520) Data: 0.0154 (0.0221) Loss: 0.0643 (0.0394)\n",
            "TRAIN(153): [ 70/391] Batch: 0.0470 (0.0511) Data: 0.0271 (0.0222) Loss: 0.0188 (0.0394)\n",
            "TRAIN(153): [ 80/391] Batch: 0.0458 (0.0510) Data: 0.0236 (0.0221) Loss: 0.0520 (0.0391)\n",
            "TRAIN(153): [ 90/391] Batch: 0.0433 (0.0510) Data: 0.0243 (0.0217) Loss: 0.0263 (0.0384)\n",
            "TRAIN(153): [100/391] Batch: 0.0549 (0.0511) Data: 0.0143 (0.0211) Loss: 0.0345 (0.0375)\n",
            "TRAIN(153): [110/391] Batch: 0.0529 (0.0513) Data: 0.0134 (0.0205) Loss: 0.0342 (0.0374)\n",
            "TRAIN(153): [120/391] Batch: 0.0469 (0.0512) Data: 0.0151 (0.0201) Loss: 0.0231 (0.0377)\n",
            "TRAIN(153): [130/391] Batch: 0.0533 (0.0512) Data: 0.0181 (0.0198) Loss: 0.0352 (0.0374)\n",
            "TRAIN(153): [140/391] Batch: 0.0564 (0.0513) Data: 0.0135 (0.0195) Loss: 0.0388 (0.0375)\n",
            "TRAIN(153): [150/391] Batch: 0.0490 (0.0514) Data: 0.0191 (0.0195) Loss: 0.0302 (0.0370)\n",
            "TRAIN(153): [160/391] Batch: 0.0558 (0.0515) Data: 0.0142 (0.0193) Loss: 0.0281 (0.0381)\n",
            "TRAIN(153): [170/391] Batch: 0.0457 (0.0512) Data: 0.0281 (0.0195) Loss: 0.0340 (0.0382)\n",
            "TRAIN(153): [180/391] Batch: 0.0476 (0.0511) Data: 0.0251 (0.0196) Loss: 0.0384 (0.0382)\n",
            "TRAIN(153): [190/391] Batch: 0.0473 (0.0510) Data: 0.0162 (0.0195) Loss: 0.1269 (0.0393)\n",
            "TRAIN(153): [200/391] Batch: 0.0570 (0.0511) Data: 0.0131 (0.0193) Loss: 0.0407 (0.0401)\n",
            "TRAIN(153): [210/391] Batch: 0.0451 (0.0510) Data: 0.0221 (0.0192) Loss: 0.0253 (0.0402)\n",
            "TRAIN(153): [220/391] Batch: 0.0448 (0.0509) Data: 0.0261 (0.0192) Loss: 0.0080 (0.0403)\n",
            "TRAIN(153): [230/391] Batch: 0.0481 (0.0507) Data: 0.0257 (0.0193) Loss: 0.0967 (0.0405)\n",
            "TRAIN(153): [240/391] Batch: 0.0466 (0.0506) Data: 0.0266 (0.0194) Loss: 0.0343 (0.0404)\n",
            "TRAIN(153): [250/391] Batch: 0.0433 (0.0505) Data: 0.0248 (0.0195) Loss: 0.0284 (0.0410)\n",
            "TRAIN(153): [260/391] Batch: 0.0469 (0.0504) Data: 0.0263 (0.0196) Loss: 0.0584 (0.0414)\n",
            "TRAIN(153): [270/391] Batch: 0.0497 (0.0504) Data: 0.0157 (0.0195) Loss: 0.1071 (0.0414)\n",
            "TRAIN(153): [280/391] Batch: 0.0464 (0.0503) Data: 0.0263 (0.0195) Loss: 0.0144 (0.0414)\n",
            "TRAIN(153): [290/391] Batch: 0.0430 (0.0503) Data: 0.0184 (0.0194) Loss: 0.0224 (0.0414)\n",
            "TRAIN(153): [300/391] Batch: 0.0532 (0.0503) Data: 0.0155 (0.0194) Loss: 0.0173 (0.0408)\n",
            "TRAIN(153): [310/391] Batch: 0.0556 (0.0503) Data: 0.0147 (0.0194) Loss: 0.0114 (0.0407)\n",
            "TRAIN(153): [320/391] Batch: 0.0455 (0.0502) Data: 0.0269 (0.0195) Loss: 0.0665 (0.0406)\n",
            "TRAIN(153): [330/391] Batch: 0.0450 (0.0501) Data: 0.0263 (0.0195) Loss: 0.0283 (0.0406)\n",
            "TRAIN(153): [340/391] Batch: 0.0520 (0.0502) Data: 0.0243 (0.0195) Loss: 0.0368 (0.0409)\n",
            "TRAIN(153): [350/391] Batch: 0.0494 (0.0502) Data: 0.0260 (0.0195) Loss: 0.0703 (0.0410)\n",
            "TRAIN(153): [360/391] Batch: 0.0495 (0.0502) Data: 0.0238 (0.0195) Loss: 0.0177 (0.0412)\n",
            "TRAIN(153): [370/391] Batch: 0.0476 (0.0503) Data: 0.0192 (0.0194) Loss: 0.0528 (0.0423)\n",
            "TRAIN(153): [380/391] Batch: 0.0474 (0.0502) Data: 0.0213 (0.0194) Loss: 0.0525 (0.0429)\n",
            "TRAIN(153): [390/391] Batch: 0.0467 (0.0502) Data: 0.0259 (0.0194) Loss: 0.0797 (0.0432)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(153)         0:00:19         0:00:07         0:00:12          0.0432\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(154): [ 10/391] Batch: 0.0515 (0.0689) Data: 0.0149 (0.0322) Loss: 0.1490 (0.0621)\n",
            "TRAIN(154): [ 20/391] Batch: 0.0644 (0.0610) Data: 0.0152 (0.0251) Loss: 0.0701 (0.0544)\n",
            "TRAIN(154): [ 30/391] Batch: 0.0542 (0.0578) Data: 0.0160 (0.0217) Loss: 0.0783 (0.0554)\n",
            "TRAIN(154): [ 40/391] Batch: 0.0440 (0.0554) Data: 0.0263 (0.0216) Loss: 0.0950 (0.0598)\n",
            "TRAIN(154): [ 50/391] Batch: 0.0476 (0.0537) Data: 0.0263 (0.0220) Loss: 0.0528 (0.0578)\n",
            "TRAIN(154): [ 60/391] Batch: 0.0444 (0.0527) Data: 0.0256 (0.0218) Loss: 0.1436 (0.0585)\n",
            "TRAIN(154): [ 70/391] Batch: 0.0476 (0.0519) Data: 0.0266 (0.0221) Loss: 0.0201 (0.0559)\n",
            "TRAIN(154): [ 80/391] Batch: 0.0443 (0.0514) Data: 0.0266 (0.0221) Loss: 0.0603 (0.0565)\n",
            "TRAIN(154): [ 90/391] Batch: 0.0653 (0.0513) Data: 0.0146 (0.0218) Loss: 0.0362 (0.0559)\n",
            "TRAIN(154): [100/391] Batch: 0.0478 (0.0509) Data: 0.0257 (0.0219) Loss: 0.0768 (0.0547)\n",
            "TRAIN(154): [110/391] Batch: 0.0423 (0.0507) Data: 0.0252 (0.0219) Loss: 0.0573 (0.0556)\n",
            "TRAIN(154): [120/391] Batch: 0.0434 (0.0506) Data: 0.0269 (0.0217) Loss: 0.0882 (0.0550)\n",
            "TRAIN(154): [130/391] Batch: 0.0529 (0.0507) Data: 0.0234 (0.0216) Loss: 0.0519 (0.0547)\n",
            "TRAIN(154): [140/391] Batch: 0.0448 (0.0504) Data: 0.0270 (0.0215) Loss: 0.0461 (0.0543)\n",
            "TRAIN(154): [150/391] Batch: 0.0533 (0.0503) Data: 0.0249 (0.0217) Loss: 0.1298 (0.0546)\n",
            "TRAIN(154): [160/391] Batch: 0.0457 (0.0501) Data: 0.0264 (0.0217) Loss: 0.1085 (0.0542)\n",
            "TRAIN(154): [170/391] Batch: 0.0596 (0.0502) Data: 0.0117 (0.0214) Loss: 0.0882 (0.0537)\n",
            "TRAIN(154): [180/391] Batch: 0.0460 (0.0500) Data: 0.0270 (0.0214) Loss: 0.0330 (0.0530)\n",
            "TRAIN(154): [190/391] Batch: 0.0460 (0.0499) Data: 0.0271 (0.0215) Loss: 0.1034 (0.0541)\n",
            "TRAIN(154): [200/391] Batch: 0.0584 (0.0498) Data: 0.0174 (0.0215) Loss: 0.0674 (0.0543)\n",
            "TRAIN(154): [210/391] Batch: 0.0469 (0.0498) Data: 0.0262 (0.0215) Loss: 0.0922 (0.0539)\n",
            "TRAIN(154): [220/391] Batch: 0.0470 (0.0497) Data: 0.0260 (0.0215) Loss: 0.0648 (0.0532)\n",
            "TRAIN(154): [230/391] Batch: 0.0461 (0.0496) Data: 0.0261 (0.0216) Loss: 0.0918 (0.0530)\n",
            "TRAIN(154): [240/391] Batch: 0.0512 (0.0496) Data: 0.0133 (0.0213) Loss: 0.0246 (0.0522)\n",
            "TRAIN(154): [250/391] Batch: 0.0402 (0.0496) Data: 0.0238 (0.0212) Loss: 0.0435 (0.0519)\n",
            "TRAIN(154): [260/391] Batch: 0.0521 (0.0497) Data: 0.0138 (0.0211) Loss: 0.0328 (0.0514)\n",
            "TRAIN(154): [270/391] Batch: 0.0447 (0.0498) Data: 0.0185 (0.0209) Loss: 0.0268 (0.0509)\n",
            "TRAIN(154): [280/391] Batch: 0.0451 (0.0498) Data: 0.0213 (0.0207) Loss: 0.0347 (0.0508)\n",
            "TRAIN(154): [290/391] Batch: 0.0438 (0.0499) Data: 0.0212 (0.0206) Loss: 0.0178 (0.0504)\n",
            "TRAIN(154): [300/391] Batch: 0.0497 (0.0500) Data: 0.0152 (0.0204) Loss: 0.0440 (0.0503)\n",
            "TRAIN(154): [310/391] Batch: 0.0471 (0.0499) Data: 0.0284 (0.0205) Loss: 0.0375 (0.0499)\n",
            "TRAIN(154): [320/391] Batch: 0.0557 (0.0500) Data: 0.0166 (0.0204) Loss: 0.0679 (0.0498)\n",
            "TRAIN(154): [330/391] Batch: 0.0448 (0.0499) Data: 0.0273 (0.0204) Loss: 0.0913 (0.0502)\n",
            "TRAIN(154): [340/391] Batch: 0.0457 (0.0499) Data: 0.0265 (0.0204) Loss: 0.0596 (0.0502)\n",
            "TRAIN(154): [350/391] Batch: 0.0548 (0.0498) Data: 0.0206 (0.0204) Loss: 0.0165 (0.0501)\n",
            "TRAIN(154): [360/391] Batch: 0.0595 (0.0499) Data: 0.0151 (0.0203) Loss: 0.0609 (0.0501)\n",
            "TRAIN(154): [370/391] Batch: 0.0420 (0.0499) Data: 0.0199 (0.0203) Loss: 0.0306 (0.0502)\n",
            "TRAIN(154): [380/391] Batch: 0.0511 (0.0499) Data: 0.0163 (0.0202) Loss: 0.0114 (0.0499)\n",
            "TRAIN(154): [390/391] Batch: 0.0462 (0.0498) Data: 0.0286 (0.0203) Loss: 0.0779 (0.0498)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(154)         0:00:19         0:00:07         0:00:11          0.0498\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(155): [ 10/391] Batch: 0.0530 (0.0672) Data: 0.0175 (0.0296) Loss: 0.0100 (0.0376)\n",
            "TRAIN(155): [ 20/391] Batch: 0.0541 (0.0574) Data: 0.0263 (0.0267) Loss: 0.0263 (0.0364)\n",
            "TRAIN(155): [ 30/391] Batch: 0.0484 (0.0539) Data: 0.0238 (0.0255) Loss: 0.0269 (0.0402)\n",
            "TRAIN(155): [ 40/391] Batch: 0.0448 (0.0525) Data: 0.0273 (0.0249) Loss: 0.0401 (0.0393)\n",
            "TRAIN(155): [ 50/391] Batch: 0.0571 (0.0521) Data: 0.0176 (0.0241) Loss: 0.0239 (0.0402)\n",
            "TRAIN(155): [ 60/391] Batch: 0.0572 (0.0516) Data: 0.0235 (0.0235) Loss: 0.0323 (0.0409)\n",
            "TRAIN(155): [ 70/391] Batch: 0.0474 (0.0510) Data: 0.0272 (0.0230) Loss: 0.0284 (0.0391)\n",
            "TRAIN(155): [ 80/391] Batch: 0.0463 (0.0506) Data: 0.0273 (0.0230) Loss: 0.0903 (0.0393)\n",
            "TRAIN(155): [ 90/391] Batch: 0.0540 (0.0503) Data: 0.0203 (0.0229) Loss: 0.0642 (0.0392)\n",
            "TRAIN(155): [100/391] Batch: 0.0471 (0.0498) Data: 0.0276 (0.0230) Loss: 0.0933 (0.0407)\n",
            "TRAIN(155): [110/391] Batch: 0.0436 (0.0498) Data: 0.0261 (0.0229) Loss: 0.0222 (0.0429)\n",
            "TRAIN(155): [120/391] Batch: 0.0503 (0.0501) Data: 0.0152 (0.0221) Loss: 0.0386 (0.0430)\n",
            "TRAIN(155): [130/391] Batch: 0.0495 (0.0501) Data: 0.0206 (0.0217) Loss: 0.0200 (0.0437)\n",
            "TRAIN(155): [140/391] Batch: 0.0403 (0.0502) Data: 0.0222 (0.0214) Loss: 0.0375 (0.0446)\n",
            "TRAIN(155): [150/391] Batch: 0.0461 (0.0504) Data: 0.0143 (0.0210) Loss: 0.0202 (0.0438)\n",
            "TRAIN(155): [160/391] Batch: 0.0477 (0.0505) Data: 0.0203 (0.0206) Loss: 0.0691 (0.0438)\n",
            "TRAIN(155): [170/391] Batch: 0.0532 (0.0504) Data: 0.0148 (0.0203) Loss: 0.0316 (0.0436)\n",
            "TRAIN(155): [180/391] Batch: 0.0464 (0.0505) Data: 0.0246 (0.0201) Loss: 0.0213 (0.0435)\n",
            "TRAIN(155): [190/391] Batch: 0.0455 (0.0503) Data: 0.0278 (0.0202) Loss: 0.0585 (0.0436)\n",
            "TRAIN(155): [200/391] Batch: 0.0452 (0.0502) Data: 0.0252 (0.0203) Loss: 0.0572 (0.0436)\n",
            "TRAIN(155): [210/391] Batch: 0.0518 (0.0503) Data: 0.0171 (0.0201) Loss: 0.0233 (0.0439)\n",
            "TRAIN(155): [220/391] Batch: 0.0478 (0.0502) Data: 0.0253 (0.0202) Loss: 0.0475 (0.0442)\n",
            "TRAIN(155): [230/391] Batch: 0.0468 (0.0501) Data: 0.0268 (0.0203) Loss: 0.0227 (0.0444)\n",
            "TRAIN(155): [240/391] Batch: 0.0436 (0.0501) Data: 0.0252 (0.0203) Loss: 0.0477 (0.0442)\n",
            "TRAIN(155): [250/391] Batch: 0.0472 (0.0500) Data: 0.0254 (0.0204) Loss: 0.0109 (0.0440)\n",
            "TRAIN(155): [260/391] Batch: 0.0460 (0.0501) Data: 0.0178 (0.0203) Loss: 0.0222 (0.0436)\n",
            "TRAIN(155): [270/391] Batch: 0.0474 (0.0500) Data: 0.0259 (0.0204) Loss: 0.1145 (0.0433)\n",
            "TRAIN(155): [280/391] Batch: 0.0581 (0.0500) Data: 0.0154 (0.0204) Loss: 0.0357 (0.0429)\n",
            "TRAIN(155): [290/391] Batch: 0.0466 (0.0498) Data: 0.0266 (0.0205) Loss: 0.0204 (0.0425)\n",
            "TRAIN(155): [300/391] Batch: 0.0478 (0.0498) Data: 0.0275 (0.0206) Loss: 0.0451 (0.0426)\n",
            "TRAIN(155): [310/391] Batch: 0.0457 (0.0498) Data: 0.0275 (0.0206) Loss: 0.0188 (0.0427)\n",
            "TRAIN(155): [320/391] Batch: 0.0461 (0.0497) Data: 0.0287 (0.0206) Loss: 0.0428 (0.0426)\n",
            "TRAIN(155): [330/391] Batch: 0.0461 (0.0496) Data: 0.0259 (0.0206) Loss: 0.0128 (0.0427)\n",
            "TRAIN(155): [340/391] Batch: 0.0398 (0.0496) Data: 0.0268 (0.0207) Loss: 0.0183 (0.0427)\n",
            "TRAIN(155): [350/391] Batch: 0.0468 (0.0495) Data: 0.0280 (0.0207) Loss: 0.0660 (0.0428)\n",
            "TRAIN(155): [360/391] Batch: 0.0467 (0.0495) Data: 0.0282 (0.0207) Loss: 0.0154 (0.0429)\n",
            "TRAIN(155): [370/391] Batch: 0.0493 (0.0495) Data: 0.0179 (0.0207) Loss: 0.0109 (0.0427)\n",
            "TRAIN(155): [380/391] Batch: 0.0402 (0.0495) Data: 0.0240 (0.0206) Loss: 0.0427 (0.0427)\n",
            "TRAIN(155): [390/391] Batch: 0.0454 (0.0495) Data: 0.0249 (0.0206) Loss: 0.0555 (0.0427)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(155)         0:00:19         0:00:08         0:00:11          0.0427\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(156): [ 10/391] Batch: 0.0531 (0.0705) Data: 0.0132 (0.0292) Loss: 0.0474 (0.0458)\n",
            "TRAIN(156): [ 20/391] Batch: 0.0398 (0.0610) Data: 0.0247 (0.0236) Loss: 0.1073 (0.0514)\n",
            "TRAIN(156): [ 30/391] Batch: 0.0415 (0.0578) Data: 0.0159 (0.0213) Loss: 0.0243 (0.0497)\n",
            "TRAIN(156): [ 40/391] Batch: 0.0595 (0.0557) Data: 0.0155 (0.0199) Loss: 0.0550 (0.0479)\n",
            "TRAIN(156): [ 50/391] Batch: 0.0502 (0.0550) Data: 0.0232 (0.0192) Loss: 0.0278 (0.0460)\n",
            "TRAIN(156): [ 60/391] Batch: 0.0474 (0.0537) Data: 0.0279 (0.0197) Loss: 0.0545 (0.0449)\n",
            "TRAIN(156): [ 70/391] Batch: 0.0620 (0.0533) Data: 0.0170 (0.0196) Loss: 0.0068 (0.0427)\n",
            "TRAIN(156): [ 80/391] Batch: 0.0551 (0.0530) Data: 0.0154 (0.0194) Loss: 0.0325 (0.0423)\n",
            "TRAIN(156): [ 90/391] Batch: 0.0454 (0.0524) Data: 0.0263 (0.0197) Loss: 0.0332 (0.0423)\n",
            "TRAIN(156): [100/391] Batch: 0.0462 (0.0519) Data: 0.0275 (0.0199) Loss: 0.0166 (0.0407)\n",
            "TRAIN(156): [110/391] Batch: 0.0610 (0.0517) Data: 0.0154 (0.0199) Loss: 0.0172 (0.0401)\n",
            "TRAIN(156): [120/391] Batch: 0.0457 (0.0513) Data: 0.0275 (0.0202) Loss: 0.0108 (0.0399)\n",
            "TRAIN(156): [130/391] Batch: 0.0457 (0.0512) Data: 0.0223 (0.0202) Loss: 0.0174 (0.0394)\n",
            "TRAIN(156): [140/391] Batch: 0.0520 (0.0511) Data: 0.0252 (0.0202) Loss: 0.0364 (0.0394)\n",
            "TRAIN(156): [150/391] Batch: 0.0448 (0.0509) Data: 0.0258 (0.0203) Loss: 0.0124 (0.0385)\n",
            "TRAIN(156): [160/391] Batch: 0.0464 (0.0506) Data: 0.0263 (0.0205) Loss: 0.0362 (0.0383)\n",
            "TRAIN(156): [170/391] Batch: 0.0476 (0.0505) Data: 0.0240 (0.0206) Loss: 0.0569 (0.0382)\n",
            "TRAIN(156): [180/391] Batch: 0.0476 (0.0503) Data: 0.0254 (0.0207) Loss: 0.0501 (0.0392)\n",
            "TRAIN(156): [190/391] Batch: 0.0455 (0.0501) Data: 0.0265 (0.0207) Loss: 0.0500 (0.0390)\n",
            "TRAIN(156): [200/391] Batch: 0.0473 (0.0500) Data: 0.0267 (0.0208) Loss: 0.0357 (0.0395)\n",
            "TRAIN(156): [210/391] Batch: 0.0475 (0.0499) Data: 0.0260 (0.0210) Loss: 0.0377 (0.0391)\n",
            "TRAIN(156): [220/391] Batch: 0.0460 (0.0498) Data: 0.0240 (0.0210) Loss: 0.0176 (0.0393)\n",
            "TRAIN(156): [230/391] Batch: 0.0468 (0.0498) Data: 0.0273 (0.0211) Loss: 0.0374 (0.0393)\n",
            "TRAIN(156): [240/391] Batch: 0.0473 (0.0498) Data: 0.0204 (0.0210) Loss: 0.0207 (0.0396)\n",
            "TRAIN(156): [250/391] Batch: 0.0450 (0.0498) Data: 0.0259 (0.0209) Loss: 0.0450 (0.0392)\n",
            "TRAIN(156): [260/391] Batch: 0.0497 (0.0499) Data: 0.0154 (0.0207) Loss: 0.0286 (0.0394)\n",
            "TRAIN(156): [270/391] Batch: 0.0536 (0.0500) Data: 0.0185 (0.0205) Loss: 0.0381 (0.0394)\n",
            "TRAIN(156): [280/391] Batch: 0.0501 (0.0501) Data: 0.0202 (0.0204) Loss: 0.0297 (0.0395)\n",
            "TRAIN(156): [290/391] Batch: 0.0531 (0.0502) Data: 0.0151 (0.0202) Loss: 0.0464 (0.0393)\n",
            "TRAIN(156): [300/391] Batch: 0.0538 (0.0503) Data: 0.0145 (0.0200) Loss: 0.0341 (0.0398)\n",
            "TRAIN(156): [310/391] Batch: 0.0514 (0.0503) Data: 0.0151 (0.0198) Loss: 0.0288 (0.0397)\n",
            "TRAIN(156): [320/391] Batch: 0.0340 (0.0505) Data: 0.0203 (0.0197) Loss: 0.0178 (0.0403)\n",
            "TRAIN(156): [330/391] Batch: 0.0462 (0.0504) Data: 0.0261 (0.0197) Loss: 0.0801 (0.0403)\n",
            "TRAIN(156): [340/391] Batch: 0.0490 (0.0503) Data: 0.0235 (0.0198) Loss: 0.0593 (0.0399)\n",
            "TRAIN(156): [350/391] Batch: 0.0438 (0.0503) Data: 0.0269 (0.0199) Loss: 0.0420 (0.0396)\n",
            "TRAIN(156): [360/391] Batch: 0.0460 (0.0502) Data: 0.0202 (0.0199) Loss: 0.0231 (0.0396)\n",
            "TRAIN(156): [370/391] Batch: 0.0492 (0.0502) Data: 0.0161 (0.0198) Loss: 0.0402 (0.0396)\n",
            "TRAIN(156): [380/391] Batch: 0.0475 (0.0502) Data: 0.0267 (0.0198) Loss: 0.0512 (0.0395)\n",
            "TRAIN(156): [390/391] Batch: 0.0486 (0.0502) Data: 0.0265 (0.0199) Loss: 0.0621 (0.0393)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(156)         0:00:19         0:00:07         0:00:11          0.0393\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(157): [ 10/391] Batch: 0.0530 (0.0660) Data: 0.0215 (0.0301) Loss: 0.0259 (0.0324)\n",
            "TRAIN(157): [ 20/391] Batch: 0.0488 (0.0574) Data: 0.0251 (0.0259) Loss: 0.0522 (0.0351)\n",
            "TRAIN(157): [ 30/391] Batch: 0.0471 (0.0539) Data: 0.0259 (0.0249) Loss: 0.0542 (0.0380)\n",
            "TRAIN(157): [ 40/391] Batch: 0.0529 (0.0524) Data: 0.0245 (0.0239) Loss: 0.0172 (0.0378)\n",
            "TRAIN(157): [ 50/391] Batch: 0.0458 (0.0518) Data: 0.0254 (0.0231) Loss: 0.0473 (0.0382)\n",
            "TRAIN(157): [ 60/391] Batch: 0.0465 (0.0511) Data: 0.0271 (0.0231) Loss: 0.0237 (0.0388)\n",
            "TRAIN(157): [ 70/391] Batch: 0.0467 (0.0509) Data: 0.0216 (0.0227) Loss: 0.0480 (0.0384)\n",
            "TRAIN(157): [ 80/391] Batch: 0.0418 (0.0505) Data: 0.0268 (0.0225) Loss: 0.0218 (0.0403)\n",
            "TRAIN(157): [ 90/391] Batch: 0.0455 (0.0503) Data: 0.0233 (0.0224) Loss: 0.0492 (0.0405)\n",
            "TRAIN(157): [100/391] Batch: 0.0456 (0.0504) Data: 0.0275 (0.0220) Loss: 0.0262 (0.0416)\n",
            "TRAIN(157): [110/391] Batch: 0.0521 (0.0503) Data: 0.0208 (0.0218) Loss: 0.0474 (0.0428)\n",
            "TRAIN(157): [120/391] Batch: 0.0487 (0.0500) Data: 0.0235 (0.0218) Loss: 0.0602 (0.0427)\n",
            "TRAIN(157): [130/391] Batch: 0.0535 (0.0500) Data: 0.0207 (0.0217) Loss: 0.0334 (0.0429)\n",
            "TRAIN(157): [140/391] Batch: 0.0504 (0.0500) Data: 0.0109 (0.0210) Loss: 0.0467 (0.0434)\n",
            "TRAIN(157): [150/391] Batch: 0.0616 (0.0501) Data: 0.0137 (0.0207) Loss: 0.0536 (0.0434)\n",
            "TRAIN(157): [160/391] Batch: 0.0532 (0.0504) Data: 0.0177 (0.0204) Loss: 0.0315 (0.0429)\n",
            "TRAIN(157): [170/391] Batch: 0.0628 (0.0505) Data: 0.0177 (0.0201) Loss: 0.0549 (0.0431)\n",
            "TRAIN(157): [180/391] Batch: 0.0620 (0.0507) Data: 0.0135 (0.0198) Loss: 0.0085 (0.0427)\n",
            "TRAIN(157): [190/391] Batch: 0.0478 (0.0507) Data: 0.0196 (0.0196) Loss: 0.0615 (0.0428)\n",
            "TRAIN(157): [200/391] Batch: 0.0469 (0.0506) Data: 0.0275 (0.0196) Loss: 0.0197 (0.0427)\n",
            "TRAIN(157): [210/391] Batch: 0.0452 (0.0505) Data: 0.0273 (0.0197) Loss: 0.0502 (0.0423)\n",
            "TRAIN(157): [220/391] Batch: 0.0471 (0.0503) Data: 0.0268 (0.0199) Loss: 0.0448 (0.0417)\n",
            "TRAIN(157): [230/391] Batch: 0.0402 (0.0502) Data: 0.0273 (0.0200) Loss: 0.0165 (0.0418)\n",
            "TRAIN(157): [240/391] Batch: 0.0365 (0.0502) Data: 0.0262 (0.0200) Loss: 0.0593 (0.0420)\n",
            "TRAIN(157): [250/391] Batch: 0.0447 (0.0501) Data: 0.0283 (0.0201) Loss: 0.0372 (0.0415)\n",
            "TRAIN(157): [260/391] Batch: 0.0464 (0.0500) Data: 0.0257 (0.0201) Loss: 0.0147 (0.0413)\n",
            "TRAIN(157): [270/391] Batch: 0.0436 (0.0499) Data: 0.0266 (0.0202) Loss: 0.0515 (0.0410)\n",
            "TRAIN(157): [280/391] Batch: 0.0440 (0.0498) Data: 0.0253 (0.0202) Loss: 0.0106 (0.0411)\n",
            "TRAIN(157): [290/391] Batch: 0.0544 (0.0498) Data: 0.0239 (0.0203) Loss: 0.0288 (0.0412)\n",
            "TRAIN(157): [300/391] Batch: 0.0428 (0.0497) Data: 0.0252 (0.0203) Loss: 0.0319 (0.0410)\n",
            "TRAIN(157): [310/391] Batch: 0.0452 (0.0497) Data: 0.0258 (0.0204) Loss: 0.0485 (0.0409)\n",
            "TRAIN(157): [320/391] Batch: 0.0572 (0.0497) Data: 0.0224 (0.0203) Loss: 0.0598 (0.0407)\n",
            "TRAIN(157): [330/391] Batch: 0.0474 (0.0496) Data: 0.0273 (0.0204) Loss: 0.0776 (0.0407)\n",
            "TRAIN(157): [340/391] Batch: 0.0406 (0.0496) Data: 0.0264 (0.0204) Loss: 0.0138 (0.0406)\n",
            "TRAIN(157): [350/391] Batch: 0.0467 (0.0495) Data: 0.0266 (0.0205) Loss: 0.0876 (0.0405)\n",
            "TRAIN(157): [360/391] Batch: 0.0450 (0.0495) Data: 0.0243 (0.0205) Loss: 0.0853 (0.0403)\n",
            "TRAIN(157): [370/391] Batch: 0.0447 (0.0495) Data: 0.0280 (0.0205) Loss: 0.0105 (0.0399)\n",
            "TRAIN(157): [380/391] Batch: 0.0507 (0.0495) Data: 0.0240 (0.0205) Loss: 0.0180 (0.0399)\n",
            "TRAIN(157): [390/391] Batch: 0.0459 (0.0494) Data: 0.0274 (0.0206) Loss: 0.0356 (0.0398)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(157)         0:00:19         0:00:08         0:00:11          0.0398\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(158): [ 10/391] Batch: 0.0516 (0.0654) Data: 0.0160 (0.0314) Loss: 0.0460 (0.0377)\n",
            "TRAIN(158): [ 20/391] Batch: 0.0470 (0.0577) Data: 0.0171 (0.0243) Loss: 0.1426 (0.0449)\n",
            "TRAIN(158): [ 30/391] Batch: 0.0470 (0.0550) Data: 0.0205 (0.0219) Loss: 0.0348 (0.0444)\n",
            "TRAIN(158): [ 40/391] Batch: 0.0391 (0.0540) Data: 0.0241 (0.0209) Loss: 0.0505 (0.0447)\n",
            "TRAIN(158): [ 50/391] Batch: 0.0488 (0.0535) Data: 0.0187 (0.0202) Loss: 0.0201 (0.0440)\n",
            "TRAIN(158): [ 60/391] Batch: 0.0549 (0.0531) Data: 0.0148 (0.0196) Loss: 0.0132 (0.0439)\n",
            "TRAIN(158): [ 70/391] Batch: 0.0510 (0.0528) Data: 0.0187 (0.0191) Loss: 0.0125 (0.0452)\n",
            "TRAIN(158): [ 80/391] Batch: 0.0451 (0.0524) Data: 0.0270 (0.0190) Loss: 0.0358 (0.0432)\n",
            "TRAIN(158): [ 90/391] Batch: 0.0440 (0.0520) Data: 0.0232 (0.0193) Loss: 0.0268 (0.0438)\n",
            "TRAIN(158): [100/391] Batch: 0.0473 (0.0517) Data: 0.0241 (0.0195) Loss: 0.0329 (0.0430)\n",
            "TRAIN(158): [110/391] Batch: 0.0396 (0.0516) Data: 0.0232 (0.0192) Loss: 0.0204 (0.0431)\n",
            "TRAIN(158): [120/391] Batch: 0.0633 (0.0515) Data: 0.0157 (0.0194) Loss: 0.0383 (0.0431)\n",
            "TRAIN(158): [130/391] Batch: 0.0491 (0.0514) Data: 0.0169 (0.0193) Loss: 0.0795 (0.0429)\n",
            "TRAIN(158): [140/391] Batch: 0.0632 (0.0514) Data: 0.0150 (0.0190) Loss: 0.0599 (0.0428)\n",
            "TRAIN(158): [150/391] Batch: 0.0412 (0.0513) Data: 0.0261 (0.0190) Loss: 0.0265 (0.0423)\n",
            "TRAIN(158): [160/391] Batch: 0.0542 (0.0511) Data: 0.0186 (0.0192) Loss: 0.0687 (0.0425)\n",
            "TRAIN(158): [170/391] Batch: 0.0456 (0.0510) Data: 0.0252 (0.0192) Loss: 0.0272 (0.0424)\n",
            "TRAIN(158): [180/391] Batch: 0.0405 (0.0509) Data: 0.0264 (0.0193) Loss: 0.0540 (0.0421)\n",
            "TRAIN(158): [190/391] Batch: 0.0442 (0.0507) Data: 0.0241 (0.0194) Loss: 0.0620 (0.0425)\n",
            "TRAIN(158): [200/391] Batch: 0.0470 (0.0507) Data: 0.0221 (0.0195) Loss: 0.0285 (0.0425)\n",
            "TRAIN(158): [210/391] Batch: 0.0428 (0.0506) Data: 0.0174 (0.0195) Loss: 0.0818 (0.0427)\n",
            "TRAIN(158): [220/391] Batch: 0.0463 (0.0504) Data: 0.0274 (0.0197) Loss: 0.0355 (0.0428)\n",
            "TRAIN(158): [230/391] Batch: 0.0457 (0.0502) Data: 0.0271 (0.0198) Loss: 0.0238 (0.0428)\n",
            "TRAIN(158): [240/391] Batch: 0.0472 (0.0501) Data: 0.0270 (0.0199) Loss: 0.0205 (0.0429)\n",
            "TRAIN(158): [250/391] Batch: 0.0545 (0.0500) Data: 0.0218 (0.0200) Loss: 0.0326 (0.0426)\n",
            "TRAIN(158): [260/391] Batch: 0.0563 (0.0499) Data: 0.0170 (0.0201) Loss: 0.0189 (0.0425)\n",
            "TRAIN(158): [270/391] Batch: 0.0459 (0.0499) Data: 0.0273 (0.0202) Loss: 0.1035 (0.0430)\n",
            "TRAIN(158): [280/391] Batch: 0.0476 (0.0499) Data: 0.0182 (0.0201) Loss: 0.0693 (0.0431)\n",
            "TRAIN(158): [290/391] Batch: 0.0402 (0.0499) Data: 0.0259 (0.0200) Loss: 0.0401 (0.0427)\n",
            "TRAIN(158): [300/391] Batch: 0.0524 (0.0500) Data: 0.0130 (0.0198) Loss: 0.0185 (0.0422)\n",
            "TRAIN(158): [310/391] Batch: 0.0480 (0.0500) Data: 0.0158 (0.0197) Loss: 0.0465 (0.0417)\n",
            "TRAIN(158): [320/391] Batch: 0.0425 (0.0501) Data: 0.0212 (0.0196) Loss: 0.0396 (0.0414)\n",
            "TRAIN(158): [330/391] Batch: 0.0462 (0.0501) Data: 0.0172 (0.0195) Loss: 0.0327 (0.0413)\n",
            "TRAIN(158): [340/391] Batch: 0.0493 (0.0501) Data: 0.0164 (0.0193) Loss: 0.0164 (0.0411)\n",
            "TRAIN(158): [350/391] Batch: 0.0402 (0.0501) Data: 0.0260 (0.0192) Loss: 0.0379 (0.0411)\n",
            "TRAIN(158): [360/391] Batch: 0.0448 (0.0501) Data: 0.0245 (0.0193) Loss: 0.0508 (0.0411)\n",
            "TRAIN(158): [370/391] Batch: 0.0562 (0.0501) Data: 0.0239 (0.0193) Loss: 0.0126 (0.0411)\n",
            "TRAIN(158): [380/391] Batch: 0.0486 (0.0500) Data: 0.0247 (0.0194) Loss: 0.0341 (0.0410)\n",
            "TRAIN(158): [390/391] Batch: 0.0476 (0.0500) Data: 0.0270 (0.0195) Loss: 0.0201 (0.0409)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(158)         0:00:19         0:00:07         0:00:11          0.0409\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(159): [ 10/391] Batch: 0.0473 (0.0616) Data: 0.0251 (0.0375) Loss: 0.0112 (0.0384)\n",
            "TRAIN(159): [ 20/391] Batch: 0.0497 (0.0542) Data: 0.0322 (0.0312) Loss: 0.0533 (0.0362)\n",
            "TRAIN(159): [ 30/391] Batch: 0.0556 (0.0526) Data: 0.0141 (0.0277) Loss: 0.0325 (0.0360)\n",
            "TRAIN(159): [ 40/391] Batch: 0.0457 (0.0512) Data: 0.0204 (0.0258) Loss: 0.0321 (0.0357)\n",
            "TRAIN(159): [ 50/391] Batch: 0.0422 (0.0510) Data: 0.0261 (0.0242) Loss: 0.0308 (0.0369)\n",
            "TRAIN(159): [ 60/391] Batch: 0.0545 (0.0509) Data: 0.0223 (0.0232) Loss: 0.0344 (0.0384)\n",
            "TRAIN(159): [ 70/391] Batch: 0.0462 (0.0505) Data: 0.0255 (0.0229) Loss: 0.0170 (0.0383)\n",
            "TRAIN(159): [ 80/391] Batch: 0.0473 (0.0501) Data: 0.0260 (0.0228) Loss: 0.0041 (0.0375)\n",
            "TRAIN(159): [ 90/391] Batch: 0.0451 (0.0498) Data: 0.0266 (0.0228) Loss: 0.0167 (0.0365)\n",
            "TRAIN(159): [100/391] Batch: 0.0393 (0.0498) Data: 0.0245 (0.0225) Loss: 0.0792 (0.0368)\n",
            "TRAIN(159): [110/391] Batch: 0.0563 (0.0497) Data: 0.0224 (0.0224) Loss: 0.0273 (0.0359)\n",
            "TRAIN(159): [120/391] Batch: 0.0462 (0.0497) Data: 0.0239 (0.0220) Loss: 0.0209 (0.0364)\n",
            "TRAIN(159): [130/391] Batch: 0.0456 (0.0496) Data: 0.0260 (0.0221) Loss: 0.0344 (0.0368)\n",
            "TRAIN(159): [140/391] Batch: 0.0484 (0.0494) Data: 0.0251 (0.0221) Loss: 0.0285 (0.0376)\n",
            "TRAIN(159): [150/391] Batch: 0.0470 (0.0493) Data: 0.0262 (0.0221) Loss: 0.0329 (0.0377)\n",
            "TRAIN(159): [160/391] Batch: 0.0675 (0.0494) Data: 0.0137 (0.0219) Loss: 0.0922 (0.0380)\n",
            "TRAIN(159): [170/391] Batch: 0.0386 (0.0495) Data: 0.0245 (0.0216) Loss: 0.0807 (0.0380)\n",
            "TRAIN(159): [180/391] Batch: 0.0463 (0.0497) Data: 0.0202 (0.0214) Loss: 0.0972 (0.0385)\n",
            "TRAIN(159): [190/391] Batch: 0.0542 (0.0498) Data: 0.0150 (0.0212) Loss: 0.0544 (0.0385)\n",
            "TRAIN(159): [200/391] Batch: 0.0465 (0.0498) Data: 0.0250 (0.0211) Loss: 0.0127 (0.0381)\n",
            "TRAIN(159): [210/391] Batch: 0.0612 (0.0499) Data: 0.0151 (0.0209) Loss: 0.0849 (0.0379)\n",
            "TRAIN(159): [220/391] Batch: 0.0515 (0.0500) Data: 0.0158 (0.0206) Loss: 0.1348 (0.0382)\n",
            "TRAIN(159): [230/391] Batch: 0.0464 (0.0498) Data: 0.0276 (0.0206) Loss: 0.0276 (0.0380)\n",
            "TRAIN(159): [240/391] Batch: 0.0462 (0.0499) Data: 0.0215 (0.0205) Loss: 0.0134 (0.0380)\n",
            "TRAIN(159): [250/391] Batch: 0.0381 (0.0498) Data: 0.0251 (0.0205) Loss: 0.0426 (0.0377)\n",
            "TRAIN(159): [260/391] Batch: 0.0582 (0.0498) Data: 0.0225 (0.0204) Loss: 0.0267 (0.0377)\n",
            "TRAIN(159): [270/391] Batch: 0.0466 (0.0497) Data: 0.0267 (0.0205) Loss: 0.0277 (0.0372)\n",
            "TRAIN(159): [280/391] Batch: 0.0435 (0.0497) Data: 0.0276 (0.0205) Loss: 0.0223 (0.0372)\n",
            "TRAIN(159): [290/391] Batch: 0.0537 (0.0496) Data: 0.0176 (0.0206) Loss: 0.0467 (0.0376)\n",
            "TRAIN(159): [300/391] Batch: 0.0546 (0.0497) Data: 0.0220 (0.0206) Loss: 0.0511 (0.0376)\n",
            "TRAIN(159): [310/391] Batch: 0.0461 (0.0497) Data: 0.0217 (0.0204) Loss: 0.0276 (0.0378)\n",
            "TRAIN(159): [320/391] Batch: 0.0525 (0.0498) Data: 0.0190 (0.0203) Loss: 0.0643 (0.0383)\n",
            "TRAIN(159): [330/391] Batch: 0.0464 (0.0498) Data: 0.0223 (0.0202) Loss: 0.0256 (0.0384)\n",
            "TRAIN(159): [340/391] Batch: 0.0456 (0.0498) Data: 0.0272 (0.0202) Loss: 0.0591 (0.0386)\n",
            "TRAIN(159): [350/391] Batch: 0.0461 (0.0497) Data: 0.0267 (0.0203) Loss: 0.0645 (0.0388)\n",
            "TRAIN(159): [360/391] Batch: 0.0455 (0.0497) Data: 0.0273 (0.0204) Loss: 0.0249 (0.0386)\n",
            "TRAIN(159): [370/391] Batch: 0.0456 (0.0497) Data: 0.0270 (0.0204) Loss: 0.0495 (0.0387)\n",
            "TRAIN(159): [380/391] Batch: 0.0481 (0.0497) Data: 0.0224 (0.0204) Loss: 0.0338 (0.0386)\n",
            "TRAIN(159): [390/391] Batch: 0.0463 (0.0496) Data: 0.0293 (0.0204) Loss: 0.0685 (0.0387)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(159)         0:00:19         0:00:07         0:00:11          0.0387\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(160): [ 10/391] Batch: 0.0561 (0.0667) Data: 0.0189 (0.0303) Loss: 0.0562 (0.0409)\n",
            "TRAIN(160): [ 20/391] Batch: 0.0490 (0.0580) Data: 0.0175 (0.0255) Loss: 0.0607 (0.0427)\n",
            "TRAIN(160): [ 30/391] Batch: 0.0547 (0.0549) Data: 0.0215 (0.0234) Loss: 0.0592 (0.0480)\n",
            "TRAIN(160): [ 40/391] Batch: 0.0545 (0.0540) Data: 0.0187 (0.0215) Loss: 0.0415 (0.0442)\n",
            "TRAIN(160): [ 50/391] Batch: 0.0501 (0.0535) Data: 0.0194 (0.0209) Loss: 0.0444 (0.0450)\n",
            "TRAIN(160): [ 60/391] Batch: 0.0477 (0.0529) Data: 0.0158 (0.0198) Loss: 0.0360 (0.0448)\n",
            "TRAIN(160): [ 70/391] Batch: 0.0486 (0.0528) Data: 0.0185 (0.0191) Loss: 0.0175 (0.0448)\n",
            "TRAIN(160): [ 80/391] Batch: 0.0502 (0.0523) Data: 0.0203 (0.0189) Loss: 0.0249 (0.0445)\n",
            "TRAIN(160): [ 90/391] Batch: 0.0604 (0.0523) Data: 0.0144 (0.0186) Loss: 0.0436 (0.0448)\n",
            "TRAIN(160): [100/391] Batch: 0.0542 (0.0523) Data: 0.0214 (0.0184) Loss: 0.0427 (0.0440)\n",
            "TRAIN(160): [110/391] Batch: 0.0545 (0.0522) Data: 0.0165 (0.0184) Loss: 0.0476 (0.0441)\n",
            "TRAIN(160): [120/391] Batch: 0.0472 (0.0518) Data: 0.0257 (0.0188) Loss: 0.0173 (0.0432)\n",
            "TRAIN(160): [130/391] Batch: 0.0461 (0.0515) Data: 0.0264 (0.0190) Loss: 0.0578 (0.0436)\n",
            "TRAIN(160): [140/391] Batch: 0.0522 (0.0513) Data: 0.0239 (0.0192) Loss: 0.0149 (0.0430)\n",
            "TRAIN(160): [150/391] Batch: 0.0551 (0.0510) Data: 0.0186 (0.0194) Loss: 0.0261 (0.0431)\n",
            "TRAIN(160): [160/391] Batch: 0.0486 (0.0508) Data: 0.0245 (0.0196) Loss: 0.0301 (0.0436)\n",
            "TRAIN(160): [170/391] Batch: 0.0491 (0.0507) Data: 0.0187 (0.0196) Loss: 0.0188 (0.0434)\n",
            "TRAIN(160): [180/391] Batch: 0.0454 (0.0505) Data: 0.0262 (0.0197) Loss: 0.0050 (0.0430)\n",
            "TRAIN(160): [190/391] Batch: 0.0417 (0.0504) Data: 0.0240 (0.0198) Loss: 0.0283 (0.0431)\n",
            "TRAIN(160): [200/391] Batch: 0.0454 (0.0503) Data: 0.0268 (0.0199) Loss: 0.0938 (0.0440)\n",
            "TRAIN(160): [210/391] Batch: 0.0531 (0.0502) Data: 0.0215 (0.0201) Loss: 0.0042 (0.0431)\n",
            "TRAIN(160): [220/391] Batch: 0.0452 (0.0500) Data: 0.0273 (0.0202) Loss: 0.0348 (0.0427)\n",
            "TRAIN(160): [230/391] Batch: 0.0454 (0.0500) Data: 0.0250 (0.0203) Loss: 0.0667 (0.0427)\n",
            "TRAIN(160): [240/391] Batch: 0.0470 (0.0498) Data: 0.0269 (0.0204) Loss: 0.0219 (0.0424)\n",
            "TRAIN(160): [250/391] Batch: 0.0513 (0.0498) Data: 0.0244 (0.0204) Loss: 0.0350 (0.0418)\n",
            "TRAIN(160): [260/391] Batch: 0.0440 (0.0497) Data: 0.0271 (0.0205) Loss: 0.0276 (0.0420)\n",
            "TRAIN(160): [270/391] Batch: 0.0424 (0.0497) Data: 0.0262 (0.0205) Loss: 0.0139 (0.0417)\n",
            "TRAIN(160): [280/391] Batch: 0.0474 (0.0496) Data: 0.0274 (0.0206) Loss: 0.0311 (0.0416)\n",
            "TRAIN(160): [290/391] Batch: 0.0449 (0.0495) Data: 0.0264 (0.0207) Loss: 0.0299 (0.0413)\n",
            "TRAIN(160): [300/391] Batch: 0.0429 (0.0495) Data: 0.0186 (0.0206) Loss: 0.0341 (0.0412)\n",
            "TRAIN(160): [310/391] Batch: 0.0416 (0.0495) Data: 0.0213 (0.0205) Loss: 0.0418 (0.0413)\n",
            "TRAIN(160): [320/391] Batch: 0.0549 (0.0496) Data: 0.0195 (0.0204) Loss: 0.0357 (0.0409)\n",
            "TRAIN(160): [330/391] Batch: 0.0473 (0.0497) Data: 0.0153 (0.0203) Loss: 0.0599 (0.0406)\n",
            "TRAIN(160): [340/391] Batch: 0.0582 (0.0497) Data: 0.0145 (0.0201) Loss: 0.0525 (0.0405)\n",
            "TRAIN(160): [350/391] Batch: 0.0511 (0.0497) Data: 0.0113 (0.0199) Loss: 0.0503 (0.0405)\n",
            "TRAIN(160): [360/391] Batch: 0.0472 (0.0497) Data: 0.0214 (0.0198) Loss: 0.0268 (0.0403)\n",
            "TRAIN(160): [370/391] Batch: 0.0457 (0.0497) Data: 0.0214 (0.0198) Loss: 0.0235 (0.0404)\n",
            "TRAIN(160): [380/391] Batch: 0.0345 (0.0498) Data: 0.0253 (0.0197) Loss: 0.0452 (0.0401)\n",
            "TRAIN(160): [390/391] Batch: 0.0461 (0.0497) Data: 0.0281 (0.0197) Loss: 0.0379 (0.0398)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(160)         0:00:19         0:00:07         0:00:11          0.0398\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(161): [ 10/391] Batch: 0.0336 (0.0640) Data: 0.0258 (0.0296) Loss: 0.0327 (0.0425)\n",
            "TRAIN(161): [ 20/391] Batch: 0.0484 (0.0567) Data: 0.0272 (0.0249) Loss: 0.0408 (0.0434)\n",
            "TRAIN(161): [ 30/391] Batch: 0.0444 (0.0538) Data: 0.0263 (0.0235) Loss: 0.0519 (0.0426)\n",
            "TRAIN(161): [ 40/391] Batch: 0.0451 (0.0522) Data: 0.0273 (0.0234) Loss: 0.0369 (0.0439)\n",
            "TRAIN(161): [ 50/391] Batch: 0.0434 (0.0516) Data: 0.0182 (0.0226) Loss: 0.0429 (0.0413)\n",
            "TRAIN(161): [ 60/391] Batch: 0.0490 (0.0510) Data: 0.0254 (0.0225) Loss: 0.0152 (0.0417)\n",
            "TRAIN(161): [ 70/391] Batch: 0.0454 (0.0508) Data: 0.0269 (0.0219) Loss: 0.0165 (0.0419)\n",
            "TRAIN(161): [ 80/391] Batch: 0.0479 (0.0503) Data: 0.0267 (0.0221) Loss: 0.0386 (0.0406)\n",
            "TRAIN(161): [ 90/391] Batch: 0.0440 (0.0503) Data: 0.0262 (0.0219) Loss: 0.0262 (0.0393)\n",
            "TRAIN(161): [100/391] Batch: 0.0392 (0.0500) Data: 0.0274 (0.0220) Loss: 0.0483 (0.0374)\n",
            "TRAIN(161): [110/391] Batch: 0.0443 (0.0499) Data: 0.0240 (0.0218) Loss: 0.0086 (0.0375)\n",
            "TRAIN(161): [120/391] Batch: 0.0492 (0.0499) Data: 0.0247 (0.0218) Loss: 0.0171 (0.0367)\n",
            "TRAIN(161): [130/391] Batch: 0.0544 (0.0499) Data: 0.0168 (0.0217) Loss: 0.0079 (0.0361)\n",
            "TRAIN(161): [140/391] Batch: 0.0410 (0.0497) Data: 0.0261 (0.0216) Loss: 0.0279 (0.0360)\n",
            "TRAIN(161): [150/391] Batch: 0.0510 (0.0498) Data: 0.0187 (0.0214) Loss: 0.0278 (0.0350)\n",
            "TRAIN(161): [160/391] Batch: 0.0493 (0.0497) Data: 0.0248 (0.0213) Loss: 0.0468 (0.0347)\n",
            "TRAIN(161): [170/391] Batch: 0.0531 (0.0497) Data: 0.0166 (0.0212) Loss: 0.0381 (0.0345)\n",
            "TRAIN(161): [180/391] Batch: 0.0446 (0.0497) Data: 0.0190 (0.0210) Loss: 0.0220 (0.0340)\n",
            "TRAIN(161): [190/391] Batch: 0.0565 (0.0498) Data: 0.0180 (0.0208) Loss: 0.0506 (0.0340)\n",
            "TRAIN(161): [200/391] Batch: 0.0484 (0.0499) Data: 0.0208 (0.0207) Loss: 0.0240 (0.0338)\n",
            "TRAIN(161): [210/391] Batch: 0.0539 (0.0499) Data: 0.0189 (0.0206) Loss: 0.0076 (0.0334)\n",
            "TRAIN(161): [220/391] Batch: 0.0435 (0.0500) Data: 0.0228 (0.0203) Loss: 0.0435 (0.0334)\n",
            "TRAIN(161): [230/391] Batch: 0.0519 (0.0501) Data: 0.0196 (0.0202) Loss: 0.0156 (0.0331)\n",
            "TRAIN(161): [240/391] Batch: 0.0476 (0.0501) Data: 0.0217 (0.0201) Loss: 0.0239 (0.0329)\n",
            "TRAIN(161): [250/391] Batch: 0.0467 (0.0501) Data: 0.0218 (0.0200) Loss: 0.0503 (0.0335)\n",
            "TRAIN(161): [260/391] Batch: 0.0470 (0.0501) Data: 0.0264 (0.0199) Loss: 0.0169 (0.0336)\n",
            "TRAIN(161): [270/391] Batch: 0.0435 (0.0500) Data: 0.0265 (0.0200) Loss: 0.0424 (0.0337)\n",
            "TRAIN(161): [280/391] Batch: 0.0466 (0.0499) Data: 0.0266 (0.0201) Loss: 0.0422 (0.0333)\n",
            "TRAIN(161): [290/391] Batch: 0.0473 (0.0498) Data: 0.0262 (0.0202) Loss: 0.0236 (0.0330)\n",
            "TRAIN(161): [300/391] Batch: 0.0473 (0.0497) Data: 0.0264 (0.0203) Loss: 0.0045 (0.0326)\n",
            "TRAIN(161): [310/391] Batch: 0.0459 (0.0496) Data: 0.0269 (0.0204) Loss: 0.0350 (0.0324)\n",
            "TRAIN(161): [320/391] Batch: 0.0462 (0.0496) Data: 0.0268 (0.0204) Loss: 0.0172 (0.0325)\n",
            "TRAIN(161): [330/391] Batch: 0.0570 (0.0496) Data: 0.0235 (0.0204) Loss: 0.0392 (0.0324)\n",
            "TRAIN(161): [340/391] Batch: 0.0473 (0.0496) Data: 0.0230 (0.0204) Loss: 0.0360 (0.0322)\n",
            "TRAIN(161): [350/391] Batch: 0.0464 (0.0496) Data: 0.0271 (0.0204) Loss: 0.0126 (0.0320)\n",
            "TRAIN(161): [360/391] Batch: 0.0511 (0.0496) Data: 0.0221 (0.0204) Loss: 0.0531 (0.0320)\n",
            "TRAIN(161): [370/391] Batch: 0.0464 (0.0496) Data: 0.0257 (0.0203) Loss: 0.0210 (0.0318)\n",
            "TRAIN(161): [380/391] Batch: 0.0493 (0.0496) Data: 0.0179 (0.0203) Loss: 0.0541 (0.0316)\n",
            "TRAIN(161): [390/391] Batch: 0.0465 (0.0496) Data: 0.0254 (0.0203) Loss: 0.0811 (0.0316)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(161)         0:00:19         0:00:07         0:00:11          0.0316\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(162): [ 10/391] Batch: 0.0444 (0.0664) Data: 0.0255 (0.0305) Loss: 0.0177 (0.0255)\n",
            "TRAIN(162): [ 20/391] Batch: 0.0521 (0.0593) Data: 0.0232 (0.0248) Loss: 0.0150 (0.0260)\n",
            "TRAIN(162): [ 30/391] Batch: 0.0446 (0.0560) Data: 0.0240 (0.0237) Loss: 0.0165 (0.0272)\n",
            "TRAIN(162): [ 40/391] Batch: 0.0471 (0.0538) Data: 0.0262 (0.0233) Loss: 0.0155 (0.0280)\n",
            "TRAIN(162): [ 50/391] Batch: 0.0463 (0.0529) Data: 0.0271 (0.0228) Loss: 0.0169 (0.0279)\n",
            "TRAIN(162): [ 60/391] Batch: 0.0451 (0.0522) Data: 0.0208 (0.0223) Loss: 0.1404 (0.0300)\n",
            "TRAIN(162): [ 70/391] Batch: 0.0468 (0.0523) Data: 0.0150 (0.0212) Loss: 0.0505 (0.0325)\n",
            "TRAIN(162): [ 80/391] Batch: 0.0582 (0.0523) Data: 0.0175 (0.0206) Loss: 0.0203 (0.0310)\n",
            "TRAIN(162): [ 90/391] Batch: 0.0497 (0.0524) Data: 0.0136 (0.0200) Loss: 0.0302 (0.0300)\n",
            "TRAIN(162): [100/391] Batch: 0.0439 (0.0525) Data: 0.0190 (0.0196) Loss: 0.0382 (0.0295)\n",
            "TRAIN(162): [110/391] Batch: 0.0415 (0.0524) Data: 0.0224 (0.0192) Loss: 0.0572 (0.0301)\n",
            "TRAIN(162): [120/391] Batch: 0.0530 (0.0525) Data: 0.0181 (0.0189) Loss: 0.0120 (0.0294)\n",
            "TRAIN(162): [130/391] Batch: 0.0458 (0.0524) Data: 0.0249 (0.0189) Loss: 0.0538 (0.0298)\n",
            "TRAIN(162): [140/391] Batch: 0.0474 (0.0521) Data: 0.0261 (0.0191) Loss: 0.0324 (0.0297)\n",
            "TRAIN(162): [150/391] Batch: 0.0443 (0.0521) Data: 0.0226 (0.0190) Loss: 0.0146 (0.0294)\n",
            "TRAIN(162): [160/391] Batch: 0.0480 (0.0518) Data: 0.0254 (0.0191) Loss: 0.0075 (0.0289)\n",
            "TRAIN(162): [170/391] Batch: 0.0474 (0.0516) Data: 0.0235 (0.0192) Loss: 0.0143 (0.0286)\n",
            "TRAIN(162): [180/391] Batch: 0.0464 (0.0513) Data: 0.0252 (0.0193) Loss: 0.0121 (0.0284)\n",
            "TRAIN(162): [190/391] Batch: 0.0494 (0.0512) Data: 0.0228 (0.0194) Loss: 0.0308 (0.0283)\n",
            "TRAIN(162): [200/391] Batch: 0.0458 (0.0511) Data: 0.0257 (0.0194) Loss: 0.0185 (0.0280)\n",
            "TRAIN(162): [210/391] Batch: 0.0444 (0.0511) Data: 0.0224 (0.0194) Loss: 0.0104 (0.0279)\n",
            "TRAIN(162): [220/391] Batch: 0.0470 (0.0509) Data: 0.0248 (0.0195) Loss: 0.0124 (0.0277)\n",
            "TRAIN(162): [230/391] Batch: 0.0392 (0.0508) Data: 0.0265 (0.0195) Loss: 0.0137 (0.0276)\n",
            "TRAIN(162): [240/391] Batch: 0.0448 (0.0508) Data: 0.0212 (0.0194) Loss: 0.0085 (0.0271)\n",
            "TRAIN(162): [250/391] Batch: 0.0510 (0.0509) Data: 0.0143 (0.0192) Loss: 0.0057 (0.0270)\n",
            "TRAIN(162): [260/391] Batch: 0.0435 (0.0508) Data: 0.0241 (0.0192) Loss: 0.0178 (0.0268)\n",
            "TRAIN(162): [270/391] Batch: 0.0510 (0.0508) Data: 0.0224 (0.0192) Loss: 0.0529 (0.0267)\n",
            "TRAIN(162): [280/391] Batch: 0.0462 (0.0509) Data: 0.0230 (0.0191) Loss: 0.0198 (0.0264)\n",
            "TRAIN(162): [290/391] Batch: 0.0471 (0.0507) Data: 0.0265 (0.0192) Loss: 0.0295 (0.0263)\n",
            "TRAIN(162): [300/391] Batch: 0.0657 (0.0507) Data: 0.0147 (0.0193) Loss: 0.0103 (0.0260)\n",
            "TRAIN(162): [310/391] Batch: 0.0485 (0.0506) Data: 0.0208 (0.0194) Loss: 0.0196 (0.0260)\n",
            "TRAIN(162): [320/391] Batch: 0.0550 (0.0506) Data: 0.0223 (0.0194) Loss: 0.0427 (0.0264)\n",
            "TRAIN(162): [330/391] Batch: 0.0492 (0.0506) Data: 0.0117 (0.0193) Loss: 0.0481 (0.0262)\n",
            "TRAIN(162): [340/391] Batch: 0.0619 (0.0507) Data: 0.0126 (0.0192) Loss: 0.0598 (0.0262)\n",
            "TRAIN(162): [350/391] Batch: 0.0530 (0.0508) Data: 0.0152 (0.0190) Loss: 0.0138 (0.0260)\n",
            "TRAIN(162): [360/391] Batch: 0.0543 (0.0508) Data: 0.0144 (0.0189) Loss: 0.0659 (0.0259)\n",
            "TRAIN(162): [370/391] Batch: 0.0502 (0.0508) Data: 0.0184 (0.0189) Loss: 0.0196 (0.0258)\n",
            "TRAIN(162): [380/391] Batch: 0.0560 (0.0508) Data: 0.0173 (0.0188) Loss: 0.0109 (0.0258)\n",
            "TRAIN(162): [390/391] Batch: 0.0487 (0.0509) Data: 0.0219 (0.0187) Loss: 0.0209 (0.0258)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(162)         0:00:19         0:00:07         0:00:12          0.0258\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(163): [ 10/391] Batch: 0.0472 (0.0657) Data: 0.0253 (0.0334) Loss: 0.0102 (0.0260)\n",
            "TRAIN(163): [ 20/391] Batch: 0.0500 (0.0578) Data: 0.0255 (0.0275) Loss: 0.0192 (0.0226)\n",
            "TRAIN(163): [ 30/391] Batch: 0.0583 (0.0561) Data: 0.0151 (0.0236) Loss: 0.0306 (0.0254)\n",
            "TRAIN(163): [ 40/391] Batch: 0.0414 (0.0548) Data: 0.0240 (0.0219) Loss: 0.1082 (0.0281)\n",
            "TRAIN(163): [ 50/391] Batch: 0.0564 (0.0537) Data: 0.0215 (0.0214) Loss: 0.0106 (0.0273)\n",
            "TRAIN(163): [ 60/391] Batch: 0.0579 (0.0532) Data: 0.0172 (0.0209) Loss: 0.0115 (0.0261)\n",
            "TRAIN(163): [ 70/391] Batch: 0.0506 (0.0528) Data: 0.0154 (0.0205) Loss: 0.0344 (0.0252)\n",
            "TRAIN(163): [ 80/391] Batch: 0.0444 (0.0524) Data: 0.0247 (0.0204) Loss: 0.0460 (0.0257)\n",
            "TRAIN(163): [ 90/391] Batch: 0.0475 (0.0519) Data: 0.0247 (0.0203) Loss: 0.0087 (0.0256)\n",
            "TRAIN(163): [100/391] Batch: 0.0484 (0.0517) Data: 0.0233 (0.0200) Loss: 0.0284 (0.0257)\n",
            "TRAIN(163): [110/391] Batch: 0.0467 (0.0517) Data: 0.0214 (0.0201) Loss: 0.0543 (0.0262)\n",
            "TRAIN(163): [120/391] Batch: 0.0380 (0.0516) Data: 0.0237 (0.0199) Loss: 0.0576 (0.0260)\n",
            "TRAIN(163): [130/391] Batch: 0.0458 (0.0513) Data: 0.0246 (0.0200) Loss: 0.0245 (0.0264)\n",
            "TRAIN(163): [140/391] Batch: 0.0445 (0.0512) Data: 0.0248 (0.0198) Loss: 0.0112 (0.0262)\n",
            "TRAIN(163): [150/391] Batch: 0.0543 (0.0512) Data: 0.0145 (0.0197) Loss: 0.0174 (0.0264)\n",
            "TRAIN(163): [160/391] Batch: 0.0422 (0.0512) Data: 0.0235 (0.0196) Loss: 0.0063 (0.0263)\n",
            "TRAIN(163): [170/391] Batch: 0.0459 (0.0510) Data: 0.0278 (0.0197) Loss: 0.0317 (0.0262)\n",
            "TRAIN(163): [180/391] Batch: 0.0503 (0.0510) Data: 0.0165 (0.0197) Loss: 0.0191 (0.0257)\n",
            "TRAIN(163): [190/391] Batch: 0.0500 (0.0508) Data: 0.0224 (0.0198) Loss: 0.0618 (0.0255)\n",
            "TRAIN(163): [200/391] Batch: 0.0564 (0.0509) Data: 0.0136 (0.0196) Loss: 0.0267 (0.0253)\n",
            "TRAIN(163): [210/391] Batch: 0.0608 (0.0510) Data: 0.0160 (0.0195) Loss: 0.0098 (0.0251)\n",
            "TRAIN(163): [220/391] Batch: 0.0437 (0.0511) Data: 0.0238 (0.0193) Loss: 0.0196 (0.0249)\n",
            "TRAIN(163): [230/391] Batch: 0.0609 (0.0512) Data: 0.0139 (0.0191) Loss: 0.0277 (0.0247)\n",
            "TRAIN(163): [240/391] Batch: 0.0482 (0.0513) Data: 0.0134 (0.0189) Loss: 0.0138 (0.0247)\n",
            "TRAIN(163): [250/391] Batch: 0.0497 (0.0512) Data: 0.0150 (0.0186) Loss: 0.0368 (0.0248)\n",
            "TRAIN(163): [260/391] Batch: 0.0511 (0.0512) Data: 0.0135 (0.0185) Loss: 0.0106 (0.0246)\n",
            "TRAIN(163): [270/391] Batch: 0.0442 (0.0512) Data: 0.0238 (0.0184) Loss: 0.0331 (0.0244)\n",
            "TRAIN(163): [280/391] Batch: 0.0463 (0.0511) Data: 0.0249 (0.0185) Loss: 0.0222 (0.0241)\n",
            "TRAIN(163): [290/391] Batch: 0.0531 (0.0511) Data: 0.0150 (0.0184) Loss: 0.0068 (0.0240)\n",
            "TRAIN(163): [300/391] Batch: 0.0462 (0.0509) Data: 0.0270 (0.0186) Loss: 0.0140 (0.0240)\n",
            "TRAIN(163): [310/391] Batch: 0.0479 (0.0508) Data: 0.0258 (0.0186) Loss: 0.0135 (0.0240)\n",
            "TRAIN(163): [320/391] Batch: 0.0549 (0.0508) Data: 0.0148 (0.0186) Loss: 0.0406 (0.0239)\n",
            "TRAIN(163): [330/391] Batch: 0.0462 (0.0507) Data: 0.0253 (0.0187) Loss: 0.0407 (0.0239)\n",
            "TRAIN(163): [340/391] Batch: 0.0604 (0.0506) Data: 0.0172 (0.0188) Loss: 0.0082 (0.0237)\n",
            "TRAIN(163): [350/391] Batch: 0.0517 (0.0506) Data: 0.0183 (0.0188) Loss: 0.0219 (0.0235)\n",
            "TRAIN(163): [360/391] Batch: 0.0455 (0.0505) Data: 0.0247 (0.0189) Loss: 0.0345 (0.0235)\n",
            "TRAIN(163): [370/391] Batch: 0.0535 (0.0505) Data: 0.0179 (0.0189) Loss: 0.0062 (0.0234)\n",
            "TRAIN(163): [380/391] Batch: 0.0448 (0.0504) Data: 0.0261 (0.0190) Loss: 0.0584 (0.0233)\n",
            "TRAIN(163): [390/391] Batch: 0.0474 (0.0504) Data: 0.0262 (0.0190) Loss: 0.0279 (0.0234)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(163)         0:00:19         0:00:07         0:00:12          0.0234\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(164): [ 10/391] Batch: 0.0457 (0.0658) Data: 0.0218 (0.0318) Loss: 0.0309 (0.0291)\n",
            "TRAIN(164): [ 20/391] Batch: 0.0554 (0.0585) Data: 0.0230 (0.0259) Loss: 0.0313 (0.0267)\n",
            "TRAIN(164): [ 30/391] Batch: 0.0584 (0.0553) Data: 0.0152 (0.0243) Loss: 0.0169 (0.0225)\n",
            "TRAIN(164): [ 40/391] Batch: 0.0464 (0.0534) Data: 0.0281 (0.0239) Loss: 0.0099 (0.0229)\n",
            "TRAIN(164): [ 50/391] Batch: 0.0451 (0.0523) Data: 0.0282 (0.0236) Loss: 0.0127 (0.0217)\n",
            "TRAIN(164): [ 60/391] Batch: 0.0445 (0.0515) Data: 0.0259 (0.0236) Loss: 0.0083 (0.0218)\n",
            "TRAIN(164): [ 70/391] Batch: 0.0404 (0.0513) Data: 0.0233 (0.0229) Loss: 0.0078 (0.0218)\n",
            "TRAIN(164): [ 80/391] Batch: 0.0535 (0.0514) Data: 0.0185 (0.0220) Loss: 0.0118 (0.0209)\n",
            "TRAIN(164): [ 90/391] Batch: 0.0558 (0.0514) Data: 0.0138 (0.0214) Loss: 0.0115 (0.0204)\n",
            "TRAIN(164): [100/391] Batch: 0.0541 (0.0511) Data: 0.0176 (0.0212) Loss: 0.0171 (0.0199)\n",
            "TRAIN(164): [110/391] Batch: 0.0504 (0.0510) Data: 0.0200 (0.0207) Loss: 0.0180 (0.0201)\n",
            "TRAIN(164): [120/391] Batch: 0.0498 (0.0509) Data: 0.0175 (0.0204) Loss: 0.0650 (0.0202)\n",
            "TRAIN(164): [130/391] Batch: 0.0559 (0.0508) Data: 0.0211 (0.0200) Loss: 0.0141 (0.0207)\n",
            "TRAIN(164): [140/391] Batch: 0.0419 (0.0509) Data: 0.0248 (0.0198) Loss: 0.0187 (0.0210)\n",
            "TRAIN(164): [150/391] Batch: 0.0538 (0.0508) Data: 0.0179 (0.0198) Loss: 0.0173 (0.0212)\n",
            "TRAIN(164): [160/391] Batch: 0.0460 (0.0505) Data: 0.0258 (0.0201) Loss: 0.0423 (0.0210)\n",
            "TRAIN(164): [170/391] Batch: 0.0440 (0.0504) Data: 0.0272 (0.0201) Loss: 0.0244 (0.0209)\n",
            "TRAIN(164): [180/391] Batch: 0.0492 (0.0503) Data: 0.0198 (0.0202) Loss: 0.0095 (0.0209)\n",
            "TRAIN(164): [190/391] Batch: 0.0451 (0.0501) Data: 0.0274 (0.0203) Loss: 0.0065 (0.0206)\n",
            "TRAIN(164): [200/391] Batch: 0.0569 (0.0501) Data: 0.0170 (0.0204) Loss: 0.0199 (0.0204)\n",
            "TRAIN(164): [210/391] Batch: 0.0466 (0.0500) Data: 0.0280 (0.0204) Loss: 0.0653 (0.0206)\n",
            "TRAIN(164): [220/391] Batch: 0.0462 (0.0499) Data: 0.0281 (0.0205) Loss: 0.0107 (0.0204)\n",
            "TRAIN(164): [230/391] Batch: 0.0434 (0.0498) Data: 0.0258 (0.0205) Loss: 0.0196 (0.0206)\n",
            "TRAIN(164): [240/391] Batch: 0.0462 (0.0498) Data: 0.0272 (0.0206) Loss: 0.0213 (0.0207)\n",
            "TRAIN(164): [250/391] Batch: 0.0491 (0.0497) Data: 0.0183 (0.0207) Loss: 0.0616 (0.0206)\n",
            "TRAIN(164): [260/391] Batch: 0.0457 (0.0497) Data: 0.0277 (0.0206) Loss: 0.0157 (0.0206)\n",
            "TRAIN(164): [270/391] Batch: 0.0447 (0.0496) Data: 0.0266 (0.0207) Loss: 0.0243 (0.0208)\n",
            "TRAIN(164): [280/391] Batch: 0.0541 (0.0496) Data: 0.0247 (0.0207) Loss: 0.0168 (0.0208)\n",
            "TRAIN(164): [290/391] Batch: 0.0456 (0.0496) Data: 0.0224 (0.0206) Loss: 0.0088 (0.0206)\n",
            "TRAIN(164): [300/391] Batch: 0.0456 (0.0496) Data: 0.0272 (0.0206) Loss: 0.0195 (0.0208)\n",
            "TRAIN(164): [310/391] Batch: 0.0522 (0.0495) Data: 0.0203 (0.0207) Loss: 0.0169 (0.0208)\n",
            "TRAIN(164): [320/391] Batch: 0.0480 (0.0495) Data: 0.0212 (0.0207) Loss: 0.0428 (0.0206)\n",
            "TRAIN(164): [330/391] Batch: 0.0419 (0.0496) Data: 0.0189 (0.0205) Loss: 0.0131 (0.0205)\n",
            "TRAIN(164): [340/391] Batch: 0.0556 (0.0496) Data: 0.0198 (0.0206) Loss: 0.0146 (0.0204)\n",
            "TRAIN(164): [350/391] Batch: 0.0485 (0.0496) Data: 0.0162 (0.0204) Loss: 0.0271 (0.0202)\n",
            "TRAIN(164): [360/391] Batch: 0.0540 (0.0497) Data: 0.0139 (0.0203) Loss: 0.0156 (0.0201)\n",
            "TRAIN(164): [370/391] Batch: 0.0415 (0.0497) Data: 0.0219 (0.0202) Loss: 0.0136 (0.0199)\n",
            "TRAIN(164): [380/391] Batch: 0.0643 (0.0498) Data: 0.0128 (0.0201) Loss: 0.0412 (0.0200)\n",
            "TRAIN(164): [390/391] Batch: 0.0444 (0.0498) Data: 0.0275 (0.0200) Loss: 0.0431 (0.0201)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(164)         0:00:19         0:00:07         0:00:11          0.0201\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(165): [ 10/391] Batch: 0.0475 (0.0703) Data: 0.0179 (0.0311) Loss: 0.0090 (0.0174)\n",
            "TRAIN(165): [ 20/391] Batch: 0.0488 (0.0598) Data: 0.0261 (0.0259) Loss: 0.0056 (0.0169)\n",
            "TRAIN(165): [ 30/391] Batch: 0.0505 (0.0569) Data: 0.0242 (0.0238) Loss: 0.0067 (0.0211)\n",
            "TRAIN(165): [ 40/391] Batch: 0.0563 (0.0553) Data: 0.0234 (0.0232) Loss: 0.0399 (0.0202)\n",
            "TRAIN(165): [ 50/391] Batch: 0.0503 (0.0541) Data: 0.0246 (0.0227) Loss: 0.0154 (0.0201)\n",
            "TRAIN(165): [ 60/391] Batch: 0.0535 (0.0534) Data: 0.0175 (0.0221) Loss: 0.0145 (0.0199)\n",
            "TRAIN(165): [ 70/391] Batch: 0.0376 (0.0525) Data: 0.0266 (0.0221) Loss: 0.0403 (0.0198)\n",
            "TRAIN(165): [ 80/391] Batch: 0.0471 (0.0521) Data: 0.0171 (0.0217) Loss: 0.0451 (0.0194)\n",
            "TRAIN(165): [ 90/391] Batch: 0.0462 (0.0517) Data: 0.0266 (0.0215) Loss: 0.0343 (0.0201)\n",
            "TRAIN(165): [100/391] Batch: 0.0522 (0.0517) Data: 0.0228 (0.0211) Loss: 0.0264 (0.0200)\n",
            "TRAIN(165): [110/391] Batch: 0.0459 (0.0513) Data: 0.0271 (0.0209) Loss: 0.0226 (0.0201)\n",
            "TRAIN(165): [120/391] Batch: 0.0475 (0.0510) Data: 0.0275 (0.0212) Loss: 0.0298 (0.0196)\n",
            "TRAIN(165): [130/391] Batch: 0.0479 (0.0508) Data: 0.0232 (0.0213) Loss: 0.0144 (0.0201)\n",
            "TRAIN(165): [140/391] Batch: 0.0469 (0.0506) Data: 0.0230 (0.0213) Loss: 0.0374 (0.0199)\n",
            "TRAIN(165): [150/391] Batch: 0.0585 (0.0506) Data: 0.0196 (0.0212) Loss: 0.0353 (0.0198)\n",
            "TRAIN(165): [160/391] Batch: 0.0478 (0.0506) Data: 0.0192 (0.0210) Loss: 0.0164 (0.0204)\n",
            "TRAIN(165): [170/391] Batch: 0.0519 (0.0505) Data: 0.0218 (0.0210) Loss: 0.0231 (0.0201)\n",
            "TRAIN(165): [180/391] Batch: 0.0465 (0.0503) Data: 0.0265 (0.0211) Loss: 0.0300 (0.0205)\n",
            "TRAIN(165): [190/391] Batch: 0.0463 (0.0502) Data: 0.0273 (0.0211) Loss: 0.0168 (0.0204)\n",
            "TRAIN(165): [200/391] Batch: 0.0475 (0.0503) Data: 0.0238 (0.0210) Loss: 0.0123 (0.0203)\n",
            "TRAIN(165): [210/391] Batch: 0.0430 (0.0502) Data: 0.0257 (0.0209) Loss: 0.0096 (0.0203)\n",
            "TRAIN(165): [220/391] Batch: 0.0495 (0.0502) Data: 0.0169 (0.0209) Loss: 0.0172 (0.0207)\n",
            "TRAIN(165): [230/391] Batch: 0.0617 (0.0502) Data: 0.0138 (0.0205) Loss: 0.0112 (0.0205)\n",
            "TRAIN(165): [240/391] Batch: 0.0501 (0.0502) Data: 0.0152 (0.0203) Loss: 0.0125 (0.0205)\n",
            "TRAIN(165): [250/391] Batch: 0.0457 (0.0501) Data: 0.0215 (0.0201) Loss: 0.0516 (0.0205)\n",
            "TRAIN(165): [260/391] Batch: 0.0457 (0.0502) Data: 0.0152 (0.0199) Loss: 0.0086 (0.0205)\n",
            "TRAIN(165): [270/391] Batch: 0.0462 (0.0502) Data: 0.0179 (0.0197) Loss: 0.0199 (0.0203)\n",
            "TRAIN(165): [280/391] Batch: 0.0426 (0.0502) Data: 0.0225 (0.0197) Loss: 0.0165 (0.0203)\n",
            "TRAIN(165): [290/391] Batch: 0.0470 (0.0502) Data: 0.0271 (0.0197) Loss: 0.0614 (0.0203)\n",
            "TRAIN(165): [300/391] Batch: 0.0468 (0.0501) Data: 0.0257 (0.0197) Loss: 0.0029 (0.0201)\n",
            "TRAIN(165): [310/391] Batch: 0.0463 (0.0501) Data: 0.0281 (0.0199) Loss: 0.0089 (0.0201)\n",
            "TRAIN(165): [320/391] Batch: 0.0458 (0.0500) Data: 0.0271 (0.0200) Loss: 0.0289 (0.0200)\n",
            "TRAIN(165): [330/391] Batch: 0.0465 (0.0499) Data: 0.0264 (0.0201) Loss: 0.0259 (0.0201)\n",
            "TRAIN(165): [340/391] Batch: 0.0523 (0.0499) Data: 0.0155 (0.0201) Loss: 0.0071 (0.0200)\n",
            "TRAIN(165): [350/391] Batch: 0.0537 (0.0499) Data: 0.0218 (0.0201) Loss: 0.0099 (0.0199)\n",
            "TRAIN(165): [360/391] Batch: 0.0391 (0.0498) Data: 0.0261 (0.0201) Loss: 0.0777 (0.0200)\n",
            "TRAIN(165): [370/391] Batch: 0.0478 (0.0499) Data: 0.0223 (0.0200) Loss: 0.0162 (0.0199)\n",
            "TRAIN(165): [380/391] Batch: 0.0524 (0.0499) Data: 0.0234 (0.0200) Loss: 0.0242 (0.0199)\n",
            "TRAIN(165): [390/391] Batch: 0.0474 (0.0498) Data: 0.0273 (0.0200) Loss: 0.0352 (0.0198)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(165)         0:00:19         0:00:07         0:00:11          0.0198\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(166): [ 10/391] Batch: 0.0419 (0.0675) Data: 0.0228 (0.0286) Loss: 0.0110 (0.0117)\n",
            "TRAIN(166): [ 20/391] Batch: 0.0477 (0.0578) Data: 0.0268 (0.0254) Loss: 0.0184 (0.0134)\n",
            "TRAIN(166): [ 30/391] Batch: 0.0473 (0.0543) Data: 0.0255 (0.0245) Loss: 0.0125 (0.0167)\n",
            "TRAIN(166): [ 40/391] Batch: 0.0484 (0.0533) Data: 0.0261 (0.0233) Loss: 0.0126 (0.0190)\n",
            "TRAIN(166): [ 50/391] Batch: 0.0476 (0.0527) Data: 0.0237 (0.0224) Loss: 0.0157 (0.0179)\n",
            "TRAIN(166): [ 60/391] Batch: 0.0476 (0.0518) Data: 0.0265 (0.0223) Loss: 0.0140 (0.0176)\n",
            "TRAIN(166): [ 70/391] Batch: 0.0525 (0.0517) Data: 0.0169 (0.0217) Loss: 0.0137 (0.0177)\n",
            "TRAIN(166): [ 80/391] Batch: 0.0535 (0.0514) Data: 0.0237 (0.0214) Loss: 0.0136 (0.0176)\n",
            "TRAIN(166): [ 90/391] Batch: 0.0497 (0.0511) Data: 0.0256 (0.0214) Loss: 0.0114 (0.0175)\n",
            "TRAIN(166): [100/391] Batch: 0.0516 (0.0512) Data: 0.0200 (0.0208) Loss: 0.0205 (0.0180)\n",
            "TRAIN(166): [110/391] Batch: 0.0536 (0.0511) Data: 0.0149 (0.0203) Loss: 0.0167 (0.0182)\n",
            "TRAIN(166): [120/391] Batch: 0.0473 (0.0509) Data: 0.0212 (0.0202) Loss: 0.0263 (0.0184)\n",
            "TRAIN(166): [130/391] Batch: 0.0486 (0.0508) Data: 0.0155 (0.0196) Loss: 0.0265 (0.0183)\n",
            "TRAIN(166): [140/391] Batch: 0.0406 (0.0509) Data: 0.0195 (0.0193) Loss: 0.0261 (0.0185)\n",
            "TRAIN(166): [150/391] Batch: 0.0639 (0.0509) Data: 0.0135 (0.0191) Loss: 0.0214 (0.0182)\n",
            "TRAIN(166): [160/391] Batch: 0.0515 (0.0509) Data: 0.0213 (0.0190) Loss: 0.0144 (0.0181)\n",
            "TRAIN(166): [170/391] Batch: 0.0433 (0.0508) Data: 0.0189 (0.0188) Loss: 0.0261 (0.0184)\n",
            "TRAIN(166): [180/391] Batch: 0.0452 (0.0507) Data: 0.0266 (0.0189) Loss: 0.0282 (0.0188)\n",
            "TRAIN(166): [190/391] Batch: 0.0541 (0.0507) Data: 0.0234 (0.0190) Loss: 0.0073 (0.0183)\n",
            "TRAIN(166): [200/391] Batch: 0.0612 (0.0507) Data: 0.0148 (0.0191) Loss: 0.0082 (0.0180)\n",
            "TRAIN(166): [210/391] Batch: 0.0513 (0.0505) Data: 0.0176 (0.0192) Loss: 0.0343 (0.0180)\n",
            "TRAIN(166): [220/391] Batch: 0.0466 (0.0504) Data: 0.0276 (0.0194) Loss: 0.0202 (0.0181)\n",
            "TRAIN(166): [230/391] Batch: 0.0472 (0.0503) Data: 0.0251 (0.0193) Loss: 0.0176 (0.0181)\n",
            "TRAIN(166): [240/391] Batch: 0.0469 (0.0503) Data: 0.0229 (0.0192) Loss: 0.0119 (0.0182)\n",
            "TRAIN(166): [250/391] Batch: 0.0538 (0.0504) Data: 0.0168 (0.0192) Loss: 0.0098 (0.0181)\n",
            "TRAIN(166): [260/391] Batch: 0.0443 (0.0503) Data: 0.0271 (0.0193) Loss: 0.0509 (0.0185)\n",
            "TRAIN(166): [270/391] Batch: 0.0472 (0.0501) Data: 0.0262 (0.0194) Loss: 0.0083 (0.0187)\n",
            "TRAIN(166): [280/391] Batch: 0.0462 (0.0501) Data: 0.0263 (0.0195) Loss: 0.0280 (0.0186)\n",
            "TRAIN(166): [290/391] Batch: 0.0469 (0.0500) Data: 0.0266 (0.0196) Loss: 0.0090 (0.0185)\n",
            "TRAIN(166): [300/391] Batch: 0.0423 (0.0499) Data: 0.0279 (0.0197) Loss: 0.0259 (0.0187)\n",
            "TRAIN(166): [310/391] Batch: 0.0459 (0.0498) Data: 0.0275 (0.0198) Loss: 0.0084 (0.0185)\n",
            "TRAIN(166): [320/391] Batch: 0.0479 (0.0498) Data: 0.0254 (0.0199) Loss: 0.0130 (0.0183)\n",
            "TRAIN(166): [330/391] Batch: 0.0463 (0.0497) Data: 0.0270 (0.0200) Loss: 0.0134 (0.0184)\n",
            "TRAIN(166): [340/391] Batch: 0.0463 (0.0496) Data: 0.0274 (0.0201) Loss: 0.0239 (0.0183)\n",
            "TRAIN(166): [350/391] Batch: 0.0449 (0.0496) Data: 0.0268 (0.0201) Loss: 0.0541 (0.0183)\n",
            "TRAIN(166): [360/391] Batch: 0.0456 (0.0496) Data: 0.0273 (0.0202) Loss: 0.0239 (0.0182)\n",
            "TRAIN(166): [370/391] Batch: 0.0464 (0.0496) Data: 0.0203 (0.0202) Loss: 0.0137 (0.0183)\n",
            "TRAIN(166): [380/391] Batch: 0.0476 (0.0496) Data: 0.0203 (0.0201) Loss: 0.0053 (0.0184)\n",
            "TRAIN(166): [390/391] Batch: 0.0463 (0.0496) Data: 0.0270 (0.0201) Loss: 0.0241 (0.0184)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(166)         0:00:19         0:00:07         0:00:11          0.0184\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(167): [ 10/391] Batch: 0.0459 (0.0676) Data: 0.0206 (0.0360) Loss: 0.0095 (0.0154)\n",
            "TRAIN(167): [ 20/391] Batch: 0.0460 (0.0613) Data: 0.0149 (0.0246) Loss: 0.0494 (0.0190)\n",
            "TRAIN(167): [ 30/391] Batch: 0.0511 (0.0578) Data: 0.0165 (0.0222) Loss: 0.0127 (0.0185)\n",
            "TRAIN(167): [ 40/391] Batch: 0.0446 (0.0558) Data: 0.0287 (0.0210) Loss: 0.0047 (0.0192)\n",
            "TRAIN(167): [ 50/391] Batch: 0.0450 (0.0543) Data: 0.0282 (0.0211) Loss: 0.0092 (0.0180)\n",
            "TRAIN(167): [ 60/391] Batch: 0.0420 (0.0533) Data: 0.0243 (0.0209) Loss: 0.0375 (0.0187)\n",
            "TRAIN(167): [ 70/391] Batch: 0.0468 (0.0529) Data: 0.0189 (0.0206) Loss: 0.0197 (0.0184)\n",
            "TRAIN(167): [ 80/391] Batch: 0.0448 (0.0521) Data: 0.0259 (0.0208) Loss: 0.0092 (0.0181)\n",
            "TRAIN(167): [ 90/391] Batch: 0.0496 (0.0519) Data: 0.0167 (0.0206) Loss: 0.0064 (0.0187)\n",
            "TRAIN(167): [100/391] Batch: 0.0473 (0.0514) Data: 0.0278 (0.0206) Loss: 0.0083 (0.0185)\n",
            "TRAIN(167): [110/391] Batch: 0.0426 (0.0513) Data: 0.0227 (0.0204) Loss: 0.0358 (0.0183)\n",
            "TRAIN(167): [120/391] Batch: 0.0465 (0.0509) Data: 0.0280 (0.0207) Loss: 0.0143 (0.0184)\n",
            "TRAIN(167): [130/391] Batch: 0.0483 (0.0508) Data: 0.0251 (0.0207) Loss: 0.0092 (0.0186)\n",
            "TRAIN(167): [140/391] Batch: 0.0459 (0.0506) Data: 0.0277 (0.0208) Loss: 0.0596 (0.0186)\n",
            "TRAIN(167): [150/391] Batch: 0.0462 (0.0505) Data: 0.0275 (0.0208) Loss: 0.0566 (0.0189)\n",
            "TRAIN(167): [160/391] Batch: 0.0497 (0.0503) Data: 0.0185 (0.0209) Loss: 0.0159 (0.0187)\n",
            "TRAIN(167): [170/391] Batch: 0.0466 (0.0501) Data: 0.0268 (0.0210) Loss: 0.0342 (0.0187)\n",
            "TRAIN(167): [180/391] Batch: 0.0456 (0.0500) Data: 0.0268 (0.0211) Loss: 0.0060 (0.0183)\n",
            "TRAIN(167): [190/391] Batch: 0.0438 (0.0500) Data: 0.0180 (0.0210) Loss: 0.0188 (0.0183)\n",
            "TRAIN(167): [200/391] Batch: 0.0456 (0.0499) Data: 0.0272 (0.0210) Loss: 0.0077 (0.0181)\n",
            "TRAIN(167): [210/391] Batch: 0.0507 (0.0498) Data: 0.0249 (0.0211) Loss: 0.0052 (0.0179)\n",
            "TRAIN(167): [220/391] Batch: 0.0377 (0.0498) Data: 0.0280 (0.0211) Loss: 0.0238 (0.0181)\n",
            "TRAIN(167): [230/391] Batch: 0.0538 (0.0498) Data: 0.0246 (0.0210) Loss: 0.0219 (0.0181)\n",
            "TRAIN(167): [240/391] Batch: 0.0553 (0.0498) Data: 0.0198 (0.0210) Loss: 0.0105 (0.0179)\n",
            "TRAIN(167): [250/391] Batch: 0.0479 (0.0499) Data: 0.0155 (0.0208) Loss: 0.0075 (0.0178)\n",
            "TRAIN(167): [260/391] Batch: 0.0554 (0.0499) Data: 0.0136 (0.0205) Loss: 0.0097 (0.0177)\n",
            "TRAIN(167): [270/391] Batch: 0.0604 (0.0499) Data: 0.0156 (0.0204) Loss: 0.0160 (0.0178)\n",
            "TRAIN(167): [280/391] Batch: 0.0408 (0.0499) Data: 0.0248 (0.0203) Loss: 0.0087 (0.0178)\n",
            "TRAIN(167): [290/391] Batch: 0.0477 (0.0500) Data: 0.0161 (0.0201) Loss: 0.0157 (0.0180)\n",
            "TRAIN(167): [300/391] Batch: 0.0485 (0.0501) Data: 0.0195 (0.0199) Loss: 0.0202 (0.0178)\n",
            "TRAIN(167): [310/391] Batch: 0.0448 (0.0501) Data: 0.0265 (0.0199) Loss: 0.0147 (0.0179)\n",
            "TRAIN(167): [320/391] Batch: 0.0443 (0.0501) Data: 0.0262 (0.0199) Loss: 0.0244 (0.0178)\n",
            "TRAIN(167): [330/391] Batch: 0.0441 (0.0500) Data: 0.0264 (0.0200) Loss: 0.0268 (0.0177)\n",
            "TRAIN(167): [340/391] Batch: 0.0359 (0.0500) Data: 0.0263 (0.0200) Loss: 0.0123 (0.0177)\n",
            "TRAIN(167): [350/391] Batch: 0.0508 (0.0500) Data: 0.0254 (0.0201) Loss: 0.0134 (0.0175)\n",
            "TRAIN(167): [360/391] Batch: 0.0454 (0.0499) Data: 0.0261 (0.0201) Loss: 0.0300 (0.0175)\n",
            "TRAIN(167): [370/391] Batch: 0.0518 (0.0499) Data: 0.0157 (0.0200) Loss: 0.0133 (0.0176)\n",
            "TRAIN(167): [380/391] Batch: 0.0474 (0.0499) Data: 0.0252 (0.0201) Loss: 0.0195 (0.0177)\n",
            "TRAIN(167): [390/391] Batch: 0.0473 (0.0498) Data: 0.0262 (0.0201) Loss: 0.0138 (0.0177)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(167)         0:00:19         0:00:07         0:00:11          0.0177\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(168): [ 10/391] Batch: 0.0447 (0.0660) Data: 0.0261 (0.0344) Loss: 0.0043 (0.0145)\n",
            "TRAIN(168): [ 20/391] Batch: 0.0445 (0.0579) Data: 0.0230 (0.0282) Loss: 0.0179 (0.0136)\n",
            "TRAIN(168): [ 30/391] Batch: 0.0467 (0.0552) Data: 0.0206 (0.0257) Loss: 0.0057 (0.0137)\n",
            "TRAIN(168): [ 40/391] Batch: 0.0481 (0.0533) Data: 0.0265 (0.0247) Loss: 0.0081 (0.0137)\n",
            "TRAIN(168): [ 50/391] Batch: 0.0499 (0.0522) Data: 0.0249 (0.0243) Loss: 0.0248 (0.0167)\n",
            "TRAIN(168): [ 60/391] Batch: 0.0461 (0.0514) Data: 0.0265 (0.0241) Loss: 0.0095 (0.0170)\n",
            "TRAIN(168): [ 70/391] Batch: 0.0480 (0.0513) Data: 0.0180 (0.0233) Loss: 0.0035 (0.0179)\n",
            "TRAIN(168): [ 80/391] Batch: 0.0461 (0.0508) Data: 0.0276 (0.0232) Loss: 0.0133 (0.0188)\n",
            "TRAIN(168): [ 90/391] Batch: 0.0460 (0.0505) Data: 0.0280 (0.0231) Loss: 0.0152 (0.0181)\n",
            "TRAIN(168): [100/391] Batch: 0.0508 (0.0505) Data: 0.0217 (0.0227) Loss: 0.0159 (0.0175)\n",
            "TRAIN(168): [110/391] Batch: 0.0432 (0.0505) Data: 0.0257 (0.0223) Loss: 0.0143 (0.0170)\n",
            "TRAIN(168): [120/391] Batch: 0.0424 (0.0506) Data: 0.0189 (0.0219) Loss: 0.0123 (0.0175)\n",
            "TRAIN(168): [130/391] Batch: 0.0478 (0.0506) Data: 0.0199 (0.0215) Loss: 0.0208 (0.0174)\n",
            "TRAIN(168): [140/391] Batch: 0.0555 (0.0505) Data: 0.0151 (0.0211) Loss: 0.0176 (0.0172)\n",
            "TRAIN(168): [150/391] Batch: 0.0606 (0.0506) Data: 0.0126 (0.0205) Loss: 0.0114 (0.0172)\n",
            "TRAIN(168): [160/391] Batch: 0.0619 (0.0507) Data: 0.0157 (0.0203) Loss: 0.0177 (0.0170)\n",
            "TRAIN(168): [170/391] Batch: 0.0479 (0.0507) Data: 0.0196 (0.0202) Loss: 0.0294 (0.0175)\n",
            "TRAIN(168): [180/391] Batch: 0.0500 (0.0508) Data: 0.0205 (0.0199) Loss: 0.0058 (0.0176)\n",
            "TRAIN(168): [190/391] Batch: 0.0390 (0.0506) Data: 0.0240 (0.0200) Loss: 0.0119 (0.0176)\n",
            "TRAIN(168): [200/391] Batch: 0.0455 (0.0504) Data: 0.0279 (0.0202) Loss: 0.0180 (0.0179)\n",
            "TRAIN(168): [210/391] Batch: 0.0475 (0.0503) Data: 0.0262 (0.0204) Loss: 0.0193 (0.0176)\n",
            "TRAIN(168): [220/391] Batch: 0.0466 (0.0502) Data: 0.0271 (0.0205) Loss: 0.0166 (0.0175)\n",
            "TRAIN(168): [230/391] Batch: 0.0471 (0.0501) Data: 0.0271 (0.0205) Loss: 0.0245 (0.0173)\n",
            "TRAIN(168): [240/391] Batch: 0.0468 (0.0501) Data: 0.0275 (0.0204) Loss: 0.0080 (0.0170)\n",
            "TRAIN(168): [250/391] Batch: 0.0451 (0.0500) Data: 0.0269 (0.0204) Loss: 0.0115 (0.0168)\n",
            "TRAIN(168): [260/391] Batch: 0.0452 (0.0499) Data: 0.0276 (0.0205) Loss: 0.0043 (0.0171)\n",
            "TRAIN(168): [270/391] Batch: 0.0525 (0.0499) Data: 0.0247 (0.0205) Loss: 0.0147 (0.0170)\n",
            "TRAIN(168): [280/391] Batch: 0.0461 (0.0498) Data: 0.0269 (0.0205) Loss: 0.0147 (0.0171)\n",
            "TRAIN(168): [290/391] Batch: 0.0450 (0.0498) Data: 0.0265 (0.0206) Loss: 0.0167 (0.0171)\n",
            "TRAIN(168): [300/391] Batch: 0.0446 (0.0497) Data: 0.0175 (0.0206) Loss: 0.0108 (0.0169)\n",
            "TRAIN(168): [310/391] Batch: 0.0517 (0.0497) Data: 0.0248 (0.0207) Loss: 0.0159 (0.0167)\n",
            "TRAIN(168): [320/391] Batch: 0.0472 (0.0497) Data: 0.0239 (0.0206) Loss: 0.0371 (0.0171)\n",
            "TRAIN(168): [330/391] Batch: 0.0465 (0.0496) Data: 0.0265 (0.0207) Loss: 0.0108 (0.0173)\n",
            "TRAIN(168): [340/391] Batch: 0.0396 (0.0496) Data: 0.0249 (0.0207) Loss: 0.0134 (0.0172)\n",
            "TRAIN(168): [350/391] Batch: 0.0472 (0.0496) Data: 0.0263 (0.0208) Loss: 0.0083 (0.0171)\n",
            "TRAIN(168): [360/391] Batch: 0.0452 (0.0495) Data: 0.0260 (0.0208) Loss: 0.0072 (0.0170)\n",
            "TRAIN(168): [370/391] Batch: 0.0381 (0.0495) Data: 0.0246 (0.0208) Loss: 0.0124 (0.0170)\n",
            "TRAIN(168): [380/391] Batch: 0.0517 (0.0495) Data: 0.0150 (0.0207) Loss: 0.0095 (0.0172)\n",
            "TRAIN(168): [390/391] Batch: 0.0478 (0.0494) Data: 0.0260 (0.0207) Loss: 0.0084 (0.0171)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(168)         0:00:19         0:00:08         0:00:11          0.0171\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(169): [ 10/391] Batch: 0.0465 (0.0711) Data: 0.0162 (0.0332) Loss: 0.0230 (0.0173)\n",
            "TRAIN(169): [ 20/391] Batch: 0.0699 (0.0615) Data: 0.0175 (0.0258) Loss: 0.0023 (0.0197)\n",
            "TRAIN(169): [ 30/391] Batch: 0.0460 (0.0575) Data: 0.0157 (0.0225) Loss: 0.0174 (0.0174)\n",
            "TRAIN(169): [ 40/391] Batch: 0.0627 (0.0558) Data: 0.0138 (0.0207) Loss: 0.0094 (0.0172)\n",
            "TRAIN(169): [ 50/391] Batch: 0.0565 (0.0548) Data: 0.0149 (0.0193) Loss: 0.0069 (0.0165)\n",
            "TRAIN(169): [ 60/391] Batch: 0.0530 (0.0538) Data: 0.0252 (0.0192) Loss: 0.0091 (0.0177)\n",
            "TRAIN(169): [ 70/391] Batch: 0.0476 (0.0531) Data: 0.0267 (0.0193) Loss: 0.0123 (0.0171)\n",
            "TRAIN(169): [ 80/391] Batch: 0.0450 (0.0526) Data: 0.0254 (0.0192) Loss: 0.0223 (0.0177)\n",
            "TRAIN(169): [ 90/391] Batch: 0.0644 (0.0523) Data: 0.0154 (0.0193) Loss: 0.0176 (0.0181)\n",
            "TRAIN(169): [100/391] Batch: 0.0517 (0.0519) Data: 0.0258 (0.0196) Loss: 0.0056 (0.0177)\n",
            "TRAIN(169): [110/391] Batch: 0.0442 (0.0516) Data: 0.0272 (0.0198) Loss: 0.0057 (0.0175)\n",
            "TRAIN(169): [120/391] Batch: 0.0441 (0.0514) Data: 0.0278 (0.0199) Loss: 0.0129 (0.0173)\n",
            "TRAIN(169): [130/391] Batch: 0.0495 (0.0512) Data: 0.0258 (0.0201) Loss: 0.0084 (0.0171)\n",
            "TRAIN(169): [140/391] Batch: 0.0520 (0.0510) Data: 0.0186 (0.0201) Loss: 0.0657 (0.0175)\n",
            "TRAIN(169): [150/391] Batch: 0.0470 (0.0508) Data: 0.0280 (0.0203) Loss: 0.0064 (0.0173)\n",
            "TRAIN(169): [160/391] Batch: 0.0365 (0.0507) Data: 0.0267 (0.0201) Loss: 0.0155 (0.0169)\n",
            "TRAIN(169): [170/391] Batch: 0.0546 (0.0506) Data: 0.0159 (0.0201) Loss: 0.0089 (0.0171)\n",
            "TRAIN(169): [180/391] Batch: 0.0516 (0.0506) Data: 0.0236 (0.0200) Loss: 0.0044 (0.0168)\n",
            "TRAIN(169): [190/391] Batch: 0.0438 (0.0505) Data: 0.0270 (0.0202) Loss: 0.0092 (0.0168)\n",
            "TRAIN(169): [200/391] Batch: 0.0468 (0.0504) Data: 0.0268 (0.0203) Loss: 0.0071 (0.0166)\n",
            "TRAIN(169): [210/391] Batch: 0.0463 (0.0502) Data: 0.0280 (0.0205) Loss: 0.0031 (0.0166)\n",
            "TRAIN(169): [220/391] Batch: 0.0545 (0.0502) Data: 0.0166 (0.0204) Loss: 0.0176 (0.0165)\n",
            "TRAIN(169): [230/391] Batch: 0.0446 (0.0502) Data: 0.0200 (0.0204) Loss: 0.0235 (0.0164)\n",
            "TRAIN(169): [240/391] Batch: 0.0482 (0.0500) Data: 0.0253 (0.0205) Loss: 0.0184 (0.0162)\n",
            "TRAIN(169): [250/391] Batch: 0.0496 (0.0500) Data: 0.0217 (0.0205) Loss: 0.0040 (0.0159)\n",
            "TRAIN(169): [260/391] Batch: 0.0595 (0.0500) Data: 0.0142 (0.0204) Loss: 0.0041 (0.0158)\n",
            "TRAIN(169): [270/391] Batch: 0.0472 (0.0501) Data: 0.0189 (0.0202) Loss: 0.0326 (0.0156)\n",
            "TRAIN(169): [280/391] Batch: 0.0605 (0.0502) Data: 0.0153 (0.0200) Loss: 0.0080 (0.0158)\n",
            "TRAIN(169): [290/391] Batch: 0.0399 (0.0503) Data: 0.0206 (0.0199) Loss: 0.0038 (0.0158)\n",
            "TRAIN(169): [300/391] Batch: 0.0489 (0.0503) Data: 0.0148 (0.0198) Loss: 0.0062 (0.0158)\n",
            "TRAIN(169): [310/391] Batch: 0.0455 (0.0504) Data: 0.0159 (0.0196) Loss: 0.0022 (0.0161)\n",
            "TRAIN(169): [320/391] Batch: 0.0526 (0.0504) Data: 0.0192 (0.0195) Loss: 0.0218 (0.0163)\n",
            "TRAIN(169): [330/391] Batch: 0.0529 (0.0504) Data: 0.0152 (0.0195) Loss: 0.0185 (0.0161)\n",
            "TRAIN(169): [340/391] Batch: 0.0467 (0.0504) Data: 0.0223 (0.0194) Loss: 0.0161 (0.0160)\n",
            "TRAIN(169): [350/391] Batch: 0.0560 (0.0503) Data: 0.0189 (0.0195) Loss: 0.0038 (0.0159)\n",
            "TRAIN(169): [360/391] Batch: 0.0457 (0.0503) Data: 0.0267 (0.0196) Loss: 0.0146 (0.0161)\n",
            "TRAIN(169): [370/391] Batch: 0.0490 (0.0502) Data: 0.0253 (0.0196) Loss: 0.0077 (0.0162)\n",
            "TRAIN(169): [380/391] Batch: 0.0464 (0.0502) Data: 0.0219 (0.0196) Loss: 0.0271 (0.0161)\n",
            "TRAIN(169): [390/391] Batch: 0.0465 (0.0502) Data: 0.0268 (0.0196) Loss: 0.0161 (0.0163)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(169)         0:00:19         0:00:07         0:00:11          0.0163\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(170): [ 10/391] Batch: 0.0433 (0.0628) Data: 0.0228 (0.0353) Loss: 0.0301 (0.0184)\n",
            "TRAIN(170): [ 20/391] Batch: 0.0506 (0.0564) Data: 0.0195 (0.0277) Loss: 0.0246 (0.0156)\n",
            "TRAIN(170): [ 30/391] Batch: 0.0447 (0.0532) Data: 0.0287 (0.0260) Loss: 0.0080 (0.0164)\n",
            "TRAIN(170): [ 40/391] Batch: 0.0485 (0.0521) Data: 0.0176 (0.0248) Loss: 0.0036 (0.0170)\n",
            "TRAIN(170): [ 50/391] Batch: 0.0545 (0.0516) Data: 0.0173 (0.0237) Loss: 0.0060 (0.0154)\n",
            "TRAIN(170): [ 60/391] Batch: 0.0472 (0.0507) Data: 0.0265 (0.0236) Loss: 0.0068 (0.0152)\n",
            "TRAIN(170): [ 70/391] Batch: 0.0480 (0.0503) Data: 0.0240 (0.0233) Loss: 0.0211 (0.0153)\n",
            "TRAIN(170): [ 80/391] Batch: 0.0483 (0.0498) Data: 0.0250 (0.0233) Loss: 0.0157 (0.0155)\n",
            "TRAIN(170): [ 90/391] Batch: 0.0456 (0.0495) Data: 0.0285 (0.0234) Loss: 0.0098 (0.0152)\n",
            "TRAIN(170): [100/391] Batch: 0.0482 (0.0495) Data: 0.0227 (0.0229) Loss: 0.0102 (0.0150)\n",
            "TRAIN(170): [110/391] Batch: 0.0472 (0.0494) Data: 0.0282 (0.0229) Loss: 0.0099 (0.0146)\n",
            "TRAIN(170): [120/391] Batch: 0.0455 (0.0492) Data: 0.0277 (0.0229) Loss: 0.0174 (0.0143)\n",
            "TRAIN(170): [130/391] Batch: 0.0455 (0.0491) Data: 0.0268 (0.0230) Loss: 0.0128 (0.0138)\n",
            "TRAIN(170): [140/391] Batch: 0.0447 (0.0491) Data: 0.0209 (0.0227) Loss: 0.0191 (0.0138)\n",
            "TRAIN(170): [150/391] Batch: 0.0463 (0.0492) Data: 0.0178 (0.0223) Loss: 0.0096 (0.0139)\n",
            "TRAIN(170): [160/391] Batch: 0.0555 (0.0492) Data: 0.0156 (0.0219) Loss: 0.0342 (0.0139)\n",
            "TRAIN(170): [170/391] Batch: 0.0531 (0.0493) Data: 0.0200 (0.0216) Loss: 0.0264 (0.0139)\n",
            "TRAIN(170): [180/391] Batch: 0.0527 (0.0494) Data: 0.0178 (0.0214) Loss: 0.0126 (0.0139)\n",
            "TRAIN(170): [190/391] Batch: 0.0475 (0.0494) Data: 0.0219 (0.0212) Loss: 0.0113 (0.0140)\n",
            "TRAIN(170): [200/391] Batch: 0.0536 (0.0495) Data: 0.0153 (0.0211) Loss: 0.0179 (0.0141)\n",
            "TRAIN(170): [210/391] Batch: 0.0445 (0.0495) Data: 0.0263 (0.0210) Loss: 0.0373 (0.0145)\n",
            "TRAIN(170): [220/391] Batch: 0.0457 (0.0495) Data: 0.0218 (0.0211) Loss: 0.0115 (0.0147)\n",
            "TRAIN(170): [230/391] Batch: 0.0416 (0.0494) Data: 0.0256 (0.0211) Loss: 0.0101 (0.0148)\n",
            "TRAIN(170): [240/391] Batch: 0.0458 (0.0494) Data: 0.0278 (0.0212) Loss: 0.0204 (0.0146)\n",
            "TRAIN(170): [250/391] Batch: 0.0478 (0.0494) Data: 0.0245 (0.0212) Loss: 0.0222 (0.0147)\n",
            "TRAIN(170): [260/391] Batch: 0.0437 (0.0494) Data: 0.0272 (0.0211) Loss: 0.0164 (0.0149)\n",
            "TRAIN(170): [270/391] Batch: 0.0437 (0.0494) Data: 0.0235 (0.0211) Loss: 0.0033 (0.0148)\n",
            "TRAIN(170): [280/391] Batch: 0.0466 (0.0493) Data: 0.0270 (0.0212) Loss: 0.0074 (0.0148)\n",
            "TRAIN(170): [290/391] Batch: 0.0429 (0.0493) Data: 0.0271 (0.0213) Loss: 0.0277 (0.0149)\n",
            "TRAIN(170): [300/391] Batch: 0.0360 (0.0492) Data: 0.0269 (0.0213) Loss: 0.0081 (0.0147)\n",
            "TRAIN(170): [310/391] Batch: 0.0461 (0.0492) Data: 0.0231 (0.0212) Loss: 0.0104 (0.0146)\n",
            "TRAIN(170): [320/391] Batch: 0.0535 (0.0493) Data: 0.0164 (0.0211) Loss: 0.0174 (0.0147)\n",
            "TRAIN(170): [330/391] Batch: 0.0466 (0.0493) Data: 0.0270 (0.0211) Loss: 0.0040 (0.0147)\n",
            "TRAIN(170): [340/391] Batch: 0.0513 (0.0492) Data: 0.0256 (0.0212) Loss: 0.0276 (0.0147)\n",
            "TRAIN(170): [350/391] Batch: 0.0471 (0.0492) Data: 0.0284 (0.0212) Loss: 0.0103 (0.0146)\n",
            "TRAIN(170): [360/391] Batch: 0.0455 (0.0492) Data: 0.0273 (0.0212) Loss: 0.0075 (0.0147)\n",
            "TRAIN(170): [370/391] Batch: 0.0458 (0.0491) Data: 0.0273 (0.0213) Loss: 0.0038 (0.0145)\n",
            "TRAIN(170): [380/391] Batch: 0.0382 (0.0491) Data: 0.0270 (0.0213) Loss: 0.0093 (0.0147)\n",
            "TRAIN(170): [390/391] Batch: 0.0442 (0.0490) Data: 0.0285 (0.0213) Loss: 0.0133 (0.0149)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(170)         0:00:19         0:00:08         0:00:10          0.0149\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(171): [ 10/391] Batch: 0.0453 (0.0637) Data: 0.0258 (0.0322) Loss: 0.0373 (0.0187)\n",
            "TRAIN(171): [ 20/391] Batch: 0.0471 (0.0588) Data: 0.0190 (0.0248) Loss: 0.0072 (0.0183)\n",
            "TRAIN(171): [ 30/391] Batch: 0.0479 (0.0564) Data: 0.0198 (0.0223) Loss: 0.0250 (0.0174)\n",
            "TRAIN(171): [ 40/391] Batch: 0.0585 (0.0553) Data: 0.0138 (0.0208) Loss: 0.0122 (0.0166)\n",
            "TRAIN(171): [ 50/391] Batch: 0.0485 (0.0542) Data: 0.0200 (0.0201) Loss: 0.0118 (0.0177)\n",
            "TRAIN(171): [ 60/391] Batch: 0.0706 (0.0539) Data: 0.0129 (0.0194) Loss: 0.0154 (0.0167)\n",
            "TRAIN(171): [ 70/391] Batch: 0.0490 (0.0535) Data: 0.0185 (0.0188) Loss: 0.0190 (0.0163)\n",
            "TRAIN(171): [ 80/391] Batch: 0.0429 (0.0533) Data: 0.0282 (0.0186) Loss: 0.0166 (0.0156)\n",
            "TRAIN(171): [ 90/391] Batch: 0.0460 (0.0528) Data: 0.0275 (0.0188) Loss: 0.0186 (0.0156)\n",
            "TRAIN(171): [100/391] Batch: 0.0531 (0.0526) Data: 0.0242 (0.0187) Loss: 0.0072 (0.0149)\n",
            "TRAIN(171): [110/391] Batch: 0.0536 (0.0522) Data: 0.0253 (0.0191) Loss: 0.0145 (0.0148)\n",
            "TRAIN(171): [120/391] Batch: 0.0480 (0.0520) Data: 0.0258 (0.0191) Loss: 0.0099 (0.0145)\n",
            "TRAIN(171): [130/391] Batch: 0.0509 (0.0519) Data: 0.0172 (0.0192) Loss: 0.0309 (0.0147)\n",
            "TRAIN(171): [140/391] Batch: 0.0448 (0.0516) Data: 0.0265 (0.0193) Loss: 0.0053 (0.0146)\n",
            "TRAIN(171): [150/391] Batch: 0.0473 (0.0515) Data: 0.0270 (0.0195) Loss: 0.0031 (0.0144)\n",
            "TRAIN(171): [160/391] Batch: 0.0463 (0.0512) Data: 0.0275 (0.0197) Loss: 0.0102 (0.0144)\n",
            "TRAIN(171): [170/391] Batch: 0.0457 (0.0512) Data: 0.0225 (0.0196) Loss: 0.0223 (0.0149)\n",
            "TRAIN(171): [180/391] Batch: 0.0465 (0.0512) Data: 0.0225 (0.0194) Loss: 0.0139 (0.0146)\n",
            "TRAIN(171): [190/391] Batch: 0.0574 (0.0512) Data: 0.0150 (0.0193) Loss: 0.0060 (0.0145)\n",
            "TRAIN(171): [200/391] Batch: 0.0466 (0.0510) Data: 0.0270 (0.0193) Loss: 0.0077 (0.0146)\n",
            "TRAIN(171): [210/391] Batch: 0.0473 (0.0510) Data: 0.0277 (0.0194) Loss: 0.0082 (0.0145)\n",
            "TRAIN(171): [220/391] Batch: 0.0425 (0.0509) Data: 0.0258 (0.0195) Loss: 0.0071 (0.0147)\n",
            "TRAIN(171): [230/391] Batch: 0.0460 (0.0508) Data: 0.0157 (0.0193) Loss: 0.0119 (0.0145)\n",
            "TRAIN(171): [240/391] Batch: 0.0527 (0.0508) Data: 0.0158 (0.0192) Loss: 0.0139 (0.0148)\n",
            "TRAIN(171): [250/391] Batch: 0.0445 (0.0508) Data: 0.0188 (0.0192) Loss: 0.0153 (0.0147)\n",
            "TRAIN(171): [260/391] Batch: 0.0452 (0.0507) Data: 0.0281 (0.0193) Loss: 0.0090 (0.0147)\n",
            "TRAIN(171): [270/391] Batch: 0.0581 (0.0506) Data: 0.0171 (0.0194) Loss: 0.0442 (0.0148)\n",
            "TRAIN(171): [280/391] Batch: 0.0580 (0.0505) Data: 0.0176 (0.0195) Loss: 0.0116 (0.0146)\n",
            "TRAIN(171): [290/391] Batch: 0.0521 (0.0506) Data: 0.0183 (0.0194) Loss: 0.0044 (0.0145)\n",
            "TRAIN(171): [300/391] Batch: 0.0531 (0.0506) Data: 0.0200 (0.0193) Loss: 0.0122 (0.0145)\n",
            "TRAIN(171): [310/391] Batch: 0.0502 (0.0506) Data: 0.0201 (0.0193) Loss: 0.0360 (0.0145)\n",
            "TRAIN(171): [320/391] Batch: 0.0463 (0.0506) Data: 0.0218 (0.0193) Loss: 0.0029 (0.0143)\n",
            "TRAIN(171): [330/391] Batch: 0.0415 (0.0506) Data: 0.0244 (0.0191) Loss: 0.0053 (0.0142)\n",
            "TRAIN(171): [340/391] Batch: 0.0423 (0.0506) Data: 0.0189 (0.0190) Loss: 0.0028 (0.0142)\n",
            "TRAIN(171): [350/391] Batch: 0.0462 (0.0506) Data: 0.0254 (0.0190) Loss: 0.0077 (0.0141)\n",
            "TRAIN(171): [360/391] Batch: 0.0552 (0.0506) Data: 0.0238 (0.0192) Loss: 0.0387 (0.0142)\n",
            "TRAIN(171): [370/391] Batch: 0.0556 (0.0505) Data: 0.0219 (0.0192) Loss: 0.0117 (0.0142)\n",
            "TRAIN(171): [380/391] Batch: 0.0398 (0.0504) Data: 0.0281 (0.0193) Loss: 0.0065 (0.0141)\n",
            "TRAIN(171): [390/391] Batch: 0.0463 (0.0504) Data: 0.0266 (0.0194) Loss: 0.0069 (0.0141)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(171)         0:00:19         0:00:07         0:00:12          0.0141\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(172): [ 10/391] Batch: 0.0444 (0.0649) Data: 0.0261 (0.0324) Loss: 0.0035 (0.0159)\n",
            "TRAIN(172): [ 20/391] Batch: 0.0415 (0.0572) Data: 0.0280 (0.0275) Loss: 0.0110 (0.0172)\n",
            "TRAIN(172): [ 30/391] Batch: 0.0370 (0.0542) Data: 0.0269 (0.0251) Loss: 0.0133 (0.0153)\n",
            "TRAIN(172): [ 40/391] Batch: 0.0532 (0.0531) Data: 0.0245 (0.0239) Loss: 0.0156 (0.0137)\n",
            "TRAIN(172): [ 50/391] Batch: 0.0594 (0.0528) Data: 0.0179 (0.0233) Loss: 0.0180 (0.0131)\n",
            "TRAIN(172): [ 60/391] Batch: 0.0457 (0.0522) Data: 0.0199 (0.0226) Loss: 0.0070 (0.0132)\n",
            "TRAIN(172): [ 70/391] Batch: 0.0522 (0.0518) Data: 0.0240 (0.0223) Loss: 0.0071 (0.0129)\n",
            "TRAIN(172): [ 80/391] Batch: 0.0601 (0.0516) Data: 0.0181 (0.0221) Loss: 0.0108 (0.0133)\n",
            "TRAIN(172): [ 90/391] Batch: 0.0455 (0.0511) Data: 0.0278 (0.0221) Loss: 0.0577 (0.0137)\n",
            "TRAIN(172): [100/391] Batch: 0.0498 (0.0508) Data: 0.0229 (0.0220) Loss: 0.0047 (0.0134)\n",
            "TRAIN(172): [110/391] Batch: 0.0446 (0.0505) Data: 0.0279 (0.0220) Loss: 0.0038 (0.0136)\n",
            "TRAIN(172): [120/391] Batch: 0.0560 (0.0503) Data: 0.0201 (0.0220) Loss: 0.0041 (0.0138)\n",
            "TRAIN(172): [130/391] Batch: 0.0518 (0.0502) Data: 0.0214 (0.0220) Loss: 0.0025 (0.0137)\n",
            "TRAIN(172): [140/391] Batch: 0.0472 (0.0501) Data: 0.0245 (0.0218) Loss: 0.0481 (0.0141)\n",
            "TRAIN(172): [150/391] Batch: 0.0538 (0.0503) Data: 0.0156 (0.0214) Loss: 0.0085 (0.0141)\n",
            "TRAIN(172): [160/391] Batch: 0.0452 (0.0503) Data: 0.0223 (0.0212) Loss: 0.0078 (0.0143)\n",
            "TRAIN(172): [170/391] Batch: 0.0492 (0.0503) Data: 0.0157 (0.0209) Loss: 0.0096 (0.0143)\n",
            "TRAIN(172): [180/391] Batch: 0.0707 (0.0504) Data: 0.0120 (0.0205) Loss: 0.0049 (0.0147)\n",
            "TRAIN(172): [190/391] Batch: 0.0395 (0.0504) Data: 0.0244 (0.0203) Loss: 0.0169 (0.0146)\n",
            "TRAIN(172): [200/391] Batch: 0.0465 (0.0505) Data: 0.0153 (0.0202) Loss: 0.0077 (0.0145)\n",
            "TRAIN(172): [210/391] Batch: 0.0388 (0.0505) Data: 0.0240 (0.0201) Loss: 0.0114 (0.0148)\n",
            "TRAIN(172): [220/391] Batch: 0.0551 (0.0506) Data: 0.0152 (0.0199) Loss: 0.0215 (0.0147)\n",
            "TRAIN(172): [230/391] Batch: 0.0444 (0.0505) Data: 0.0266 (0.0199) Loss: 0.0030 (0.0147)\n",
            "TRAIN(172): [240/391] Batch: 0.0525 (0.0506) Data: 0.0163 (0.0197) Loss: 0.0162 (0.0148)\n",
            "TRAIN(172): [250/391] Batch: 0.0425 (0.0504) Data: 0.0259 (0.0197) Loss: 0.0301 (0.0150)\n",
            "TRAIN(172): [260/391] Batch: 0.0429 (0.0504) Data: 0.0259 (0.0198) Loss: 0.0082 (0.0148)\n",
            "TRAIN(172): [270/391] Batch: 0.0573 (0.0504) Data: 0.0162 (0.0198) Loss: 0.0054 (0.0147)\n",
            "TRAIN(172): [280/391] Batch: 0.0443 (0.0503) Data: 0.0264 (0.0197) Loss: 0.0048 (0.0146)\n",
            "TRAIN(172): [290/391] Batch: 0.0434 (0.0503) Data: 0.0273 (0.0198) Loss: 0.0122 (0.0145)\n",
            "TRAIN(172): [300/391] Batch: 0.0446 (0.0502) Data: 0.0256 (0.0199) Loss: 0.0250 (0.0145)\n",
            "TRAIN(172): [310/391] Batch: 0.0549 (0.0501) Data: 0.0244 (0.0200) Loss: 0.0098 (0.0144)\n",
            "TRAIN(172): [320/391] Batch: 0.0484 (0.0501) Data: 0.0230 (0.0200) Loss: 0.0167 (0.0144)\n",
            "TRAIN(172): [330/391] Batch: 0.0445 (0.0500) Data: 0.0247 (0.0200) Loss: 0.0183 (0.0143)\n",
            "TRAIN(172): [340/391] Batch: 0.0592 (0.0500) Data: 0.0175 (0.0201) Loss: 0.0183 (0.0143)\n",
            "TRAIN(172): [350/391] Batch: 0.0465 (0.0500) Data: 0.0279 (0.0201) Loss: 0.0078 (0.0143)\n",
            "TRAIN(172): [360/391] Batch: 0.0468 (0.0499) Data: 0.0268 (0.0202) Loss: 0.0153 (0.0145)\n",
            "TRAIN(172): [370/391] Batch: 0.0359 (0.0498) Data: 0.0276 (0.0202) Loss: 0.0111 (0.0144)\n",
            "TRAIN(172): [380/391] Batch: 0.0422 (0.0498) Data: 0.0197 (0.0202) Loss: 0.0076 (0.0144)\n",
            "TRAIN(172): [390/391] Batch: 0.0467 (0.0498) Data: 0.0268 (0.0202) Loss: 0.0223 (0.0143)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(172)         0:00:19         0:00:07         0:00:11          0.0143\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(173): [ 10/391] Batch: 0.0405 (0.0650) Data: 0.0235 (0.0310) Loss: 0.0090 (0.0124)\n",
            "TRAIN(173): [ 20/391] Batch: 0.0425 (0.0574) Data: 0.0268 (0.0253) Loss: 0.0038 (0.0135)\n",
            "TRAIN(173): [ 30/391] Batch: 0.0524 (0.0552) Data: 0.0187 (0.0235) Loss: 0.0105 (0.0168)\n",
            "TRAIN(173): [ 40/391] Batch: 0.0434 (0.0538) Data: 0.0227 (0.0219) Loss: 0.0125 (0.0160)\n",
            "TRAIN(173): [ 50/391] Batch: 0.0533 (0.0533) Data: 0.0156 (0.0210) Loss: 0.0068 (0.0149)\n",
            "TRAIN(173): [ 60/391] Batch: 0.0550 (0.0528) Data: 0.0121 (0.0199) Loss: 0.0038 (0.0155)\n",
            "TRAIN(173): [ 70/391] Batch: 0.0597 (0.0524) Data: 0.0158 (0.0195) Loss: 0.0068 (0.0148)\n",
            "TRAIN(173): [ 80/391] Batch: 0.0470 (0.0520) Data: 0.0240 (0.0195) Loss: 0.0093 (0.0146)\n",
            "TRAIN(173): [ 90/391] Batch: 0.0540 (0.0520) Data: 0.0163 (0.0190) Loss: 0.0076 (0.0139)\n",
            "TRAIN(173): [100/391] Batch: 0.0451 (0.0518) Data: 0.0258 (0.0189) Loss: 0.0304 (0.0141)\n",
            "TRAIN(173): [110/391] Batch: 0.0485 (0.0514) Data: 0.0259 (0.0193) Loss: 0.0120 (0.0140)\n",
            "TRAIN(173): [120/391] Batch: 0.0474 (0.0511) Data: 0.0253 (0.0196) Loss: 0.0042 (0.0142)\n",
            "TRAIN(173): [130/391] Batch: 0.0498 (0.0508) Data: 0.0242 (0.0199) Loss: 0.0051 (0.0142)\n",
            "TRAIN(173): [140/391] Batch: 0.0458 (0.0505) Data: 0.0274 (0.0201) Loss: 0.0410 (0.0141)\n",
            "TRAIN(173): [150/391] Batch: 0.0502 (0.0503) Data: 0.0251 (0.0204) Loss: 0.0239 (0.0141)\n",
            "TRAIN(173): [160/391] Batch: 0.0521 (0.0502) Data: 0.0171 (0.0205) Loss: 0.0196 (0.0140)\n",
            "TRAIN(173): [170/391] Batch: 0.0544 (0.0502) Data: 0.0175 (0.0203) Loss: 0.0158 (0.0143)\n",
            "TRAIN(173): [180/391] Batch: 0.0514 (0.0502) Data: 0.0246 (0.0202) Loss: 0.0116 (0.0141)\n",
            "TRAIN(173): [190/391] Batch: 0.0576 (0.0502) Data: 0.0221 (0.0202) Loss: 0.0178 (0.0139)\n",
            "TRAIN(173): [200/391] Batch: 0.0499 (0.0501) Data: 0.0201 (0.0201) Loss: 0.0073 (0.0138)\n",
            "TRAIN(173): [210/391] Batch: 0.0445 (0.0500) Data: 0.0252 (0.0202) Loss: 0.0140 (0.0137)\n",
            "TRAIN(173): [220/391] Batch: 0.0453 (0.0500) Data: 0.0248 (0.0202) Loss: 0.0063 (0.0138)\n",
            "TRAIN(173): [230/391] Batch: 0.0469 (0.0500) Data: 0.0271 (0.0203) Loss: 0.0185 (0.0141)\n",
            "TRAIN(173): [240/391] Batch: 0.0468 (0.0499) Data: 0.0273 (0.0204) Loss: 0.0135 (0.0140)\n",
            "TRAIN(173): [250/391] Batch: 0.0542 (0.0499) Data: 0.0232 (0.0204) Loss: 0.0176 (0.0141)\n",
            "TRAIN(173): [260/391] Batch: 0.0466 (0.0498) Data: 0.0280 (0.0205) Loss: 0.0096 (0.0139)\n",
            "TRAIN(173): [270/391] Batch: 0.0486 (0.0498) Data: 0.0163 (0.0205) Loss: 0.0167 (0.0138)\n",
            "TRAIN(173): [280/391] Batch: 0.0485 (0.0497) Data: 0.0252 (0.0205) Loss: 0.0077 (0.0137)\n",
            "TRAIN(173): [290/391] Batch: 0.0465 (0.0497) Data: 0.0250 (0.0206) Loss: 0.0064 (0.0137)\n",
            "TRAIN(173): [300/391] Batch: 0.0514 (0.0496) Data: 0.0267 (0.0206) Loss: 0.0153 (0.0137)\n",
            "TRAIN(173): [310/391] Batch: 0.0467 (0.0497) Data: 0.0212 (0.0205) Loss: 0.0169 (0.0138)\n",
            "TRAIN(173): [320/391] Batch: 0.0519 (0.0497) Data: 0.0145 (0.0204) Loss: 0.0364 (0.0138)\n",
            "TRAIN(173): [330/391] Batch: 0.0461 (0.0498) Data: 0.0206 (0.0203) Loss: 0.0062 (0.0138)\n",
            "TRAIN(173): [340/391] Batch: 0.0496 (0.0498) Data: 0.0214 (0.0203) Loss: 0.0202 (0.0139)\n",
            "TRAIN(173): [350/391] Batch: 0.0385 (0.0499) Data: 0.0244 (0.0201) Loss: 0.0138 (0.0139)\n",
            "TRAIN(173): [360/391] Batch: 0.0537 (0.0499) Data: 0.0148 (0.0201) Loss: 0.0056 (0.0138)\n",
            "TRAIN(173): [370/391] Batch: 0.0518 (0.0500) Data: 0.0171 (0.0200) Loss: 0.0044 (0.0138)\n",
            "TRAIN(173): [380/391] Batch: 0.0501 (0.0499) Data: 0.0179 (0.0199) Loss: 0.0181 (0.0139)\n",
            "TRAIN(173): [390/391] Batch: 0.0488 (0.0499) Data: 0.0258 (0.0200) Loss: 0.0170 (0.0139)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(173)         0:00:19         0:00:07         0:00:11          0.0139\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(174): [ 10/391] Batch: 0.0472 (0.0601) Data: 0.0254 (0.0376) Loss: 0.0050 (0.0131)\n",
            "TRAIN(174): [ 20/391] Batch: 0.0673 (0.0545) Data: 0.0199 (0.0310) Loss: 0.0195 (0.0152)\n",
            "TRAIN(174): [ 30/391] Batch: 0.0441 (0.0517) Data: 0.0274 (0.0286) Loss: 0.0212 (0.0146)\n",
            "TRAIN(174): [ 40/391] Batch: 0.0507 (0.0516) Data: 0.0253 (0.0264) Loss: 0.0074 (0.0135)\n",
            "TRAIN(174): [ 50/391] Batch: 0.0478 (0.0510) Data: 0.0270 (0.0255) Loss: 0.0133 (0.0132)\n",
            "TRAIN(174): [ 60/391] Batch: 0.0557 (0.0507) Data: 0.0169 (0.0245) Loss: 0.0064 (0.0128)\n",
            "TRAIN(174): [ 70/391] Batch: 0.0546 (0.0507) Data: 0.0175 (0.0234) Loss: 0.0141 (0.0128)\n",
            "TRAIN(174): [ 80/391] Batch: 0.0482 (0.0504) Data: 0.0268 (0.0229) Loss: 0.0817 (0.0135)\n",
            "TRAIN(174): [ 90/391] Batch: 0.0560 (0.0504) Data: 0.0154 (0.0225) Loss: 0.0076 (0.0130)\n",
            "TRAIN(174): [100/391] Batch: 0.0438 (0.0505) Data: 0.0206 (0.0218) Loss: 0.0097 (0.0132)\n",
            "TRAIN(174): [110/391] Batch: 0.0445 (0.0502) Data: 0.0281 (0.0219) Loss: 0.0145 (0.0129)\n",
            "TRAIN(174): [120/391] Batch: 0.0499 (0.0503) Data: 0.0186 (0.0217) Loss: 0.0094 (0.0134)\n",
            "TRAIN(174): [130/391] Batch: 0.0605 (0.0505) Data: 0.0135 (0.0213) Loss: 0.0092 (0.0133)\n",
            "TRAIN(174): [140/391] Batch: 0.0456 (0.0502) Data: 0.0269 (0.0214) Loss: 0.0068 (0.0132)\n",
            "TRAIN(174): [150/391] Batch: 0.0441 (0.0500) Data: 0.0278 (0.0215) Loss: 0.0041 (0.0130)\n",
            "TRAIN(174): [160/391] Batch: 0.0453 (0.0499) Data: 0.0265 (0.0215) Loss: 0.0217 (0.0128)\n",
            "TRAIN(174): [170/391] Batch: 0.0445 (0.0499) Data: 0.0279 (0.0214) Loss: 0.0200 (0.0128)\n",
            "TRAIN(174): [180/391] Batch: 0.0532 (0.0500) Data: 0.0168 (0.0211) Loss: 0.0042 (0.0127)\n",
            "TRAIN(174): [190/391] Batch: 0.0632 (0.0501) Data: 0.0144 (0.0209) Loss: 0.0122 (0.0125)\n",
            "TRAIN(174): [200/391] Batch: 0.0630 (0.0502) Data: 0.0148 (0.0206) Loss: 0.0067 (0.0128)\n",
            "TRAIN(174): [210/391] Batch: 0.0506 (0.0502) Data: 0.0239 (0.0205) Loss: 0.0035 (0.0126)\n",
            "TRAIN(174): [220/391] Batch: 0.0444 (0.0503) Data: 0.0215 (0.0203) Loss: 0.0159 (0.0128)\n",
            "TRAIN(174): [230/391] Batch: 0.0508 (0.0504) Data: 0.0189 (0.0201) Loss: 0.0216 (0.0128)\n",
            "TRAIN(174): [240/391] Batch: 0.0611 (0.0505) Data: 0.0127 (0.0199) Loss: 0.0098 (0.0126)\n",
            "TRAIN(174): [250/391] Batch: 0.0461 (0.0504) Data: 0.0279 (0.0199) Loss: 0.0078 (0.0127)\n",
            "TRAIN(174): [260/391] Batch: 0.0433 (0.0504) Data: 0.0211 (0.0199) Loss: 0.0215 (0.0125)\n",
            "TRAIN(174): [270/391] Batch: 0.0500 (0.0503) Data: 0.0248 (0.0200) Loss: 0.0194 (0.0129)\n",
            "TRAIN(174): [280/391] Batch: 0.0537 (0.0503) Data: 0.0180 (0.0200) Loss: 0.0125 (0.0130)\n",
            "TRAIN(174): [290/391] Batch: 0.0505 (0.0503) Data: 0.0164 (0.0199) Loss: 0.0074 (0.0129)\n",
            "TRAIN(174): [300/391] Batch: 0.0544 (0.0503) Data: 0.0163 (0.0198) Loss: 0.0271 (0.0129)\n",
            "TRAIN(174): [310/391] Batch: 0.0395 (0.0502) Data: 0.0271 (0.0198) Loss: 0.0167 (0.0132)\n",
            "TRAIN(174): [320/391] Batch: 0.0503 (0.0502) Data: 0.0248 (0.0198) Loss: 0.0293 (0.0132)\n",
            "TRAIN(174): [330/391] Batch: 0.0471 (0.0501) Data: 0.0270 (0.0199) Loss: 0.0106 (0.0133)\n",
            "TRAIN(174): [340/391] Batch: 0.0430 (0.0501) Data: 0.0224 (0.0198) Loss: 0.0064 (0.0133)\n",
            "TRAIN(174): [350/391] Batch: 0.0529 (0.0501) Data: 0.0242 (0.0199) Loss: 0.0050 (0.0132)\n",
            "TRAIN(174): [360/391] Batch: 0.0514 (0.0501) Data: 0.0192 (0.0199) Loss: 0.0078 (0.0130)\n",
            "TRAIN(174): [370/391] Batch: 0.0416 (0.0500) Data: 0.0263 (0.0199) Loss: 0.0139 (0.0129)\n",
            "TRAIN(174): [380/391] Batch: 0.0584 (0.0500) Data: 0.0206 (0.0199) Loss: 0.0038 (0.0128)\n",
            "TRAIN(174): [390/391] Batch: 0.0467 (0.0500) Data: 0.0291 (0.0200) Loss: 0.0073 (0.0128)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(174)         0:00:19         0:00:07         0:00:11          0.0128\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(175): [ 10/391] Batch: 0.0444 (0.0642) Data: 0.0263 (0.0316) Loss: 0.0030 (0.0125)\n",
            "TRAIN(175): [ 20/391] Batch: 0.0478 (0.0568) Data: 0.0271 (0.0269) Loss: 0.0036 (0.0134)\n",
            "TRAIN(175): [ 30/391] Batch: 0.0516 (0.0544) Data: 0.0158 (0.0248) Loss: 0.0145 (0.0148)\n",
            "TRAIN(175): [ 40/391] Batch: 0.0503 (0.0528) Data: 0.0169 (0.0237) Loss: 0.0077 (0.0144)\n",
            "TRAIN(175): [ 50/391] Batch: 0.0414 (0.0520) Data: 0.0209 (0.0230) Loss: 0.0115 (0.0142)\n",
            "TRAIN(175): [ 60/391] Batch: 0.0512 (0.0521) Data: 0.0196 (0.0218) Loss: 0.0093 (0.0138)\n",
            "TRAIN(175): [ 70/391] Batch: 0.0467 (0.0522) Data: 0.0184 (0.0210) Loss: 0.0042 (0.0129)\n",
            "TRAIN(175): [ 80/391] Batch: 0.0596 (0.0523) Data: 0.0175 (0.0206) Loss: 0.0140 (0.0129)\n",
            "TRAIN(175): [ 90/391] Batch: 0.0405 (0.0521) Data: 0.0221 (0.0203) Loss: 0.0050 (0.0126)\n",
            "TRAIN(175): [100/391] Batch: 0.0384 (0.0519) Data: 0.0246 (0.0199) Loss: 0.0329 (0.0129)\n",
            "TRAIN(175): [110/391] Batch: 0.0478 (0.0520) Data: 0.0191 (0.0195) Loss: 0.0105 (0.0129)\n",
            "TRAIN(175): [120/391] Batch: 0.0578 (0.0519) Data: 0.0188 (0.0194) Loss: 0.0105 (0.0129)\n",
            "TRAIN(175): [130/391] Batch: 0.0413 (0.0516) Data: 0.0253 (0.0196) Loss: 0.0025 (0.0130)\n",
            "TRAIN(175): [140/391] Batch: 0.0540 (0.0516) Data: 0.0175 (0.0196) Loss: 0.0111 (0.0130)\n",
            "TRAIN(175): [150/391] Batch: 0.0385 (0.0514) Data: 0.0264 (0.0196) Loss: 0.0053 (0.0127)\n",
            "TRAIN(175): [160/391] Batch: 0.0473 (0.0512) Data: 0.0248 (0.0198) Loss: 0.0134 (0.0127)\n",
            "TRAIN(175): [170/391] Batch: 0.0481 (0.0511) Data: 0.0237 (0.0198) Loss: 0.0043 (0.0126)\n",
            "TRAIN(175): [180/391] Batch: 0.0556 (0.0511) Data: 0.0161 (0.0197) Loss: 0.0039 (0.0128)\n",
            "TRAIN(175): [190/391] Batch: 0.0561 (0.0510) Data: 0.0220 (0.0197) Loss: 0.0053 (0.0129)\n",
            "TRAIN(175): [200/391] Batch: 0.0483 (0.0509) Data: 0.0223 (0.0197) Loss: 0.0233 (0.0132)\n",
            "TRAIN(175): [210/391] Batch: 0.0463 (0.0507) Data: 0.0269 (0.0198) Loss: 0.0060 (0.0131)\n",
            "TRAIN(175): [220/391] Batch: 0.0477 (0.0506) Data: 0.0264 (0.0200) Loss: 0.0072 (0.0130)\n",
            "TRAIN(175): [230/391] Batch: 0.0445 (0.0504) Data: 0.0271 (0.0201) Loss: 0.0216 (0.0130)\n",
            "TRAIN(175): [240/391] Batch: 0.0479 (0.0503) Data: 0.0233 (0.0202) Loss: 0.0056 (0.0131)\n",
            "TRAIN(175): [250/391] Batch: 0.0549 (0.0504) Data: 0.0150 (0.0200) Loss: 0.0162 (0.0130)\n",
            "TRAIN(175): [260/391] Batch: 0.0434 (0.0503) Data: 0.0270 (0.0199) Loss: 0.0122 (0.0132)\n",
            "TRAIN(175): [270/391] Batch: 0.0475 (0.0502) Data: 0.0276 (0.0200) Loss: 0.0049 (0.0131)\n",
            "TRAIN(175): [280/391] Batch: 0.0472 (0.0501) Data: 0.0264 (0.0201) Loss: 0.0136 (0.0130)\n",
            "TRAIN(175): [290/391] Batch: 0.0496 (0.0501) Data: 0.0206 (0.0202) Loss: 0.0052 (0.0130)\n",
            "TRAIN(175): [300/391] Batch: 0.0461 (0.0500) Data: 0.0253 (0.0203) Loss: 0.0129 (0.0130)\n",
            "TRAIN(175): [310/391] Batch: 0.0455 (0.0499) Data: 0.0281 (0.0204) Loss: 0.0042 (0.0129)\n",
            "TRAIN(175): [320/391] Batch: 0.0499 (0.0498) Data: 0.0255 (0.0204) Loss: 0.0083 (0.0129)\n",
            "TRAIN(175): [330/391] Batch: 0.0466 (0.0498) Data: 0.0219 (0.0204) Loss: 0.0149 (0.0131)\n",
            "TRAIN(175): [340/391] Batch: 0.0501 (0.0498) Data: 0.0190 (0.0203) Loss: 0.0335 (0.0132)\n",
            "TRAIN(175): [350/391] Batch: 0.0464 (0.0499) Data: 0.0212 (0.0203) Loss: 0.0091 (0.0132)\n",
            "TRAIN(175): [360/391] Batch: 0.0405 (0.0499) Data: 0.0197 (0.0201) Loss: 0.0250 (0.0132)\n",
            "TRAIN(175): [370/391] Batch: 0.0483 (0.0499) Data: 0.0163 (0.0200) Loss: 0.0053 (0.0132)\n",
            "TRAIN(175): [380/391] Batch: 0.0577 (0.0499) Data: 0.0141 (0.0199) Loss: 0.0160 (0.0133)\n",
            "TRAIN(175): [390/391] Batch: 0.0471 (0.0498) Data: 0.0282 (0.0199) Loss: 0.0127 (0.0133)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(175)         0:00:19         0:00:07         0:00:11          0.0133\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(176): [ 10/391] Batch: 0.0480 (0.0666) Data: 0.0170 (0.0316) Loss: 0.0072 (0.0148)\n",
            "TRAIN(176): [ 20/391] Batch: 0.0499 (0.0581) Data: 0.0209 (0.0257) Loss: 0.0102 (0.0118)\n",
            "TRAIN(176): [ 30/391] Batch: 0.0464 (0.0551) Data: 0.0240 (0.0241) Loss: 0.0043 (0.0108)\n",
            "TRAIN(176): [ 40/391] Batch: 0.0459 (0.0534) Data: 0.0284 (0.0239) Loss: 0.0109 (0.0116)\n",
            "TRAIN(176): [ 50/391] Batch: 0.0372 (0.0527) Data: 0.0273 (0.0229) Loss: 0.0086 (0.0113)\n",
            "TRAIN(176): [ 60/391] Batch: 0.0484 (0.0519) Data: 0.0254 (0.0228) Loss: 0.0213 (0.0115)\n",
            "TRAIN(176): [ 70/391] Batch: 0.0428 (0.0515) Data: 0.0214 (0.0223) Loss: 0.0095 (0.0114)\n",
            "TRAIN(176): [ 80/391] Batch: 0.0507 (0.0512) Data: 0.0166 (0.0222) Loss: 0.0066 (0.0113)\n",
            "TRAIN(176): [ 90/391] Batch: 0.0513 (0.0511) Data: 0.0225 (0.0217) Loss: 0.0110 (0.0112)\n",
            "TRAIN(176): [100/391] Batch: 0.0539 (0.0509) Data: 0.0239 (0.0217) Loss: 0.0041 (0.0114)\n",
            "TRAIN(176): [110/391] Batch: 0.0428 (0.0508) Data: 0.0190 (0.0215) Loss: 0.0072 (0.0112)\n",
            "TRAIN(176): [120/391] Batch: 0.0501 (0.0506) Data: 0.0247 (0.0216) Loss: 0.0106 (0.0113)\n",
            "TRAIN(176): [130/391] Batch: 0.0485 (0.0504) Data: 0.0266 (0.0213) Loss: 0.0034 (0.0112)\n",
            "TRAIN(176): [140/391] Batch: 0.0497 (0.0503) Data: 0.0195 (0.0213) Loss: 0.0101 (0.0110)\n",
            "TRAIN(176): [150/391] Batch: 0.0445 (0.0502) Data: 0.0276 (0.0213) Loss: 0.0114 (0.0111)\n",
            "TRAIN(176): [160/391] Batch: 0.0474 (0.0502) Data: 0.0226 (0.0212) Loss: 0.0066 (0.0108)\n",
            "TRAIN(176): [170/391] Batch: 0.0489 (0.0501) Data: 0.0254 (0.0211) Loss: 0.0048 (0.0110)\n",
            "TRAIN(176): [180/391] Batch: 0.0449 (0.0502) Data: 0.0162 (0.0208) Loss: 0.0031 (0.0108)\n",
            "TRAIN(176): [190/391] Batch: 0.0467 (0.0500) Data: 0.0284 (0.0210) Loss: 0.0283 (0.0111)\n",
            "TRAIN(176): [200/391] Batch: 0.0546 (0.0500) Data: 0.0200 (0.0208) Loss: 0.0121 (0.0112)\n",
            "TRAIN(176): [210/391] Batch: 0.0462 (0.0499) Data: 0.0185 (0.0206) Loss: 0.0052 (0.0111)\n",
            "TRAIN(176): [220/391] Batch: 0.0472 (0.0499) Data: 0.0214 (0.0204) Loss: 0.0117 (0.0111)\n",
            "TRAIN(176): [230/391] Batch: 0.0501 (0.0499) Data: 0.0163 (0.0203) Loss: 0.0339 (0.0112)\n",
            "TRAIN(176): [240/391] Batch: 0.0449 (0.0499) Data: 0.0201 (0.0201) Loss: 0.0067 (0.0110)\n",
            "TRAIN(176): [250/391] Batch: 0.0541 (0.0500) Data: 0.0151 (0.0200) Loss: 0.0057 (0.0111)\n",
            "TRAIN(176): [260/391] Batch: 0.0488 (0.0500) Data: 0.0199 (0.0199) Loss: 0.0107 (0.0113)\n",
            "TRAIN(176): [270/391] Batch: 0.0451 (0.0499) Data: 0.0239 (0.0198) Loss: 0.0033 (0.0113)\n",
            "TRAIN(176): [280/391] Batch: 0.0462 (0.0499) Data: 0.0276 (0.0199) Loss: 0.0056 (0.0114)\n",
            "TRAIN(176): [290/391] Batch: 0.0450 (0.0498) Data: 0.0270 (0.0201) Loss: 0.0071 (0.0114)\n",
            "TRAIN(176): [300/391] Batch: 0.0458 (0.0497) Data: 0.0275 (0.0202) Loss: 0.0078 (0.0114)\n",
            "TRAIN(176): [310/391] Batch: 0.0505 (0.0497) Data: 0.0252 (0.0203) Loss: 0.0194 (0.0113)\n",
            "TRAIN(176): [320/391] Batch: 0.0414 (0.0497) Data: 0.0243 (0.0203) Loss: 0.0069 (0.0114)\n",
            "TRAIN(176): [330/391] Batch: 0.0535 (0.0496) Data: 0.0246 (0.0203) Loss: 0.0070 (0.0114)\n",
            "TRAIN(176): [340/391] Batch: 0.0465 (0.0496) Data: 0.0252 (0.0203) Loss: 0.0068 (0.0116)\n",
            "TRAIN(176): [350/391] Batch: 0.0464 (0.0496) Data: 0.0279 (0.0204) Loss: 0.0038 (0.0115)\n",
            "TRAIN(176): [360/391] Batch: 0.0440 (0.0495) Data: 0.0262 (0.0205) Loss: 0.0046 (0.0115)\n",
            "TRAIN(176): [370/391] Batch: 0.0480 (0.0495) Data: 0.0190 (0.0204) Loss: 0.0062 (0.0114)\n",
            "TRAIN(176): [380/391] Batch: 0.0429 (0.0495) Data: 0.0269 (0.0205) Loss: 0.0123 (0.0114)\n",
            "TRAIN(176): [390/391] Batch: 0.0487 (0.0495) Data: 0.0285 (0.0205) Loss: 0.0411 (0.0115)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(176)         0:00:19         0:00:08         0:00:11          0.0115\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(177): [ 10/391] Batch: 0.0482 (0.0652) Data: 0.0236 (0.0290) Loss: 0.0049 (0.0097)\n",
            "TRAIN(177): [ 20/391] Batch: 0.0521 (0.0577) Data: 0.0189 (0.0246) Loss: 0.0171 (0.0132)\n",
            "TRAIN(177): [ 30/391] Batch: 0.0559 (0.0544) Data: 0.0226 (0.0238) Loss: 0.0057 (0.0131)\n",
            "TRAIN(177): [ 40/391] Batch: 0.0451 (0.0527) Data: 0.0284 (0.0237) Loss: 0.0079 (0.0134)\n",
            "TRAIN(177): [ 50/391] Batch: 0.0506 (0.0518) Data: 0.0261 (0.0234) Loss: 0.0124 (0.0150)\n",
            "TRAIN(177): [ 60/391] Batch: 0.0480 (0.0513) Data: 0.0250 (0.0233) Loss: 0.0271 (0.0149)\n",
            "TRAIN(177): [ 70/391] Batch: 0.0638 (0.0511) Data: 0.0156 (0.0228) Loss: 0.0170 (0.0141)\n",
            "TRAIN(177): [ 80/391] Batch: 0.0456 (0.0510) Data: 0.0215 (0.0219) Loss: 0.0089 (0.0137)\n",
            "TRAIN(177): [ 90/391] Batch: 0.0599 (0.0510) Data: 0.0174 (0.0216) Loss: 0.0162 (0.0135)\n",
            "TRAIN(177): [100/391] Batch: 0.0544 (0.0510) Data: 0.0189 (0.0211) Loss: 0.0065 (0.0141)\n",
            "TRAIN(177): [110/391] Batch: 0.0543 (0.0511) Data: 0.0177 (0.0206) Loss: 0.0028 (0.0142)\n",
            "TRAIN(177): [120/391] Batch: 0.0467 (0.0511) Data: 0.0158 (0.0203) Loss: 0.0080 (0.0140)\n",
            "TRAIN(177): [130/391] Batch: 0.0518 (0.0513) Data: 0.0145 (0.0198) Loss: 0.0012 (0.0137)\n",
            "TRAIN(177): [140/391] Batch: 0.0499 (0.0513) Data: 0.0222 (0.0196) Loss: 0.0188 (0.0135)\n",
            "TRAIN(177): [150/391] Batch: 0.0486 (0.0511) Data: 0.0208 (0.0198) Loss: 0.0265 (0.0137)\n",
            "TRAIN(177): [160/391] Batch: 0.0468 (0.0508) Data: 0.0271 (0.0200) Loss: 0.0070 (0.0136)\n",
            "TRAIN(177): [170/391] Batch: 0.0454 (0.0507) Data: 0.0253 (0.0201) Loss: 0.0077 (0.0134)\n",
            "TRAIN(177): [180/391] Batch: 0.0519 (0.0506) Data: 0.0195 (0.0202) Loss: 0.0042 (0.0135)\n",
            "TRAIN(177): [190/391] Batch: 0.0489 (0.0505) Data: 0.0221 (0.0201) Loss: 0.0061 (0.0133)\n",
            "TRAIN(177): [200/391] Batch: 0.0480 (0.0504) Data: 0.0245 (0.0202) Loss: 0.0619 (0.0134)\n",
            "TRAIN(177): [210/391] Batch: 0.0430 (0.0503) Data: 0.0250 (0.0202) Loss: 0.0258 (0.0136)\n",
            "TRAIN(177): [220/391] Batch: 0.0466 (0.0501) Data: 0.0278 (0.0204) Loss: 0.0134 (0.0133)\n",
            "TRAIN(177): [230/391] Batch: 0.0471 (0.0500) Data: 0.0270 (0.0205) Loss: 0.0057 (0.0133)\n",
            "TRAIN(177): [240/391] Batch: 0.0567 (0.0499) Data: 0.0183 (0.0205) Loss: 0.0296 (0.0132)\n",
            "TRAIN(177): [250/391] Batch: 0.0419 (0.0499) Data: 0.0287 (0.0206) Loss: 0.0083 (0.0131)\n",
            "TRAIN(177): [260/391] Batch: 0.0489 (0.0498) Data: 0.0277 (0.0207) Loss: 0.0113 (0.0129)\n",
            "TRAIN(177): [270/391] Batch: 0.0464 (0.0497) Data: 0.0291 (0.0208) Loss: 0.0066 (0.0130)\n",
            "TRAIN(177): [280/391] Batch: 0.0534 (0.0497) Data: 0.0166 (0.0208) Loss: 0.0630 (0.0133)\n",
            "TRAIN(177): [290/391] Batch: 0.0472 (0.0497) Data: 0.0279 (0.0208) Loss: 0.0084 (0.0132)\n",
            "TRAIN(177): [300/391] Batch: 0.0462 (0.0496) Data: 0.0280 (0.0209) Loss: 0.0037 (0.0130)\n",
            "TRAIN(177): [310/391] Batch: 0.0380 (0.0495) Data: 0.0273 (0.0210) Loss: 0.0111 (0.0128)\n",
            "TRAIN(177): [320/391] Batch: 0.0476 (0.0495) Data: 0.0206 (0.0209) Loss: 0.0052 (0.0129)\n",
            "TRAIN(177): [330/391] Batch: 0.0470 (0.0494) Data: 0.0281 (0.0210) Loss: 0.0180 (0.0129)\n",
            "TRAIN(177): [340/391] Batch: 0.0404 (0.0494) Data: 0.0271 (0.0210) Loss: 0.0016 (0.0129)\n",
            "TRAIN(177): [350/391] Batch: 0.0565 (0.0495) Data: 0.0157 (0.0209) Loss: 0.0133 (0.0128)\n",
            "TRAIN(177): [360/391] Batch: 0.0477 (0.0495) Data: 0.0175 (0.0208) Loss: 0.0034 (0.0127)\n",
            "TRAIN(177): [370/391] Batch: 0.0641 (0.0495) Data: 0.0159 (0.0207) Loss: 0.0149 (0.0126)\n",
            "TRAIN(177): [380/391] Batch: 0.0577 (0.0496) Data: 0.0140 (0.0205) Loss: 0.0087 (0.0126)\n",
            "TRAIN(177): [390/391] Batch: 0.0451 (0.0496) Data: 0.0236 (0.0205) Loss: 0.0042 (0.0126)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(177)         0:00:19         0:00:08         0:00:11          0.0126\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(178): [ 10/391] Batch: 0.0639 (0.0722) Data: 0.0129 (0.0341) Loss: 0.0069 (0.0112)\n",
            "TRAIN(178): [ 20/391] Batch: 0.0447 (0.0606) Data: 0.0231 (0.0287) Loss: 0.0084 (0.0109)\n",
            "TRAIN(178): [ 30/391] Batch: 0.0481 (0.0572) Data: 0.0175 (0.0252) Loss: 0.0269 (0.0108)\n",
            "TRAIN(178): [ 40/391] Batch: 0.0560 (0.0558) Data: 0.0154 (0.0231) Loss: 0.0031 (0.0099)\n",
            "TRAIN(178): [ 50/391] Batch: 0.0565 (0.0541) Data: 0.0189 (0.0228) Loss: 0.0300 (0.0107)\n",
            "TRAIN(178): [ 60/391] Batch: 0.0372 (0.0531) Data: 0.0262 (0.0225) Loss: 0.0114 (0.0108)\n",
            "TRAIN(178): [ 70/391] Batch: 0.0456 (0.0523) Data: 0.0280 (0.0227) Loss: 0.0166 (0.0106)\n",
            "TRAIN(178): [ 80/391] Batch: 0.0449 (0.0518) Data: 0.0274 (0.0227) Loss: 0.0158 (0.0106)\n",
            "TRAIN(178): [ 90/391] Batch: 0.0468 (0.0517) Data: 0.0221 (0.0221) Loss: 0.0054 (0.0106)\n",
            "TRAIN(178): [100/391] Batch: 0.0369 (0.0515) Data: 0.0257 (0.0217) Loss: 0.0056 (0.0109)\n",
            "TRAIN(178): [110/391] Batch: 0.0426 (0.0516) Data: 0.0193 (0.0212) Loss: 0.0034 (0.0106)\n",
            "TRAIN(178): [120/391] Batch: 0.0415 (0.0513) Data: 0.0266 (0.0211) Loss: 0.0325 (0.0111)\n",
            "TRAIN(178): [130/391] Batch: 0.0441 (0.0511) Data: 0.0254 (0.0212) Loss: 0.0082 (0.0113)\n",
            "TRAIN(178): [140/391] Batch: 0.0507 (0.0510) Data: 0.0179 (0.0209) Loss: 0.0070 (0.0111)\n",
            "TRAIN(178): [150/391] Batch: 0.0483 (0.0508) Data: 0.0267 (0.0210) Loss: 0.0048 (0.0109)\n",
            "TRAIN(178): [160/391] Batch: 0.0500 (0.0508) Data: 0.0174 (0.0209) Loss: 0.0132 (0.0113)\n",
            "TRAIN(178): [170/391] Batch: 0.0485 (0.0506) Data: 0.0251 (0.0210) Loss: 0.0036 (0.0113)\n",
            "TRAIN(178): [180/391] Batch: 0.0447 (0.0505) Data: 0.0283 (0.0209) Loss: 0.0142 (0.0113)\n",
            "TRAIN(178): [190/391] Batch: 0.0554 (0.0504) Data: 0.0214 (0.0210) Loss: 0.0041 (0.0113)\n",
            "TRAIN(178): [200/391] Batch: 0.0420 (0.0502) Data: 0.0232 (0.0211) Loss: 0.0064 (0.0112)\n",
            "TRAIN(178): [210/391] Batch: 0.0500 (0.0501) Data: 0.0169 (0.0210) Loss: 0.0031 (0.0111)\n",
            "TRAIN(178): [220/391] Batch: 0.0483 (0.0501) Data: 0.0207 (0.0211) Loss: 0.0103 (0.0111)\n",
            "TRAIN(178): [230/391] Batch: 0.0542 (0.0501) Data: 0.0194 (0.0209) Loss: 0.0058 (0.0111)\n",
            "TRAIN(178): [240/391] Batch: 0.0415 (0.0501) Data: 0.0243 (0.0208) Loss: 0.0193 (0.0111)\n",
            "TRAIN(178): [250/391] Batch: 0.0415 (0.0501) Data: 0.0242 (0.0207) Loss: 0.0040 (0.0113)\n",
            "TRAIN(178): [260/391] Batch: 0.0497 (0.0502) Data: 0.0156 (0.0206) Loss: 0.0064 (0.0112)\n",
            "TRAIN(178): [270/391] Batch: 0.0523 (0.0502) Data: 0.0145 (0.0204) Loss: 0.0084 (0.0112)\n",
            "TRAIN(178): [280/391] Batch: 0.0395 (0.0501) Data: 0.0254 (0.0203) Loss: 0.0124 (0.0113)\n",
            "TRAIN(178): [290/391] Batch: 0.0395 (0.0501) Data: 0.0280 (0.0203) Loss: 0.0035 (0.0113)\n",
            "TRAIN(178): [300/391] Batch: 0.0421 (0.0501) Data: 0.0241 (0.0204) Loss: 0.0071 (0.0114)\n",
            "TRAIN(178): [310/391] Batch: 0.0451 (0.0500) Data: 0.0193 (0.0204) Loss: 0.0271 (0.0115)\n",
            "TRAIN(178): [320/391] Batch: 0.0484 (0.0500) Data: 0.0265 (0.0205) Loss: 0.0190 (0.0115)\n",
            "TRAIN(178): [330/391] Batch: 0.0430 (0.0499) Data: 0.0252 (0.0205) Loss: 0.0357 (0.0117)\n",
            "TRAIN(178): [340/391] Batch: 0.0466 (0.0499) Data: 0.0255 (0.0205) Loss: 0.0207 (0.0116)\n",
            "TRAIN(178): [350/391] Batch: 0.0468 (0.0498) Data: 0.0267 (0.0206) Loss: 0.0328 (0.0116)\n",
            "TRAIN(178): [360/391] Batch: 0.0472 (0.0498) Data: 0.0271 (0.0207) Loss: 0.0138 (0.0116)\n",
            "TRAIN(178): [370/391] Batch: 0.0373 (0.0497) Data: 0.0256 (0.0207) Loss: 0.0149 (0.0116)\n",
            "TRAIN(178): [380/391] Batch: 0.0473 (0.0497) Data: 0.0278 (0.0207) Loss: 0.0315 (0.0118)\n",
            "TRAIN(178): [390/391] Batch: 0.0470 (0.0497) Data: 0.0287 (0.0207) Loss: 0.0073 (0.0118)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(178)         0:00:19         0:00:08         0:00:11          0.0118\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(179): [ 10/391] Batch: 0.0453 (0.0612) Data: 0.0256 (0.0379) Loss: 0.0035 (0.0119)\n",
            "TRAIN(179): [ 20/391] Batch: 0.0531 (0.0552) Data: 0.0289 (0.0303) Loss: 0.0086 (0.0096)\n",
            "TRAIN(179): [ 30/391] Batch: 0.0454 (0.0538) Data: 0.0234 (0.0266) Loss: 0.0112 (0.0096)\n",
            "TRAIN(179): [ 40/391] Batch: 0.0514 (0.0527) Data: 0.0192 (0.0249) Loss: 0.0094 (0.0090)\n",
            "TRAIN(179): [ 50/391] Batch: 0.0470 (0.0524) Data: 0.0228 (0.0234) Loss: 0.0039 (0.0097)\n",
            "TRAIN(179): [ 60/391] Batch: 0.0483 (0.0518) Data: 0.0228 (0.0232) Loss: 0.0080 (0.0108)\n",
            "TRAIN(179): [ 70/391] Batch: 0.0540 (0.0515) Data: 0.0238 (0.0230) Loss: 0.0096 (0.0114)\n",
            "TRAIN(179): [ 80/391] Batch: 0.0571 (0.0513) Data: 0.0230 (0.0226) Loss: 0.0040 (0.0119)\n",
            "TRAIN(179): [ 90/391] Batch: 0.0478 (0.0511) Data: 0.0265 (0.0224) Loss: 0.0144 (0.0127)\n",
            "TRAIN(179): [100/391] Batch: 0.0467 (0.0510) Data: 0.0213 (0.0218) Loss: 0.0123 (0.0125)\n",
            "TRAIN(179): [110/391] Batch: 0.0513 (0.0508) Data: 0.0204 (0.0215) Loss: 0.0225 (0.0125)\n",
            "TRAIN(179): [120/391] Batch: 0.0536 (0.0509) Data: 0.0148 (0.0210) Loss: 0.0263 (0.0123)\n",
            "TRAIN(179): [130/391] Batch: 0.0362 (0.0509) Data: 0.0254 (0.0207) Loss: 0.0069 (0.0120)\n",
            "TRAIN(179): [140/391] Batch: 0.0619 (0.0511) Data: 0.0158 (0.0204) Loss: 0.0116 (0.0117)\n",
            "TRAIN(179): [150/391] Batch: 0.0552 (0.0511) Data: 0.0126 (0.0201) Loss: 0.0116 (0.0117)\n",
            "TRAIN(179): [160/391] Batch: 0.0454 (0.0512) Data: 0.0225 (0.0198) Loss: 0.0129 (0.0115)\n",
            "TRAIN(179): [170/391] Batch: 0.0469 (0.0510) Data: 0.0279 (0.0200) Loss: 0.0156 (0.0116)\n",
            "TRAIN(179): [180/391] Batch: 0.0456 (0.0509) Data: 0.0276 (0.0201) Loss: 0.0046 (0.0113)\n",
            "TRAIN(179): [190/391] Batch: 0.0601 (0.0508) Data: 0.0181 (0.0201) Loss: 0.0059 (0.0113)\n",
            "TRAIN(179): [200/391] Batch: 0.0470 (0.0507) Data: 0.0281 (0.0203) Loss: 0.0051 (0.0113)\n",
            "TRAIN(179): [210/391] Batch: 0.0465 (0.0506) Data: 0.0265 (0.0203) Loss: 0.0524 (0.0116)\n",
            "TRAIN(179): [220/391] Batch: 0.0496 (0.0505) Data: 0.0170 (0.0204) Loss: 0.0060 (0.0115)\n",
            "TRAIN(179): [230/391] Batch: 0.0468 (0.0505) Data: 0.0224 (0.0202) Loss: 0.0090 (0.0116)\n",
            "TRAIN(179): [240/391] Batch: 0.0525 (0.0505) Data: 0.0169 (0.0201) Loss: 0.0096 (0.0116)\n",
            "TRAIN(179): [250/391] Batch: 0.0454 (0.0505) Data: 0.0241 (0.0200) Loss: 0.0074 (0.0116)\n",
            "TRAIN(179): [260/391] Batch: 0.0446 (0.0504) Data: 0.0281 (0.0201) Loss: 0.0044 (0.0115)\n",
            "TRAIN(179): [270/391] Batch: 0.0492 (0.0503) Data: 0.0249 (0.0202) Loss: 0.0146 (0.0115)\n",
            "TRAIN(179): [280/391] Batch: 0.0418 (0.0503) Data: 0.0251 (0.0201) Loss: 0.0058 (0.0116)\n",
            "TRAIN(179): [290/391] Batch: 0.0541 (0.0503) Data: 0.0203 (0.0202) Loss: 0.0238 (0.0115)\n",
            "TRAIN(179): [300/391] Batch: 0.0542 (0.0502) Data: 0.0221 (0.0202) Loss: 0.0397 (0.0116)\n",
            "TRAIN(179): [310/391] Batch: 0.0473 (0.0502) Data: 0.0270 (0.0202) Loss: 0.0092 (0.0116)\n",
            "TRAIN(179): [320/391] Batch: 0.0483 (0.0501) Data: 0.0262 (0.0203) Loss: 0.0049 (0.0116)\n",
            "TRAIN(179): [330/391] Batch: 0.0455 (0.0500) Data: 0.0275 (0.0204) Loss: 0.0351 (0.0117)\n",
            "TRAIN(179): [340/391] Batch: 0.0475 (0.0500) Data: 0.0264 (0.0205) Loss: 0.0134 (0.0116)\n",
            "TRAIN(179): [350/391] Batch: 0.0503 (0.0499) Data: 0.0182 (0.0205) Loss: 0.0105 (0.0118)\n",
            "TRAIN(179): [360/391] Batch: 0.0473 (0.0499) Data: 0.0253 (0.0204) Loss: 0.0040 (0.0117)\n",
            "TRAIN(179): [370/391] Batch: 0.0643 (0.0500) Data: 0.0187 (0.0203) Loss: 0.0062 (0.0116)\n",
            "TRAIN(179): [380/391] Batch: 0.0495 (0.0501) Data: 0.0178 (0.0201) Loss: 0.0157 (0.0116)\n",
            "TRAIN(179): [390/391] Batch: 0.0515 (0.0502) Data: 0.0214 (0.0200) Loss: 0.0061 (0.0115)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(179)         0:00:19         0:00:07         0:00:11          0.0115\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(180): [ 10/391] Batch: 0.0541 (0.0715) Data: 0.0127 (0.0305) Loss: 0.0066 (0.0147)\n",
            "TRAIN(180): [ 20/391] Batch: 0.0499 (0.0619) Data: 0.0190 (0.0238) Loss: 0.0053 (0.0124)\n",
            "TRAIN(180): [ 30/391] Batch: 0.0380 (0.0580) Data: 0.0248 (0.0215) Loss: 0.0256 (0.0121)\n",
            "TRAIN(180): [ 40/391] Batch: 0.0497 (0.0561) Data: 0.0240 (0.0213) Loss: 0.0155 (0.0118)\n",
            "TRAIN(180): [ 50/391] Batch: 0.0584 (0.0553) Data: 0.0141 (0.0200) Loss: 0.0113 (0.0121)\n",
            "TRAIN(180): [ 60/391] Batch: 0.0498 (0.0545) Data: 0.0162 (0.0198) Loss: 0.0246 (0.0117)\n",
            "TRAIN(180): [ 70/391] Batch: 0.0521 (0.0538) Data: 0.0238 (0.0199) Loss: 0.0084 (0.0111)\n",
            "TRAIN(180): [ 80/391] Batch: 0.0363 (0.0531) Data: 0.0243 (0.0201) Loss: 0.0053 (0.0112)\n",
            "TRAIN(180): [ 90/391] Batch: 0.0463 (0.0527) Data: 0.0256 (0.0202) Loss: 0.0035 (0.0108)\n",
            "TRAIN(180): [100/391] Batch: 0.0490 (0.0524) Data: 0.0271 (0.0202) Loss: 0.0178 (0.0108)\n",
            "TRAIN(180): [110/391] Batch: 0.0598 (0.0523) Data: 0.0143 (0.0201) Loss: 0.0029 (0.0107)\n",
            "TRAIN(180): [120/391] Batch: 0.0614 (0.0522) Data: 0.0187 (0.0201) Loss: 0.0050 (0.0105)\n",
            "TRAIN(180): [130/391] Batch: 0.0464 (0.0518) Data: 0.0268 (0.0200) Loss: 0.0052 (0.0104)\n",
            "TRAIN(180): [140/391] Batch: 0.0648 (0.0518) Data: 0.0145 (0.0199) Loss: 0.0246 (0.0109)\n",
            "TRAIN(180): [150/391] Batch: 0.0463 (0.0516) Data: 0.0259 (0.0199) Loss: 0.0055 (0.0111)\n",
            "TRAIN(180): [160/391] Batch: 0.0468 (0.0515) Data: 0.0250 (0.0200) Loss: 0.0153 (0.0109)\n",
            "TRAIN(180): [170/391] Batch: 0.0376 (0.0514) Data: 0.0260 (0.0201) Loss: 0.0023 (0.0110)\n",
            "TRAIN(180): [180/391] Batch: 0.0549 (0.0513) Data: 0.0224 (0.0201) Loss: 0.0273 (0.0112)\n",
            "TRAIN(180): [190/391] Batch: 0.0520 (0.0513) Data: 0.0155 (0.0199) Loss: 0.0024 (0.0109)\n",
            "TRAIN(180): [200/391] Batch: 0.0523 (0.0512) Data: 0.0228 (0.0200) Loss: 0.0046 (0.0108)\n",
            "TRAIN(180): [210/391] Batch: 0.0455 (0.0511) Data: 0.0250 (0.0200) Loss: 0.0065 (0.0109)\n",
            "TRAIN(180): [220/391] Batch: 0.0606 (0.0511) Data: 0.0144 (0.0199) Loss: 0.0313 (0.0112)\n",
            "TRAIN(180): [230/391] Batch: 0.0485 (0.0511) Data: 0.0131 (0.0199) Loss: 0.0326 (0.0113)\n",
            "TRAIN(180): [240/391] Batch: 0.0493 (0.0512) Data: 0.0178 (0.0197) Loss: 0.0088 (0.0113)\n",
            "TRAIN(180): [250/391] Batch: 0.0474 (0.0512) Data: 0.0166 (0.0195) Loss: 0.0189 (0.0111)\n",
            "TRAIN(180): [260/391] Batch: 0.0575 (0.0512) Data: 0.0139 (0.0193) Loss: 0.0086 (0.0110)\n",
            "TRAIN(180): [270/391] Batch: 0.0491 (0.0513) Data: 0.0136 (0.0191) Loss: 0.0016 (0.0111)\n",
            "TRAIN(180): [280/391] Batch: 0.0507 (0.0513) Data: 0.0185 (0.0190) Loss: 0.0063 (0.0110)\n",
            "TRAIN(180): [290/391] Batch: 0.0470 (0.0513) Data: 0.0198 (0.0189) Loss: 0.0151 (0.0111)\n",
            "TRAIN(180): [300/391] Batch: 0.0439 (0.0513) Data: 0.0250 (0.0188) Loss: 0.0165 (0.0110)\n",
            "TRAIN(180): [310/391] Batch: 0.0465 (0.0512) Data: 0.0255 (0.0189) Loss: 0.0420 (0.0114)\n",
            "TRAIN(180): [320/391] Batch: 0.0463 (0.0511) Data: 0.0260 (0.0190) Loss: 0.0152 (0.0113)\n",
            "TRAIN(180): [330/391] Batch: 0.0467 (0.0510) Data: 0.0251 (0.0191) Loss: 0.0176 (0.0112)\n",
            "TRAIN(180): [340/391] Batch: 0.0547 (0.0510) Data: 0.0127 (0.0191) Loss: 0.0192 (0.0113)\n",
            "TRAIN(180): [350/391] Batch: 0.0368 (0.0509) Data: 0.0258 (0.0191) Loss: 0.0296 (0.0113)\n",
            "TRAIN(180): [360/391] Batch: 0.0479 (0.0508) Data: 0.0219 (0.0191) Loss: 0.0112 (0.0112)\n",
            "TRAIN(180): [370/391] Batch: 0.0470 (0.0508) Data: 0.0262 (0.0191) Loss: 0.0201 (0.0113)\n",
            "TRAIN(180): [380/391] Batch: 0.0511 (0.0507) Data: 0.0156 (0.0192) Loss: 0.0102 (0.0113)\n",
            "TRAIN(180): [390/391] Batch: 0.0459 (0.0506) Data: 0.0259 (0.0192) Loss: 0.0032 (0.0112)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(180)         0:00:19         0:00:07         0:00:12          0.0112\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(181): [ 10/391] Batch: 0.0540 (0.0686) Data: 0.0214 (0.0304) Loss: 0.0089 (0.0093)\n",
            "TRAIN(181): [ 20/391] Batch: 0.0483 (0.0578) Data: 0.0263 (0.0267) Loss: 0.0052 (0.0106)\n",
            "TRAIN(181): [ 30/391] Batch: 0.0574 (0.0555) Data: 0.0205 (0.0241) Loss: 0.0355 (0.0108)\n",
            "TRAIN(181): [ 40/391] Batch: 0.0488 (0.0536) Data: 0.0248 (0.0233) Loss: 0.0023 (0.0104)\n",
            "TRAIN(181): [ 50/391] Batch: 0.0479 (0.0526) Data: 0.0243 (0.0230) Loss: 0.0050 (0.0104)\n",
            "TRAIN(181): [ 60/391] Batch: 0.0471 (0.0519) Data: 0.0261 (0.0224) Loss: 0.0067 (0.0109)\n",
            "TRAIN(181): [ 70/391] Batch: 0.0477 (0.0515) Data: 0.0262 (0.0221) Loss: 0.0044 (0.0116)\n",
            "TRAIN(181): [ 80/391] Batch: 0.0501 (0.0517) Data: 0.0145 (0.0213) Loss: 0.0039 (0.0119)\n",
            "TRAIN(181): [ 90/391] Batch: 0.0490 (0.0515) Data: 0.0228 (0.0208) Loss: 0.0119 (0.0117)\n",
            "TRAIN(181): [100/391] Batch: 0.0472 (0.0515) Data: 0.0219 (0.0205) Loss: 0.0091 (0.0115)\n",
            "TRAIN(181): [110/391] Batch: 0.0433 (0.0517) Data: 0.0216 (0.0200) Loss: 0.0050 (0.0117)\n",
            "TRAIN(181): [120/391] Batch: 0.0497 (0.0519) Data: 0.0204 (0.0197) Loss: 0.0088 (0.0116)\n",
            "TRAIN(181): [130/391] Batch: 0.0454 (0.0520) Data: 0.0186 (0.0192) Loss: 0.0201 (0.0116)\n",
            "TRAIN(181): [140/391] Batch: 0.0532 (0.0520) Data: 0.0177 (0.0190) Loss: 0.0206 (0.0118)\n",
            "TRAIN(181): [150/391] Batch: 0.0521 (0.0521) Data: 0.0174 (0.0187) Loss: 0.0060 (0.0120)\n",
            "TRAIN(181): [160/391] Batch: 0.0472 (0.0520) Data: 0.0183 (0.0186) Loss: 0.0066 (0.0116)\n",
            "TRAIN(181): [170/391] Batch: 0.0465 (0.0519) Data: 0.0245 (0.0186) Loss: 0.0064 (0.0114)\n",
            "TRAIN(181): [180/391] Batch: 0.0379 (0.0518) Data: 0.0251 (0.0186) Loss: 0.0156 (0.0113)\n",
            "TRAIN(181): [190/391] Batch: 0.0501 (0.0517) Data: 0.0142 (0.0187) Loss: 0.0085 (0.0111)\n",
            "TRAIN(181): [200/391] Batch: 0.0487 (0.0516) Data: 0.0202 (0.0186) Loss: 0.0105 (0.0112)\n",
            "TRAIN(181): [210/391] Batch: 0.0470 (0.0515) Data: 0.0252 (0.0186) Loss: 0.0278 (0.0112)\n",
            "TRAIN(181): [220/391] Batch: 0.0471 (0.0514) Data: 0.0256 (0.0188) Loss: 0.0062 (0.0110)\n",
            "TRAIN(181): [230/391] Batch: 0.0489 (0.0512) Data: 0.0233 (0.0189) Loss: 0.0042 (0.0112)\n",
            "TRAIN(181): [240/391] Batch: 0.0516 (0.0512) Data: 0.0166 (0.0189) Loss: 0.0167 (0.0112)\n",
            "TRAIN(181): [250/391] Batch: 0.0464 (0.0511) Data: 0.0251 (0.0190) Loss: 0.0119 (0.0111)\n",
            "TRAIN(181): [260/391] Batch: 0.0474 (0.0510) Data: 0.0262 (0.0191) Loss: 0.0098 (0.0109)\n",
            "TRAIN(181): [270/391] Batch: 0.0481 (0.0509) Data: 0.0228 (0.0192) Loss: 0.0385 (0.0110)\n",
            "TRAIN(181): [280/391] Batch: 0.0497 (0.0508) Data: 0.0162 (0.0191) Loss: 0.0079 (0.0109)\n",
            "TRAIN(181): [290/391] Batch: 0.0470 (0.0507) Data: 0.0267 (0.0192) Loss: 0.0173 (0.0110)\n",
            "TRAIN(181): [300/391] Batch: 0.0466 (0.0506) Data: 0.0223 (0.0193) Loss: 0.0717 (0.0111)\n",
            "TRAIN(181): [310/391] Batch: 0.0485 (0.0506) Data: 0.0254 (0.0193) Loss: 0.0536 (0.0114)\n",
            "TRAIN(181): [320/391] Batch: 0.0437 (0.0506) Data: 0.0223 (0.0192) Loss: 0.0080 (0.0114)\n",
            "TRAIN(181): [330/391] Batch: 0.0582 (0.0507) Data: 0.0163 (0.0192) Loss: 0.0075 (0.0114)\n",
            "TRAIN(181): [340/391] Batch: 0.0516 (0.0506) Data: 0.0176 (0.0192) Loss: 0.0149 (0.0116)\n",
            "TRAIN(181): [350/391] Batch: 0.0621 (0.0507) Data: 0.0122 (0.0191) Loss: 0.0027 (0.0115)\n",
            "TRAIN(181): [360/391] Batch: 0.0459 (0.0506) Data: 0.0231 (0.0191) Loss: 0.0158 (0.0114)\n",
            "TRAIN(181): [370/391] Batch: 0.0429 (0.0507) Data: 0.0205 (0.0190) Loss: 0.0083 (0.0112)\n",
            "TRAIN(181): [380/391] Batch: 0.0483 (0.0507) Data: 0.0146 (0.0189) Loss: 0.0156 (0.0113)\n",
            "TRAIN(181): [390/391] Batch: 0.0462 (0.0506) Data: 0.0270 (0.0189) Loss: 0.0038 (0.0111)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(181)         0:00:19         0:00:07         0:00:12          0.0111\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(182): [ 10/391] Batch: 0.0489 (0.0728) Data: 0.0158 (0.0311) Loss: 0.0097 (0.0170)\n",
            "TRAIN(182): [ 20/391] Batch: 0.0473 (0.0608) Data: 0.0201 (0.0256) Loss: 0.0094 (0.0121)\n",
            "TRAIN(182): [ 30/391] Batch: 0.0446 (0.0573) Data: 0.0249 (0.0226) Loss: 0.0032 (0.0104)\n",
            "TRAIN(182): [ 40/391] Batch: 0.0466 (0.0550) Data: 0.0279 (0.0228) Loss: 0.0111 (0.0101)\n",
            "TRAIN(182): [ 50/391] Batch: 0.0573 (0.0542) Data: 0.0223 (0.0222) Loss: 0.0176 (0.0102)\n",
            "TRAIN(182): [ 60/391] Batch: 0.0516 (0.0533) Data: 0.0245 (0.0222) Loss: 0.0258 (0.0105)\n",
            "TRAIN(182): [ 70/391] Batch: 0.0463 (0.0526) Data: 0.0268 (0.0222) Loss: 0.0050 (0.0102)\n",
            "TRAIN(182): [ 80/391] Batch: 0.0466 (0.0519) Data: 0.0282 (0.0223) Loss: 0.0173 (0.0101)\n",
            "TRAIN(182): [ 90/391] Batch: 0.0455 (0.0515) Data: 0.0266 (0.0224) Loss: 0.0048 (0.0107)\n",
            "TRAIN(182): [100/391] Batch: 0.0464 (0.0512) Data: 0.0265 (0.0224) Loss: 0.0436 (0.0110)\n",
            "TRAIN(182): [110/391] Batch: 0.0423 (0.0509) Data: 0.0257 (0.0224) Loss: 0.0180 (0.0111)\n",
            "TRAIN(182): [120/391] Batch: 0.0497 (0.0508) Data: 0.0235 (0.0224) Loss: 0.0027 (0.0108)\n",
            "TRAIN(182): [130/391] Batch: 0.0526 (0.0508) Data: 0.0172 (0.0221) Loss: 0.0217 (0.0111)\n",
            "TRAIN(182): [140/391] Batch: 0.0433 (0.0506) Data: 0.0248 (0.0221) Loss: 0.0240 (0.0112)\n",
            "TRAIN(182): [150/391] Batch: 0.0466 (0.0505) Data: 0.0232 (0.0220) Loss: 0.0195 (0.0109)\n",
            "TRAIN(182): [160/391] Batch: 0.0479 (0.0504) Data: 0.0252 (0.0219) Loss: 0.0043 (0.0109)\n",
            "TRAIN(182): [170/391] Batch: 0.0419 (0.0504) Data: 0.0252 (0.0218) Loss: 0.0051 (0.0110)\n",
            "TRAIN(182): [180/391] Batch: 0.0446 (0.0504) Data: 0.0271 (0.0217) Loss: 0.0077 (0.0109)\n",
            "TRAIN(182): [190/391] Batch: 0.0439 (0.0502) Data: 0.0254 (0.0217) Loss: 0.0103 (0.0110)\n",
            "TRAIN(182): [200/391] Batch: 0.0482 (0.0502) Data: 0.0172 (0.0216) Loss: 0.0179 (0.0110)\n",
            "TRAIN(182): [210/391] Batch: 0.0418 (0.0501) Data: 0.0275 (0.0216) Loss: 0.0108 (0.0112)\n",
            "TRAIN(182): [220/391] Batch: 0.0430 (0.0501) Data: 0.0272 (0.0215) Loss: 0.0464 (0.0113)\n",
            "TRAIN(182): [230/391] Batch: 0.0616 (0.0500) Data: 0.0191 (0.0216) Loss: 0.0093 (0.0115)\n",
            "TRAIN(182): [240/391] Batch: 0.0457 (0.0500) Data: 0.0208 (0.0215) Loss: 0.0366 (0.0116)\n",
            "TRAIN(182): [250/391] Batch: 0.0612 (0.0501) Data: 0.0162 (0.0214) Loss: 0.0071 (0.0117)\n",
            "TRAIN(182): [260/391] Batch: 0.0581 (0.0501) Data: 0.0184 (0.0212) Loss: 0.0273 (0.0118)\n",
            "TRAIN(182): [270/391] Batch: 0.0598 (0.0502) Data: 0.0145 (0.0210) Loss: 0.0229 (0.0118)\n",
            "TRAIN(182): [280/391] Batch: 0.0410 (0.0502) Data: 0.0220 (0.0208) Loss: 0.0084 (0.0118)\n",
            "TRAIN(182): [290/391] Batch: 0.0530 (0.0502) Data: 0.0122 (0.0206) Loss: 0.0228 (0.0118)\n",
            "TRAIN(182): [300/391] Batch: 0.0526 (0.0502) Data: 0.0175 (0.0204) Loss: 0.0520 (0.0120)\n",
            "TRAIN(182): [310/391] Batch: 0.0459 (0.0501) Data: 0.0266 (0.0205) Loss: 0.0042 (0.0120)\n",
            "TRAIN(182): [320/391] Batch: 0.0519 (0.0501) Data: 0.0171 (0.0205) Loss: 0.0033 (0.0118)\n",
            "TRAIN(182): [330/391] Batch: 0.0506 (0.0501) Data: 0.0178 (0.0204) Loss: 0.0037 (0.0116)\n",
            "TRAIN(182): [340/391] Batch: 0.0544 (0.0500) Data: 0.0158 (0.0204) Loss: 0.0121 (0.0117)\n",
            "TRAIN(182): [350/391] Batch: 0.0422 (0.0500) Data: 0.0237 (0.0204) Loss: 0.0053 (0.0116)\n",
            "TRAIN(182): [360/391] Batch: 0.0448 (0.0500) Data: 0.0240 (0.0204) Loss: 0.0458 (0.0117)\n",
            "TRAIN(182): [370/391] Batch: 0.0463 (0.0499) Data: 0.0275 (0.0205) Loss: 0.0093 (0.0118)\n",
            "TRAIN(182): [380/391] Batch: 0.0563 (0.0499) Data: 0.0227 (0.0205) Loss: 0.0057 (0.0117)\n",
            "TRAIN(182): [390/391] Batch: 0.0463 (0.0498) Data: 0.0242 (0.0205) Loss: 0.0270 (0.0117)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(182)         0:00:19         0:00:08         0:00:11          0.0117\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(183): [ 10/391] Batch: 0.0515 (0.0661) Data: 0.0164 (0.0285) Loss: 0.0117 (0.0078)\n",
            "TRAIN(183): [ 20/391] Batch: 0.0475 (0.0570) Data: 0.0278 (0.0250) Loss: 0.0176 (0.0093)\n",
            "TRAIN(183): [ 30/391] Batch: 0.0455 (0.0547) Data: 0.0233 (0.0227) Loss: 0.0033 (0.0087)\n",
            "TRAIN(183): [ 40/391] Batch: 0.0445 (0.0537) Data: 0.0180 (0.0214) Loss: 0.0036 (0.0078)\n",
            "TRAIN(183): [ 50/391] Batch: 0.0362 (0.0523) Data: 0.0267 (0.0214) Loss: 0.0106 (0.0079)\n",
            "TRAIN(183): [ 60/391] Batch: 0.0449 (0.0516) Data: 0.0277 (0.0217) Loss: 0.0043 (0.0080)\n",
            "TRAIN(183): [ 70/391] Batch: 0.0452 (0.0512) Data: 0.0239 (0.0215) Loss: 0.0221 (0.0087)\n",
            "TRAIN(183): [ 80/391] Batch: 0.0367 (0.0509) Data: 0.0255 (0.0213) Loss: 0.0064 (0.0091)\n",
            "TRAIN(183): [ 90/391] Batch: 0.0373 (0.0508) Data: 0.0243 (0.0208) Loss: 0.0045 (0.0094)\n",
            "TRAIN(183): [100/391] Batch: 0.0563 (0.0508) Data: 0.0156 (0.0205) Loss: 0.0032 (0.0096)\n",
            "TRAIN(183): [110/391] Batch: 0.0469 (0.0506) Data: 0.0258 (0.0205) Loss: 0.0059 (0.0096)\n",
            "TRAIN(183): [120/391] Batch: 0.0415 (0.0507) Data: 0.0247 (0.0203) Loss: 0.0081 (0.0096)\n",
            "TRAIN(183): [130/391] Batch: 0.0469 (0.0509) Data: 0.0165 (0.0200) Loss: 0.0057 (0.0098)\n",
            "TRAIN(183): [140/391] Batch: 0.0479 (0.0509) Data: 0.0205 (0.0197) Loss: 0.0058 (0.0097)\n",
            "TRAIN(183): [150/391] Batch: 0.0518 (0.0509) Data: 0.0202 (0.0196) Loss: 0.0095 (0.0101)\n",
            "TRAIN(183): [160/391] Batch: 0.0488 (0.0510) Data: 0.0149 (0.0193) Loss: 0.0063 (0.0102)\n",
            "TRAIN(183): [170/391] Batch: 0.0504 (0.0510) Data: 0.0076 (0.0190) Loss: 0.0097 (0.0101)\n",
            "TRAIN(183): [180/391] Batch: 0.0559 (0.0510) Data: 0.0167 (0.0188) Loss: 0.0037 (0.0100)\n",
            "TRAIN(183): [190/391] Batch: 0.0505 (0.0509) Data: 0.0242 (0.0189) Loss: 0.0213 (0.0101)\n",
            "TRAIN(183): [200/391] Batch: 0.0456 (0.0507) Data: 0.0269 (0.0191) Loss: 0.0397 (0.0101)\n",
            "TRAIN(183): [210/391] Batch: 0.0466 (0.0506) Data: 0.0264 (0.0192) Loss: 0.0065 (0.0101)\n",
            "TRAIN(183): [220/391] Batch: 0.0472 (0.0505) Data: 0.0277 (0.0193) Loss: 0.0232 (0.0103)\n",
            "TRAIN(183): [230/391] Batch: 0.0436 (0.0505) Data: 0.0251 (0.0194) Loss: 0.0015 (0.0101)\n",
            "TRAIN(183): [240/391] Batch: 0.0563 (0.0505) Data: 0.0211 (0.0193) Loss: 0.0019 (0.0100)\n",
            "TRAIN(183): [250/391] Batch: 0.0470 (0.0504) Data: 0.0270 (0.0194) Loss: 0.0205 (0.0101)\n",
            "TRAIN(183): [260/391] Batch: 0.0549 (0.0504) Data: 0.0181 (0.0193) Loss: 0.0076 (0.0100)\n",
            "TRAIN(183): [270/391] Batch: 0.0489 (0.0504) Data: 0.0247 (0.0193) Loss: 0.0090 (0.0100)\n",
            "TRAIN(183): [280/391] Batch: 0.0461 (0.0504) Data: 0.0270 (0.0194) Loss: 0.0086 (0.0100)\n",
            "TRAIN(183): [290/391] Batch: 0.0433 (0.0503) Data: 0.0270 (0.0195) Loss: 0.0054 (0.0100)\n",
            "TRAIN(183): [300/391] Batch: 0.0527 (0.0503) Data: 0.0220 (0.0195) Loss: 0.0168 (0.0099)\n",
            "TRAIN(183): [310/391] Batch: 0.0461 (0.0502) Data: 0.0232 (0.0195) Loss: 0.0075 (0.0099)\n",
            "TRAIN(183): [320/391] Batch: 0.0449 (0.0501) Data: 0.0258 (0.0196) Loss: 0.0259 (0.0100)\n",
            "TRAIN(183): [330/391] Batch: 0.0496 (0.0501) Data: 0.0265 (0.0197) Loss: 0.0034 (0.0101)\n",
            "TRAIN(183): [340/391] Batch: 0.0497 (0.0500) Data: 0.0266 (0.0198) Loss: 0.0173 (0.0104)\n",
            "TRAIN(183): [350/391] Batch: 0.0420 (0.0500) Data: 0.0259 (0.0199) Loss: 0.0073 (0.0103)\n",
            "TRAIN(183): [360/391] Batch: 0.0419 (0.0500) Data: 0.0266 (0.0200) Loss: 0.0183 (0.0103)\n",
            "TRAIN(183): [370/391] Batch: 0.0438 (0.0499) Data: 0.0263 (0.0200) Loss: 0.0039 (0.0102)\n",
            "TRAIN(183): [380/391] Batch: 0.0532 (0.0499) Data: 0.0184 (0.0200) Loss: 0.0108 (0.0102)\n",
            "TRAIN(183): [390/391] Batch: 0.0477 (0.0499) Data: 0.0235 (0.0199) Loss: 0.0092 (0.0102)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(183)         0:00:19         0:00:07         0:00:11          0.0102\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(184): [ 10/391] Batch: 0.0461 (0.0676) Data: 0.0163 (0.0288) Loss: 0.0125 (0.0118)\n",
            "TRAIN(184): [ 20/391] Batch: 0.0344 (0.0589) Data: 0.0257 (0.0231) Loss: 0.0034 (0.0119)\n",
            "TRAIN(184): [ 30/391] Batch: 0.0461 (0.0561) Data: 0.0155 (0.0213) Loss: 0.0208 (0.0119)\n",
            "TRAIN(184): [ 40/391] Batch: 0.0417 (0.0547) Data: 0.0158 (0.0196) Loss: 0.0061 (0.0116)\n",
            "TRAIN(184): [ 50/391] Batch: 0.0464 (0.0536) Data: 0.0270 (0.0196) Loss: 0.0032 (0.0117)\n",
            "TRAIN(184): [ 60/391] Batch: 0.0530 (0.0533) Data: 0.0159 (0.0191) Loss: 0.0066 (0.0114)\n",
            "TRAIN(184): [ 70/391] Batch: 0.0571 (0.0528) Data: 0.0218 (0.0190) Loss: 0.0634 (0.0122)\n",
            "TRAIN(184): [ 80/391] Batch: 0.0500 (0.0523) Data: 0.0258 (0.0193) Loss: 0.0063 (0.0120)\n",
            "TRAIN(184): [ 90/391] Batch: 0.0466 (0.0517) Data: 0.0280 (0.0198) Loss: 0.0272 (0.0122)\n",
            "TRAIN(184): [100/391] Batch: 0.0368 (0.0515) Data: 0.0252 (0.0199) Loss: 0.0159 (0.0120)\n",
            "TRAIN(184): [110/391] Batch: 0.0472 (0.0512) Data: 0.0272 (0.0201) Loss: 0.0555 (0.0122)\n",
            "TRAIN(184): [120/391] Batch: 0.0466 (0.0508) Data: 0.0272 (0.0204) Loss: 0.0032 (0.0121)\n",
            "TRAIN(184): [130/391] Batch: 0.0458 (0.0508) Data: 0.0177 (0.0202) Loss: 0.0053 (0.0122)\n",
            "TRAIN(184): [140/391] Batch: 0.0464 (0.0506) Data: 0.0281 (0.0203) Loss: 0.0042 (0.0121)\n",
            "TRAIN(184): [150/391] Batch: 0.0528 (0.0506) Data: 0.0245 (0.0202) Loss: 0.0028 (0.0119)\n",
            "TRAIN(184): [160/391] Batch: 0.0423 (0.0504) Data: 0.0276 (0.0204) Loss: 0.0070 (0.0116)\n",
            "TRAIN(184): [170/391] Batch: 0.0466 (0.0503) Data: 0.0271 (0.0205) Loss: 0.0065 (0.0116)\n",
            "TRAIN(184): [180/391] Batch: 0.0470 (0.0502) Data: 0.0272 (0.0206) Loss: 0.0174 (0.0116)\n",
            "TRAIN(184): [190/391] Batch: 0.0458 (0.0501) Data: 0.0272 (0.0206) Loss: 0.0084 (0.0118)\n",
            "TRAIN(184): [200/391] Batch: 0.0452 (0.0500) Data: 0.0281 (0.0206) Loss: 0.0078 (0.0116)\n",
            "TRAIN(184): [210/391] Batch: 0.0448 (0.0499) Data: 0.0280 (0.0207) Loss: 0.0106 (0.0116)\n",
            "TRAIN(184): [220/391] Batch: 0.0422 (0.0498) Data: 0.0254 (0.0208) Loss: 0.0371 (0.0117)\n",
            "TRAIN(184): [230/391] Batch: 0.0448 (0.0498) Data: 0.0272 (0.0207) Loss: 0.0144 (0.0117)\n",
            "TRAIN(184): [240/391] Batch: 0.0513 (0.0498) Data: 0.0185 (0.0207) Loss: 0.0069 (0.0115)\n",
            "TRAIN(184): [250/391] Batch: 0.0471 (0.0497) Data: 0.0278 (0.0207) Loss: 0.0096 (0.0114)\n",
            "TRAIN(184): [260/391] Batch: 0.0338 (0.0498) Data: 0.0258 (0.0205) Loss: 0.0112 (0.0114)\n",
            "TRAIN(184): [270/391] Batch: 0.0470 (0.0499) Data: 0.0207 (0.0204) Loss: 0.0058 (0.0115)\n",
            "TRAIN(184): [280/391] Batch: 0.0464 (0.0498) Data: 0.0207 (0.0204) Loss: 0.0039 (0.0113)\n",
            "TRAIN(184): [290/391] Batch: 0.0488 (0.0499) Data: 0.0154 (0.0202) Loss: 0.0334 (0.0116)\n",
            "TRAIN(184): [300/391] Batch: 0.0624 (0.0499) Data: 0.0163 (0.0200) Loss: 0.0098 (0.0117)\n",
            "TRAIN(184): [310/391] Batch: 0.0469 (0.0500) Data: 0.0216 (0.0199) Loss: 0.0131 (0.0116)\n",
            "TRAIN(184): [320/391] Batch: 0.0502 (0.0500) Data: 0.0185 (0.0199) Loss: 0.0132 (0.0116)\n",
            "TRAIN(184): [330/391] Batch: 0.0513 (0.0500) Data: 0.0165 (0.0198) Loss: 0.0017 (0.0116)\n",
            "TRAIN(184): [340/391] Batch: 0.0537 (0.0501) Data: 0.0169 (0.0197) Loss: 0.0181 (0.0116)\n",
            "TRAIN(184): [350/391] Batch: 0.0458 (0.0500) Data: 0.0262 (0.0197) Loss: 0.0109 (0.0115)\n",
            "TRAIN(184): [360/391] Batch: 0.0487 (0.0500) Data: 0.0256 (0.0198) Loss: 0.0107 (0.0116)\n",
            "TRAIN(184): [370/391] Batch: 0.0459 (0.0499) Data: 0.0277 (0.0199) Loss: 0.0065 (0.0117)\n",
            "TRAIN(184): [380/391] Batch: 0.0411 (0.0499) Data: 0.0263 (0.0199) Loss: 0.0220 (0.0117)\n",
            "TRAIN(184): [390/391] Batch: 0.0441 (0.0499) Data: 0.0258 (0.0199) Loss: 0.0063 (0.0116)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(184)         0:00:19         0:00:07         0:00:11          0.0116\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(185): [ 10/391] Batch: 0.0456 (0.0651) Data: 0.0189 (0.0304) Loss: 0.0152 (0.0126)\n",
            "TRAIN(185): [ 20/391] Batch: 0.0584 (0.0564) Data: 0.0266 (0.0276) Loss: 0.0046 (0.0113)\n",
            "TRAIN(185): [ 30/391] Batch: 0.0544 (0.0537) Data: 0.0216 (0.0261) Loss: 0.0102 (0.0104)\n",
            "TRAIN(185): [ 40/391] Batch: 0.0423 (0.0526) Data: 0.0192 (0.0244) Loss: 0.0046 (0.0096)\n",
            "TRAIN(185): [ 50/391] Batch: 0.0468 (0.0518) Data: 0.0272 (0.0240) Loss: 0.0043 (0.0111)\n",
            "TRAIN(185): [ 60/391] Batch: 0.0454 (0.0511) Data: 0.0276 (0.0236) Loss: 0.0063 (0.0114)\n",
            "TRAIN(185): [ 70/391] Batch: 0.0488 (0.0508) Data: 0.0262 (0.0232) Loss: 0.0327 (0.0122)\n",
            "TRAIN(185): [ 80/391] Batch: 0.0487 (0.0507) Data: 0.0252 (0.0229) Loss: 0.0171 (0.0122)\n",
            "TRAIN(185): [ 90/391] Batch: 0.0462 (0.0505) Data: 0.0277 (0.0228) Loss: 0.0048 (0.0123)\n",
            "TRAIN(185): [100/391] Batch: 0.0424 (0.0502) Data: 0.0273 (0.0227) Loss: 0.0300 (0.0125)\n",
            "TRAIN(185): [110/391] Batch: 0.0537 (0.0503) Data: 0.0160 (0.0223) Loss: 0.0100 (0.0130)\n",
            "TRAIN(185): [120/391] Batch: 0.0389 (0.0501) Data: 0.0277 (0.0221) Loss: 0.0053 (0.0125)\n",
            "TRAIN(185): [130/391] Batch: 0.0451 (0.0501) Data: 0.0233 (0.0220) Loss: 0.0023 (0.0124)\n",
            "TRAIN(185): [140/391] Batch: 0.0410 (0.0502) Data: 0.0244 (0.0216) Loss: 0.0117 (0.0124)\n",
            "TRAIN(185): [150/391] Batch: 0.0625 (0.0503) Data: 0.0140 (0.0213) Loss: 0.0068 (0.0119)\n",
            "TRAIN(185): [160/391] Batch: 0.0518 (0.0504) Data: 0.0193 (0.0210) Loss: 0.0360 (0.0119)\n",
            "TRAIN(185): [170/391] Batch: 0.0491 (0.0505) Data: 0.0190 (0.0208) Loss: 0.0284 (0.0119)\n",
            "TRAIN(185): [180/391] Batch: 0.0575 (0.0506) Data: 0.0147 (0.0206) Loss: 0.0029 (0.0114)\n",
            "TRAIN(185): [190/391] Batch: 0.0490 (0.0507) Data: 0.0177 (0.0203) Loss: 0.0079 (0.0113)\n",
            "TRAIN(185): [200/391] Batch: 0.0525 (0.0506) Data: 0.0239 (0.0204) Loss: 0.0275 (0.0114)\n",
            "TRAIN(185): [210/391] Batch: 0.0533 (0.0506) Data: 0.0168 (0.0202) Loss: 0.0048 (0.0113)\n",
            "TRAIN(185): [220/391] Batch: 0.0489 (0.0505) Data: 0.0254 (0.0202) Loss: 0.0189 (0.0111)\n",
            "TRAIN(185): [230/391] Batch: 0.0499 (0.0504) Data: 0.0188 (0.0203) Loss: 0.0033 (0.0111)\n",
            "TRAIN(185): [240/391] Batch: 0.0444 (0.0502) Data: 0.0268 (0.0204) Loss: 0.0068 (0.0111)\n",
            "TRAIN(185): [250/391] Batch: 0.0545 (0.0503) Data: 0.0186 (0.0203) Loss: 0.0069 (0.0109)\n",
            "TRAIN(185): [260/391] Batch: 0.0452 (0.0503) Data: 0.0177 (0.0202) Loss: 0.0051 (0.0109)\n",
            "TRAIN(185): [270/391] Batch: 0.0498 (0.0502) Data: 0.0253 (0.0203) Loss: 0.0050 (0.0108)\n",
            "TRAIN(185): [280/391] Batch: 0.0420 (0.0502) Data: 0.0262 (0.0203) Loss: 0.0069 (0.0108)\n",
            "TRAIN(185): [290/391] Batch: 0.0450 (0.0502) Data: 0.0260 (0.0202) Loss: 0.0065 (0.0108)\n",
            "TRAIN(185): [300/391] Batch: 0.0512 (0.0502) Data: 0.0156 (0.0202) Loss: 0.0254 (0.0108)\n",
            "TRAIN(185): [310/391] Batch: 0.0443 (0.0502) Data: 0.0246 (0.0202) Loss: 0.0069 (0.0108)\n",
            "TRAIN(185): [320/391] Batch: 0.0565 (0.0502) Data: 0.0228 (0.0203) Loss: 0.0122 (0.0109)\n",
            "TRAIN(185): [330/391] Batch: 0.0446 (0.0501) Data: 0.0272 (0.0203) Loss: 0.0077 (0.0110)\n",
            "TRAIN(185): [340/391] Batch: 0.0521 (0.0501) Data: 0.0175 (0.0203) Loss: 0.0095 (0.0109)\n",
            "TRAIN(185): [350/391] Batch: 0.0453 (0.0500) Data: 0.0273 (0.0203) Loss: 0.0065 (0.0108)\n",
            "TRAIN(185): [360/391] Batch: 0.0479 (0.0500) Data: 0.0259 (0.0203) Loss: 0.0123 (0.0108)\n",
            "TRAIN(185): [370/391] Batch: 0.0620 (0.0500) Data: 0.0161 (0.0203) Loss: 0.0040 (0.0107)\n",
            "TRAIN(185): [380/391] Batch: 0.0426 (0.0500) Data: 0.0264 (0.0203) Loss: 0.0067 (0.0107)\n",
            "TRAIN(185): [390/391] Batch: 0.0483 (0.0500) Data: 0.0262 (0.0203) Loss: 0.0166 (0.0107)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(185)         0:00:19         0:00:07         0:00:11          0.0107\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(186): [ 10/391] Batch: 0.0452 (0.0703) Data: 0.0137 (0.0295) Loss: 0.0051 (0.0086)\n",
            "TRAIN(186): [ 20/391] Batch: 0.0464 (0.0604) Data: 0.0154 (0.0219) Loss: 0.0065 (0.0086)\n",
            "TRAIN(186): [ 30/391] Batch: 0.0562 (0.0578) Data: 0.0173 (0.0192) Loss: 0.0041 (0.0085)\n",
            "TRAIN(186): [ 40/391] Batch: 0.0476 (0.0555) Data: 0.0219 (0.0187) Loss: 0.0056 (0.0086)\n",
            "TRAIN(186): [ 50/391] Batch: 0.0661 (0.0548) Data: 0.0141 (0.0181) Loss: 0.0033 (0.0093)\n",
            "TRAIN(186): [ 60/391] Batch: 0.0472 (0.0545) Data: 0.0140 (0.0175) Loss: 0.0041 (0.0100)\n",
            "TRAIN(186): [ 70/391] Batch: 0.0559 (0.0536) Data: 0.0198 (0.0177) Loss: 0.0154 (0.0110)\n",
            "TRAIN(186): [ 80/391] Batch: 0.0342 (0.0531) Data: 0.0255 (0.0176) Loss: 0.0176 (0.0105)\n",
            "TRAIN(186): [ 90/391] Batch: 0.0462 (0.0525) Data: 0.0268 (0.0182) Loss: 0.0165 (0.0104)\n",
            "TRAIN(186): [100/391] Batch: 0.0564 (0.0523) Data: 0.0185 (0.0186) Loss: 0.0139 (0.0110)\n",
            "TRAIN(186): [110/391] Batch: 0.0395 (0.0519) Data: 0.0228 (0.0187) Loss: 0.0035 (0.0111)\n",
            "TRAIN(186): [120/391] Batch: 0.0469 (0.0517) Data: 0.0271 (0.0189) Loss: 0.0057 (0.0109)\n",
            "TRAIN(186): [130/391] Batch: 0.0406 (0.0513) Data: 0.0265 (0.0192) Loss: 0.0077 (0.0109)\n",
            "TRAIN(186): [140/391] Batch: 0.0479 (0.0512) Data: 0.0169 (0.0192) Loss: 0.0160 (0.0111)\n",
            "TRAIN(186): [150/391] Batch: 0.0440 (0.0511) Data: 0.0276 (0.0193) Loss: 0.0095 (0.0112)\n",
            "TRAIN(186): [160/391] Batch: 0.0475 (0.0509) Data: 0.0270 (0.0196) Loss: 0.0053 (0.0110)\n",
            "TRAIN(186): [170/391] Batch: 0.0463 (0.0507) Data: 0.0269 (0.0198) Loss: 0.0035 (0.0109)\n",
            "TRAIN(186): [180/391] Batch: 0.0371 (0.0506) Data: 0.0269 (0.0198) Loss: 0.0045 (0.0108)\n",
            "TRAIN(186): [190/391] Batch: 0.0447 (0.0505) Data: 0.0265 (0.0199) Loss: 0.0096 (0.0112)\n",
            "TRAIN(186): [200/391] Batch: 0.0537 (0.0505) Data: 0.0177 (0.0198) Loss: 0.0085 (0.0112)\n",
            "TRAIN(186): [210/391] Batch: 0.0498 (0.0504) Data: 0.0269 (0.0199) Loss: 0.0444 (0.0114)\n",
            "TRAIN(186): [220/391] Batch: 0.0464 (0.0504) Data: 0.0232 (0.0198) Loss: 0.0199 (0.0113)\n",
            "TRAIN(186): [230/391] Batch: 0.0454 (0.0502) Data: 0.0291 (0.0200) Loss: 0.0106 (0.0113)\n",
            "TRAIN(186): [240/391] Batch: 0.0551 (0.0502) Data: 0.0239 (0.0201) Loss: 0.0076 (0.0115)\n",
            "TRAIN(186): [250/391] Batch: 0.0545 (0.0502) Data: 0.0178 (0.0200) Loss: 0.0225 (0.0115)\n",
            "TRAIN(186): [260/391] Batch: 0.0501 (0.0502) Data: 0.0190 (0.0199) Loss: 0.0131 (0.0115)\n",
            "TRAIN(186): [270/391] Batch: 0.0597 (0.0502) Data: 0.0151 (0.0199) Loss: 0.0034 (0.0114)\n",
            "TRAIN(186): [280/391] Batch: 0.0505 (0.0503) Data: 0.0193 (0.0197) Loss: 0.0071 (0.0113)\n",
            "TRAIN(186): [290/391] Batch: 0.0528 (0.0504) Data: 0.0163 (0.0196) Loss: 0.0447 (0.0114)\n",
            "TRAIN(186): [300/391] Batch: 0.0439 (0.0504) Data: 0.0179 (0.0194) Loss: 0.0066 (0.0115)\n",
            "TRAIN(186): [310/391] Batch: 0.0469 (0.0505) Data: 0.0195 (0.0193) Loss: 0.0043 (0.0115)\n",
            "TRAIN(186): [320/391] Batch: 0.0493 (0.0505) Data: 0.0240 (0.0193) Loss: 0.0106 (0.0113)\n",
            "TRAIN(186): [330/391] Batch: 0.0633 (0.0506) Data: 0.0130 (0.0192) Loss: 0.0094 (0.0113)\n",
            "TRAIN(186): [340/391] Batch: 0.0504 (0.0505) Data: 0.0237 (0.0192) Loss: 0.0074 (0.0114)\n",
            "TRAIN(186): [350/391] Batch: 0.0531 (0.0505) Data: 0.0258 (0.0193) Loss: 0.0060 (0.0114)\n",
            "TRAIN(186): [360/391] Batch: 0.0524 (0.0504) Data: 0.0191 (0.0194) Loss: 0.0049 (0.0115)\n",
            "TRAIN(186): [370/391] Batch: 0.0459 (0.0504) Data: 0.0261 (0.0194) Loss: 0.0176 (0.0115)\n",
            "TRAIN(186): [380/391] Batch: 0.0412 (0.0503) Data: 0.0249 (0.0195) Loss: 0.0159 (0.0115)\n",
            "TRAIN(186): [390/391] Batch: 0.0461 (0.0503) Data: 0.0282 (0.0196) Loss: 0.0347 (0.0116)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(186)         0:00:19         0:00:07         0:00:12          0.0116\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(187): [ 10/391] Batch: 0.0534 (0.0646) Data: 0.0170 (0.0299) Loss: 0.0152 (0.0090)\n",
            "TRAIN(187): [ 20/391] Batch: 0.0448 (0.0568) Data: 0.0257 (0.0253) Loss: 0.0146 (0.0086)\n",
            "TRAIN(187): [ 30/391] Batch: 0.0445 (0.0543) Data: 0.0200 (0.0237) Loss: 0.0055 (0.0085)\n",
            "TRAIN(187): [ 40/391] Batch: 0.0437 (0.0534) Data: 0.0262 (0.0222) Loss: 0.0136 (0.0085)\n",
            "TRAIN(187): [ 50/391] Batch: 0.0565 (0.0529) Data: 0.0164 (0.0217) Loss: 0.0166 (0.0091)\n",
            "TRAIN(187): [ 60/391] Batch: 0.0645 (0.0524) Data: 0.0154 (0.0214) Loss: 0.0148 (0.0097)\n",
            "TRAIN(187): [ 70/391] Batch: 0.0433 (0.0520) Data: 0.0253 (0.0212) Loss: 0.0178 (0.0097)\n",
            "TRAIN(187): [ 80/391] Batch: 0.0426 (0.0516) Data: 0.0271 (0.0211) Loss: 0.0137 (0.0100)\n",
            "TRAIN(187): [ 90/391] Batch: 0.0482 (0.0515) Data: 0.0242 (0.0212) Loss: 0.0029 (0.0098)\n",
            "TRAIN(187): [100/391] Batch: 0.0379 (0.0513) Data: 0.0276 (0.0210) Loss: 0.0040 (0.0104)\n",
            "TRAIN(187): [110/391] Batch: 0.0470 (0.0511) Data: 0.0277 (0.0211) Loss: 0.0154 (0.0108)\n",
            "TRAIN(187): [120/391] Batch: 0.0465 (0.0509) Data: 0.0272 (0.0212) Loss: 0.0055 (0.0104)\n",
            "TRAIN(187): [130/391] Batch: 0.0441 (0.0509) Data: 0.0175 (0.0210) Loss: 0.0096 (0.0103)\n",
            "TRAIN(187): [140/391] Batch: 0.0529 (0.0506) Data: 0.0255 (0.0212) Loss: 0.0217 (0.0103)\n",
            "TRAIN(187): [150/391] Batch: 0.0703 (0.0508) Data: 0.0129 (0.0209) Loss: 0.0090 (0.0105)\n",
            "TRAIN(187): [160/391] Batch: 0.0566 (0.0507) Data: 0.0141 (0.0205) Loss: 0.0039 (0.0108)\n",
            "TRAIN(187): [170/391] Batch: 0.0573 (0.0507) Data: 0.0160 (0.0201) Loss: 0.0189 (0.0109)\n",
            "TRAIN(187): [180/391] Batch: 0.0438 (0.0507) Data: 0.0169 (0.0198) Loss: 0.0040 (0.0107)\n",
            "TRAIN(187): [190/391] Batch: 0.0538 (0.0507) Data: 0.0131 (0.0194) Loss: 0.0160 (0.0106)\n",
            "TRAIN(187): [200/391] Batch: 0.0626 (0.0508) Data: 0.0150 (0.0193) Loss: 0.0129 (0.0106)\n",
            "TRAIN(187): [210/391] Batch: 0.0551 (0.0508) Data: 0.0221 (0.0191) Loss: 0.0149 (0.0109)\n",
            "TRAIN(187): [220/391] Batch: 0.0529 (0.0507) Data: 0.0170 (0.0191) Loss: 0.0049 (0.0109)\n",
            "TRAIN(187): [230/391] Batch: 0.0552 (0.0506) Data: 0.0177 (0.0192) Loss: 0.0072 (0.0108)\n",
            "TRAIN(187): [240/391] Batch: 0.0462 (0.0505) Data: 0.0280 (0.0192) Loss: 0.0152 (0.0108)\n",
            "TRAIN(187): [250/391] Batch: 0.0498 (0.0504) Data: 0.0269 (0.0194) Loss: 0.0227 (0.0108)\n",
            "TRAIN(187): [260/391] Batch: 0.0515 (0.0503) Data: 0.0242 (0.0195) Loss: 0.0352 (0.0109)\n",
            "TRAIN(187): [270/391] Batch: 0.0467 (0.0503) Data: 0.0278 (0.0195) Loss: 0.0049 (0.0108)\n",
            "TRAIN(187): [280/391] Batch: 0.0579 (0.0503) Data: 0.0152 (0.0196) Loss: 0.0162 (0.0108)\n",
            "TRAIN(187): [290/391] Batch: 0.0466 (0.0502) Data: 0.0277 (0.0197) Loss: 0.0039 (0.0107)\n",
            "TRAIN(187): [300/391] Batch: 0.0474 (0.0501) Data: 0.0259 (0.0198) Loss: 0.0089 (0.0108)\n",
            "TRAIN(187): [310/391] Batch: 0.0453 (0.0500) Data: 0.0275 (0.0199) Loss: 0.0128 (0.0107)\n",
            "TRAIN(187): [320/391] Batch: 0.0455 (0.0500) Data: 0.0183 (0.0200) Loss: 0.0027 (0.0107)\n",
            "TRAIN(187): [330/391] Batch: 0.0515 (0.0499) Data: 0.0176 (0.0200) Loss: 0.0066 (0.0107)\n",
            "TRAIN(187): [340/391] Batch: 0.0485 (0.0499) Data: 0.0251 (0.0200) Loss: 0.0106 (0.0107)\n",
            "TRAIN(187): [350/391] Batch: 0.0470 (0.0499) Data: 0.0234 (0.0201) Loss: 0.0152 (0.0106)\n",
            "TRAIN(187): [360/391] Batch: 0.0528 (0.0499) Data: 0.0237 (0.0201) Loss: 0.0042 (0.0107)\n",
            "TRAIN(187): [370/391] Batch: 0.0456 (0.0499) Data: 0.0268 (0.0200) Loss: 0.0050 (0.0107)\n",
            "TRAIN(187): [380/391] Batch: 0.0424 (0.0499) Data: 0.0196 (0.0200) Loss: 0.0385 (0.0108)\n",
            "TRAIN(187): [390/391] Batch: 0.0456 (0.0499) Data: 0.0278 (0.0200) Loss: 0.0363 (0.0107)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(187)         0:00:19         0:00:07         0:00:11          0.0107\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(188): [ 10/391] Batch: 0.0462 (0.0640) Data: 0.0265 (0.0363) Loss: 0.0067 (0.0076)\n",
            "TRAIN(188): [ 20/391] Batch: 0.0648 (0.0569) Data: 0.0145 (0.0293) Loss: 0.0030 (0.0092)\n",
            "TRAIN(188): [ 30/391] Batch: 0.0501 (0.0551) Data: 0.0204 (0.0248) Loss: 0.0022 (0.0101)\n",
            "TRAIN(188): [ 40/391] Batch: 0.0588 (0.0537) Data: 0.0157 (0.0232) Loss: 0.0061 (0.0106)\n",
            "TRAIN(188): [ 50/391] Batch: 0.0490 (0.0536) Data: 0.0131 (0.0215) Loss: 0.0067 (0.0121)\n",
            "TRAIN(188): [ 60/391] Batch: 0.0385 (0.0532) Data: 0.0255 (0.0209) Loss: 0.0021 (0.0124)\n",
            "TRAIN(188): [ 70/391] Batch: 0.0469 (0.0531) Data: 0.0209 (0.0204) Loss: 0.0042 (0.0122)\n",
            "TRAIN(188): [ 80/391] Batch: 0.0501 (0.0528) Data: 0.0253 (0.0202) Loss: 0.0054 (0.0123)\n",
            "TRAIN(188): [ 90/391] Batch: 0.0481 (0.0525) Data: 0.0272 (0.0200) Loss: 0.0121 (0.0121)\n",
            "TRAIN(188): [100/391] Batch: 0.0468 (0.0520) Data: 0.0275 (0.0204) Loss: 0.0015 (0.0124)\n",
            "TRAIN(188): [110/391] Batch: 0.0478 (0.0516) Data: 0.0263 (0.0206) Loss: 0.0089 (0.0120)\n",
            "TRAIN(188): [120/391] Batch: 0.0466 (0.0512) Data: 0.0273 (0.0208) Loss: 0.0062 (0.0123)\n",
            "TRAIN(188): [130/391] Batch: 0.0522 (0.0511) Data: 0.0221 (0.0208) Loss: 0.0120 (0.0121)\n",
            "TRAIN(188): [140/391] Batch: 0.0510 (0.0509) Data: 0.0255 (0.0210) Loss: 0.0053 (0.0121)\n",
            "TRAIN(188): [150/391] Batch: 0.0464 (0.0508) Data: 0.0269 (0.0211) Loss: 0.0398 (0.0120)\n",
            "TRAIN(188): [160/391] Batch: 0.0456 (0.0508) Data: 0.0224 (0.0211) Loss: 0.0128 (0.0120)\n",
            "TRAIN(188): [170/391] Batch: 0.0444 (0.0507) Data: 0.0271 (0.0212) Loss: 0.0084 (0.0121)\n",
            "TRAIN(188): [180/391] Batch: 0.0377 (0.0506) Data: 0.0288 (0.0212) Loss: 0.0069 (0.0119)\n",
            "TRAIN(188): [190/391] Batch: 0.0451 (0.0505) Data: 0.0253 (0.0212) Loss: 0.0072 (0.0117)\n",
            "TRAIN(188): [200/391] Batch: 0.0471 (0.0503) Data: 0.0272 (0.0213) Loss: 0.0055 (0.0115)\n",
            "TRAIN(188): [210/391] Batch: 0.0426 (0.0503) Data: 0.0242 (0.0213) Loss: 0.0038 (0.0116)\n",
            "TRAIN(188): [220/391] Batch: 0.0460 (0.0501) Data: 0.0283 (0.0214) Loss: 0.0102 (0.0114)\n",
            "TRAIN(188): [230/391] Batch: 0.0457 (0.0501) Data: 0.0279 (0.0214) Loss: 0.0192 (0.0114)\n",
            "TRAIN(188): [240/391] Batch: 0.0523 (0.0500) Data: 0.0251 (0.0214) Loss: 0.0089 (0.0115)\n",
            "TRAIN(188): [250/391] Batch: 0.0444 (0.0500) Data: 0.0275 (0.0213) Loss: 0.0147 (0.0115)\n",
            "TRAIN(188): [260/391] Batch: 0.0536 (0.0501) Data: 0.0178 (0.0212) Loss: 0.0057 (0.0115)\n",
            "TRAIN(188): [270/391] Batch: 0.0450 (0.0500) Data: 0.0198 (0.0211) Loss: 0.0090 (0.0115)\n",
            "TRAIN(188): [280/391] Batch: 0.0508 (0.0500) Data: 0.0254 (0.0211) Loss: 0.0058 (0.0113)\n",
            "TRAIN(188): [290/391] Batch: 0.0490 (0.0499) Data: 0.0265 (0.0212) Loss: 0.0075 (0.0112)\n",
            "TRAIN(188): [300/391] Batch: 0.0332 (0.0499) Data: 0.0271 (0.0211) Loss: 0.0045 (0.0113)\n",
            "TRAIN(188): [310/391] Batch: 0.0472 (0.0500) Data: 0.0189 (0.0209) Loss: 0.0039 (0.0112)\n",
            "TRAIN(188): [320/391] Batch: 0.0497 (0.0500) Data: 0.0159 (0.0208) Loss: 0.0080 (0.0111)\n",
            "TRAIN(188): [330/391] Batch: 0.0604 (0.0501) Data: 0.0153 (0.0207) Loss: 0.0071 (0.0111)\n",
            "TRAIN(188): [340/391] Batch: 0.0411 (0.0501) Data: 0.0238 (0.0206) Loss: 0.0101 (0.0111)\n",
            "TRAIN(188): [350/391] Batch: 0.0475 (0.0501) Data: 0.0212 (0.0205) Loss: 0.0131 (0.0112)\n",
            "TRAIN(188): [360/391] Batch: 0.0553 (0.0501) Data: 0.0234 (0.0205) Loss: 0.0075 (0.0111)\n",
            "TRAIN(188): [370/391] Batch: 0.0495 (0.0501) Data: 0.0248 (0.0205) Loss: 0.0054 (0.0112)\n",
            "TRAIN(188): [380/391] Batch: 0.0493 (0.0501) Data: 0.0241 (0.0205) Loss: 0.0099 (0.0112)\n",
            "TRAIN(188): [390/391] Batch: 0.0467 (0.0500) Data: 0.0281 (0.0205) Loss: 0.0558 (0.0113)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(188)         0:00:19         0:00:08         0:00:11          0.0113\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(189): [ 10/391] Batch: 0.0444 (0.0664) Data: 0.0216 (0.0311) Loss: 0.0036 (0.0105)\n",
            "TRAIN(189): [ 20/391] Batch: 0.0446 (0.0575) Data: 0.0252 (0.0266) Loss: 0.0072 (0.0139)\n",
            "TRAIN(189): [ 30/391] Batch: 0.0462 (0.0542) Data: 0.0273 (0.0254) Loss: 0.0326 (0.0144)\n",
            "TRAIN(189): [ 40/391] Batch: 0.0486 (0.0531) Data: 0.0250 (0.0246) Loss: 0.0043 (0.0128)\n",
            "TRAIN(189): [ 50/391] Batch: 0.0513 (0.0525) Data: 0.0196 (0.0237) Loss: 0.0089 (0.0123)\n",
            "TRAIN(189): [ 60/391] Batch: 0.0377 (0.0521) Data: 0.0270 (0.0226) Loss: 0.0030 (0.0121)\n",
            "TRAIN(189): [ 70/391] Batch: 0.0521 (0.0518) Data: 0.0189 (0.0221) Loss: 0.0075 (0.0119)\n",
            "TRAIN(189): [ 80/391] Batch: 0.0469 (0.0513) Data: 0.0275 (0.0220) Loss: 0.0135 (0.0128)\n",
            "TRAIN(189): [ 90/391] Batch: 0.0485 (0.0511) Data: 0.0276 (0.0219) Loss: 0.0094 (0.0131)\n",
            "TRAIN(189): [100/391] Batch: 0.0487 (0.0508) Data: 0.0257 (0.0220) Loss: 0.0058 (0.0129)\n",
            "TRAIN(189): [110/391] Batch: 0.0398 (0.0507) Data: 0.0262 (0.0218) Loss: 0.0229 (0.0125)\n",
            "TRAIN(189): [120/391] Batch: 0.0453 (0.0504) Data: 0.0281 (0.0219) Loss: 0.0062 (0.0128)\n",
            "TRAIN(189): [130/391] Batch: 0.0415 (0.0503) Data: 0.0259 (0.0218) Loss: 0.0111 (0.0126)\n",
            "TRAIN(189): [140/391] Batch: 0.0530 (0.0503) Data: 0.0152 (0.0217) Loss: 0.0045 (0.0126)\n",
            "TRAIN(189): [150/391] Batch: 0.0523 (0.0502) Data: 0.0198 (0.0215) Loss: 0.0088 (0.0125)\n",
            "TRAIN(189): [160/391] Batch: 0.0545 (0.0501) Data: 0.0242 (0.0215) Loss: 0.0185 (0.0126)\n",
            "TRAIN(189): [170/391] Batch: 0.0490 (0.0501) Data: 0.0203 (0.0213) Loss: 0.0171 (0.0126)\n",
            "TRAIN(189): [180/391] Batch: 0.0491 (0.0501) Data: 0.0209 (0.0212) Loss: 0.0081 (0.0125)\n",
            "TRAIN(189): [190/391] Batch: 0.0626 (0.0502) Data: 0.0147 (0.0210) Loss: 0.0064 (0.0123)\n",
            "TRAIN(189): [200/391] Batch: 0.0520 (0.0503) Data: 0.0238 (0.0207) Loss: 0.0046 (0.0121)\n",
            "TRAIN(189): [210/391] Batch: 0.0475 (0.0504) Data: 0.0145 (0.0205) Loss: 0.0051 (0.0119)\n",
            "TRAIN(189): [220/391] Batch: 0.0402 (0.0504) Data: 0.0243 (0.0204) Loss: 0.0016 (0.0119)\n",
            "TRAIN(189): [230/391] Batch: 0.0567 (0.0505) Data: 0.0156 (0.0202) Loss: 0.0094 (0.0117)\n",
            "TRAIN(189): [240/391] Batch: 0.0452 (0.0505) Data: 0.0170 (0.0201) Loss: 0.0030 (0.0116)\n",
            "TRAIN(189): [250/391] Batch: 0.0471 (0.0505) Data: 0.0277 (0.0201) Loss: 0.0060 (0.0116)\n",
            "TRAIN(189): [260/391] Batch: 0.0476 (0.0503) Data: 0.0270 (0.0202) Loss: 0.0164 (0.0115)\n",
            "TRAIN(189): [270/391] Batch: 0.0434 (0.0503) Data: 0.0256 (0.0203) Loss: 0.0050 (0.0113)\n",
            "TRAIN(189): [280/391] Batch: 0.0469 (0.0503) Data: 0.0277 (0.0203) Loss: 0.0103 (0.0115)\n",
            "TRAIN(189): [290/391] Batch: 0.0448 (0.0502) Data: 0.0273 (0.0204) Loss: 0.0084 (0.0115)\n",
            "TRAIN(189): [300/391] Batch: 0.0451 (0.0501) Data: 0.0278 (0.0204) Loss: 0.0252 (0.0116)\n",
            "TRAIN(189): [310/391] Batch: 0.0456 (0.0501) Data: 0.0278 (0.0204) Loss: 0.0089 (0.0118)\n",
            "TRAIN(189): [320/391] Batch: 0.0459 (0.0500) Data: 0.0255 (0.0205) Loss: 0.0038 (0.0116)\n",
            "TRAIN(189): [330/391] Batch: 0.0459 (0.0500) Data: 0.0278 (0.0205) Loss: 0.0188 (0.0117)\n",
            "TRAIN(189): [340/391] Batch: 0.0461 (0.0499) Data: 0.0277 (0.0205) Loss: 0.0130 (0.0115)\n",
            "TRAIN(189): [350/391] Batch: 0.0458 (0.0498) Data: 0.0267 (0.0206) Loss: 0.0065 (0.0114)\n",
            "TRAIN(189): [360/391] Batch: 0.0470 (0.0498) Data: 0.0271 (0.0206) Loss: 0.0046 (0.0113)\n",
            "TRAIN(189): [370/391] Batch: 0.0481 (0.0498) Data: 0.0209 (0.0206) Loss: 0.0097 (0.0114)\n",
            "TRAIN(189): [380/391] Batch: 0.0461 (0.0497) Data: 0.0271 (0.0206) Loss: 0.0110 (0.0114)\n",
            "TRAIN(189): [390/391] Batch: 0.0443 (0.0497) Data: 0.0270 (0.0207) Loss: 0.0097 (0.0114)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(189)         0:00:19         0:00:08         0:00:11          0.0114\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(190): [ 10/391] Batch: 0.0458 (0.0651) Data: 0.0264 (0.0317) Loss: 0.0180 (0.0091)\n",
            "TRAIN(190): [ 20/391] Batch: 0.0491 (0.0566) Data: 0.0271 (0.0280) Loss: 0.0100 (0.0081)\n",
            "TRAIN(190): [ 30/391] Batch: 0.0468 (0.0542) Data: 0.0252 (0.0261) Loss: 0.0039 (0.0083)\n",
            "TRAIN(190): [ 40/391] Batch: 0.0518 (0.0529) Data: 0.0258 (0.0252) Loss: 0.0034 (0.0076)\n",
            "TRAIN(190): [ 50/391] Batch: 0.0388 (0.0527) Data: 0.0249 (0.0234) Loss: 0.0034 (0.0076)\n",
            "TRAIN(190): [ 60/391] Batch: 0.0492 (0.0524) Data: 0.0200 (0.0224) Loss: 0.0276 (0.0085)\n",
            "TRAIN(190): [ 70/391] Batch: 0.0502 (0.0519) Data: 0.0201 (0.0219) Loss: 0.0204 (0.0094)\n",
            "TRAIN(190): [ 80/391] Batch: 0.0376 (0.0517) Data: 0.0274 (0.0214) Loss: 0.0099 (0.0096)\n",
            "TRAIN(190): [ 90/391] Batch: 0.0479 (0.0517) Data: 0.0152 (0.0208) Loss: 0.0080 (0.0092)\n",
            "TRAIN(190): [100/391] Batch: 0.0426 (0.0516) Data: 0.0235 (0.0204) Loss: 0.0035 (0.0091)\n",
            "TRAIN(190): [110/391] Batch: 0.0452 (0.0514) Data: 0.0278 (0.0203) Loss: 0.0069 (0.0090)\n",
            "TRAIN(190): [120/391] Batch: 0.0464 (0.0511) Data: 0.0276 (0.0205) Loss: 0.0069 (0.0090)\n",
            "TRAIN(190): [130/391] Batch: 0.0460 (0.0508) Data: 0.0251 (0.0206) Loss: 0.0034 (0.0089)\n",
            "TRAIN(190): [140/391] Batch: 0.0462 (0.0505) Data: 0.0283 (0.0208) Loss: 0.0075 (0.0089)\n",
            "TRAIN(190): [150/391] Batch: 0.0501 (0.0504) Data: 0.0253 (0.0209) Loss: 0.0162 (0.0094)\n",
            "TRAIN(190): [160/391] Batch: 0.0466 (0.0502) Data: 0.0285 (0.0211) Loss: 0.0055 (0.0095)\n",
            "TRAIN(190): [170/391] Batch: 0.0444 (0.0501) Data: 0.0206 (0.0211) Loss: 0.0066 (0.0095)\n",
            "TRAIN(190): [180/391] Batch: 0.0508 (0.0501) Data: 0.0191 (0.0210) Loss: 0.0092 (0.0097)\n",
            "TRAIN(190): [190/391] Batch: 0.0471 (0.0499) Data: 0.0274 (0.0211) Loss: 0.0037 (0.0095)\n",
            "TRAIN(190): [200/391] Batch: 0.0429 (0.0498) Data: 0.0206 (0.0211) Loss: 0.0117 (0.0100)\n",
            "TRAIN(190): [210/391] Batch: 0.0409 (0.0498) Data: 0.0274 (0.0210) Loss: 0.0085 (0.0100)\n",
            "TRAIN(190): [220/391] Batch: 0.0440 (0.0497) Data: 0.0259 (0.0211) Loss: 0.0066 (0.0100)\n",
            "TRAIN(190): [230/391] Batch: 0.0463 (0.0497) Data: 0.0284 (0.0212) Loss: 0.0066 (0.0101)\n",
            "TRAIN(190): [240/391] Batch: 0.0465 (0.0496) Data: 0.0280 (0.0213) Loss: 0.0114 (0.0100)\n",
            "TRAIN(190): [250/391] Batch: 0.0608 (0.0496) Data: 0.0177 (0.0213) Loss: 0.0101 (0.0101)\n",
            "TRAIN(190): [260/391] Batch: 0.0519 (0.0496) Data: 0.0170 (0.0211) Loss: 0.0100 (0.0101)\n",
            "TRAIN(190): [270/391] Batch: 0.0529 (0.0496) Data: 0.0203 (0.0212) Loss: 0.0060 (0.0102)\n",
            "TRAIN(190): [280/391] Batch: 0.0469 (0.0495) Data: 0.0277 (0.0213) Loss: 0.0048 (0.0102)\n",
            "TRAIN(190): [290/391] Batch: 0.0565 (0.0494) Data: 0.0190 (0.0213) Loss: 0.0096 (0.0103)\n",
            "TRAIN(190): [300/391] Batch: 0.0472 (0.0493) Data: 0.0268 (0.0214) Loss: 0.0137 (0.0102)\n",
            "TRAIN(190): [310/391] Batch: 0.0522 (0.0493) Data: 0.0246 (0.0214) Loss: 0.0086 (0.0103)\n",
            "TRAIN(190): [320/391] Batch: 0.0511 (0.0494) Data: 0.0175 (0.0213) Loss: 0.0111 (0.0102)\n",
            "TRAIN(190): [330/391] Batch: 0.0478 (0.0494) Data: 0.0172 (0.0211) Loss: 0.0026 (0.0105)\n",
            "TRAIN(190): [340/391] Batch: 0.0561 (0.0494) Data: 0.0155 (0.0209) Loss: 0.0158 (0.0104)\n",
            "TRAIN(190): [350/391] Batch: 0.0505 (0.0495) Data: 0.0202 (0.0208) Loss: 0.0115 (0.0103)\n",
            "TRAIN(190): [360/391] Batch: 0.0573 (0.0496) Data: 0.0185 (0.0207) Loss: 0.0119 (0.0103)\n",
            "TRAIN(190): [370/391] Batch: 0.0491 (0.0496) Data: 0.0154 (0.0206) Loss: 0.0075 (0.0102)\n",
            "TRAIN(190): [380/391] Batch: 0.0512 (0.0497) Data: 0.0242 (0.0205) Loss: 0.0128 (0.0102)\n",
            "TRAIN(190): [390/391] Batch: 0.0457 (0.0496) Data: 0.0285 (0.0206) Loss: 0.0066 (0.0103)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(190)         0:00:19         0:00:08         0:00:11          0.0103\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(191): [ 10/391] Batch: 0.0559 (0.0704) Data: 0.0164 (0.0296) Loss: 0.0013 (0.0143)\n",
            "TRAIN(191): [ 20/391] Batch: 0.0440 (0.0600) Data: 0.0265 (0.0246) Loss: 0.0101 (0.0132)\n",
            "TRAIN(191): [ 30/391] Batch: 0.0444 (0.0560) Data: 0.0281 (0.0235) Loss: 0.0035 (0.0122)\n",
            "TRAIN(191): [ 40/391] Batch: 0.0465 (0.0544) Data: 0.0232 (0.0225) Loss: 0.0041 (0.0104)\n",
            "TRAIN(191): [ 50/391] Batch: 0.0524 (0.0530) Data: 0.0249 (0.0227) Loss: 0.0054 (0.0109)\n",
            "TRAIN(191): [ 60/391] Batch: 0.0486 (0.0525) Data: 0.0207 (0.0221) Loss: 0.0066 (0.0123)\n",
            "TRAIN(191): [ 70/391] Batch: 0.0540 (0.0518) Data: 0.0233 (0.0222) Loss: 0.0396 (0.0124)\n",
            "TRAIN(191): [ 80/391] Batch: 0.0456 (0.0512) Data: 0.0277 (0.0224) Loss: 0.0135 (0.0122)\n",
            "TRAIN(191): [ 90/391] Batch: 0.0456 (0.0507) Data: 0.0289 (0.0225) Loss: 0.0055 (0.0118)\n",
            "TRAIN(191): [100/391] Batch: 0.0594 (0.0506) Data: 0.0185 (0.0223) Loss: 0.0127 (0.0114)\n",
            "TRAIN(191): [110/391] Batch: 0.0522 (0.0504) Data: 0.0200 (0.0221) Loss: 0.0063 (0.0120)\n",
            "TRAIN(191): [120/391] Batch: 0.0406 (0.0502) Data: 0.0231 (0.0220) Loss: 0.0036 (0.0117)\n",
            "TRAIN(191): [130/391] Batch: 0.0373 (0.0501) Data: 0.0268 (0.0218) Loss: 0.0093 (0.0117)\n",
            "TRAIN(191): [140/391] Batch: 0.0484 (0.0501) Data: 0.0223 (0.0216) Loss: 0.0144 (0.0115)\n",
            "TRAIN(191): [150/391] Batch: 0.0502 (0.0500) Data: 0.0214 (0.0215) Loss: 0.0051 (0.0115)\n",
            "TRAIN(191): [160/391] Batch: 0.0450 (0.0499) Data: 0.0245 (0.0213) Loss: 0.0100 (0.0114)\n",
            "TRAIN(191): [170/391] Batch: 0.0476 (0.0500) Data: 0.0187 (0.0211) Loss: 0.0065 (0.0113)\n",
            "TRAIN(191): [180/391] Batch: 0.0397 (0.0498) Data: 0.0281 (0.0212) Loss: 0.0249 (0.0113)\n",
            "TRAIN(191): [190/391] Batch: 0.0601 (0.0498) Data: 0.0148 (0.0212) Loss: 0.0088 (0.0113)\n",
            "TRAIN(191): [200/391] Batch: 0.0527 (0.0498) Data: 0.0209 (0.0211) Loss: 0.0113 (0.0112)\n",
            "TRAIN(191): [210/391] Batch: 0.0511 (0.0499) Data: 0.0198 (0.0209) Loss: 0.0059 (0.0112)\n",
            "TRAIN(191): [220/391] Batch: 0.0467 (0.0499) Data: 0.0221 (0.0208) Loss: 0.0100 (0.0112)\n",
            "TRAIN(191): [230/391] Batch: 0.0597 (0.0498) Data: 0.0150 (0.0207) Loss: 0.0180 (0.0112)\n",
            "TRAIN(191): [240/391] Batch: 0.0467 (0.0498) Data: 0.0210 (0.0207) Loss: 0.0069 (0.0110)\n",
            "TRAIN(191): [250/391] Batch: 0.0613 (0.0499) Data: 0.0163 (0.0206) Loss: 0.0048 (0.0109)\n",
            "TRAIN(191): [260/391] Batch: 0.0644 (0.0499) Data: 0.0156 (0.0204) Loss: 0.0066 (0.0110)\n",
            "TRAIN(191): [270/391] Batch: 0.0432 (0.0499) Data: 0.0184 (0.0204) Loss: 0.0038 (0.0110)\n",
            "TRAIN(191): [280/391] Batch: 0.0444 (0.0498) Data: 0.0220 (0.0204) Loss: 0.0363 (0.0111)\n",
            "TRAIN(191): [290/391] Batch: 0.0485 (0.0498) Data: 0.0177 (0.0204) Loss: 0.0309 (0.0112)\n",
            "TRAIN(191): [300/391] Batch: 0.0544 (0.0498) Data: 0.0221 (0.0204) Loss: 0.0088 (0.0113)\n",
            "TRAIN(191): [310/391] Batch: 0.0460 (0.0498) Data: 0.0267 (0.0204) Loss: 0.0106 (0.0113)\n",
            "TRAIN(191): [320/391] Batch: 0.0454 (0.0497) Data: 0.0169 (0.0204) Loss: 0.0055 (0.0113)\n",
            "TRAIN(191): [330/391] Batch: 0.0454 (0.0496) Data: 0.0264 (0.0204) Loss: 0.0164 (0.0112)\n",
            "TRAIN(191): [340/391] Batch: 0.0526 (0.0496) Data: 0.0238 (0.0205) Loss: 0.0092 (0.0112)\n",
            "TRAIN(191): [350/391] Batch: 0.0488 (0.0495) Data: 0.0264 (0.0206) Loss: 0.0041 (0.0111)\n",
            "TRAIN(191): [360/391] Batch: 0.0471 (0.0495) Data: 0.0269 (0.0206) Loss: 0.0056 (0.0111)\n",
            "TRAIN(191): [370/391] Batch: 0.0441 (0.0495) Data: 0.0245 (0.0206) Loss: 0.0044 (0.0112)\n",
            "TRAIN(191): [380/391] Batch: 0.0463 (0.0495) Data: 0.0284 (0.0207) Loss: 0.0187 (0.0111)\n",
            "TRAIN(191): [390/391] Batch: 0.0459 (0.0495) Data: 0.0284 (0.0207) Loss: 0.0047 (0.0111)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(191)         0:00:19         0:00:08         0:00:11          0.0111\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(192): [ 10/391] Batch: 0.0420 (0.0659) Data: 0.0250 (0.0308) Loss: 0.0011 (0.0121)\n",
            "TRAIN(192): [ 20/391] Batch: 0.0457 (0.0586) Data: 0.0248 (0.0246) Loss: 0.0176 (0.0133)\n",
            "TRAIN(192): [ 30/391] Batch: 0.0485 (0.0555) Data: 0.0233 (0.0230) Loss: 0.0095 (0.0115)\n",
            "TRAIN(192): [ 40/391] Batch: 0.0466 (0.0542) Data: 0.0186 (0.0216) Loss: 0.0056 (0.0115)\n",
            "TRAIN(192): [ 50/391] Batch: 0.0590 (0.0534) Data: 0.0166 (0.0207) Loss: 0.0111 (0.0118)\n",
            "TRAIN(192): [ 60/391] Batch: 0.0363 (0.0524) Data: 0.0293 (0.0209) Loss: 0.0152 (0.0120)\n",
            "TRAIN(192): [ 70/391] Batch: 0.0479 (0.0522) Data: 0.0210 (0.0205) Loss: 0.0073 (0.0114)\n",
            "TRAIN(192): [ 80/391] Batch: 0.0562 (0.0522) Data: 0.0163 (0.0201) Loss: 0.0084 (0.0116)\n",
            "TRAIN(192): [ 90/391] Batch: 0.0532 (0.0523) Data: 0.0142 (0.0198) Loss: 0.0063 (0.0115)\n",
            "TRAIN(192): [100/391] Batch: 0.0565 (0.0523) Data: 0.0186 (0.0195) Loss: 0.0051 (0.0114)\n",
            "TRAIN(192): [110/391] Batch: 0.0570 (0.0521) Data: 0.0194 (0.0195) Loss: 0.0127 (0.0112)\n",
            "TRAIN(192): [120/391] Batch: 0.0486 (0.0520) Data: 0.0218 (0.0195) Loss: 0.0146 (0.0113)\n",
            "TRAIN(192): [130/391] Batch: 0.0468 (0.0516) Data: 0.0276 (0.0196) Loss: 0.0133 (0.0113)\n",
            "TRAIN(192): [140/391] Batch: 0.0484 (0.0514) Data: 0.0266 (0.0198) Loss: 0.0047 (0.0111)\n",
            "TRAIN(192): [150/391] Batch: 0.0468 (0.0513) Data: 0.0257 (0.0198) Loss: 0.0103 (0.0110)\n",
            "TRAIN(192): [160/391] Batch: 0.0598 (0.0511) Data: 0.0180 (0.0200) Loss: 0.0158 (0.0112)\n",
            "TRAIN(192): [170/391] Batch: 0.0493 (0.0509) Data: 0.0249 (0.0200) Loss: 0.0085 (0.0109)\n",
            "TRAIN(192): [180/391] Batch: 0.0526 (0.0509) Data: 0.0182 (0.0199) Loss: 0.0017 (0.0107)\n",
            "TRAIN(192): [190/391] Batch: 0.0530 (0.0507) Data: 0.0244 (0.0200) Loss: 0.0087 (0.0109)\n",
            "TRAIN(192): [200/391] Batch: 0.0508 (0.0506) Data: 0.0234 (0.0201) Loss: 0.0031 (0.0108)\n",
            "TRAIN(192): [210/391] Batch: 0.0386 (0.0505) Data: 0.0278 (0.0200) Loss: 0.0046 (0.0108)\n",
            "TRAIN(192): [220/391] Batch: 0.0575 (0.0505) Data: 0.0192 (0.0200) Loss: 0.0029 (0.0110)\n",
            "TRAIN(192): [230/391] Batch: 0.0521 (0.0504) Data: 0.0256 (0.0201) Loss: 0.0153 (0.0112)\n",
            "TRAIN(192): [240/391] Batch: 0.0574 (0.0504) Data: 0.0230 (0.0201) Loss: 0.0057 (0.0111)\n",
            "TRAIN(192): [250/391] Batch: 0.0468 (0.0503) Data: 0.0259 (0.0201) Loss: 0.0042 (0.0113)\n",
            "TRAIN(192): [260/391] Batch: 0.0468 (0.0503) Data: 0.0263 (0.0201) Loss: 0.0064 (0.0113)\n",
            "TRAIN(192): [270/391] Batch: 0.0561 (0.0502) Data: 0.0232 (0.0202) Loss: 0.0215 (0.0113)\n",
            "TRAIN(192): [280/391] Batch: 0.0426 (0.0502) Data: 0.0255 (0.0203) Loss: 0.0031 (0.0112)\n",
            "TRAIN(192): [290/391] Batch: 0.0452 (0.0501) Data: 0.0290 (0.0203) Loss: 0.0116 (0.0112)\n",
            "TRAIN(192): [300/391] Batch: 0.0579 (0.0502) Data: 0.0168 (0.0203) Loss: 0.0043 (0.0113)\n",
            "TRAIN(192): [310/391] Batch: 0.0516 (0.0502) Data: 0.0166 (0.0202) Loss: 0.0232 (0.0113)\n",
            "TRAIN(192): [320/391] Batch: 0.0430 (0.0501) Data: 0.0270 (0.0202) Loss: 0.0106 (0.0112)\n",
            "TRAIN(192): [330/391] Batch: 0.0703 (0.0502) Data: 0.0146 (0.0202) Loss: 0.0342 (0.0114)\n",
            "TRAIN(192): [340/391] Batch: 0.0439 (0.0502) Data: 0.0184 (0.0200) Loss: 0.0024 (0.0113)\n",
            "TRAIN(192): [350/391] Batch: 0.0578 (0.0502) Data: 0.0184 (0.0199) Loss: 0.0047 (0.0114)\n",
            "TRAIN(192): [360/391] Batch: 0.0543 (0.0503) Data: 0.0190 (0.0198) Loss: 0.0172 (0.0114)\n",
            "TRAIN(192): [370/391] Batch: 0.0418 (0.0503) Data: 0.0238 (0.0197) Loss: 0.0136 (0.0114)\n",
            "TRAIN(192): [380/391] Batch: 0.0390 (0.0503) Data: 0.0252 (0.0197) Loss: 0.0048 (0.0115)\n",
            "TRAIN(192): [390/391] Batch: 0.0486 (0.0503) Data: 0.0236 (0.0197) Loss: 0.0038 (0.0114)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(192)         0:00:19         0:00:07         0:00:12          0.0114\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(193): [ 10/391] Batch: 0.0495 (0.0668) Data: 0.0175 (0.0310) Loss: 0.0027 (0.0116)\n",
            "TRAIN(193): [ 20/391] Batch: 0.0399 (0.0585) Data: 0.0250 (0.0242) Loss: 0.0126 (0.0137)\n",
            "TRAIN(193): [ 30/391] Batch: 0.0533 (0.0558) Data: 0.0228 (0.0233) Loss: 0.0315 (0.0138)\n",
            "TRAIN(193): [ 40/391] Batch: 0.0479 (0.0542) Data: 0.0263 (0.0230) Loss: 0.0209 (0.0129)\n",
            "TRAIN(193): [ 50/391] Batch: 0.0446 (0.0538) Data: 0.0174 (0.0217) Loss: 0.0018 (0.0118)\n",
            "TRAIN(193): [ 60/391] Batch: 0.0488 (0.0532) Data: 0.0226 (0.0210) Loss: 0.0088 (0.0114)\n",
            "TRAIN(193): [ 70/391] Batch: 0.0435 (0.0527) Data: 0.0255 (0.0206) Loss: 0.0119 (0.0108)\n",
            "TRAIN(193): [ 80/391] Batch: 0.0491 (0.0522) Data: 0.0245 (0.0208) Loss: 0.0046 (0.0107)\n",
            "TRAIN(193): [ 90/391] Batch: 0.0439 (0.0518) Data: 0.0288 (0.0208) Loss: 0.0068 (0.0108)\n",
            "TRAIN(193): [100/391] Batch: 0.0599 (0.0517) Data: 0.0192 (0.0208) Loss: 0.0024 (0.0105)\n",
            "TRAIN(193): [110/391] Batch: 0.0461 (0.0512) Data: 0.0286 (0.0211) Loss: 0.0113 (0.0105)\n",
            "TRAIN(193): [120/391] Batch: 0.0471 (0.0510) Data: 0.0266 (0.0212) Loss: 0.0110 (0.0108)\n",
            "TRAIN(193): [130/391] Batch: 0.0546 (0.0510) Data: 0.0239 (0.0212) Loss: 0.0058 (0.0104)\n",
            "TRAIN(193): [140/391] Batch: 0.0536 (0.0509) Data: 0.0236 (0.0211) Loss: 0.0085 (0.0104)\n",
            "TRAIN(193): [150/391] Batch: 0.0480 (0.0506) Data: 0.0259 (0.0212) Loss: 0.0049 (0.0102)\n",
            "TRAIN(193): [160/391] Batch: 0.0580 (0.0506) Data: 0.0171 (0.0212) Loss: 0.0050 (0.0103)\n",
            "TRAIN(193): [170/391] Batch: 0.0527 (0.0504) Data: 0.0258 (0.0212) Loss: 0.0044 (0.0102)\n",
            "TRAIN(193): [180/391] Batch: 0.0428 (0.0503) Data: 0.0265 (0.0212) Loss: 0.0050 (0.0102)\n",
            "TRAIN(193): [190/391] Batch: 0.0447 (0.0503) Data: 0.0251 (0.0212) Loss: 0.0042 (0.0102)\n",
            "TRAIN(193): [200/391] Batch: 0.0518 (0.0503) Data: 0.0190 (0.0211) Loss: 0.0040 (0.0102)\n",
            "TRAIN(193): [210/391] Batch: 0.0475 (0.0503) Data: 0.0207 (0.0209) Loss: 0.0085 (0.0103)\n",
            "TRAIN(193): [220/391] Batch: 0.0525 (0.0504) Data: 0.0184 (0.0207) Loss: 0.0073 (0.0105)\n",
            "TRAIN(193): [230/391] Batch: 0.0451 (0.0504) Data: 0.0216 (0.0206) Loss: 0.0039 (0.0104)\n",
            "TRAIN(193): [240/391] Batch: 0.0446 (0.0505) Data: 0.0234 (0.0205) Loss: 0.0078 (0.0104)\n",
            "TRAIN(193): [250/391] Batch: 0.0620 (0.0506) Data: 0.0135 (0.0203) Loss: 0.0280 (0.0106)\n",
            "TRAIN(193): [260/391] Batch: 0.0507 (0.0506) Data: 0.0147 (0.0201) Loss: 0.0054 (0.0104)\n",
            "TRAIN(193): [270/391] Batch: 0.0468 (0.0506) Data: 0.0267 (0.0200) Loss: 0.0023 (0.0104)\n",
            "TRAIN(193): [280/391] Batch: 0.0459 (0.0505) Data: 0.0275 (0.0201) Loss: 0.0034 (0.0102)\n",
            "TRAIN(193): [290/391] Batch: 0.0476 (0.0504) Data: 0.0258 (0.0202) Loss: 0.0137 (0.0102)\n",
            "TRAIN(193): [300/391] Batch: 0.0554 (0.0504) Data: 0.0158 (0.0203) Loss: 0.0045 (0.0103)\n",
            "TRAIN(193): [310/391] Batch: 0.0482 (0.0503) Data: 0.0244 (0.0203) Loss: 0.0189 (0.0105)\n",
            "TRAIN(193): [320/391] Batch: 0.0456 (0.0503) Data: 0.0274 (0.0203) Loss: 0.0086 (0.0105)\n",
            "TRAIN(193): [330/391] Batch: 0.0421 (0.0503) Data: 0.0258 (0.0203) Loss: 0.0063 (0.0104)\n",
            "TRAIN(193): [340/391] Batch: 0.0465 (0.0502) Data: 0.0277 (0.0203) Loss: 0.0035 (0.0104)\n",
            "TRAIN(193): [350/391] Batch: 0.0627 (0.0503) Data: 0.0133 (0.0202) Loss: 0.0061 (0.0104)\n",
            "TRAIN(193): [360/391] Batch: 0.0510 (0.0503) Data: 0.0181 (0.0202) Loss: 0.0316 (0.0104)\n",
            "TRAIN(193): [370/391] Batch: 0.0558 (0.0503) Data: 0.0228 (0.0202) Loss: 0.0085 (0.0106)\n",
            "TRAIN(193): [380/391] Batch: 0.0462 (0.0502) Data: 0.0263 (0.0203) Loss: 0.0042 (0.0106)\n",
            "TRAIN(193): [390/391] Batch: 0.0486 (0.0502) Data: 0.0257 (0.0203) Loss: 0.0048 (0.0106)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(193)         0:00:19         0:00:07         0:00:11          0.0106\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(194): [ 10/391] Batch: 0.0432 (0.0669) Data: 0.0227 (0.0300) Loss: 0.0604 (0.0160)\n",
            "TRAIN(194): [ 20/391] Batch: 0.0545 (0.0583) Data: 0.0243 (0.0262) Loss: 0.0070 (0.0128)\n",
            "TRAIN(194): [ 30/391] Batch: 0.0457 (0.0551) Data: 0.0275 (0.0249) Loss: 0.0139 (0.0121)\n",
            "TRAIN(194): [ 40/391] Batch: 0.0520 (0.0535) Data: 0.0230 (0.0241) Loss: 0.0087 (0.0120)\n",
            "TRAIN(194): [ 50/391] Batch: 0.0379 (0.0526) Data: 0.0254 (0.0231) Loss: 0.0118 (0.0110)\n",
            "TRAIN(194): [ 60/391] Batch: 0.0602 (0.0527) Data: 0.0147 (0.0222) Loss: 0.0041 (0.0101)\n",
            "TRAIN(194): [ 70/391] Batch: 0.0551 (0.0524) Data: 0.0157 (0.0214) Loss: 0.0120 (0.0097)\n",
            "TRAIN(194): [ 80/391] Batch: 0.0439 (0.0523) Data: 0.0194 (0.0205) Loss: 0.0128 (0.0097)\n",
            "TRAIN(194): [ 90/391] Batch: 0.0589 (0.0519) Data: 0.0182 (0.0203) Loss: 0.0033 (0.0100)\n",
            "TRAIN(194): [100/391] Batch: 0.0475 (0.0518) Data: 0.0155 (0.0198) Loss: 0.0156 (0.0098)\n",
            "TRAIN(194): [110/391] Batch: 0.0486 (0.0516) Data: 0.0204 (0.0195) Loss: 0.0061 (0.0096)\n",
            "TRAIN(194): [120/391] Batch: 0.0550 (0.0516) Data: 0.0159 (0.0194) Loss: 0.0158 (0.0096)\n",
            "TRAIN(194): [130/391] Batch: 0.0528 (0.0515) Data: 0.0188 (0.0193) Loss: 0.0109 (0.0097)\n",
            "TRAIN(194): [140/391] Batch: 0.0570 (0.0516) Data: 0.0175 (0.0191) Loss: 0.0099 (0.0100)\n",
            "TRAIN(194): [150/391] Batch: 0.0469 (0.0514) Data: 0.0261 (0.0191) Loss: 0.0034 (0.0097)\n",
            "TRAIN(194): [160/391] Batch: 0.0564 (0.0514) Data: 0.0205 (0.0192) Loss: 0.0096 (0.0096)\n",
            "TRAIN(194): [170/391] Batch: 0.0508 (0.0513) Data: 0.0161 (0.0192) Loss: 0.0110 (0.0096)\n",
            "TRAIN(194): [180/391] Batch: 0.0463 (0.0513) Data: 0.0239 (0.0191) Loss: 0.0084 (0.0099)\n",
            "TRAIN(194): [190/391] Batch: 0.0464 (0.0513) Data: 0.0231 (0.0191) Loss: 0.0264 (0.0099)\n",
            "TRAIN(194): [200/391] Batch: 0.0581 (0.0513) Data: 0.0134 (0.0190) Loss: 0.0068 (0.0102)\n",
            "TRAIN(194): [210/391] Batch: 0.0375 (0.0513) Data: 0.0230 (0.0190) Loss: 0.0155 (0.0104)\n",
            "TRAIN(194): [220/391] Batch: 0.0550 (0.0513) Data: 0.0160 (0.0189) Loss: 0.0031 (0.0104)\n",
            "TRAIN(194): [230/391] Batch: 0.0462 (0.0511) Data: 0.0249 (0.0189) Loss: 0.0085 (0.0104)\n",
            "TRAIN(194): [240/391] Batch: 0.0556 (0.0512) Data: 0.0125 (0.0188) Loss: 0.0084 (0.0105)\n",
            "TRAIN(194): [250/391] Batch: 0.0445 (0.0511) Data: 0.0232 (0.0188) Loss: 0.0093 (0.0104)\n",
            "TRAIN(194): [260/391] Batch: 0.0468 (0.0512) Data: 0.0219 (0.0188) Loss: 0.0285 (0.0104)\n",
            "TRAIN(194): [270/391] Batch: 0.0464 (0.0511) Data: 0.0253 (0.0188) Loss: 0.0253 (0.0105)\n",
            "TRAIN(194): [280/391] Batch: 0.0578 (0.0511) Data: 0.0202 (0.0188) Loss: 0.0048 (0.0104)\n",
            "TRAIN(194): [290/391] Batch: 0.0508 (0.0511) Data: 0.0226 (0.0188) Loss: 0.0076 (0.0106)\n",
            "TRAIN(194): [300/391] Batch: 0.0483 (0.0511) Data: 0.0238 (0.0188) Loss: 0.0028 (0.0106)\n",
            "TRAIN(194): [310/391] Batch: 0.0357 (0.0511) Data: 0.0237 (0.0187) Loss: 0.0130 (0.0106)\n",
            "TRAIN(194): [320/391] Batch: 0.0515 (0.0511) Data: 0.0150 (0.0188) Loss: 0.0260 (0.0106)\n",
            "TRAIN(194): [330/391] Batch: 0.0426 (0.0510) Data: 0.0181 (0.0188) Loss: 0.0061 (0.0106)\n",
            "TRAIN(194): [340/391] Batch: 0.0457 (0.0511) Data: 0.0172 (0.0187) Loss: 0.0070 (0.0107)\n",
            "TRAIN(194): [350/391] Batch: 0.0665 (0.0512) Data: 0.0099 (0.0186) Loss: 0.0036 (0.0106)\n",
            "TRAIN(194): [360/391] Batch: 0.0656 (0.0513) Data: 0.0101 (0.0185) Loss: 0.0030 (0.0106)\n",
            "TRAIN(194): [370/391] Batch: 0.0525 (0.0514) Data: 0.0163 (0.0184) Loss: 0.0128 (0.0107)\n",
            "TRAIN(194): [380/391] Batch: 0.0483 (0.0514) Data: 0.0178 (0.0183) Loss: 0.0087 (0.0106)\n",
            "TRAIN(194): [390/391] Batch: 0.0469 (0.0514) Data: 0.0207 (0.0183) Loss: 0.0043 (0.0106)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(194)         0:00:20         0:00:07         0:00:12          0.0106\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(195): [ 10/391] Batch: 0.0488 (0.0734) Data: 0.0129 (0.0286) Loss: 0.0104 (0.0083)\n",
            "TRAIN(195): [ 20/391] Batch: 0.0458 (0.0610) Data: 0.0239 (0.0244) Loss: 0.0094 (0.0098)\n",
            "TRAIN(195): [ 30/391] Batch: 0.0473 (0.0566) Data: 0.0238 (0.0235) Loss: 0.0173 (0.0104)\n",
            "TRAIN(195): [ 40/391] Batch: 0.0470 (0.0544) Data: 0.0241 (0.0230) Loss: 0.0057 (0.0099)\n",
            "TRAIN(195): [ 50/391] Batch: 0.0472 (0.0529) Data: 0.0238 (0.0227) Loss: 0.0057 (0.0101)\n",
            "TRAIN(195): [ 60/391] Batch: 0.0484 (0.0521) Data: 0.0236 (0.0226) Loss: 0.0051 (0.0105)\n",
            "TRAIN(195): [ 70/391] Batch: 0.0459 (0.0514) Data: 0.0252 (0.0224) Loss: 0.0100 (0.0104)\n",
            "TRAIN(195): [ 80/391] Batch: 0.0482 (0.0512) Data: 0.0238 (0.0222) Loss: 0.0214 (0.0106)\n",
            "TRAIN(195): [ 90/391] Batch: 0.0453 (0.0508) Data: 0.0253 (0.0222) Loss: 0.0142 (0.0104)\n",
            "TRAIN(195): [100/391] Batch: 0.0464 (0.0509) Data: 0.0208 (0.0216) Loss: 0.0068 (0.0103)\n",
            "TRAIN(195): [110/391] Batch: 0.0474 (0.0511) Data: 0.0207 (0.0210) Loss: 0.0043 (0.0105)\n",
            "TRAIN(195): [120/391] Batch: 0.0658 (0.0512) Data: 0.0134 (0.0208) Loss: 0.0090 (0.0101)\n",
            "TRAIN(195): [130/391] Batch: 0.0468 (0.0509) Data: 0.0250 (0.0208) Loss: 0.0063 (0.0103)\n",
            "TRAIN(195): [140/391] Batch: 0.0463 (0.0507) Data: 0.0245 (0.0208) Loss: 0.0108 (0.0109)\n",
            "TRAIN(195): [150/391] Batch: 0.0661 (0.0507) Data: 0.0129 (0.0206) Loss: 0.0109 (0.0106)\n",
            "TRAIN(195): [160/391] Batch: 0.0464 (0.0507) Data: 0.0156 (0.0205) Loss: 0.0080 (0.0109)\n",
            "TRAIN(195): [170/391] Batch: 0.0503 (0.0508) Data: 0.0151 (0.0201) Loss: 0.0063 (0.0111)\n",
            "TRAIN(195): [180/391] Batch: 0.0385 (0.0507) Data: 0.0230 (0.0200) Loss: 0.0054 (0.0110)\n",
            "TRAIN(195): [190/391] Batch: 0.0472 (0.0505) Data: 0.0241 (0.0201) Loss: 0.0112 (0.0112)\n",
            "TRAIN(195): [200/391] Batch: 0.0514 (0.0504) Data: 0.0154 (0.0201) Loss: 0.0260 (0.0111)\n",
            "TRAIN(195): [210/391] Batch: 0.0576 (0.0505) Data: 0.0200 (0.0199) Loss: 0.0038 (0.0112)\n",
            "TRAIN(195): [220/391] Batch: 0.0498 (0.0507) Data: 0.0126 (0.0196) Loss: 0.0096 (0.0113)\n",
            "TRAIN(195): [230/391] Batch: 0.0469 (0.0507) Data: 0.0182 (0.0194) Loss: 0.0065 (0.0112)\n",
            "TRAIN(195): [240/391] Batch: 0.0478 (0.0508) Data: 0.0187 (0.0192) Loss: 0.0039 (0.0113)\n",
            "TRAIN(195): [250/391] Batch: 0.0704 (0.0510) Data: 0.0094 (0.0190) Loss: 0.0055 (0.0113)\n",
            "TRAIN(195): [260/391] Batch: 0.0507 (0.0510) Data: 0.0144 (0.0186) Loss: 0.0105 (0.0118)\n",
            "TRAIN(195): [270/391] Batch: 0.0572 (0.0511) Data: 0.0109 (0.0184) Loss: 0.0543 (0.0120)\n",
            "TRAIN(195): [280/391] Batch: 0.0576 (0.0511) Data: 0.0211 (0.0184) Loss: 0.0133 (0.0120)\n",
            "TRAIN(195): [290/391] Batch: 0.0479 (0.0510) Data: 0.0247 (0.0185) Loss: 0.0030 (0.0117)\n",
            "TRAIN(195): [300/391] Batch: 0.0499 (0.0510) Data: 0.0254 (0.0185) Loss: 0.0037 (0.0119)\n",
            "TRAIN(195): [310/391] Batch: 0.0531 (0.0510) Data: 0.0220 (0.0185) Loss: 0.0079 (0.0119)\n",
            "TRAIN(195): [320/391] Batch: 0.0408 (0.0509) Data: 0.0249 (0.0185) Loss: 0.0047 (0.0119)\n",
            "TRAIN(195): [330/391] Batch: 0.0541 (0.0509) Data: 0.0232 (0.0186) Loss: 0.0080 (0.0118)\n",
            "TRAIN(195): [340/391] Batch: 0.0592 (0.0509) Data: 0.0161 (0.0186) Loss: 0.0511 (0.0119)\n",
            "TRAIN(195): [350/391] Batch: 0.0466 (0.0509) Data: 0.0235 (0.0185) Loss: 0.0124 (0.0119)\n",
            "TRAIN(195): [360/391] Batch: 0.0400 (0.0509) Data: 0.0226 (0.0185) Loss: 0.0091 (0.0120)\n",
            "TRAIN(195): [370/391] Batch: 0.0555 (0.0509) Data: 0.0160 (0.0185) Loss: 0.0048 (0.0119)\n",
            "TRAIN(195): [380/391] Batch: 0.0595 (0.0509) Data: 0.0148 (0.0184) Loss: 0.0188 (0.0120)\n",
            "TRAIN(195): [390/391] Batch: 0.0475 (0.0508) Data: 0.0270 (0.0185) Loss: 0.0026 (0.0119)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(195)         0:00:19         0:00:07         0:00:12          0.0119\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(196): [ 10/391] Batch: 0.0493 (0.0689) Data: 0.0138 (0.0300) Loss: 0.0057 (0.0093)\n",
            "TRAIN(196): [ 20/391] Batch: 0.0528 (0.0604) Data: 0.0237 (0.0237) Loss: 0.0026 (0.0090)\n",
            "TRAIN(196): [ 30/391] Batch: 0.0553 (0.0574) Data: 0.0229 (0.0220) Loss: 0.0076 (0.0103)\n",
            "TRAIN(196): [ 40/391] Batch: 0.0433 (0.0559) Data: 0.0259 (0.0209) Loss: 0.0178 (0.0110)\n",
            "TRAIN(196): [ 50/391] Batch: 0.0533 (0.0550) Data: 0.0227 (0.0205) Loss: 0.0037 (0.0103)\n",
            "TRAIN(196): [ 60/391] Batch: 0.0341 (0.0540) Data: 0.0257 (0.0204) Loss: 0.0036 (0.0106)\n",
            "TRAIN(196): [ 70/391] Batch: 0.0507 (0.0537) Data: 0.0159 (0.0201) Loss: 0.0038 (0.0106)\n",
            "TRAIN(196): [ 80/391] Batch: 0.0559 (0.0532) Data: 0.0135 (0.0200) Loss: 0.0147 (0.0103)\n",
            "TRAIN(196): [ 90/391] Batch: 0.0545 (0.0531) Data: 0.0146 (0.0191) Loss: 0.0041 (0.0108)\n",
            "TRAIN(196): [100/391] Batch: 0.0446 (0.0530) Data: 0.0217 (0.0186) Loss: 0.0143 (0.0111)\n",
            "TRAIN(196): [110/391] Batch: 0.0530 (0.0529) Data: 0.0150 (0.0183) Loss: 0.0108 (0.0109)\n",
            "TRAIN(196): [120/391] Batch: 0.0569 (0.0527) Data: 0.0144 (0.0181) Loss: 0.0140 (0.0108)\n",
            "TRAIN(196): [130/391] Batch: 0.0629 (0.0528) Data: 0.0137 (0.0179) Loss: 0.0213 (0.0108)\n",
            "TRAIN(196): [140/391] Batch: 0.0486 (0.0528) Data: 0.0163 (0.0178) Loss: 0.0172 (0.0107)\n",
            "TRAIN(196): [150/391] Batch: 0.0455 (0.0527) Data: 0.0245 (0.0178) Loss: 0.0060 (0.0104)\n",
            "TRAIN(196): [160/391] Batch: 0.0464 (0.0523) Data: 0.0270 (0.0179) Loss: 0.0069 (0.0104)\n",
            "TRAIN(196): [170/391] Batch: 0.0573 (0.0522) Data: 0.0229 (0.0180) Loss: 0.0494 (0.0107)\n",
            "TRAIN(196): [180/391] Batch: 0.0465 (0.0520) Data: 0.0270 (0.0183) Loss: 0.0099 (0.0107)\n",
            "TRAIN(196): [190/391] Batch: 0.0518 (0.0520) Data: 0.0145 (0.0182) Loss: 0.0258 (0.0109)\n",
            "TRAIN(196): [200/391] Batch: 0.0414 (0.0518) Data: 0.0194 (0.0183) Loss: 0.0127 (0.0109)\n",
            "TRAIN(196): [210/391] Batch: 0.0451 (0.0516) Data: 0.0263 (0.0185) Loss: 0.0041 (0.0109)\n",
            "TRAIN(196): [220/391] Batch: 0.0419 (0.0516) Data: 0.0249 (0.0184) Loss: 0.0139 (0.0109)\n",
            "TRAIN(196): [230/391] Batch: 0.0483 (0.0516) Data: 0.0143 (0.0183) Loss: 0.0137 (0.0107)\n",
            "TRAIN(196): [240/391] Batch: 0.0460 (0.0514) Data: 0.0267 (0.0185) Loss: 0.0038 (0.0107)\n",
            "TRAIN(196): [250/391] Batch: 0.0463 (0.0513) Data: 0.0256 (0.0185) Loss: 0.0055 (0.0108)\n",
            "TRAIN(196): [260/391] Batch: 0.0506 (0.0513) Data: 0.0152 (0.0185) Loss: 0.0117 (0.0107)\n",
            "TRAIN(196): [270/391] Batch: 0.0432 (0.0513) Data: 0.0165 (0.0184) Loss: 0.0205 (0.0107)\n",
            "TRAIN(196): [280/391] Batch: 0.0452 (0.0513) Data: 0.0218 (0.0184) Loss: 0.0104 (0.0107)\n",
            "TRAIN(196): [290/391] Batch: 0.0393 (0.0513) Data: 0.0245 (0.0184) Loss: 0.0125 (0.0107)\n",
            "TRAIN(196): [300/391] Batch: 0.0512 (0.0513) Data: 0.0151 (0.0184) Loss: 0.0103 (0.0106)\n",
            "TRAIN(196): [310/391] Batch: 0.0526 (0.0513) Data: 0.0152 (0.0183) Loss: 0.0386 (0.0106)\n",
            "TRAIN(196): [320/391] Batch: 0.0549 (0.0513) Data: 0.0218 (0.0183) Loss: 0.0042 (0.0106)\n",
            "TRAIN(196): [330/391] Batch: 0.0536 (0.0513) Data: 0.0231 (0.0183) Loss: 0.0086 (0.0106)\n",
            "TRAIN(196): [340/391] Batch: 0.0553 (0.0513) Data: 0.0155 (0.0183) Loss: 0.0130 (0.0107)\n",
            "TRAIN(196): [350/391] Batch: 0.0510 (0.0514) Data: 0.0190 (0.0183) Loss: 0.0099 (0.0106)\n",
            "TRAIN(196): [360/391] Batch: 0.0609 (0.0514) Data: 0.0143 (0.0182) Loss: 0.0078 (0.0106)\n",
            "TRAIN(196): [370/391] Batch: 0.0485 (0.0514) Data: 0.0194 (0.0182) Loss: 0.0043 (0.0106)\n",
            "TRAIN(196): [380/391] Batch: 0.0586 (0.0514) Data: 0.0125 (0.0181) Loss: 0.0143 (0.0106)\n",
            "TRAIN(196): [390/391] Batch: 0.0448 (0.0513) Data: 0.0242 (0.0181) Loss: 0.0111 (0.0105)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(196)         0:00:20         0:00:07         0:00:12          0.0105\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(197): [ 10/391] Batch: 0.0468 (0.0686) Data: 0.0203 (0.0343) Loss: 0.0144 (0.0145)\n",
            "TRAIN(197): [ 20/391] Batch: 0.0539 (0.0614) Data: 0.0209 (0.0251) Loss: 0.0253 (0.0122)\n",
            "TRAIN(197): [ 30/391] Batch: 0.0492 (0.0579) Data: 0.0230 (0.0236) Loss: 0.0077 (0.0118)\n",
            "TRAIN(197): [ 40/391] Batch: 0.0540 (0.0561) Data: 0.0180 (0.0224) Loss: 0.0018 (0.0107)\n",
            "TRAIN(197): [ 50/391] Batch: 0.0452 (0.0548) Data: 0.0218 (0.0218) Loss: 0.0059 (0.0105)\n",
            "TRAIN(197): [ 60/391] Batch: 0.0454 (0.0539) Data: 0.0253 (0.0217) Loss: 0.0446 (0.0116)\n",
            "TRAIN(197): [ 70/391] Batch: 0.0577 (0.0536) Data: 0.0139 (0.0213) Loss: 0.0147 (0.0111)\n",
            "TRAIN(197): [ 80/391] Batch: 0.0460 (0.0526) Data: 0.0266 (0.0215) Loss: 0.0283 (0.0113)\n",
            "TRAIN(197): [ 90/391] Batch: 0.0568 (0.0526) Data: 0.0164 (0.0209) Loss: 0.0134 (0.0108)\n",
            "TRAIN(197): [100/391] Batch: 0.0459 (0.0520) Data: 0.0277 (0.0211) Loss: 0.0076 (0.0114)\n",
            "TRAIN(197): [110/391] Batch: 0.0458 (0.0516) Data: 0.0254 (0.0213) Loss: 0.0041 (0.0114)\n",
            "TRAIN(197): [120/391] Batch: 0.0454 (0.0513) Data: 0.0244 (0.0212) Loss: 0.0068 (0.0113)\n",
            "TRAIN(197): [130/391] Batch: 0.0579 (0.0512) Data: 0.0144 (0.0210) Loss: 0.0050 (0.0116)\n",
            "TRAIN(197): [140/391] Batch: 0.0463 (0.0510) Data: 0.0268 (0.0211) Loss: 0.0042 (0.0117)\n",
            "TRAIN(197): [150/391] Batch: 0.0492 (0.0509) Data: 0.0177 (0.0210) Loss: 0.0030 (0.0118)\n",
            "TRAIN(197): [160/391] Batch: 0.0407 (0.0507) Data: 0.0230 (0.0210) Loss: 0.0105 (0.0117)\n",
            "TRAIN(197): [170/391] Batch: 0.0476 (0.0505) Data: 0.0256 (0.0212) Loss: 0.0052 (0.0115)\n",
            "TRAIN(197): [180/391] Batch: 0.0471 (0.0503) Data: 0.0244 (0.0212) Loss: 0.0163 (0.0114)\n",
            "TRAIN(197): [190/391] Batch: 0.0451 (0.0502) Data: 0.0249 (0.0212) Loss: 0.0103 (0.0112)\n",
            "TRAIN(197): [200/391] Batch: 0.0457 (0.0501) Data: 0.0249 (0.0212) Loss: 0.0030 (0.0115)\n",
            "TRAIN(197): [210/391] Batch: 0.0472 (0.0500) Data: 0.0253 (0.0213) Loss: 0.0070 (0.0116)\n",
            "TRAIN(197): [220/391] Batch: 0.0483 (0.0499) Data: 0.0200 (0.0213) Loss: 0.0066 (0.0116)\n",
            "TRAIN(197): [230/391] Batch: 0.0464 (0.0500) Data: 0.0147 (0.0210) Loss: 0.0020 (0.0114)\n",
            "TRAIN(197): [240/391] Batch: 0.0649 (0.0501) Data: 0.0103 (0.0207) Loss: 0.0039 (0.0115)\n",
            "TRAIN(197): [250/391] Batch: 0.0481 (0.0501) Data: 0.0185 (0.0206) Loss: 0.0161 (0.0114)\n",
            "TRAIN(197): [260/391] Batch: 0.0478 (0.0503) Data: 0.0200 (0.0203) Loss: 0.0023 (0.0116)\n",
            "TRAIN(197): [270/391] Batch: 0.0530 (0.0503) Data: 0.0190 (0.0202) Loss: 0.0053 (0.0115)\n",
            "TRAIN(197): [280/391] Batch: 0.0535 (0.0504) Data: 0.0181 (0.0201) Loss: 0.0083 (0.0116)\n",
            "TRAIN(197): [290/391] Batch: 0.0473 (0.0504) Data: 0.0256 (0.0200) Loss: 0.0092 (0.0115)\n",
            "TRAIN(197): [300/391] Batch: 0.0519 (0.0503) Data: 0.0200 (0.0200) Loss: 0.0186 (0.0116)\n",
            "TRAIN(197): [310/391] Batch: 0.0446 (0.0502) Data: 0.0265 (0.0200) Loss: 0.0035 (0.0115)\n",
            "TRAIN(197): [320/391] Batch: 0.0469 (0.0503) Data: 0.0206 (0.0200) Loss: 0.0037 (0.0115)\n",
            "TRAIN(197): [330/391] Batch: 0.0466 (0.0502) Data: 0.0266 (0.0199) Loss: 0.0090 (0.0114)\n",
            "TRAIN(197): [340/391] Batch: 0.0463 (0.0501) Data: 0.0265 (0.0200) Loss: 0.0080 (0.0113)\n",
            "TRAIN(197): [350/391] Batch: 0.0584 (0.0502) Data: 0.0130 (0.0200) Loss: 0.0097 (0.0111)\n",
            "TRAIN(197): [360/391] Batch: 0.0463 (0.0502) Data: 0.0224 (0.0199) Loss: 0.0057 (0.0113)\n",
            "TRAIN(197): [370/391] Batch: 0.0537 (0.0502) Data: 0.0230 (0.0199) Loss: 0.0071 (0.0113)\n",
            "TRAIN(197): [380/391] Batch: 0.0447 (0.0501) Data: 0.0253 (0.0199) Loss: 0.0107 (0.0113)\n",
            "TRAIN(197): [390/391] Batch: 0.0477 (0.0501) Data: 0.0270 (0.0199) Loss: 0.0209 (0.0114)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(197)         0:00:19         0:00:07         0:00:11          0.0114\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(198): [ 10/391] Batch: 0.0568 (0.0681) Data: 0.0217 (0.0307) Loss: 0.0081 (0.0158)\n",
            "TRAIN(198): [ 20/391] Batch: 0.0364 (0.0593) Data: 0.0247 (0.0241) Loss: 0.0091 (0.0128)\n",
            "TRAIN(198): [ 30/391] Batch: 0.0508 (0.0570) Data: 0.0141 (0.0215) Loss: 0.0234 (0.0129)\n",
            "TRAIN(198): [ 40/391] Batch: 0.0461 (0.0545) Data: 0.0260 (0.0216) Loss: 0.0224 (0.0118)\n",
            "TRAIN(198): [ 50/391] Batch: 0.0472 (0.0534) Data: 0.0253 (0.0217) Loss: 0.0044 (0.0113)\n",
            "TRAIN(198): [ 60/391] Batch: 0.0491 (0.0527) Data: 0.0236 (0.0212) Loss: 0.0145 (0.0112)\n",
            "TRAIN(198): [ 70/391] Batch: 0.0499 (0.0525) Data: 0.0166 (0.0210) Loss: 0.0234 (0.0111)\n",
            "TRAIN(198): [ 80/391] Batch: 0.0580 (0.0520) Data: 0.0153 (0.0207) Loss: 0.0046 (0.0108)\n",
            "TRAIN(198): [ 90/391] Batch: 0.0454 (0.0517) Data: 0.0263 (0.0207) Loss: 0.0248 (0.0111)\n",
            "TRAIN(198): [100/391] Batch: 0.0488 (0.0516) Data: 0.0156 (0.0201) Loss: 0.0028 (0.0108)\n",
            "TRAIN(198): [110/391] Batch: 0.0623 (0.0518) Data: 0.0121 (0.0197) Loss: 0.0087 (0.0107)\n",
            "TRAIN(198): [120/391] Batch: 0.0513 (0.0519) Data: 0.0134 (0.0193) Loss: 0.0228 (0.0106)\n",
            "TRAIN(198): [130/391] Batch: 0.0444 (0.0519) Data: 0.0218 (0.0190) Loss: 0.0032 (0.0105)\n",
            "TRAIN(198): [140/391] Batch: 0.0443 (0.0520) Data: 0.0219 (0.0187) Loss: 0.0081 (0.0103)\n",
            "TRAIN(198): [150/391] Batch: 0.0575 (0.0519) Data: 0.0126 (0.0185) Loss: 0.0118 (0.0104)\n",
            "TRAIN(198): [160/391] Batch: 0.0567 (0.0520) Data: 0.0151 (0.0182) Loss: 0.0145 (0.0104)\n",
            "TRAIN(198): [170/391] Batch: 0.0559 (0.0520) Data: 0.0150 (0.0181) Loss: 0.0107 (0.0104)\n",
            "TRAIN(198): [180/391] Batch: 0.0467 (0.0516) Data: 0.0277 (0.0183) Loss: 0.0140 (0.0104)\n",
            "TRAIN(198): [190/391] Batch: 0.0567 (0.0516) Data: 0.0191 (0.0184) Loss: 0.0082 (0.0104)\n",
            "TRAIN(198): [200/391] Batch: 0.0435 (0.0515) Data: 0.0195 (0.0183) Loss: 0.0064 (0.0105)\n",
            "TRAIN(198): [210/391] Batch: 0.0382 (0.0514) Data: 0.0278 (0.0183) Loss: 0.0106 (0.0105)\n",
            "TRAIN(198): [220/391] Batch: 0.0508 (0.0513) Data: 0.0240 (0.0185) Loss: 0.0070 (0.0105)\n",
            "TRAIN(198): [230/391] Batch: 0.0505 (0.0513) Data: 0.0160 (0.0185) Loss: 0.0079 (0.0105)\n",
            "TRAIN(198): [240/391] Batch: 0.0499 (0.0513) Data: 0.0236 (0.0186) Loss: 0.0060 (0.0105)\n",
            "TRAIN(198): [250/391] Batch: 0.0385 (0.0512) Data: 0.0220 (0.0186) Loss: 0.0355 (0.0108)\n",
            "TRAIN(198): [260/391] Batch: 0.0465 (0.0510) Data: 0.0262 (0.0187) Loss: 0.0087 (0.0109)\n",
            "TRAIN(198): [270/391] Batch: 0.0534 (0.0510) Data: 0.0146 (0.0187) Loss: 0.0181 (0.0108)\n",
            "TRAIN(198): [280/391] Batch: 0.0454 (0.0509) Data: 0.0272 (0.0187) Loss: 0.0084 (0.0108)\n",
            "TRAIN(198): [290/391] Batch: 0.0441 (0.0509) Data: 0.0264 (0.0188) Loss: 0.0045 (0.0108)\n",
            "TRAIN(198): [300/391] Batch: 0.0465 (0.0508) Data: 0.0271 (0.0188) Loss: 0.0022 (0.0107)\n",
            "TRAIN(198): [310/391] Batch: 0.0482 (0.0508) Data: 0.0256 (0.0189) Loss: 0.0130 (0.0108)\n",
            "TRAIN(198): [320/391] Batch: 0.0562 (0.0508) Data: 0.0159 (0.0189) Loss: 0.0163 (0.0107)\n",
            "TRAIN(198): [330/391] Batch: 0.0489 (0.0507) Data: 0.0220 (0.0189) Loss: 0.0194 (0.0107)\n",
            "TRAIN(198): [340/391] Batch: 0.0457 (0.0507) Data: 0.0265 (0.0189) Loss: 0.0188 (0.0107)\n",
            "TRAIN(198): [350/391] Batch: 0.0346 (0.0506) Data: 0.0250 (0.0189) Loss: 0.0060 (0.0106)\n",
            "TRAIN(198): [360/391] Batch: 0.0471 (0.0505) Data: 0.0249 (0.0190) Loss: 0.0084 (0.0106)\n",
            "TRAIN(198): [370/391] Batch: 0.0436 (0.0506) Data: 0.0183 (0.0189) Loss: 0.0075 (0.0106)\n",
            "TRAIN(198): [380/391] Batch: 0.0451 (0.0506) Data: 0.0194 (0.0188) Loss: 0.0043 (0.0107)\n",
            "TRAIN(198): [390/391] Batch: 0.0538 (0.0506) Data: 0.0206 (0.0188) Loss: 0.0249 (0.0106)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(198)         0:00:19         0:00:07         0:00:12          0.0106\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(199): [ 10/391] Batch: 0.0484 (0.0705) Data: 0.0160 (0.0312) Loss: 0.0128 (0.0105)\n",
            "TRAIN(199): [ 20/391] Batch: 0.0464 (0.0623) Data: 0.0172 (0.0227) Loss: 0.0157 (0.0121)\n",
            "TRAIN(199): [ 30/391] Batch: 0.0546 (0.0588) Data: 0.0232 (0.0202) Loss: 0.0035 (0.0106)\n",
            "TRAIN(199): [ 40/391] Batch: 0.0520 (0.0566) Data: 0.0148 (0.0197) Loss: 0.0044 (0.0100)\n",
            "TRAIN(199): [ 50/391] Batch: 0.0465 (0.0553) Data: 0.0216 (0.0194) Loss: 0.0291 (0.0108)\n",
            "TRAIN(199): [ 60/391] Batch: 0.0476 (0.0546) Data: 0.0202 (0.0192) Loss: 0.0046 (0.0110)\n",
            "TRAIN(199): [ 70/391] Batch: 0.0517 (0.0541) Data: 0.0225 (0.0194) Loss: 0.0251 (0.0108)\n",
            "TRAIN(199): [ 80/391] Batch: 0.0501 (0.0538) Data: 0.0153 (0.0194) Loss: 0.0161 (0.0106)\n",
            "TRAIN(199): [ 90/391] Batch: 0.0455 (0.0532) Data: 0.0240 (0.0195) Loss: 0.0146 (0.0111)\n",
            "TRAIN(199): [100/391] Batch: 0.0490 (0.0530) Data: 0.0247 (0.0195) Loss: 0.0241 (0.0115)\n",
            "TRAIN(199): [110/391] Batch: 0.0460 (0.0525) Data: 0.0249 (0.0197) Loss: 0.0091 (0.0112)\n",
            "TRAIN(199): [120/391] Batch: 0.0387 (0.0521) Data: 0.0247 (0.0196) Loss: 0.0044 (0.0107)\n",
            "TRAIN(199): [130/391] Batch: 0.0478 (0.0518) Data: 0.0257 (0.0197) Loss: 0.0044 (0.0104)\n",
            "TRAIN(199): [140/391] Batch: 0.0532 (0.0518) Data: 0.0140 (0.0197) Loss: 0.0247 (0.0107)\n",
            "TRAIN(199): [150/391] Batch: 0.0530 (0.0517) Data: 0.0175 (0.0195) Loss: 0.0098 (0.0109)\n",
            "TRAIN(199): [160/391] Batch: 0.0531 (0.0517) Data: 0.0236 (0.0196) Loss: 0.0177 (0.0108)\n",
            "TRAIN(199): [170/391] Batch: 0.0560 (0.0517) Data: 0.0134 (0.0193) Loss: 0.0017 (0.0106)\n",
            "TRAIN(199): [180/391] Batch: 0.0525 (0.0516) Data: 0.0226 (0.0193) Loss: 0.0061 (0.0104)\n",
            "TRAIN(199): [190/391] Batch: 0.0440 (0.0517) Data: 0.0198 (0.0191) Loss: 0.0042 (0.0103)\n",
            "TRAIN(199): [200/391] Batch: 0.0493 (0.0514) Data: 0.0249 (0.0192) Loss: 0.0095 (0.0105)\n",
            "TRAIN(199): [210/391] Batch: 0.0547 (0.0514) Data: 0.0135 (0.0192) Loss: 0.0058 (0.0105)\n",
            "TRAIN(199): [220/391] Batch: 0.0552 (0.0514) Data: 0.0216 (0.0192) Loss: 0.0025 (0.0103)\n",
            "TRAIN(199): [230/391] Batch: 0.0425 (0.0514) Data: 0.0216 (0.0190) Loss: 0.0099 (0.0103)\n",
            "TRAIN(199): [240/391] Batch: 0.0431 (0.0515) Data: 0.0236 (0.0190) Loss: 0.0031 (0.0103)\n",
            "TRAIN(199): [250/391] Batch: 0.0573 (0.0516) Data: 0.0132 (0.0188) Loss: 0.0118 (0.0103)\n",
            "TRAIN(199): [260/391] Batch: 0.0633 (0.0517) Data: 0.0117 (0.0186) Loss: 0.0050 (0.0102)\n",
            "TRAIN(199): [270/391] Batch: 0.0464 (0.0518) Data: 0.0203 (0.0185) Loss: 0.0365 (0.0103)\n",
            "TRAIN(199): [280/391] Batch: 0.0564 (0.0518) Data: 0.0178 (0.0184) Loss: 0.0035 (0.0103)\n",
            "TRAIN(199): [290/391] Batch: 0.0555 (0.0518) Data: 0.0180 (0.0183) Loss: 0.0160 (0.0103)\n",
            "TRAIN(199): [300/391] Batch: 0.0509 (0.0518) Data: 0.0200 (0.0182) Loss: 0.0071 (0.0103)\n",
            "TRAIN(199): [310/391] Batch: 0.0473 (0.0517) Data: 0.0255 (0.0183) Loss: 0.0044 (0.0104)\n",
            "TRAIN(199): [320/391] Batch: 0.0599 (0.0517) Data: 0.0171 (0.0183) Loss: 0.0041 (0.0104)\n",
            "TRAIN(199): [330/391] Batch: 0.0452 (0.0517) Data: 0.0214 (0.0182) Loss: 0.0041 (0.0103)\n",
            "TRAIN(199): [340/391] Batch: 0.0468 (0.0517) Data: 0.0156 (0.0181) Loss: 0.0126 (0.0103)\n",
            "TRAIN(199): [350/391] Batch: 0.0507 (0.0517) Data: 0.0236 (0.0182) Loss: 0.0307 (0.0104)\n",
            "TRAIN(199): [360/391] Batch: 0.0482 (0.0517) Data: 0.0243 (0.0182) Loss: 0.0172 (0.0105)\n",
            "TRAIN(199): [370/391] Batch: 0.0468 (0.0516) Data: 0.0269 (0.0183) Loss: 0.0050 (0.0104)\n",
            "TRAIN(199): [380/391] Batch: 0.0459 (0.0515) Data: 0.0268 (0.0183) Loss: 0.0078 (0.0104)\n",
            "TRAIN(199): [390/391] Batch: 0.0461 (0.0514) Data: 0.0262 (0.0184) Loss: 0.0138 (0.0104)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(199)         0:00:20         0:00:07         0:00:12          0.0104\n",
            "--------------------------------------------------------------------------------\n",
            "Finished Training & Test at 1:05:34 ....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# check submission.csv\n",
        "pd.read_csv(args.save_path / 'submission.csv', index_col=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "WyXD5h_STYBI",
        "outputId": "b5d5ec97-aeac-434e-f0cb-b37888c7c754"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id_idx  label\n",
              "0          0     68\n",
              "1          1     38\n",
              "2          2     55\n",
              "3          3     13\n",
              "4          4     71\n",
              "...      ...    ...\n",
              "9995    9995      2\n",
              "9996    9996      3\n",
              "9997    9997     97\n",
              "9998    9998     84\n",
              "9999    9999     26\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-717fd64a-fc4c-4aa7-bf8d-a51dc28b4cd3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_idx</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>84</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>26</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-717fd64a-fc4c-4aa7-bf8d-a51dc28b4cd3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-67414ca5-5170-457b-bbc7-34e90b2c21a4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67414ca5-5170-457b-bbc7-34e90b2c21a4')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-67414ca5-5170-457b-bbc7-34e90b2c21a4 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-717fd64a-fc4c-4aa7-bf8d-a51dc28b4cd3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-717fd64a-fc4c-4aa7-bf8d-a51dc28b4cd3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0HmP4ONPTZeK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}