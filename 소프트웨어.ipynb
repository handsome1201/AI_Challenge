{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/handsome1201/AI_Challenge/blob/main/%EC%86%8C%ED%94%84%ED%8A%B8%EC%9B%A8%EC%96%B4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UXgew5s6Sw7V"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "import time\n",
        "import datetime\n",
        "import easydict\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torchvision\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import SGD, AdamW\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import CIFAR100"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "2QaxRR4Tu9K0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "NISPzPEmSy_r"
      },
      "outputs": [],
      "source": [
        "args = easydict.EasyDict({\n",
        "    # device setting\n",
        "    'device': 0,\n",
        "    'seed' : 123,\n",
        "\n",
        "    # training setting\n",
        "    'batch_size' : 256,  # Increased batch size for potentially faster training\n",
        "    'num_workers' : 4,  # Increased number of workers for data loading\n",
        "    'epoch' : 250,  # Increased number of epochs for more training iterations\n",
        "    'num_cls' : 100,\n",
        "    'resample' : True,\n",
        "\n",
        "    # optimizer & criterion\n",
        "    'lr' : 0.001,  # Decreased learning rate for potentially more stable convergence\n",
        "    'momentum' : 0.9,\n",
        "    'weight_decay' : 5e-5,  # Slightly increased weight decay for better regularization\n",
        "    'nesterov' : True,\n",
        "\n",
        "    # directory\n",
        "    'data_path' : '/content/drive/MyDrive/소프트웨어/Data',\n",
        "    'save_path' : '/content/drive/MyDrive/소프트웨어/Save',\n",
        "\n",
        "    # etc\n",
        "    'print_freq' : 10,  # Increased print frequency for more frequent updates during training\n",
        "    'threshold' : 0.7,  # Adjusted threshold for binary classification based on requirements\n",
        "})\n",
        "\n",
        "def setup(args):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    if args.seed is not None:\n",
        "        random.seed(args.seed)\n",
        "        np.random.seed(args.seed)\n",
        "        torch.random.manual_seed(args.seed)\n",
        "    return device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "ExGGeqCaTG6C"
      },
      "outputs": [],
      "source": [
        "# load data_preprocessing module\n",
        "sys.path.append(args.data_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "XwfHK1N4TIq5"
      },
      "outputs": [],
      "source": [
        "def load_data(data_path, train_transform, test_transform):\n",
        "    start = time.time()\n",
        "    train_dataset = CIFAR100(root=data_path, train=True, download=True, transform=train_transform)\n",
        "    test_dataset = CIFAR100(root=data_path, train=False, download=True, transform=test_transform)\n",
        "    end = time.time()\n",
        "    sec = end - start\n",
        "    print(f\"Completed Loading dataset at {str(datetime.timedelta(seconds=sec)).split('.')[0]}\")\n",
        "    return train_dataset, test_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "7LPxBDSLTLLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26168a9-caf9-4d0b-a8da-45de9cdfeff9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Completed Loading dataset at 0:00:02\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "train_dataset, test_dataset = load_data(args.save_path, transforms.ToTensor(), transforms.ToTensor())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "qAOcw1Q3TNAR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "301ffa1e-9a8c-42db-bd37-6ba9ab6f858a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train_shape: (50000, 32, 32, 3)\n",
            "Test_shape: (10000, 32, 32, 3)\n",
            "Number of classes: 100\n"
          ]
        }
      ],
      "source": [
        "args.num_features = train_dataset.data.shape[1]\n",
        "args.num_classes = len(np.unique(train_dataset.targets))\n",
        "print(f\"Train_shape: {train_dataset.data.shape}\")\n",
        "print(f\"Test_shape: {test_dataset.data.shape}\")\n",
        "print(f\"Number of classes: {len(np.unique(train_dataset.targets))}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "5o6rmfPKTOOT"
      },
      "outputs": [],
      "source": [
        "num_balance = [np.sum(np.array(train_dataset.targets) == i) for i in np.unique(train_dataset.targets)]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize=(10,7), dpi=100)\n",
        "ax = fig.subplots()\n",
        "ax.bar(range(len(np.unique(train_dataset.targets))), num_balance, color='red')\n",
        "ax.set_title(\"Imbalanced Dataset\")\n",
        "_= ax.set_ylabel(\"Number of data\")\n",
        "_= ax.set_xlabel(\"Classes\")\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "MCen-H6fv6a9",
        "outputId": "353e8465-0b8c-4e4e-ab9d-97913252c3ea"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "Q0gR0O1hTQew"
      },
      "outputs": [],
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "\n",
        "    def __init__(self, dataset, type='train', indices=None, num_samples=None):\n",
        "\n",
        "        # if indices is not provided,\n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset.targets))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # if num_samples is not provided,\n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset\n",
        "        label_to_count = [0] * len(np.unique(dataset.targets))\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            label_to_count[label] += 1\n",
        "\n",
        "        beta = 0.9999\n",
        "        effective_num = 1.0 - np.power(beta, label_to_count)\n",
        "        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [per_cls_weights[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        return dataset.targets[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(torch.multinomial(self.weights, self.num_samples, replacement=True).tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = LambdaLayer(lambda x:\n",
        "                                        F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_s(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_s, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet32(num_classes=100):\n",
        "    return ResNet_s(BasicBlock, [5, 5, 5], num_classes=num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "EF7ilBoGTSyd"
      },
      "outputs": [],
      "source": [
        "class Metric:\n",
        "    def __init__(self, header='', fmt='{val:.4f} ({avg:.4f})'):\n",
        "        \"\"\"Base Metric Class\n",
        "        :arg\n",
        "            fmt(str): format representing metric in string\n",
        "        \"\"\"\n",
        "        self.val = 0\n",
        "        self.sum = 0\n",
        "        self.n = 0\n",
        "        self.avg = 0\n",
        "        self.header = header\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        if isinstance(val, torch.Tensor):\n",
        "            val = val.detach().clone()\n",
        "\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.n += n\n",
        "        self.avg = self.sum / self.n\n",
        "\n",
        "    def compute(self):\n",
        "        return self.avg\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.header + ' ' + self.fmt.format(**self.__dict__)\n",
        "\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    epoch = epoch + 1\n",
        "    if epoch <= 5:\n",
        "        lr = args.lr * epoch / 5\n",
        "    elif epoch > 180:\n",
        "        lr = args.lr * 0.0001\n",
        "    elif epoch > 160:\n",
        "        lr = args.lr * 0.01\n",
        "    else:\n",
        "        lr = args.lr\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "def train_one_epoch(model, train_dataloader, optimizer, criterion, epoch, args):\n",
        "    # 1. create metric\n",
        "    data_m = Metric(header='Data:')\n",
        "    batch_m = Metric(header='Batch:')\n",
        "    loss_m = Metric(header='Loss:')\n",
        "\n",
        "    # 2. start validate\n",
        "    model.train()\n",
        "\n",
        "    total_iter = len(train_dataloader)\n",
        "    start_time = time.time()\n",
        "\n",
        "    for batch_idx, (x, y) in enumerate(train_dataloader):\n",
        "        batch_size = x.size(0)\n",
        "\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "\n",
        "        data_m.update(time.time() - start_time)\n",
        "\n",
        "        y_hat = model(x)\n",
        "        loss = criterion(y_hat, y)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_m.update(loss, batch_size)\n",
        "\n",
        "        if batch_idx and args.print_freq and batch_idx % args.print_freq == 0:\n",
        "            num_digits = len(str(total_iter))\n",
        "            print(f\"TRAIN({epoch:03}): [{batch_idx:>{num_digits}}/{total_iter}] {batch_m} {data_m} {loss_m}\")\n",
        "\n",
        "        batch_m.update(time.time() - start_time)\n",
        "        start_time = time.time()\n",
        "\n",
        "    # 3. calculate metric\n",
        "    duration = str(datetime.timedelta(seconds=batch_m.sum)).split('.')[0]\n",
        "    data = str(datetime.timedelta(seconds=data_m.sum)).split('.')[0]\n",
        "    f_b_o = str(datetime.timedelta(seconds=batch_m.sum - data_m.sum)).split('.')[0]\n",
        "    loss = loss_m.compute()\n",
        "\n",
        "    # 4. print metric\n",
        "    space = 16\n",
        "    num_metric = 5\n",
        "    print('-'*space*num_metric)\n",
        "    print((\"{:>16}\"*num_metric).format('Stage', 'Batch', 'Data', 'F+B+O', 'Loss'))\n",
        "    print('-'*space*num_metric)\n",
        "    print(f\"{'TRAIN('+str(epoch)+')':>{space}}{duration:>{space}}{data:>{space}}{f_b_o:>{space}}{loss:{space}.4f}\")\n",
        "    print('-'*space*num_metric)\n",
        "\n",
        "    return loss\n",
        "\n",
        "def prediction_submission(predict, args):\n",
        "    \"\"\"\n",
        "    테스트 데이터의 idx와 예측한 prediction label을 submission.csv로 저장\n",
        "    \"\"\"\n",
        "    submission = [[idx, label] for idx, label in enumerate(predict)]\n",
        "    df = pd.DataFrame(data=submission, columns=['id_idx', 'label'], index=None)\n",
        "    args.save_path = Path(args.save_path)\n",
        "    args.save_path.mkdir(exist_ok=True)\n",
        "    df.to_csv(args.save_path / 'submission.csv', index=False)\n",
        "\n",
        "def test_submission(model, test_dataloader, args):\n",
        "    \"\"\"\n",
        "    학습한 모델로 테스트 데이터의 라벨을 예측하고 그 결과를 submission.csv로 저장\n",
        "    \"\"\"\n",
        "    model_predict = [] # for submission.csv\n",
        "    for x,y in test_dataloader:\n",
        "        x = x.to(args.device)\n",
        "        y = y.to(args.device)\n",
        "        output = model(x)\n",
        "        _, prediction = torch.max(output, 1)\n",
        "        model_predict.extend(prediction.cpu().numpy())\n",
        "    prediction_submission(model_predict, args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "0LZqTCDsTU-6"
      },
      "outputs": [],
      "source": [
        "def run(args):\n",
        "    start = time.time()\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    transform_val = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "    # 1. load train, test dataset\n",
        "    train_dataset, test_dataset = load_data(args.save_path, transform_train, transform_val)\n",
        "    train_sampler = None\n",
        "    if args.resample:\n",
        "        train_sampler = ImbalancedDatasetSampler(train_dataset)\n",
        "        per_cls_weights = None\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=(args.resample is False),\n",
        "                                  num_workers=args.num_workers, sampler=train_sampler)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=args.num_workers)\n",
        "\n",
        "    cudnn.benchmark = True\n",
        "    # 2. create model\n",
        "    model = resnet32().to(args.device)\n",
        "\n",
        "    # 3. optimizer, criterion\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, weight_decay=1e-6, momentum=0.9, nesterov=True)\n",
        "    # 각 클래스의 샘플 수를 계산합니다.\n",
        "    class_counts = np.bincount(train_dataset.targets)\n",
        "\n",
        "    # 클래스 가중치를 계산합니다. 소수 클래스에 더 큰 가중치가 부여됩니다.\n",
        "    class_weights = 1. / class_counts\n",
        "\n",
        "    # 텐서로 변환합니다.\n",
        "    weights = torch.FloatTensor(class_weights).to(args.device)\n",
        "\n",
        "    # 가중치를 손실 함수에 적용합니다.\n",
        "    criterion = nn.CrossEntropyLoss(weight=weights)\n",
        "\n",
        "#     criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 4. train & validate\n",
        "    for epoch in range(args.epoch):\n",
        "        adjust_learning_rate(optimizer, epoch, args)\n",
        "        train_loss = train_one_epoch(model, train_dataloader, optimizer, criterion, epoch, args)\n",
        "\n",
        "    test_submission(model, test_dataloader, args)\n",
        "    end = time.time()\n",
        "    sec = end - start\n",
        "    print(f\"Finished Training & Test at {str(datetime.timedelta(seconds=sec)).split('.')[0]} ....\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImbalancedDatasetSampler(torch.utils.data.sampler.Sampler):\n",
        "\n",
        "    def __init__(self, dataset, type='train', indices=None, num_samples=None):\n",
        "\n",
        "        # if indices is not provided,\n",
        "        # all elements in the dataset will be considered\n",
        "        self.indices = list(range(len(dataset.targets))) \\\n",
        "            if indices is None else indices\n",
        "\n",
        "        # if num_samples is not provided,\n",
        "        # draw `len(indices)` samples in each iteration\n",
        "        self.num_samples = len(self.indices) \\\n",
        "            if num_samples is None else num_samples\n",
        "\n",
        "        # distribution of classes in the dataset\n",
        "        label_to_count = [0] * len(np.unique(dataset.targets))\n",
        "        for idx in self.indices:\n",
        "            label = self._get_label(dataset, idx)\n",
        "            label_to_count[label] += 1\n",
        "\n",
        "        beta = 0.9999\n",
        "        effective_num = 1.0 - np.power(beta, label_to_count)\n",
        "        per_cls_weights = (1.0 - beta) / np.array(effective_num)\n",
        "\n",
        "        # weight for each sample\n",
        "        weights = [per_cls_weights[self._get_label(dataset, idx)]\n",
        "                   for idx in self.indices]\n",
        "        self.weights = torch.DoubleTensor(weights)\n",
        "\n",
        "    def _get_label(self, dataset, idx):\n",
        "        return dataset.targets[idx]\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(torch.multinomial(self.weights, self.num_samples, replacement=True).tolist())\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            self.shortcut = LambdaLayer(lambda x:\n",
        "                                        F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet_s(nn.Module):\n",
        "\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet_s, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet32(num_classes=100):\n",
        "    return ResNet_s(BasicBlock, [5, 5, 5], num_classes=num_classes)"
      ],
      "metadata": {
        "id": "AKtnENpEwKQQ"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ld6PGsznTWjs",
        "outputId": "efc713fb-0caf-476d-c4b1-7f30deab04b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "TRAIN(041): [180/196] Batch: 0.0774 (0.0789) Data: 0.0546 (0.0511) Loss: 2.7528 (2.8125)\n",
            "TRAIN(041): [190/196] Batch: 0.0759 (0.0787) Data: 0.0609 (0.0513) Loss: 2.8511 (2.8143)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(41)         0:00:15         0:00:10         0:00:05          2.8156\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(042): [ 10/196] Batch: 0.0629 (0.1191) Data: 0.0571 (0.0864) Loss: 2.7523 (2.8310)\n",
            "TRAIN(042): [ 20/196] Batch: 0.0730 (0.0982) Data: 0.0530 (0.0692) Loss: 2.7154 (2.8137)\n",
            "TRAIN(042): [ 30/196] Batch: 0.0780 (0.0911) Data: 0.0515 (0.0630) Loss: 2.8329 (2.8146)\n",
            "TRAIN(042): [ 40/196] Batch: 0.0810 (0.0872) Data: 0.0568 (0.0601) Loss: 2.6713 (2.8130)\n",
            "TRAIN(042): [ 50/196] Batch: 0.0847 (0.0852) Data: 0.0458 (0.0575) Loss: 2.7492 (2.8051)\n",
            "TRAIN(042): [ 60/196] Batch: 0.0721 (0.0837) Data: 0.0550 (0.0558) Loss: 2.8092 (2.7981)\n",
            "TRAIN(042): [ 70/196] Batch: 0.0763 (0.0829) Data: 0.0472 (0.0547) Loss: 2.8369 (2.7915)\n",
            "TRAIN(042): [ 80/196] Batch: 0.0729 (0.0821) Data: 0.0518 (0.0534) Loss: 2.6430 (2.7896)\n",
            "TRAIN(042): [ 90/196] Batch: 0.0731 (0.0815) Data: 0.0538 (0.0527) Loss: 2.8944 (2.7919)\n",
            "TRAIN(042): [100/196] Batch: 0.0742 (0.0810) Data: 0.0594 (0.0522) Loss: 2.7736 (2.7920)\n",
            "TRAIN(042): [110/196] Batch: 0.0760 (0.0805) Data: 0.0624 (0.0522) Loss: 2.8822 (2.7944)\n",
            "TRAIN(042): [120/196] Batch: 0.0689 (0.0802) Data: 0.0577 (0.0523) Loss: 2.8322 (2.7939)\n",
            "TRAIN(042): [130/196] Batch: 0.0815 (0.0799) Data: 0.0508 (0.0523) Loss: 2.7358 (2.7921)\n",
            "TRAIN(042): [140/196] Batch: 0.0775 (0.0796) Data: 0.0614 (0.0523) Loss: 2.6939 (2.7857)\n",
            "TRAIN(042): [150/196] Batch: 0.0664 (0.0794) Data: 0.0617 (0.0521) Loss: 2.7301 (2.7848)\n",
            "TRAIN(042): [160/196] Batch: 0.0754 (0.0792) Data: 0.0556 (0.0520) Loss: 2.8053 (2.7849)\n",
            "TRAIN(042): [170/196] Batch: 0.0750 (0.0790) Data: 0.0574 (0.0519) Loss: 2.6733 (2.7818)\n",
            "TRAIN(042): [180/196] Batch: 0.0761 (0.0789) Data: 0.0564 (0.0519) Loss: 2.6982 (2.7826)\n",
            "TRAIN(042): [190/196] Batch: 0.0772 (0.0787) Data: 0.0602 (0.0519) Loss: 2.7658 (2.7838)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(42)         0:00:15         0:00:10         0:00:05          2.7850\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(043): [ 10/196] Batch: 0.0703 (0.1150) Data: 0.0545 (0.0802) Loss: 2.5907 (2.7114)\n",
            "TRAIN(043): [ 20/196] Batch: 0.0691 (0.0956) Data: 0.0619 (0.0683) Loss: 2.7331 (2.7428)\n",
            "TRAIN(043): [ 30/196] Batch: 0.0762 (0.0896) Data: 0.0498 (0.0619) Loss: 2.5884 (2.7595)\n",
            "TRAIN(043): [ 40/196] Batch: 0.0958 (0.0868) Data: 0.0393 (0.0579) Loss: 2.9102 (2.7602)\n",
            "TRAIN(043): [ 50/196] Batch: 0.0777 (0.0845) Data: 0.0587 (0.0563) Loss: 2.7838 (2.7652)\n",
            "TRAIN(043): [ 60/196] Batch: 0.0769 (0.0833) Data: 0.0520 (0.0546) Loss: 2.6004 (2.7685)\n",
            "TRAIN(043): [ 70/196] Batch: 0.0770 (0.0825) Data: 0.0535 (0.0531) Loss: 2.7734 (2.7682)\n",
            "TRAIN(043): [ 80/196] Batch: 0.0670 (0.0817) Data: 0.0599 (0.0527) Loss: 2.7379 (2.7673)\n",
            "TRAIN(043): [ 90/196] Batch: 0.0777 (0.0812) Data: 0.0556 (0.0526) Loss: 2.8633 (2.7733)\n",
            "TRAIN(043): [100/196] Batch: 0.0792 (0.0807) Data: 0.0522 (0.0523) Loss: 2.8813 (2.7770)\n",
            "TRAIN(043): [110/196] Batch: 0.0772 (0.0803) Data: 0.0569 (0.0523) Loss: 2.8224 (2.7796)\n",
            "TRAIN(043): [120/196] Batch: 0.0828 (0.0799) Data: 0.0539 (0.0521) Loss: 2.8325 (2.7820)\n",
            "TRAIN(043): [130/196] Batch: 0.0838 (0.0797) Data: 0.0495 (0.0519) Loss: 2.8441 (2.7808)\n",
            "TRAIN(043): [140/196] Batch: 0.0734 (0.0794) Data: 0.0582 (0.0518) Loss: 2.9358 (2.7804)\n",
            "TRAIN(043): [150/196] Batch: 0.0767 (0.0792) Data: 0.0495 (0.0514) Loss: 2.7834 (2.7813)\n",
            "TRAIN(043): [160/196] Batch: 0.0756 (0.0790) Data: 0.0525 (0.0511) Loss: 2.8232 (2.7815)\n",
            "TRAIN(043): [170/196] Batch: 0.0684 (0.0788) Data: 0.0599 (0.0511) Loss: 2.6756 (2.7795)\n",
            "TRAIN(043): [180/196] Batch: 0.0745 (0.0786) Data: 0.0616 (0.0510) Loss: 2.7518 (2.7802)\n",
            "TRAIN(043): [190/196] Batch: 0.0757 (0.0785) Data: 0.0619 (0.0511) Loss: 2.5600 (2.7803)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(43)         0:00:15         0:00:10         0:00:05          2.7806\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(044): [ 10/196] Batch: 0.0878 (0.1243) Data: 0.0406 (0.0781) Loss: 2.6939 (2.7535)\n",
            "TRAIN(044): [ 20/196] Batch: 0.0783 (0.1006) Data: 0.0428 (0.0620) Loss: 2.7333 (2.7220)\n",
            "TRAIN(044): [ 30/196] Batch: 0.0620 (0.0924) Data: 0.0537 (0.0580) Loss: 2.5213 (2.7363)\n",
            "TRAIN(044): [ 40/196] Batch: 0.0802 (0.0885) Data: 0.0525 (0.0551) Loss: 2.7772 (2.7442)\n",
            "TRAIN(044): [ 50/196] Batch: 0.0687 (0.0860) Data: 0.0588 (0.0537) Loss: 2.7551 (2.7500)\n",
            "TRAIN(044): [ 60/196] Batch: 0.0726 (0.0844) Data: 0.0514 (0.0529) Loss: 2.8268 (2.7479)\n",
            "TRAIN(044): [ 70/196] Batch: 0.0768 (0.0832) Data: 0.0571 (0.0526) Loss: 2.8004 (2.7517)\n",
            "TRAIN(044): [ 80/196] Batch: 0.0751 (0.0823) Data: 0.0572 (0.0524) Loss: 2.8212 (2.7533)\n",
            "TRAIN(044): [ 90/196] Batch: 0.0715 (0.0816) Data: 0.0568 (0.0522) Loss: 2.6883 (2.7533)\n",
            "TRAIN(044): [100/196] Batch: 0.0759 (0.0810) Data: 0.0558 (0.0519) Loss: 2.6772 (2.7497)\n",
            "TRAIN(044): [110/196] Batch: 0.0768 (0.0806) Data: 0.0522 (0.0516) Loss: 2.6952 (2.7507)\n",
            "TRAIN(044): [120/196] Batch: 0.0799 (0.0802) Data: 0.0542 (0.0515) Loss: 2.6941 (2.7556)\n",
            "TRAIN(044): [130/196] Batch: 0.0784 (0.0799) Data: 0.0541 (0.0513) Loss: 2.7684 (2.7541)\n",
            "TRAIN(044): [140/196] Batch: 0.0708 (0.0796) Data: 0.0609 (0.0514) Loss: 2.9076 (2.7569)\n",
            "TRAIN(044): [150/196] Batch: 0.0691 (0.0794) Data: 0.0574 (0.0514) Loss: 2.6303 (2.7556)\n",
            "TRAIN(044): [160/196] Batch: 0.0795 (0.0792) Data: 0.0536 (0.0513) Loss: 2.7140 (2.7549)\n",
            "TRAIN(044): [170/196] Batch: 0.0710 (0.0790) Data: 0.0572 (0.0512) Loss: 2.8138 (2.7538)\n",
            "TRAIN(044): [180/196] Batch: 0.0741 (0.0788) Data: 0.0532 (0.0511) Loss: 2.7757 (2.7550)\n",
            "TRAIN(044): [190/196] Batch: 0.0760 (0.0787) Data: 0.0596 (0.0510) Loss: 2.9103 (2.7582)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(44)         0:00:15         0:00:09         0:00:05          2.7572\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(045): [ 10/196] Batch: 0.0663 (0.1378) Data: 0.0463 (0.1006) Loss: 2.8110 (2.7389)\n",
            "TRAIN(045): [ 20/196] Batch: 0.0618 (0.1079) Data: 0.0514 (0.0728) Loss: 2.7153 (2.7457)\n",
            "TRAIN(045): [ 30/196] Batch: 0.0771 (0.0971) Data: 0.0557 (0.0660) Loss: 2.6783 (2.7371)\n",
            "TRAIN(045): [ 40/196] Batch: 0.0745 (0.0917) Data: 0.0633 (0.0622) Loss: 2.7068 (2.7340)\n",
            "TRAIN(045): [ 50/196] Batch: 0.0767 (0.0886) Data: 0.0575 (0.0604) Loss: 2.9980 (2.7462)\n",
            "TRAIN(045): [ 60/196] Batch: 0.0750 (0.0865) Data: 0.0555 (0.0591) Loss: 2.7306 (2.7490)\n",
            "TRAIN(045): [ 70/196] Batch: 0.0855 (0.0851) Data: 0.0525 (0.0579) Loss: 2.7151 (2.7484)\n",
            "TRAIN(045): [ 80/196] Batch: 0.0785 (0.0839) Data: 0.0553 (0.0568) Loss: 2.6112 (2.7458)\n",
            "TRAIN(045): [ 90/196] Batch: 0.0752 (0.0829) Data: 0.0628 (0.0564) Loss: 2.8082 (2.7505)\n",
            "TRAIN(045): [100/196] Batch: 0.0716 (0.0823) Data: 0.0565 (0.0561) Loss: 2.6939 (2.7499)\n",
            "TRAIN(045): [110/196] Batch: 0.0814 (0.0817) Data: 0.0567 (0.0556) Loss: 2.6768 (2.7492)\n",
            "TRAIN(045): [120/196] Batch: 0.0705 (0.0813) Data: 0.0551 (0.0553) Loss: 2.5785 (2.7466)\n",
            "TRAIN(045): [130/196] Batch: 0.0795 (0.0809) Data: 0.0549 (0.0550) Loss: 2.7244 (2.7481)\n",
            "TRAIN(045): [140/196] Batch: 0.0791 (0.0806) Data: 0.0503 (0.0545) Loss: 2.6346 (2.7457)\n",
            "TRAIN(045): [150/196] Batch: 0.0717 (0.0802) Data: 0.0592 (0.0543) Loss: 2.7324 (2.7453)\n",
            "TRAIN(045): [160/196] Batch: 0.0712 (0.0799) Data: 0.0578 (0.0539) Loss: 2.7643 (2.7435)\n",
            "TRAIN(045): [170/196] Batch: 0.0746 (0.0797) Data: 0.0544 (0.0535) Loss: 2.7679 (2.7464)\n",
            "TRAIN(045): [180/196] Batch: 0.0766 (0.0796) Data: 0.0511 (0.0530) Loss: 2.8348 (2.7446)\n",
            "TRAIN(045): [190/196] Batch: 0.0741 (0.0794) Data: 0.0622 (0.0529) Loss: 2.6672 (2.7433)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(45)         0:00:15         0:00:10         0:00:05          2.7420\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(046): [ 10/196] Batch: 0.0636 (0.1150) Data: 0.0613 (0.0837) Loss: 2.8108 (2.7363)\n",
            "TRAIN(046): [ 20/196] Batch: 0.0760 (0.0960) Data: 0.0534 (0.0682) Loss: 2.7452 (2.7404)\n",
            "TRAIN(046): [ 30/196] Batch: 0.0847 (0.0895) Data: 0.0505 (0.0631) Loss: 2.8553 (2.7340)\n",
            "TRAIN(046): [ 40/196] Batch: 0.0769 (0.0860) Data: 0.0571 (0.0605) Loss: 2.7809 (2.7285)\n",
            "TRAIN(046): [ 50/196] Batch: 0.0818 (0.0840) Data: 0.0572 (0.0587) Loss: 2.8827 (2.7290)\n",
            "TRAIN(046): [ 60/196] Batch: 0.0728 (0.0827) Data: 0.0533 (0.0573) Loss: 2.7770 (2.7289)\n",
            "TRAIN(046): [ 70/196] Batch: 0.0777 (0.0817) Data: 0.0578 (0.0564) Loss: 2.7593 (2.7267)\n",
            "TRAIN(046): [ 80/196] Batch: 0.0716 (0.0810) Data: 0.0571 (0.0555) Loss: 2.5556 (2.7213)\n",
            "TRAIN(046): [ 90/196] Batch: 0.0854 (0.0804) Data: 0.0537 (0.0552) Loss: 2.6123 (2.7210)\n",
            "TRAIN(046): [100/196] Batch: 0.0721 (0.0799) Data: 0.0549 (0.0546) Loss: 2.8026 (2.7200)\n",
            "TRAIN(046): [110/196] Batch: 0.0792 (0.0796) Data: 0.0541 (0.0543) Loss: 2.7861 (2.7147)\n",
            "TRAIN(046): [120/196] Batch: 0.0849 (0.0793) Data: 0.0536 (0.0542) Loss: 2.6808 (2.7150)\n",
            "TRAIN(046): [130/196] Batch: 0.0786 (0.0791) Data: 0.0492 (0.0540) Loss: 2.7113 (2.7157)\n",
            "TRAIN(046): [140/196] Batch: 0.0638 (0.0788) Data: 0.0575 (0.0537) Loss: 2.6536 (2.7164)\n",
            "TRAIN(046): [150/196] Batch: 0.0816 (0.0787) Data: 0.0522 (0.0534) Loss: 2.7407 (2.7132)\n",
            "TRAIN(046): [160/196] Batch: 0.0767 (0.0786) Data: 0.0487 (0.0530) Loss: 2.7855 (2.7140)\n",
            "TRAIN(046): [170/196] Batch: 0.0760 (0.0785) Data: 0.0503 (0.0526) Loss: 2.7837 (2.7155)\n",
            "TRAIN(046): [180/196] Batch: 0.0760 (0.0784) Data: 0.0508 (0.0522) Loss: 2.8122 (2.7139)\n",
            "TRAIN(046): [190/196] Batch: 0.0755 (0.0782) Data: 0.0622 (0.0522) Loss: 2.7569 (2.7174)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(46)         0:00:15         0:00:10         0:00:05          2.7164\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(047): [ 10/196] Batch: 0.0612 (0.1174) Data: 0.0615 (0.0846) Loss: 2.5555 (2.7274)\n",
            "TRAIN(047): [ 20/196] Batch: 0.0716 (0.0973) Data: 0.0528 (0.0681) Loss: 2.7054 (2.7293)\n",
            "TRAIN(047): [ 30/196] Batch: 0.0677 (0.0901) Data: 0.0575 (0.0619) Loss: 2.6170 (2.7137)\n",
            "TRAIN(047): [ 40/196] Batch: 0.0849 (0.0868) Data: 0.0479 (0.0582) Loss: 2.8670 (2.7168)\n",
            "TRAIN(047): [ 50/196] Batch: 0.0652 (0.0844) Data: 0.0518 (0.0554) Loss: 2.7764 (2.7078)\n",
            "TRAIN(047): [ 60/196] Batch: 0.0817 (0.0833) Data: 0.0428 (0.0538) Loss: 2.6496 (2.7019)\n",
            "TRAIN(047): [ 70/196] Batch: 0.0688 (0.0821) Data: 0.0580 (0.0527) Loss: 2.6621 (2.6991)\n",
            "TRAIN(047): [ 80/196] Batch: 0.0760 (0.0815) Data: 0.0552 (0.0521) Loss: 2.6681 (2.6969)\n",
            "TRAIN(047): [ 90/196] Batch: 0.0805 (0.0809) Data: 0.0511 (0.0520) Loss: 2.7666 (2.7026)\n",
            "TRAIN(047): [100/196] Batch: 0.0664 (0.0803) Data: 0.0623 (0.0518) Loss: 2.6677 (2.6987)\n",
            "TRAIN(047): [110/196] Batch: 0.0748 (0.0800) Data: 0.0561 (0.0515) Loss: 2.7284 (2.6961)\n",
            "TRAIN(047): [120/196] Batch: 0.0829 (0.0797) Data: 0.0569 (0.0511) Loss: 2.5908 (2.6924)\n",
            "TRAIN(047): [130/196] Batch: 0.0774 (0.0795) Data: 0.0479 (0.0506) Loss: 2.6635 (2.6941)\n",
            "TRAIN(047): [140/196] Batch: 0.0782 (0.0794) Data: 0.0563 (0.0503) Loss: 2.5092 (2.6953)\n",
            "TRAIN(047): [150/196] Batch: 0.0611 (0.0792) Data: 0.0554 (0.0501) Loss: 2.6992 (2.6969)\n",
            "TRAIN(047): [160/196] Batch: 0.0660 (0.0790) Data: 0.0634 (0.0499) Loss: 2.7307 (2.6933)\n",
            "TRAIN(047): [170/196] Batch: 0.0763 (0.0788) Data: 0.0576 (0.0500) Loss: 2.6541 (2.6886)\n",
            "TRAIN(047): [180/196] Batch: 0.0785 (0.0787) Data: 0.0542 (0.0500) Loss: 2.7211 (2.6907)\n",
            "TRAIN(047): [190/196] Batch: 0.0762 (0.0785) Data: 0.0623 (0.0502) Loss: 2.7087 (2.6928)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(47)         0:00:15         0:00:09         0:00:05          2.6913\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(048): [ 10/196] Batch: 0.0720 (0.1169) Data: 0.0533 (0.0804) Loss: 2.5824 (2.6590)\n",
            "TRAIN(048): [ 20/196] Batch: 0.0801 (0.0968) Data: 0.0510 (0.0662) Loss: 2.7024 (2.6469)\n",
            "TRAIN(048): [ 30/196] Batch: 0.0788 (0.0898) Data: 0.0523 (0.0616) Loss: 2.5860 (2.6592)\n",
            "TRAIN(048): [ 40/196] Batch: 0.0657 (0.0862) Data: 0.0581 (0.0587) Loss: 2.7461 (2.6656)\n",
            "TRAIN(048): [ 50/196] Batch: 0.0761 (0.0841) Data: 0.0563 (0.0570) Loss: 2.6616 (2.6749)\n",
            "TRAIN(048): [ 60/196] Batch: 0.0782 (0.0828) Data: 0.0556 (0.0561) Loss: 2.7874 (2.6806)\n",
            "TRAIN(048): [ 70/196] Batch: 0.0703 (0.0817) Data: 0.0622 (0.0555) Loss: 2.5733 (2.6771)\n",
            "TRAIN(048): [ 80/196] Batch: 0.0802 (0.0811) Data: 0.0527 (0.0548) Loss: 2.6203 (2.6774)\n",
            "TRAIN(048): [ 90/196] Batch: 0.0713 (0.0806) Data: 0.0510 (0.0541) Loss: 2.7198 (2.6739)\n",
            "TRAIN(048): [100/196] Batch: 0.0733 (0.0802) Data: 0.0519 (0.0533) Loss: 2.7291 (2.6729)\n",
            "TRAIN(048): [110/196] Batch: 0.0841 (0.0799) Data: 0.0522 (0.0532) Loss: 2.6739 (2.6723)\n",
            "TRAIN(048): [120/196] Batch: 0.0924 (0.0797) Data: 0.0459 (0.0530) Loss: 2.5982 (2.6733)\n",
            "TRAIN(048): [130/196] Batch: 0.0865 (0.0796) Data: 0.0401 (0.0522) Loss: 2.7791 (2.6768)\n",
            "TRAIN(048): [140/196] Batch: 0.0750 (0.0793) Data: 0.0519 (0.0516) Loss: 2.7796 (2.6805)\n",
            "TRAIN(048): [150/196] Batch: 0.0716 (0.0790) Data: 0.0624 (0.0515) Loss: 2.7472 (2.6805)\n",
            "TRAIN(048): [160/196] Batch: 0.0851 (0.0788) Data: 0.0535 (0.0516) Loss: 2.7667 (2.6814)\n",
            "TRAIN(048): [170/196] Batch: 0.0765 (0.0786) Data: 0.0623 (0.0516) Loss: 2.4599 (2.6785)\n",
            "TRAIN(048): [180/196] Batch: 0.0675 (0.0785) Data: 0.0561 (0.0516) Loss: 2.6413 (2.6779)\n",
            "TRAIN(048): [190/196] Batch: 0.0764 (0.0784) Data: 0.0627 (0.0516) Loss: 2.4713 (2.6778)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(48)         0:00:15         0:00:10         0:00:05          2.6764\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(049): [ 10/196] Batch: 0.0839 (0.1161) Data: 0.0517 (0.0824) Loss: 2.8345 (2.6694)\n",
            "TRAIN(049): [ 20/196] Batch: 0.0729 (0.0961) Data: 0.0573 (0.0675) Loss: 2.5995 (2.6414)\n",
            "TRAIN(049): [ 30/196] Batch: 0.0838 (0.0897) Data: 0.0540 (0.0627) Loss: 2.7767 (2.6559)\n",
            "TRAIN(049): [ 40/196] Batch: 0.0742 (0.0862) Data: 0.0579 (0.0597) Loss: 2.6771 (2.6582)\n",
            "TRAIN(049): [ 50/196] Batch: 0.0804 (0.0843) Data: 0.0529 (0.0578) Loss: 2.6925 (2.6525)\n",
            "TRAIN(049): [ 60/196] Batch: 0.0711 (0.0829) Data: 0.0542 (0.0566) Loss: 2.6248 (2.6497)\n",
            "TRAIN(049): [ 70/196] Batch: 0.0739 (0.0820) Data: 0.0521 (0.0555) Loss: 2.4818 (2.6418)\n",
            "TRAIN(049): [ 80/196] Batch: 0.0835 (0.0816) Data: 0.0455 (0.0545) Loss: 2.6776 (2.6455)\n",
            "TRAIN(049): [ 90/196] Batch: 0.0726 (0.0809) Data: 0.0539 (0.0538) Loss: 2.7276 (2.6445)\n",
            "TRAIN(049): [100/196] Batch: 0.0783 (0.0806) Data: 0.0512 (0.0531) Loss: 2.6986 (2.6453)\n",
            "TRAIN(049): [110/196] Batch: 0.0822 (0.0805) Data: 0.0411 (0.0525) Loss: 2.8121 (2.6455)\n",
            "TRAIN(049): [120/196] Batch: 0.0710 (0.0801) Data: 0.0556 (0.0520) Loss: 2.6678 (2.6481)\n",
            "TRAIN(049): [130/196] Batch: 0.0823 (0.0798) Data: 0.0573 (0.0516) Loss: 2.7965 (2.6457)\n",
            "TRAIN(049): [140/196] Batch: 0.0780 (0.0796) Data: 0.0517 (0.0514) Loss: 2.7147 (2.6438)\n",
            "TRAIN(049): [150/196] Batch: 0.0853 (0.0793) Data: 0.0534 (0.0513) Loss: 2.4558 (2.6447)\n",
            "TRAIN(049): [160/196] Batch: 0.0751 (0.0791) Data: 0.0567 (0.0512) Loss: 2.6582 (2.6446)\n",
            "TRAIN(049): [170/196] Batch: 0.0773 (0.0790) Data: 0.0553 (0.0512) Loss: 2.4590 (2.6423)\n",
            "TRAIN(049): [180/196] Batch: 0.0720 (0.0788) Data: 0.0566 (0.0511) Loss: 2.7160 (2.6398)\n",
            "TRAIN(049): [190/196] Batch: 0.0762 (0.0786) Data: 0.0629 (0.0512) Loss: 2.6397 (2.6431)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(49)         0:00:15         0:00:10         0:00:05          2.6416\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(050): [ 10/196] Batch: 0.0693 (0.1183) Data: 0.0563 (0.0825) Loss: 2.5358 (2.6134)\n",
            "TRAIN(050): [ 20/196] Batch: 0.0799 (0.0978) Data: 0.0503 (0.0680) Loss: 2.6574 (2.6347)\n",
            "TRAIN(050): [ 30/196] Batch: 0.0812 (0.0906) Data: 0.0521 (0.0630) Loss: 2.6307 (2.6379)\n",
            "TRAIN(050): [ 40/196] Batch: 0.0801 (0.0870) Data: 0.0533 (0.0600) Loss: 2.8934 (2.6454)\n",
            "TRAIN(050): [ 50/196] Batch: 0.0766 (0.0848) Data: 0.0520 (0.0575) Loss: 2.4133 (2.6422)\n",
            "TRAIN(050): [ 60/196] Batch: 0.0764 (0.0836) Data: 0.0551 (0.0564) Loss: 2.7242 (2.6444)\n",
            "TRAIN(050): [ 70/196] Batch: 0.0668 (0.0827) Data: 0.0504 (0.0551) Loss: 2.6135 (2.6392)\n",
            "TRAIN(050): [ 80/196] Batch: 0.0757 (0.0818) Data: 0.0599 (0.0543) Loss: 2.6627 (2.6451)\n",
            "TRAIN(050): [ 90/196] Batch: 0.0582 (0.0813) Data: 0.0496 (0.0532) Loss: 2.6092 (2.6427)\n",
            "TRAIN(050): [100/196] Batch: 0.0690 (0.0808) Data: 0.0571 (0.0526) Loss: 2.7681 (2.6420)\n",
            "TRAIN(050): [110/196] Batch: 0.0808 (0.0804) Data: 0.0518 (0.0523) Loss: 2.8467 (2.6410)\n",
            "TRAIN(050): [120/196] Batch: 0.0814 (0.0800) Data: 0.0556 (0.0524) Loss: 2.7413 (2.6409)\n",
            "TRAIN(050): [130/196] Batch: 0.0742 (0.0796) Data: 0.0624 (0.0524) Loss: 2.7255 (2.6442)\n",
            "TRAIN(050): [140/196] Batch: 0.0708 (0.0794) Data: 0.0577 (0.0523) Loss: 2.4835 (2.6425)\n",
            "TRAIN(050): [150/196] Batch: 0.0794 (0.0792) Data: 0.0585 (0.0522) Loss: 2.6780 (2.6380)\n",
            "TRAIN(050): [160/196] Batch: 0.0830 (0.0790) Data: 0.0556 (0.0522) Loss: 2.6886 (2.6351)\n",
            "TRAIN(050): [170/196] Batch: 0.0736 (0.0789) Data: 0.0551 (0.0522) Loss: 2.6200 (2.6368)\n",
            "TRAIN(050): [180/196] Batch: 0.0663 (0.0787) Data: 0.0619 (0.0521) Loss: 2.7264 (2.6378)\n",
            "TRAIN(050): [190/196] Batch: 0.0759 (0.0786) Data: 0.0628 (0.0522) Loss: 2.7995 (2.6371)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(50)         0:00:15         0:00:10         0:00:05          2.6353\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(051): [ 10/196] Batch: 0.0867 (0.1206) Data: 0.0493 (0.0836) Loss: 2.6895 (2.6170)\n",
            "TRAIN(051): [ 20/196] Batch: 0.0810 (0.0986) Data: 0.0523 (0.0680) Loss: 2.5701 (2.6484)\n",
            "TRAIN(051): [ 30/196] Batch: 0.0811 (0.0914) Data: 0.0455 (0.0612) Loss: 2.6217 (2.6348)\n",
            "TRAIN(051): [ 40/196] Batch: 0.0632 (0.0878) Data: 0.0538 (0.0580) Loss: 2.6642 (2.6307)\n",
            "TRAIN(051): [ 50/196] Batch: 0.0887 (0.0857) Data: 0.0441 (0.0558) Loss: 2.7926 (2.6323)\n",
            "TRAIN(051): [ 60/196] Batch: 0.0696 (0.0840) Data: 0.0509 (0.0539) Loss: 2.6592 (2.6275)\n",
            "TRAIN(051): [ 70/196] Batch: 0.0795 (0.0830) Data: 0.0501 (0.0530) Loss: 2.6387 (2.6265)\n",
            "TRAIN(051): [ 80/196] Batch: 0.0763 (0.0820) Data: 0.0564 (0.0527) Loss: 2.6032 (2.6321)\n",
            "TRAIN(051): [ 90/196] Batch: 0.0702 (0.0812) Data: 0.0631 (0.0528) Loss: 2.6707 (2.6378)\n",
            "TRAIN(051): [100/196] Batch: 0.0797 (0.0808) Data: 0.0523 (0.0528) Loss: 2.5633 (2.6312)\n",
            "TRAIN(051): [110/196] Batch: 0.0734 (0.0804) Data: 0.0564 (0.0526) Loss: 2.5120 (2.6309)\n",
            "TRAIN(051): [120/196] Batch: 0.0629 (0.0800) Data: 0.0637 (0.0522) Loss: 2.7580 (2.6290)\n",
            "TRAIN(051): [130/196] Batch: 0.0750 (0.0797) Data: 0.0631 (0.0522) Loss: 2.6933 (2.6285)\n",
            "TRAIN(051): [140/196] Batch: 0.0643 (0.0794) Data: 0.0623 (0.0521) Loss: 2.4247 (2.6256)\n",
            "TRAIN(051): [150/196] Batch: 0.0838 (0.0792) Data: 0.0541 (0.0523) Loss: 2.6219 (2.6219)\n",
            "TRAIN(051): [160/196] Batch: 0.0803 (0.0791) Data: 0.0514 (0.0521) Loss: 2.6508 (2.6220)\n",
            "TRAIN(051): [170/196] Batch: 0.0751 (0.0788) Data: 0.0625 (0.0521) Loss: 2.5529 (2.6194)\n",
            "TRAIN(051): [180/196] Batch: 0.0673 (0.0787) Data: 0.0623 (0.0520) Loss: 2.5260 (2.6198)\n",
            "TRAIN(051): [190/196] Batch: 0.0756 (0.0785) Data: 0.0614 (0.0520) Loss: 2.6945 (2.6196)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(51)         0:00:15         0:00:10         0:00:05          2.6185\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(052): [ 10/196] Batch: 0.0974 (0.1210) Data: 0.0312 (0.0749) Loss: 2.4734 (2.6462)\n",
            "TRAIN(052): [ 20/196] Batch: 0.0713 (0.0987) Data: 0.0570 (0.0610) Loss: 2.7357 (2.6463)\n",
            "TRAIN(052): [ 30/196] Batch: 0.0753 (0.0917) Data: 0.0566 (0.0581) Loss: 2.7726 (2.6496)\n",
            "TRAIN(052): [ 40/196] Batch: 0.0665 (0.0888) Data: 0.0582 (0.0553) Loss: 2.4151 (2.6261)\n",
            "TRAIN(052): [ 50/196] Batch: 0.0915 (0.0871) Data: 0.0400 (0.0530) Loss: 2.4813 (2.6254)\n",
            "TRAIN(052): [ 60/196] Batch: 0.0791 (0.0851) Data: 0.0532 (0.0522) Loss: 2.5833 (2.6137)\n",
            "TRAIN(052): [ 70/196] Batch: 0.0698 (0.0837) Data: 0.0577 (0.0519) Loss: 2.7308 (2.6130)\n",
            "TRAIN(052): [ 80/196] Batch: 0.0710 (0.0827) Data: 0.0586 (0.0517) Loss: 2.5565 (2.6140)\n",
            "TRAIN(052): [ 90/196] Batch: 0.0707 (0.0820) Data: 0.0569 (0.0516) Loss: 2.5426 (2.6112)\n",
            "TRAIN(052): [100/196] Batch: 0.0700 (0.0813) Data: 0.0640 (0.0515) Loss: 2.7092 (2.6093)\n",
            "TRAIN(052): [110/196] Batch: 0.0707 (0.0809) Data: 0.0562 (0.0513) Loss: 2.5880 (2.6123)\n",
            "TRAIN(052): [120/196] Batch: 0.0727 (0.0805) Data: 0.0572 (0.0512) Loss: 2.7173 (2.6154)\n",
            "TRAIN(052): [130/196] Batch: 0.0769 (0.0801) Data: 0.0579 (0.0512) Loss: 2.4212 (2.6141)\n",
            "TRAIN(052): [140/196] Batch: 0.0759 (0.0798) Data: 0.0649 (0.0512) Loss: 2.6054 (2.6107)\n",
            "TRAIN(052): [150/196] Batch: 0.0797 (0.0797) Data: 0.0523 (0.0511) Loss: 2.6779 (2.6102)\n",
            "TRAIN(052): [160/196] Batch: 0.0759 (0.0794) Data: 0.0570 (0.0510) Loss: 2.5670 (2.6079)\n",
            "TRAIN(052): [170/196] Batch: 0.0727 (0.0792) Data: 0.0623 (0.0511) Loss: 2.6787 (2.6093)\n",
            "TRAIN(052): [180/196] Batch: 0.0750 (0.0790) Data: 0.0561 (0.0510) Loss: 2.4634 (2.6071)\n",
            "TRAIN(052): [190/196] Batch: 0.0757 (0.0788) Data: 0.0628 (0.0510) Loss: 2.4484 (2.6075)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(52)         0:00:15         0:00:09         0:00:05          2.6078\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(053): [ 10/196] Batch: 0.0728 (0.1322) Data: 0.0454 (0.0877) Loss: 2.6151 (2.6557)\n",
            "TRAIN(053): [ 20/196] Batch: 0.0640 (0.1050) Data: 0.0504 (0.0657) Loss: 2.9331 (2.6364)\n",
            "TRAIN(053): [ 30/196] Batch: 0.0782 (0.0953) Data: 0.0526 (0.0604) Loss: 2.6877 (2.6278)\n",
            "TRAIN(053): [ 40/196] Batch: 0.0789 (0.0904) Data: 0.0588 (0.0579) Loss: 2.5592 (2.6172)\n",
            "TRAIN(053): [ 50/196] Batch: 0.0742 (0.0876) Data: 0.0522 (0.0560) Loss: 2.7302 (2.6154)\n",
            "TRAIN(053): [ 60/196] Batch: 0.0755 (0.0855) Data: 0.0625 (0.0552) Loss: 2.5212 (2.6002)\n",
            "TRAIN(053): [ 70/196] Batch: 0.0770 (0.0843) Data: 0.0562 (0.0543) Loss: 2.6742 (2.6004)\n",
            "TRAIN(053): [ 80/196] Batch: 0.0819 (0.0833) Data: 0.0514 (0.0536) Loss: 2.6840 (2.6017)\n",
            "TRAIN(053): [ 90/196] Batch: 0.0715 (0.0823) Data: 0.0627 (0.0536) Loss: 2.5633 (2.6002)\n",
            "TRAIN(053): [100/196] Batch: 0.0758 (0.0817) Data: 0.0629 (0.0535) Loss: 2.4526 (2.5986)\n",
            "TRAIN(053): [110/196] Batch: 0.0788 (0.0813) Data: 0.0485 (0.0531) Loss: 2.5449 (2.5955)\n",
            "TRAIN(053): [120/196] Batch: 0.0666 (0.0807) Data: 0.0612 (0.0527) Loss: 2.5722 (2.5962)\n",
            "TRAIN(053): [130/196] Batch: 0.0870 (0.0805) Data: 0.0498 (0.0525) Loss: 2.5291 (2.5956)\n",
            "TRAIN(053): [140/196] Batch: 0.0706 (0.0801) Data: 0.0618 (0.0524) Loss: 2.5615 (2.5932)\n",
            "TRAIN(053): [150/196] Batch: 0.0661 (0.0798) Data: 0.0622 (0.0522) Loss: 2.3828 (2.5918)\n",
            "TRAIN(053): [160/196] Batch: 0.0781 (0.0797) Data: 0.0498 (0.0519) Loss: 2.6560 (2.5875)\n",
            "TRAIN(053): [170/196] Batch: 0.0779 (0.0795) Data: 0.0563 (0.0517) Loss: 2.4696 (2.5843)\n",
            "TRAIN(053): [180/196] Batch: 0.0732 (0.0793) Data: 0.0536 (0.0515) Loss: 2.4913 (2.5852)\n",
            "TRAIN(053): [190/196] Batch: 0.0770 (0.0791) Data: 0.0597 (0.0513) Loss: 2.5883 (2.5883)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(53)         0:00:15         0:00:10         0:00:05          2.5902\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(054): [ 10/196] Batch: 0.0755 (0.1165) Data: 0.0496 (0.0788) Loss: 2.4615 (2.5605)\n",
            "TRAIN(054): [ 20/196] Batch: 0.0842 (0.0965) Data: 0.0507 (0.0651) Loss: 2.5294 (2.5558)\n",
            "TRAIN(054): [ 30/196] Batch: 0.0729 (0.0894) Data: 0.0608 (0.0609) Loss: 2.4270 (2.5552)\n",
            "TRAIN(054): [ 40/196] Batch: 0.0791 (0.0863) Data: 0.0570 (0.0590) Loss: 2.5944 (2.5464)\n",
            "TRAIN(054): [ 50/196] Batch: 0.0805 (0.0843) Data: 0.0527 (0.0573) Loss: 2.5654 (2.5485)\n",
            "TRAIN(054): [ 60/196] Batch: 0.0730 (0.0828) Data: 0.0599 (0.0562) Loss: 2.5570 (2.5581)\n",
            "TRAIN(054): [ 70/196] Batch: 0.0760 (0.0819) Data: 0.0604 (0.0553) Loss: 2.5864 (2.5572)\n",
            "TRAIN(054): [ 80/196] Batch: 0.0676 (0.0812) Data: 0.0608 (0.0548) Loss: 2.7345 (2.5582)\n",
            "TRAIN(054): [ 90/196] Batch: 0.0702 (0.0807) Data: 0.0569 (0.0543) Loss: 2.5695 (2.5639)\n",
            "TRAIN(054): [100/196] Batch: 0.0728 (0.0802) Data: 0.0530 (0.0539) Loss: 2.3370 (2.5613)\n",
            "TRAIN(054): [110/196] Batch: 0.0692 (0.0798) Data: 0.0566 (0.0534) Loss: 2.5894 (2.5604)\n",
            "TRAIN(054): [120/196] Batch: 0.0807 (0.0796) Data: 0.0505 (0.0531) Loss: 2.6097 (2.5642)\n",
            "TRAIN(054): [130/196] Batch: 0.0703 (0.0793) Data: 0.0566 (0.0527) Loss: 2.3766 (2.5595)\n",
            "TRAIN(054): [140/196] Batch: 0.0698 (0.0791) Data: 0.0564 (0.0522) Loss: 2.6096 (2.5574)\n",
            "TRAIN(054): [150/196] Batch: 0.0805 (0.0789) Data: 0.0525 (0.0520) Loss: 2.6240 (2.5559)\n",
            "TRAIN(054): [160/196] Batch: 0.0773 (0.0788) Data: 0.0488 (0.0517) Loss: 2.5510 (2.5554)\n",
            "TRAIN(054): [170/196] Batch: 0.0754 (0.0787) Data: 0.0520 (0.0516) Loss: 2.5529 (2.5576)\n",
            "TRAIN(054): [180/196] Batch: 0.0763 (0.0785) Data: 0.0578 (0.0513) Loss: 2.4473 (2.5585)\n",
            "TRAIN(054): [190/196] Batch: 0.0746 (0.0783) Data: 0.0624 (0.0513) Loss: 2.4671 (2.5593)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(54)         0:00:15         0:00:10         0:00:05          2.5591\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(055): [ 10/196] Batch: 0.0863 (0.1157) Data: 0.0508 (0.0823) Loss: 2.6254 (2.5358)\n",
            "TRAIN(055): [ 20/196] Batch: 0.0737 (0.0957) Data: 0.0619 (0.0683) Loss: 2.3287 (2.5525)\n",
            "TRAIN(055): [ 30/196] Batch: 0.0691 (0.0894) Data: 0.0581 (0.0625) Loss: 2.2695 (2.5405)\n",
            "TRAIN(055): [ 40/196] Batch: 0.0714 (0.0859) Data: 0.0639 (0.0600) Loss: 2.4304 (2.5491)\n",
            "TRAIN(055): [ 50/196] Batch: 0.0772 (0.0840) Data: 0.0534 (0.0580) Loss: 2.6659 (2.5582)\n",
            "TRAIN(055): [ 60/196] Batch: 0.0686 (0.0826) Data: 0.0575 (0.0567) Loss: 2.4647 (2.5540)\n",
            "TRAIN(055): [ 70/196] Batch: 0.0659 (0.0817) Data: 0.0623 (0.0559) Loss: 2.6848 (2.5571)\n",
            "TRAIN(055): [ 80/196] Batch: 0.0852 (0.0811) Data: 0.0485 (0.0549) Loss: 2.6756 (2.5618)\n",
            "TRAIN(055): [ 90/196] Batch: 0.0792 (0.0805) Data: 0.0523 (0.0543) Loss: 2.3632 (2.5549)\n",
            "TRAIN(055): [100/196] Batch: 0.0732 (0.0801) Data: 0.0522 (0.0537) Loss: 2.5974 (2.5539)\n",
            "TRAIN(055): [110/196] Batch: 0.0749 (0.0798) Data: 0.0500 (0.0530) Loss: 2.5065 (2.5516)\n",
            "TRAIN(055): [120/196] Batch: 0.0712 (0.0794) Data: 0.0583 (0.0522) Loss: 2.6510 (2.5522)\n",
            "TRAIN(055): [130/196] Batch: 0.0858 (0.0794) Data: 0.0422 (0.0520) Loss: 2.6378 (2.5523)\n",
            "TRAIN(055): [140/196] Batch: 0.0794 (0.0792) Data: 0.0492 (0.0516) Loss: 2.4325 (2.5540)\n",
            "TRAIN(055): [150/196] Batch: 0.0607 (0.0791) Data: 0.0493 (0.0510) Loss: 2.6632 (2.5549)\n",
            "TRAIN(055): [160/196] Batch: 0.0746 (0.0790) Data: 0.0516 (0.0508) Loss: 2.5755 (2.5567)\n",
            "TRAIN(055): [170/196] Batch: 0.0685 (0.0789) Data: 0.0569 (0.0507) Loss: 2.4503 (2.5597)\n",
            "TRAIN(055): [180/196] Batch: 0.0706 (0.0787) Data: 0.0639 (0.0508) Loss: 2.5402 (2.5560)\n",
            "TRAIN(055): [190/196] Batch: 0.0763 (0.0785) Data: 0.0621 (0.0509) Loss: 2.6012 (2.5567)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(55)         0:00:15         0:00:09         0:00:05          2.5574\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(056): [ 10/196] Batch: 0.0712 (0.1162) Data: 0.0525 (0.0825) Loss: 2.4378 (2.5508)\n",
            "TRAIN(056): [ 20/196] Batch: 0.0866 (0.0967) Data: 0.0508 (0.0683) Loss: 2.4527 (2.5521)\n",
            "TRAIN(056): [ 30/196] Batch: 0.0824 (0.0898) Data: 0.0561 (0.0638) Loss: 2.6610 (2.5548)\n",
            "TRAIN(056): [ 40/196] Batch: 0.0796 (0.0864) Data: 0.0529 (0.0605) Loss: 2.3340 (2.5460)\n",
            "TRAIN(056): [ 50/196] Batch: 0.0796 (0.0843) Data: 0.0521 (0.0587) Loss: 2.3802 (2.5360)\n",
            "TRAIN(056): [ 60/196] Batch: 0.0857 (0.0829) Data: 0.0517 (0.0574) Loss: 2.5172 (2.5259)\n",
            "TRAIN(056): [ 70/196] Batch: 0.0748 (0.0819) Data: 0.0554 (0.0566) Loss: 2.5323 (2.5244)\n",
            "TRAIN(056): [ 80/196] Batch: 0.0714 (0.0812) Data: 0.0575 (0.0559) Loss: 2.4321 (2.5213)\n",
            "TRAIN(056): [ 90/196] Batch: 0.0909 (0.0807) Data: 0.0449 (0.0551) Loss: 2.3944 (2.5241)\n",
            "TRAIN(056): [100/196] Batch: 0.0776 (0.0802) Data: 0.0496 (0.0541) Loss: 2.5487 (2.5239)\n",
            "TRAIN(056): [110/196] Batch: 0.0760 (0.0798) Data: 0.0549 (0.0536) Loss: 2.5566 (2.5236)\n",
            "TRAIN(056): [120/196] Batch: 0.0782 (0.0796) Data: 0.0520 (0.0531) Loss: 2.5635 (2.5244)\n",
            "TRAIN(056): [130/196] Batch: 0.0743 (0.0793) Data: 0.0528 (0.0527) Loss: 2.5442 (2.5274)\n",
            "TRAIN(056): [140/196] Batch: 0.0862 (0.0791) Data: 0.0519 (0.0526) Loss: 2.5864 (2.5265)\n",
            "TRAIN(056): [150/196] Batch: 0.0696 (0.0788) Data: 0.0626 (0.0525) Loss: 2.3758 (2.5231)\n",
            "TRAIN(056): [160/196] Batch: 0.0659 (0.0787) Data: 0.0626 (0.0524) Loss: 2.5498 (2.5218)\n",
            "TRAIN(056): [170/196] Batch: 0.0805 (0.0786) Data: 0.0524 (0.0524) Loss: 2.5588 (2.5250)\n",
            "TRAIN(056): [180/196] Batch: 0.0684 (0.0784) Data: 0.0602 (0.0522) Loss: 2.6502 (2.5262)\n",
            "TRAIN(056): [190/196] Batch: 0.0754 (0.0783) Data: 0.0630 (0.0523) Loss: 2.5572 (2.5268)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(56)         0:00:15         0:00:10         0:00:05          2.5251\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(057): [ 10/196] Batch: 0.0734 (0.1188) Data: 0.0496 (0.0815) Loss: 2.7497 (2.5265)\n",
            "TRAIN(057): [ 20/196] Batch: 0.0701 (0.0976) Data: 0.0559 (0.0679) Loss: 2.6538 (2.5379)\n",
            "TRAIN(057): [ 30/196] Batch: 0.0726 (0.0903) Data: 0.0629 (0.0622) Loss: 2.5478 (2.5381)\n",
            "TRAIN(057): [ 40/196] Batch: 0.0812 (0.0870) Data: 0.0524 (0.0591) Loss: 2.5602 (2.5208)\n",
            "TRAIN(057): [ 50/196] Batch: 0.0690 (0.0846) Data: 0.0588 (0.0574) Loss: 2.5529 (2.5207)\n",
            "TRAIN(057): [ 60/196] Batch: 0.0828 (0.0833) Data: 0.0508 (0.0565) Loss: 2.5456 (2.5197)\n",
            "TRAIN(057): [ 70/196] Batch: 0.0754 (0.0823) Data: 0.0505 (0.0555) Loss: 2.5432 (2.5186)\n",
            "TRAIN(057): [ 80/196] Batch: 0.0780 (0.0815) Data: 0.0536 (0.0543) Loss: 2.3785 (2.5188)\n",
            "TRAIN(057): [ 90/196] Batch: 0.0787 (0.0811) Data: 0.0471 (0.0537) Loss: 2.5384 (2.5186)\n",
            "TRAIN(057): [100/196] Batch: 0.0483 (0.0809) Data: 0.0561 (0.0527) Loss: 2.6970 (2.5139)\n",
            "TRAIN(057): [110/196] Batch: 0.0774 (0.0806) Data: 0.0529 (0.0522) Loss: 2.3380 (2.5151)\n",
            "TRAIN(057): [120/196] Batch: 0.0711 (0.0802) Data: 0.0629 (0.0521) Loss: 2.3388 (2.5123)\n",
            "TRAIN(057): [130/196] Batch: 0.0692 (0.0799) Data: 0.0582 (0.0521) Loss: 2.5885 (2.5129)\n",
            "TRAIN(057): [140/196] Batch: 0.0697 (0.0796) Data: 0.0623 (0.0520) Loss: 2.4645 (2.5125)\n",
            "TRAIN(057): [150/196] Batch: 0.0822 (0.0795) Data: 0.0551 (0.0520) Loss: 2.6185 (2.5127)\n",
            "TRAIN(057): [160/196] Batch: 0.0730 (0.0793) Data: 0.0543 (0.0519) Loss: 2.6267 (2.5158)\n",
            "TRAIN(057): [170/196] Batch: 0.0770 (0.0791) Data: 0.0558 (0.0518) Loss: 2.5604 (2.5189)\n",
            "TRAIN(057): [180/196] Batch: 0.0667 (0.0789) Data: 0.0571 (0.0518) Loss: 2.4919 (2.5190)\n",
            "TRAIN(057): [190/196] Batch: 0.0771 (0.0787) Data: 0.0624 (0.0519) Loss: 2.5472 (2.5180)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(57)         0:00:15         0:00:10         0:00:05          2.5177\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(058): [ 10/196] Batch: 0.0767 (0.1184) Data: 0.0492 (0.0824) Loss: 2.4481 (2.5020)\n",
            "TRAIN(058): [ 20/196] Batch: 0.0706 (0.0971) Data: 0.0561 (0.0672) Loss: 2.6442 (2.5265)\n",
            "TRAIN(058): [ 30/196] Batch: 0.0793 (0.0904) Data: 0.0512 (0.0621) Loss: 2.5127 (2.5291)\n",
            "TRAIN(058): [ 40/196] Batch: 0.0819 (0.0868) Data: 0.0517 (0.0590) Loss: 2.5298 (2.5265)\n",
            "TRAIN(058): [ 50/196] Batch: 0.0749 (0.0847) Data: 0.0659 (0.0574) Loss: 2.3730 (2.5163)\n",
            "TRAIN(058): [ 60/196] Batch: 0.0764 (0.0837) Data: 0.0495 (0.0555) Loss: 2.5590 (2.5184)\n",
            "TRAIN(058): [ 70/196] Batch: 0.0769 (0.0829) Data: 0.0477 (0.0541) Loss: 2.4155 (2.5098)\n",
            "TRAIN(058): [ 80/196] Batch: 0.0763 (0.0820) Data: 0.0583 (0.0537) Loss: 2.2891 (2.5084)\n",
            "TRAIN(058): [ 90/196] Batch: 0.0779 (0.0814) Data: 0.0580 (0.0529) Loss: 2.4129 (2.5082)\n",
            "TRAIN(058): [100/196] Batch: 0.0820 (0.0811) Data: 0.0510 (0.0526) Loss: 2.4885 (2.5070)\n",
            "TRAIN(058): [110/196] Batch: 0.0709 (0.0806) Data: 0.0552 (0.0523) Loss: 2.4726 (2.5051)\n",
            "TRAIN(058): [120/196] Batch: 0.0882 (0.0803) Data: 0.0508 (0.0520) Loss: 2.3712 (2.5016)\n",
            "TRAIN(058): [130/196] Batch: 0.0800 (0.0800) Data: 0.0518 (0.0518) Loss: 2.4155 (2.5020)\n",
            "TRAIN(058): [140/196] Batch: 0.0707 (0.0796) Data: 0.0584 (0.0518) Loss: 2.4990 (2.5028)\n",
            "TRAIN(058): [150/196] Batch: 0.0772 (0.0794) Data: 0.0555 (0.0518) Loss: 2.4908 (2.5046)\n",
            "TRAIN(058): [160/196] Batch: 0.0754 (0.0792) Data: 0.0554 (0.0516) Loss: 2.6326 (2.5041)\n",
            "TRAIN(058): [170/196] Batch: 0.0680 (0.0790) Data: 0.0626 (0.0517) Loss: 2.5930 (2.5046)\n",
            "TRAIN(058): [180/196] Batch: 0.0796 (0.0789) Data: 0.0518 (0.0516) Loss: 2.7224 (2.5062)\n",
            "TRAIN(058): [190/196] Batch: 0.0763 (0.0787) Data: 0.0633 (0.0516) Loss: 2.4932 (2.5088)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(58)         0:00:15         0:00:10         0:00:05          2.5081\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(059): [ 10/196] Batch: 0.0637 (0.1185) Data: 0.0620 (0.0853) Loss: 2.5410 (2.4686)\n",
            "TRAIN(059): [ 20/196] Batch: 0.0651 (0.0979) Data: 0.0612 (0.0694) Loss: 2.4771 (2.4839)\n",
            "TRAIN(059): [ 30/196] Batch: 0.0714 (0.0912) Data: 0.0514 (0.0634) Loss: 2.4251 (2.4966)\n",
            "TRAIN(059): [ 40/196] Batch: 0.0756 (0.0875) Data: 0.0526 (0.0592) Loss: 2.3582 (2.4951)\n",
            "TRAIN(059): [ 50/196] Batch: 0.0664 (0.0854) Data: 0.0510 (0.0562) Loss: 2.6046 (2.4943)\n",
            "TRAIN(059): [ 60/196] Batch: 0.0714 (0.0839) Data: 0.0563 (0.0547) Loss: 2.4770 (2.4913)\n",
            "TRAIN(059): [ 70/196] Batch: 0.0724 (0.0828) Data: 0.0552 (0.0535) Loss: 2.5025 (2.4905)\n",
            "TRAIN(059): [ 80/196] Batch: 0.0892 (0.0822) Data: 0.0445 (0.0530) Loss: 2.3481 (2.4839)\n",
            "TRAIN(059): [ 90/196] Batch: 0.0748 (0.0815) Data: 0.0490 (0.0523) Loss: 2.4425 (2.4857)\n",
            "TRAIN(059): [100/196] Batch: 0.0661 (0.0809) Data: 0.0616 (0.0523) Loss: 2.5381 (2.4843)\n",
            "TRAIN(059): [110/196] Batch: 0.0709 (0.0805) Data: 0.0578 (0.0523) Loss: 2.5238 (2.4866)\n",
            "TRAIN(059): [120/196] Batch: 0.0828 (0.0802) Data: 0.0525 (0.0522) Loss: 2.6090 (2.4913)\n",
            "TRAIN(059): [130/196] Batch: 0.0800 (0.0798) Data: 0.0588 (0.0522) Loss: 2.4856 (2.4855)\n",
            "TRAIN(059): [140/196] Batch: 0.0797 (0.0796) Data: 0.0516 (0.0520) Loss: 2.4391 (2.4851)\n",
            "TRAIN(059): [150/196] Batch: 0.0675 (0.0794) Data: 0.0618 (0.0519) Loss: 2.4143 (2.4821)\n",
            "TRAIN(059): [160/196] Batch: 0.0767 (0.0792) Data: 0.0554 (0.0517) Loss: 2.5103 (2.4825)\n",
            "TRAIN(059): [170/196] Batch: 0.0868 (0.0791) Data: 0.0511 (0.0516) Loss: 2.3976 (2.4815)\n",
            "TRAIN(059): [180/196] Batch: 0.0725 (0.0788) Data: 0.0631 (0.0515) Loss: 2.5452 (2.4845)\n",
            "TRAIN(059): [190/196] Batch: 0.0763 (0.0787) Data: 0.0608 (0.0516) Loss: 2.5542 (2.4839)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(59)         0:00:15         0:00:10         0:00:05          2.4852\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(060): [ 10/196] Batch: 0.0842 (0.1234) Data: 0.0368 (0.0783) Loss: 2.3148 (2.4944)\n",
            "TRAIN(060): [ 20/196] Batch: 0.0917 (0.1021) Data: 0.0363 (0.0586) Loss: 2.5264 (2.4892)\n",
            "TRAIN(060): [ 30/196] Batch: 0.0880 (0.0944) Data: 0.0464 (0.0536) Loss: 2.4965 (2.5117)\n",
            "TRAIN(060): [ 40/196] Batch: 0.0760 (0.0897) Data: 0.0591 (0.0522) Loss: 2.4656 (2.4903)\n",
            "TRAIN(060): [ 50/196] Batch: 0.0765 (0.0880) Data: 0.0477 (0.0505) Loss: 2.4557 (2.4989)\n",
            "TRAIN(060): [ 60/196] Batch: 0.0666 (0.0859) Data: 0.0618 (0.0503) Loss: 2.4311 (2.4908)\n",
            "TRAIN(060): [ 70/196] Batch: 0.0810 (0.0846) Data: 0.0531 (0.0501) Loss: 2.6377 (2.4934)\n",
            "TRAIN(060): [ 80/196] Batch: 0.0743 (0.0834) Data: 0.0578 (0.0501) Loss: 2.3069 (2.4902)\n",
            "TRAIN(060): [ 90/196] Batch: 0.0735 (0.0826) Data: 0.0593 (0.0502) Loss: 2.4065 (2.4858)\n",
            "TRAIN(060): [100/196] Batch: 0.0770 (0.0820) Data: 0.0572 (0.0502) Loss: 2.3506 (2.4823)\n",
            "TRAIN(060): [110/196] Batch: 0.0769 (0.0815) Data: 0.0551 (0.0501) Loss: 2.3364 (2.4749)\n",
            "TRAIN(060): [120/196] Batch: 0.0642 (0.0810) Data: 0.0624 (0.0501) Loss: 2.4107 (2.4750)\n",
            "TRAIN(060): [130/196] Batch: 0.0707 (0.0806) Data: 0.0582 (0.0501) Loss: 2.4864 (2.4726)\n",
            "TRAIN(060): [140/196] Batch: 0.0779 (0.0803) Data: 0.0576 (0.0502) Loss: 2.4797 (2.4698)\n",
            "TRAIN(060): [150/196] Batch: 0.0876 (0.0801) Data: 0.0503 (0.0502) Loss: 2.4846 (2.4697)\n",
            "TRAIN(060): [160/196] Batch: 0.0741 (0.0798) Data: 0.0561 (0.0501) Loss: 2.5860 (2.4714)\n",
            "TRAIN(060): [170/196] Batch: 0.0686 (0.0795) Data: 0.0629 (0.0502) Loss: 2.5442 (2.4698)\n",
            "TRAIN(060): [180/196] Batch: 0.0680 (0.0794) Data: 0.0594 (0.0502) Loss: 2.3404 (2.4682)\n",
            "TRAIN(060): [190/196] Batch: 0.0750 (0.0792) Data: 0.0627 (0.0502) Loss: 2.4540 (2.4652)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(60)         0:00:15         0:00:09         0:00:05          2.4655\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(061): [ 10/196] Batch: 0.0892 (0.1311) Data: 0.0393 (0.0836) Loss: 2.3538 (2.4218)\n",
            "TRAIN(061): [ 20/196] Batch: 0.0751 (0.1047) Data: 0.0561 (0.0659) Loss: 2.5816 (2.4170)\n",
            "TRAIN(061): [ 30/196] Batch: 0.0698 (0.0954) Data: 0.0550 (0.0597) Loss: 2.3455 (2.4385)\n",
            "TRAIN(061): [ 40/196] Batch: 0.0710 (0.0906) Data: 0.0538 (0.0573) Loss: 2.5193 (2.4414)\n",
            "TRAIN(061): [ 50/196] Batch: 0.0684 (0.0876) Data: 0.0592 (0.0562) Loss: 2.3935 (2.4504)\n",
            "TRAIN(061): [ 60/196] Batch: 0.0762 (0.0856) Data: 0.0622 (0.0557) Loss: 2.5844 (2.4613)\n",
            "TRAIN(061): [ 70/196] Batch: 0.0708 (0.0844) Data: 0.0556 (0.0548) Loss: 2.4657 (2.4654)\n",
            "TRAIN(061): [ 80/196] Batch: 0.0816 (0.0835) Data: 0.0481 (0.0537) Loss: 2.3668 (2.4560)\n",
            "TRAIN(061): [ 90/196] Batch: 0.0875 (0.0826) Data: 0.0486 (0.0531) Loss: 2.4645 (2.4607)\n",
            "TRAIN(061): [100/196] Batch: 0.0685 (0.0819) Data: 0.0635 (0.0530) Loss: 2.4386 (2.4590)\n",
            "TRAIN(061): [110/196] Batch: 0.0819 (0.0814) Data: 0.0568 (0.0528) Loss: 2.3227 (2.4572)\n",
            "TRAIN(061): [120/196] Batch: 0.0661 (0.0809) Data: 0.0600 (0.0527) Loss: 2.5225 (2.4620)\n",
            "TRAIN(061): [130/196] Batch: 0.0771 (0.0806) Data: 0.0553 (0.0525) Loss: 2.5108 (2.4633)\n",
            "TRAIN(061): [140/196] Batch: 0.0763 (0.0802) Data: 0.0622 (0.0525) Loss: 2.6599 (2.4638)\n",
            "TRAIN(061): [150/196] Batch: 0.0798 (0.0800) Data: 0.0524 (0.0523) Loss: 2.3988 (2.4611)\n",
            "TRAIN(061): [160/196] Batch: 0.0760 (0.0798) Data: 0.0555 (0.0518) Loss: 2.4542 (2.4621)\n",
            "TRAIN(061): [170/196] Batch: 0.0761 (0.0796) Data: 0.0494 (0.0514) Loss: 2.3127 (2.4610)\n",
            "TRAIN(061): [180/196] Batch: 0.0757 (0.0795) Data: 0.0512 (0.0511) Loss: 2.4379 (2.4581)\n",
            "TRAIN(061): [190/196] Batch: 0.0712 (0.0793) Data: 0.0619 (0.0509) Loss: 2.3272 (2.4583)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(61)         0:00:15         0:00:09         0:00:05          2.4569\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(062): [ 10/196] Batch: 0.0754 (0.1179) Data: 0.0485 (0.0807) Loss: 2.5860 (2.5106)\n",
            "TRAIN(062): [ 20/196] Batch: 0.0699 (0.0972) Data: 0.0548 (0.0662) Loss: 2.4231 (2.4820)\n",
            "TRAIN(062): [ 30/196] Batch: 0.0771 (0.0906) Data: 0.0458 (0.0596) Loss: 2.6588 (2.4780)\n",
            "TRAIN(062): [ 40/196] Batch: 0.0718 (0.0866) Data: 0.0632 (0.0569) Loss: 2.2714 (2.4636)\n",
            "TRAIN(062): [ 50/196] Batch: 0.0735 (0.0844) Data: 0.0643 (0.0561) Loss: 2.4993 (2.4596)\n",
            "TRAIN(062): [ 60/196] Batch: 0.0695 (0.0831) Data: 0.0578 (0.0550) Loss: 2.3264 (2.4550)\n",
            "TRAIN(062): [ 70/196] Batch: 0.0726 (0.0822) Data: 0.0534 (0.0542) Loss: 2.3252 (2.4474)\n",
            "TRAIN(062): [ 80/196] Batch: 0.0723 (0.0814) Data: 0.0553 (0.0535) Loss: 2.5088 (2.4454)\n",
            "TRAIN(062): [ 90/196] Batch: 0.0759 (0.0808) Data: 0.0554 (0.0529) Loss: 2.3551 (2.4448)\n",
            "TRAIN(062): [100/196] Batch: 0.0678 (0.0803) Data: 0.0599 (0.0526) Loss: 2.5845 (2.4429)\n",
            "TRAIN(062): [110/196] Batch: 0.0865 (0.0800) Data: 0.0518 (0.0523) Loss: 2.2165 (2.4355)\n",
            "TRAIN(062): [120/196] Batch: 0.0762 (0.0796) Data: 0.0612 (0.0523) Loss: 2.4945 (2.4356)\n",
            "TRAIN(062): [130/196] Batch: 0.0766 (0.0795) Data: 0.0519 (0.0522) Loss: 2.5485 (2.4330)\n",
            "TRAIN(062): [140/196] Batch: 0.0749 (0.0793) Data: 0.0507 (0.0516) Loss: 2.3232 (2.4315)\n",
            "TRAIN(062): [150/196] Batch: 0.0687 (0.0791) Data: 0.0538 (0.0512) Loss: 2.4963 (2.4332)\n",
            "TRAIN(062): [160/196] Batch: 0.0783 (0.0789) Data: 0.0552 (0.0512) Loss: 2.4165 (2.4344)\n",
            "TRAIN(062): [170/196] Batch: 0.0732 (0.0788) Data: 0.0492 (0.0509) Loss: 2.3648 (2.4333)\n",
            "TRAIN(062): [180/196] Batch: 0.0759 (0.0787) Data: 0.0502 (0.0505) Loss: 2.4190 (2.4331)\n",
            "TRAIN(062): [190/196] Batch: 0.0763 (0.0785) Data: 0.0630 (0.0505) Loss: 2.4500 (2.4339)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(62)         0:00:15         0:00:09         0:00:05          2.4354\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(063): [ 10/196] Batch: 0.0693 (0.1170) Data: 0.0554 (0.0812) Loss: 2.4770 (2.4390)\n",
            "TRAIN(063): [ 20/196] Batch: 0.0738 (0.0966) Data: 0.0569 (0.0662) Loss: 2.3181 (2.4376)\n",
            "TRAIN(063): [ 30/196] Batch: 0.0884 (0.0902) Data: 0.0503 (0.0620) Loss: 2.2139 (2.4420)\n",
            "TRAIN(063): [ 40/196] Batch: 0.0802 (0.0864) Data: 0.0586 (0.0593) Loss: 2.5830 (2.4528)\n",
            "TRAIN(063): [ 50/196] Batch: 0.0673 (0.0843) Data: 0.0594 (0.0572) Loss: 2.5130 (2.4538)\n",
            "TRAIN(063): [ 60/196] Batch: 0.0731 (0.0831) Data: 0.0507 (0.0559) Loss: 2.5368 (2.4531)\n",
            "TRAIN(063): [ 70/196] Batch: 0.0613 (0.0819) Data: 0.0613 (0.0549) Loss: 2.4090 (2.4490)\n",
            "TRAIN(063): [ 80/196] Batch: 0.0657 (0.0812) Data: 0.0618 (0.0543) Loss: 2.4575 (2.4477)\n",
            "TRAIN(063): [ 90/196] Batch: 0.0730 (0.0807) Data: 0.0575 (0.0542) Loss: 2.3300 (2.4474)\n",
            "TRAIN(063): [100/196] Batch: 0.0757 (0.0802) Data: 0.0627 (0.0537) Loss: 2.6191 (2.4479)\n",
            "TRAIN(063): [110/196] Batch: 0.0693 (0.0801) Data: 0.0491 (0.0528) Loss: 2.5399 (2.4473)\n",
            "TRAIN(063): [120/196] Batch: 0.1131 (0.0805) Data: 0.0397 (0.0512) Loss: 2.4301 (2.4458)\n",
            "TRAIN(063): [130/196] Batch: 0.0837 (0.0805) Data: 0.0543 (0.0508) Loss: 2.3648 (2.4466)\n",
            "TRAIN(063): [140/196] Batch: 0.0715 (0.0808) Data: 0.0656 (0.0506) Loss: 2.2675 (2.4482)\n",
            "TRAIN(063): [150/196] Batch: 0.0702 (0.0813) Data: 0.0507 (0.0504) Loss: 2.3444 (2.4423)\n",
            "TRAIN(063): [160/196] Batch: 0.0761 (0.0818) Data: 0.0550 (0.0506) Loss: 2.4682 (2.4400)\n",
            "TRAIN(063): [170/196] Batch: 0.0835 (0.0816) Data: 0.0558 (0.0504) Loss: 2.5178 (2.4379)\n",
            "TRAIN(063): [180/196] Batch: 0.0682 (0.0812) Data: 0.0649 (0.0504) Loss: 2.4216 (2.4394)\n",
            "TRAIN(063): [190/196] Batch: 0.0755 (0.0810) Data: 0.0611 (0.0505) Loss: 2.4424 (2.4394)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(63)         0:00:15         0:00:09         0:00:05          2.4391\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(064): [ 10/196] Batch: 0.0738 (0.1200) Data: 0.0492 (0.0805) Loss: 2.4871 (2.4469)\n",
            "TRAIN(064): [ 20/196] Batch: 0.0709 (0.0977) Data: 0.0620 (0.0661) Loss: 2.3856 (2.4143)\n",
            "TRAIN(064): [ 30/196] Batch: 0.0705 (0.0907) Data: 0.0576 (0.0611) Loss: 2.4043 (2.4066)\n",
            "TRAIN(064): [ 40/196] Batch: 0.0756 (0.0872) Data: 0.0529 (0.0586) Loss: 2.3083 (2.3947)\n",
            "TRAIN(064): [ 50/196] Batch: 0.0665 (0.0848) Data: 0.0607 (0.0570) Loss: 2.5648 (2.4031)\n",
            "TRAIN(064): [ 60/196] Batch: 0.0788 (0.0834) Data: 0.0546 (0.0557) Loss: 2.4782 (2.4077)\n",
            "TRAIN(064): [ 70/196] Batch: 0.0782 (0.0824) Data: 0.0529 (0.0550) Loss: 2.5688 (2.4126)\n",
            "TRAIN(064): [ 80/196] Batch: 0.0815 (0.0816) Data: 0.0568 (0.0547) Loss: 2.4929 (2.4135)\n",
            "TRAIN(064): [ 90/196] Batch: 0.0762 (0.0810) Data: 0.0629 (0.0544) Loss: 2.4699 (2.4106)\n",
            "TRAIN(064): [100/196] Batch: 0.0601 (0.0805) Data: 0.0585 (0.0536) Loss: 2.4258 (2.4131)\n",
            "TRAIN(064): [110/196] Batch: 0.0748 (0.0803) Data: 0.0524 (0.0527) Loss: 2.2885 (2.4098)\n",
            "TRAIN(064): [120/196] Batch: 0.0797 (0.0800) Data: 0.0496 (0.0521) Loss: 2.5040 (2.4105)\n",
            "TRAIN(064): [130/196] Batch: 0.0768 (0.0797) Data: 0.0556 (0.0518) Loss: 2.4032 (2.4097)\n",
            "TRAIN(064): [140/196] Batch: 0.0833 (0.0796) Data: 0.0512 (0.0516) Loss: 2.5099 (2.4112)\n",
            "TRAIN(064): [150/196] Batch: 0.0884 (0.0794) Data: 0.0511 (0.0514) Loss: 2.3621 (2.4098)\n",
            "TRAIN(064): [160/196] Batch: 0.0847 (0.0792) Data: 0.0536 (0.0513) Loss: 2.5628 (2.4124)\n",
            "TRAIN(064): [170/196] Batch: 0.0772 (0.0790) Data: 0.0549 (0.0512) Loss: 2.1996 (2.4120)\n",
            "TRAIN(064): [180/196] Batch: 0.0726 (0.0788) Data: 0.0611 (0.0511) Loss: 2.4266 (2.4130)\n",
            "TRAIN(064): [190/196] Batch: 0.0760 (0.0787) Data: 0.0619 (0.0512) Loss: 2.4489 (2.4124)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(64)         0:00:15         0:00:10         0:00:05          2.4129\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(065): [ 10/196] Batch: 0.0642 (0.1164) Data: 0.0591 (0.0846) Loss: 2.3729 (2.4148)\n",
            "TRAIN(065): [ 20/196] Batch: 0.0800 (0.0972) Data: 0.0509 (0.0675) Loss: 2.2529 (2.3886)\n",
            "TRAIN(065): [ 30/196] Batch: 0.0669 (0.0900) Data: 0.0588 (0.0616) Loss: 2.4244 (2.3890)\n",
            "TRAIN(065): [ 40/196] Batch: 0.0764 (0.0868) Data: 0.0490 (0.0587) Loss: 2.4684 (2.3775)\n",
            "TRAIN(065): [ 50/196] Batch: 0.0702 (0.0845) Data: 0.0565 (0.0565) Loss: 2.4678 (2.3826)\n",
            "TRAIN(065): [ 60/196] Batch: 0.0789 (0.0832) Data: 0.0503 (0.0554) Loss: 2.4244 (2.3913)\n",
            "TRAIN(065): [ 70/196] Batch: 0.0729 (0.0821) Data: 0.0594 (0.0547) Loss: 2.2514 (2.3872)\n",
            "TRAIN(065): [ 80/196] Batch: 0.0824 (0.0815) Data: 0.0468 (0.0537) Loss: 2.4363 (2.3905)\n",
            "TRAIN(065): [ 90/196] Batch: 0.0838 (0.0810) Data: 0.0429 (0.0527) Loss: 2.2258 (2.3922)\n",
            "TRAIN(065): [100/196] Batch: 0.0715 (0.0805) Data: 0.0522 (0.0519) Loss: 2.3511 (2.3958)\n",
            "TRAIN(065): [110/196] Batch: 0.0734 (0.0803) Data: 0.0528 (0.0517) Loss: 2.4522 (2.3996)\n",
            "TRAIN(065): [120/196] Batch: 0.0957 (0.0802) Data: 0.0481 (0.0513) Loss: 2.3829 (2.4015)\n",
            "TRAIN(065): [130/196] Batch: 0.0822 (0.0798) Data: 0.0554 (0.0513) Loss: 2.4809 (2.4017)\n",
            "TRAIN(065): [140/196] Batch: 0.0666 (0.0796) Data: 0.0606 (0.0513) Loss: 2.3534 (2.4028)\n",
            "TRAIN(065): [150/196] Batch: 0.0702 (0.0794) Data: 0.0563 (0.0513) Loss: 2.4940 (2.4050)\n",
            "TRAIN(065): [160/196] Batch: 0.0808 (0.0792) Data: 0.0523 (0.0511) Loss: 2.5412 (2.4066)\n",
            "TRAIN(065): [170/196] Batch: 0.0815 (0.0791) Data: 0.0507 (0.0510) Loss: 2.3104 (2.4098)\n",
            "TRAIN(065): [180/196] Batch: 0.0820 (0.0789) Data: 0.0523 (0.0510) Loss: 2.3335 (2.4072)\n",
            "TRAIN(065): [190/196] Batch: 0.0765 (0.0787) Data: 0.0633 (0.0510) Loss: 2.3312 (2.4098)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(65)         0:00:15         0:00:09         0:00:05          2.4086\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(066): [ 10/196] Batch: 0.0691 (0.1213) Data: 0.0549 (0.0866) Loss: 2.3045 (2.3800)\n",
            "TRAIN(066): [ 20/196] Batch: 0.0846 (0.0991) Data: 0.0512 (0.0703) Loss: 2.4049 (2.4014)\n",
            "TRAIN(066): [ 30/196] Batch: 0.0762 (0.0916) Data: 0.0536 (0.0636) Loss: 2.2994 (2.3998)\n",
            "TRAIN(066): [ 40/196] Batch: 0.0836 (0.0877) Data: 0.0572 (0.0602) Loss: 2.5311 (2.4097)\n",
            "TRAIN(066): [ 50/196] Batch: 0.0878 (0.0854) Data: 0.0500 (0.0580) Loss: 2.3451 (2.4064)\n",
            "TRAIN(066): [ 60/196] Batch: 0.0744 (0.0839) Data: 0.0505 (0.0562) Loss: 2.3228 (2.4044)\n",
            "TRAIN(066): [ 70/196] Batch: 0.0682 (0.0829) Data: 0.0575 (0.0545) Loss: 2.5312 (2.4079)\n",
            "TRAIN(066): [ 80/196] Batch: 0.0907 (0.0827) Data: 0.0395 (0.0533) Loss: 2.4783 (2.4021)\n",
            "TRAIN(066): [ 90/196] Batch: 0.0716 (0.0819) Data: 0.0508 (0.0524) Loss: 2.4130 (2.4038)\n",
            "TRAIN(066): [100/196] Batch: 0.0800 (0.0815) Data: 0.0472 (0.0519) Loss: 2.4588 (2.4053)\n",
            "TRAIN(066): [110/196] Batch: 0.0822 (0.0810) Data: 0.0507 (0.0514) Loss: 2.3877 (2.4064)\n",
            "TRAIN(066): [120/196] Batch: 0.0667 (0.0806) Data: 0.0602 (0.0512) Loss: 2.4593 (2.4048)\n",
            "TRAIN(066): [130/196] Batch: 0.0831 (0.0803) Data: 0.0494 (0.0510) Loss: 2.3601 (2.4036)\n",
            "TRAIN(066): [140/196] Batch: 0.0740 (0.0800) Data: 0.0548 (0.0508) Loss: 2.2770 (2.4027)\n",
            "TRAIN(066): [150/196] Batch: 0.0835 (0.0798) Data: 0.0502 (0.0507) Loss: 2.4203 (2.4013)\n",
            "TRAIN(066): [160/196] Batch: 0.0719 (0.0795) Data: 0.0573 (0.0507) Loss: 2.3920 (2.4003)\n",
            "TRAIN(066): [170/196] Batch: 0.0751 (0.0793) Data: 0.0606 (0.0506) Loss: 2.3280 (2.3975)\n",
            "TRAIN(066): [180/196] Batch: 0.0806 (0.0791) Data: 0.0572 (0.0505) Loss: 2.2410 (2.3939)\n",
            "TRAIN(066): [190/196] Batch: 0.0762 (0.0790) Data: 0.0625 (0.0505) Loss: 2.5320 (2.3951)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(66)         0:00:15         0:00:09         0:00:05          2.3948\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(067): [ 10/196] Batch: 0.0741 (0.1195) Data: 0.0485 (0.0853) Loss: 2.4956 (2.3876)\n",
            "TRAIN(067): [ 20/196] Batch: 0.0880 (0.0986) Data: 0.0414 (0.0676) Loss: 2.3031 (2.3906)\n",
            "TRAIN(067): [ 30/196] Batch: 0.0811 (0.0910) Data: 0.0493 (0.0618) Loss: 2.3592 (2.4028)\n",
            "TRAIN(067): [ 40/196] Batch: 0.0728 (0.0871) Data: 0.0575 (0.0580) Loss: 2.2923 (2.3869)\n",
            "TRAIN(067): [ 50/196] Batch: 0.0652 (0.0855) Data: 0.0517 (0.0555) Loss: 2.4250 (2.3860)\n",
            "TRAIN(067): [ 60/196] Batch: 0.0640 (0.0841) Data: 0.0502 (0.0542) Loss: 2.4206 (2.3864)\n",
            "TRAIN(067): [ 70/196] Batch: 0.0671 (0.0831) Data: 0.0537 (0.0528) Loss: 2.3131 (2.3878)\n",
            "TRAIN(067): [ 80/196] Batch: 0.0804 (0.0826) Data: 0.0389 (0.0519) Loss: 2.4174 (2.3864)\n",
            "TRAIN(067): [ 90/196] Batch: 0.0879 (0.0818) Data: 0.0520 (0.0514) Loss: 2.2819 (2.3824)\n",
            "TRAIN(067): [100/196] Batch: 0.0796 (0.0812) Data: 0.0505 (0.0510) Loss: 2.2471 (2.3836)\n",
            "TRAIN(067): [110/196] Batch: 0.0855 (0.0807) Data: 0.0498 (0.0508) Loss: 2.2140 (2.3788)\n",
            "TRAIN(067): [120/196] Batch: 0.0644 (0.0803) Data: 0.0618 (0.0508) Loss: 2.2624 (2.3803)\n",
            "TRAIN(067): [130/196] Batch: 0.0763 (0.0801) Data: 0.0505 (0.0506) Loss: 2.5201 (2.3825)\n",
            "TRAIN(067): [140/196] Batch: 0.0829 (0.0798) Data: 0.0542 (0.0506) Loss: 2.3616 (2.3823)\n",
            "TRAIN(067): [150/196] Batch: 0.0700 (0.0795) Data: 0.0624 (0.0505) Loss: 2.3428 (2.3809)\n",
            "TRAIN(067): [160/196] Batch: 0.0800 (0.0793) Data: 0.0520 (0.0505) Loss: 2.3431 (2.3779)\n",
            "TRAIN(067): [170/196] Batch: 0.0717 (0.0791) Data: 0.0574 (0.0505) Loss: 2.3596 (2.3757)\n",
            "TRAIN(067): [180/196] Batch: 0.0839 (0.0790) Data: 0.0492 (0.0504) Loss: 2.3281 (2.3739)\n",
            "TRAIN(067): [190/196] Batch: 0.0740 (0.0788) Data: 0.0632 (0.0505) Loss: 2.2100 (2.3721)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(67)         0:00:15         0:00:09         0:00:05          2.3721\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(068): [ 10/196] Batch: 0.0769 (0.1193) Data: 0.0478 (0.0836) Loss: 2.3862 (2.3433)\n",
            "TRAIN(068): [ 20/196] Batch: 0.0738 (0.0980) Data: 0.0498 (0.0661) Loss: 2.3703 (2.3676)\n",
            "TRAIN(068): [ 30/196] Batch: 0.0748 (0.0913) Data: 0.0503 (0.0592) Loss: 2.1911 (2.3691)\n",
            "TRAIN(068): [ 40/196] Batch: 0.0748 (0.0877) Data: 0.0638 (0.0563) Loss: 2.3260 (2.3642)\n",
            "TRAIN(068): [ 50/196] Batch: 0.0760 (0.0858) Data: 0.0488 (0.0537) Loss: 2.3897 (2.3587)\n",
            "TRAIN(068): [ 60/196] Batch: 0.0802 (0.0844) Data: 0.0450 (0.0521) Loss: 2.1972 (2.3577)\n",
            "TRAIN(068): [ 70/196] Batch: 0.0723 (0.0832) Data: 0.0543 (0.0511) Loss: 2.4663 (2.3624)\n",
            "TRAIN(068): [ 80/196] Batch: 0.0700 (0.0823) Data: 0.0561 (0.0507) Loss: 2.3126 (2.3629)\n",
            "TRAIN(068): [ 90/196] Batch: 0.0829 (0.0817) Data: 0.0497 (0.0507) Loss: 2.4228 (2.3699)\n",
            "TRAIN(068): [100/196] Batch: 0.0713 (0.0811) Data: 0.0548 (0.0505) Loss: 2.2727 (2.3651)\n",
            "TRAIN(068): [110/196] Batch: 0.0806 (0.0807) Data: 0.0500 (0.0502) Loss: 2.1736 (2.3644)\n",
            "TRAIN(068): [120/196] Batch: 0.0724 (0.0803) Data: 0.0555 (0.0502) Loss: 2.4351 (2.3670)\n",
            "TRAIN(068): [130/196] Batch: 0.0857 (0.0800) Data: 0.0513 (0.0501) Loss: 2.2740 (2.3642)\n",
            "TRAIN(068): [140/196] Batch: 0.0765 (0.0797) Data: 0.0551 (0.0501) Loss: 2.3129 (2.3617)\n",
            "TRAIN(068): [150/196] Batch: 0.0714 (0.0794) Data: 0.0583 (0.0500) Loss: 2.1384 (2.3570)\n",
            "TRAIN(068): [160/196] Batch: 0.0707 (0.0792) Data: 0.0545 (0.0500) Loss: 2.4376 (2.3551)\n",
            "TRAIN(068): [170/196] Batch: 0.0748 (0.0790) Data: 0.0625 (0.0500) Loss: 2.2856 (2.3557)\n",
            "TRAIN(068): [180/196] Batch: 0.0782 (0.0789) Data: 0.0543 (0.0500) Loss: 2.5047 (2.3555)\n",
            "TRAIN(068): [190/196] Batch: 0.0761 (0.0787) Data: 0.0622 (0.0500) Loss: 2.2672 (2.3557)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(68)         0:00:15         0:00:09         0:00:05          2.3574\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(069): [ 10/196] Batch: 0.0701 (0.1259) Data: 0.0516 (0.0801) Loss: 2.4226 (2.3072)\n",
            "TRAIN(069): [ 20/196] Batch: 0.0746 (0.1038) Data: 0.0477 (0.0598) Loss: 2.3309 (2.3381)\n",
            "TRAIN(069): [ 30/196] Batch: 0.0601 (0.0961) Data: 0.0561 (0.0525) Loss: 2.5359 (2.3558)\n",
            "TRAIN(069): [ 40/196] Batch: 0.0820 (0.0916) Data: 0.0412 (0.0507) Loss: 2.2423 (2.3489)\n",
            "TRAIN(069): [ 50/196] Batch: 0.0796 (0.0886) Data: 0.0522 (0.0499) Loss: 2.3286 (2.3366)\n",
            "TRAIN(069): [ 60/196] Batch: 0.0857 (0.0865) Data: 0.0526 (0.0502) Loss: 2.4758 (2.3365)\n",
            "TRAIN(069): [ 70/196] Batch: 0.0765 (0.0849) Data: 0.0545 (0.0503) Loss: 2.3240 (2.3393)\n",
            "TRAIN(069): [ 80/196] Batch: 0.0680 (0.0838) Data: 0.0592 (0.0501) Loss: 2.4012 (2.3446)\n",
            "TRAIN(069): [ 90/196] Batch: 0.0757 (0.0829) Data: 0.0613 (0.0499) Loss: 2.5075 (2.3467)\n",
            "TRAIN(069): [100/196] Batch: 0.0658 (0.0822) Data: 0.0598 (0.0497) Loss: 2.4065 (2.3484)\n",
            "TRAIN(069): [110/196] Batch: 0.0800 (0.0818) Data: 0.0497 (0.0498) Loss: 2.2865 (2.3522)\n",
            "TRAIN(069): [120/196] Batch: 0.0749 (0.0813) Data: 0.0554 (0.0497) Loss: 2.3383 (2.3510)\n",
            "TRAIN(069): [130/196] Batch: 0.0812 (0.0809) Data: 0.0517 (0.0496) Loss: 2.3029 (2.3476)\n",
            "TRAIN(069): [140/196] Batch: 0.0748 (0.0805) Data: 0.0609 (0.0498) Loss: 2.3819 (2.3435)\n",
            "TRAIN(069): [150/196] Batch: 0.0754 (0.0803) Data: 0.0518 (0.0497) Loss: 2.5674 (2.3428)\n",
            "TRAIN(069): [160/196] Batch: 0.0729 (0.0800) Data: 0.0542 (0.0496) Loss: 2.2150 (2.3441)\n",
            "TRAIN(069): [170/196] Batch: 0.0812 (0.0798) Data: 0.0494 (0.0496) Loss: 2.2334 (2.3446)\n",
            "TRAIN(069): [180/196] Batch: 0.0741 (0.0796) Data: 0.0516 (0.0494) Loss: 2.5146 (2.3440)\n",
            "TRAIN(069): [190/196] Batch: 0.0704 (0.0794) Data: 0.0616 (0.0493) Loss: 2.4951 (2.3454)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(69)         0:00:15         0:00:09         0:00:05          2.3454\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(070): [ 10/196] Batch: 0.0835 (0.1305) Data: 0.0293 (0.0822) Loss: 2.2976 (2.3265)\n",
            "TRAIN(070): [ 20/196] Batch: 0.0879 (0.1047) Data: 0.0489 (0.0625) Loss: 2.4094 (2.3399)\n",
            "TRAIN(070): [ 30/196] Batch: 0.0806 (0.0951) Data: 0.0493 (0.0585) Loss: 2.3209 (2.3323)\n",
            "TRAIN(070): [ 40/196] Batch: 0.0705 (0.0903) Data: 0.0543 (0.0558) Loss: 2.2988 (2.3366)\n",
            "TRAIN(070): [ 50/196] Batch: 0.0886 (0.0875) Data: 0.0503 (0.0543) Loss: 2.4506 (2.3381)\n",
            "TRAIN(070): [ 60/196] Batch: 0.0869 (0.0856) Data: 0.0506 (0.0535) Loss: 2.5017 (2.3480)\n",
            "TRAIN(070): [ 70/196] Batch: 0.0713 (0.0841) Data: 0.0603 (0.0531) Loss: 2.2336 (2.3397)\n",
            "TRAIN(070): [ 80/196] Batch: 0.0659 (0.0831) Data: 0.0578 (0.0526) Loss: 2.4105 (2.3406)\n",
            "TRAIN(070): [ 90/196] Batch: 0.0763 (0.0824) Data: 0.0553 (0.0522) Loss: 2.2115 (2.3406)\n",
            "TRAIN(070): [100/196] Batch: 0.0730 (0.0817) Data: 0.0533 (0.0517) Loss: 2.4211 (2.3406)\n",
            "TRAIN(070): [110/196] Batch: 0.0703 (0.0812) Data: 0.0553 (0.0513) Loss: 2.3465 (2.3435)\n",
            "TRAIN(070): [120/196] Batch: 0.0830 (0.0809) Data: 0.0476 (0.0511) Loss: 2.2743 (2.3382)\n",
            "TRAIN(070): [130/196] Batch: 0.0759 (0.0804) Data: 0.0622 (0.0512) Loss: 2.6513 (2.3441)\n",
            "TRAIN(070): [140/196] Batch: 0.0701 (0.0801) Data: 0.0616 (0.0511) Loss: 2.4110 (2.3420)\n",
            "TRAIN(070): [150/196] Batch: 0.0684 (0.0799) Data: 0.0521 (0.0508) Loss: 2.4298 (2.3421)\n",
            "TRAIN(070): [160/196] Batch: 0.0676 (0.0798) Data: 0.0571 (0.0505) Loss: 2.2278 (2.3404)\n",
            "TRAIN(070): [170/196] Batch: 0.0937 (0.0797) Data: 0.0436 (0.0505) Loss: 2.2665 (2.3372)\n",
            "TRAIN(070): [180/196] Batch: 0.0666 (0.0796) Data: 0.0472 (0.0499) Loss: 2.4498 (2.3395)\n",
            "TRAIN(070): [190/196] Batch: 0.0732 (0.0794) Data: 0.0628 (0.0496) Loss: 2.4102 (2.3404)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(70)         0:00:15         0:00:09         0:00:05          2.3408\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(071): [ 10/196] Batch: 0.0745 (0.1193) Data: 0.0482 (0.0843) Loss: 2.3503 (2.3697)\n",
            "TRAIN(071): [ 20/196] Batch: 0.0660 (0.0975) Data: 0.0555 (0.0680) Loss: 2.3075 (2.3338)\n",
            "TRAIN(071): [ 30/196] Batch: 0.0707 (0.0905) Data: 0.0531 (0.0611) Loss: 2.3622 (2.3327)\n",
            "TRAIN(071): [ 40/196] Batch: 0.0768 (0.0868) Data: 0.0561 (0.0577) Loss: 2.3025 (2.3329)\n",
            "TRAIN(071): [ 50/196] Batch: 0.0860 (0.0848) Data: 0.0480 (0.0559) Loss: 2.5094 (2.3257)\n",
            "TRAIN(071): [ 60/196] Batch: 0.0711 (0.0833) Data: 0.0537 (0.0548) Loss: 2.2319 (2.3280)\n",
            "TRAIN(071): [ 70/196] Batch: 0.0779 (0.0823) Data: 0.0537 (0.0541) Loss: 2.3079 (2.3391)\n",
            "TRAIN(071): [ 80/196] Batch: 0.0722 (0.0815) Data: 0.0557 (0.0536) Loss: 2.3631 (2.3369)\n",
            "TRAIN(071): [ 90/196] Batch: 0.0722 (0.0808) Data: 0.0633 (0.0532) Loss: 2.2749 (2.3348)\n",
            "TRAIN(071): [100/196] Batch: 0.0806 (0.0805) Data: 0.0478 (0.0527) Loss: 2.1595 (2.3288)\n",
            "TRAIN(071): [110/196] Batch: 0.0861 (0.0801) Data: 0.0464 (0.0523) Loss: 2.2648 (2.3288)\n",
            "TRAIN(071): [120/196] Batch: 0.0639 (0.0797) Data: 0.0610 (0.0518) Loss: 2.4862 (2.3275)\n",
            "TRAIN(071): [130/196] Batch: 0.0735 (0.0795) Data: 0.0485 (0.0513) Loss: 2.3591 (2.3277)\n",
            "TRAIN(071): [140/196] Batch: 0.0903 (0.0794) Data: 0.0478 (0.0508) Loss: 2.3717 (2.3295)\n",
            "TRAIN(071): [150/196] Batch: 0.0790 (0.0792) Data: 0.0545 (0.0508) Loss: 2.1671 (2.3270)\n",
            "TRAIN(071): [160/196] Batch: 0.0634 (0.0791) Data: 0.0510 (0.0504) Loss: 2.2819 (2.3277)\n",
            "TRAIN(071): [170/196] Batch: 0.0303 (0.0791) Data: 0.0526 (0.0501) Loss: 2.1811 (2.3268)\n",
            "TRAIN(071): [180/196] Batch: 0.0745 (0.0789) Data: 0.0571 (0.0499) Loss: 2.3760 (2.3249)\n",
            "TRAIN(071): [190/196] Batch: 0.0756 (0.0788) Data: 0.0615 (0.0500) Loss: 2.4394 (2.3224)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(71)         0:00:15         0:00:09         0:00:05          2.3210\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(072): [ 10/196] Batch: 0.0698 (0.1180) Data: 0.0550 (0.0836) Loss: 2.2856 (2.2716)\n",
            "TRAIN(072): [ 20/196] Batch: 0.0802 (0.0979) Data: 0.0489 (0.0676) Loss: 2.5402 (2.3276)\n",
            "TRAIN(072): [ 30/196] Batch: 0.0783 (0.0906) Data: 0.0518 (0.0623) Loss: 2.1805 (2.3119)\n",
            "TRAIN(072): [ 40/196] Batch: 0.0796 (0.0869) Data: 0.0569 (0.0593) Loss: 2.2375 (2.3134)\n",
            "TRAIN(072): [ 50/196] Batch: 0.0720 (0.0847) Data: 0.0567 (0.0575) Loss: 2.1464 (2.3091)\n",
            "TRAIN(072): [ 60/196] Batch: 0.0736 (0.0834) Data: 0.0510 (0.0558) Loss: 2.2877 (2.3087)\n",
            "TRAIN(072): [ 70/196] Batch: 0.0851 (0.0824) Data: 0.0507 (0.0550) Loss: 2.2660 (2.3134)\n",
            "TRAIN(072): [ 80/196] Batch: 0.0709 (0.0815) Data: 0.0588 (0.0544) Loss: 2.4201 (2.3091)\n",
            "TRAIN(072): [ 90/196] Batch: 0.0749 (0.0809) Data: 0.0576 (0.0539) Loss: 2.2557 (2.3046)\n",
            "TRAIN(072): [100/196] Batch: 0.0753 (0.0805) Data: 0.0557 (0.0534) Loss: 2.2551 (2.3076)\n",
            "TRAIN(072): [110/196] Batch: 0.0611 (0.0803) Data: 0.0541 (0.0527) Loss: 2.2744 (2.3063)\n",
            "TRAIN(072): [120/196] Batch: 0.0778 (0.0800) Data: 0.0518 (0.0518) Loss: 2.4984 (2.3099)\n",
            "TRAIN(072): [130/196] Batch: 0.0852 (0.0799) Data: 0.0443 (0.0513) Loss: 2.3843 (2.3127)\n",
            "TRAIN(072): [140/196] Batch: 0.0748 (0.0797) Data: 0.0527 (0.0508) Loss: 2.3374 (2.3129)\n",
            "TRAIN(072): [150/196] Batch: 0.0617 (0.0795) Data: 0.0557 (0.0505) Loss: 2.2586 (2.3107)\n",
            "TRAIN(072): [160/196] Batch: 0.0801 (0.0793) Data: 0.0506 (0.0503) Loss: 2.5331 (2.3117)\n",
            "TRAIN(072): [170/196] Batch: 0.0865 (0.0792) Data: 0.0523 (0.0503) Loss: 2.2237 (2.3144)\n",
            "TRAIN(072): [180/196] Batch: 0.0697 (0.0789) Data: 0.0613 (0.0502) Loss: 2.2581 (2.3133)\n",
            "TRAIN(072): [190/196] Batch: 0.0747 (0.0788) Data: 0.0622 (0.0501) Loss: 2.3429 (2.3107)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(72)         0:00:15         0:00:09         0:00:05          2.3092\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(073): [ 10/196] Batch: 0.0824 (0.1208) Data: 0.0384 (0.0782) Loss: 2.1841 (2.3054)\n",
            "TRAIN(073): [ 20/196] Batch: 0.0725 (0.0977) Data: 0.0547 (0.0643) Loss: 2.3878 (2.2783)\n",
            "TRAIN(073): [ 30/196] Batch: 0.0823 (0.0906) Data: 0.0557 (0.0596) Loss: 2.2575 (2.2889)\n",
            "TRAIN(073): [ 40/196] Batch: 0.0856 (0.0872) Data: 0.0477 (0.0570) Loss: 2.3123 (2.2876)\n",
            "TRAIN(073): [ 50/196] Batch: 0.0753 (0.0846) Data: 0.0616 (0.0555) Loss: 2.2712 (2.2902)\n",
            "TRAIN(073): [ 60/196] Batch: 0.0707 (0.0834) Data: 0.0543 (0.0540) Loss: 2.4235 (2.2900)\n",
            "TRAIN(073): [ 70/196] Batch: 0.0824 (0.0824) Data: 0.0494 (0.0534) Loss: 2.2679 (2.2863)\n",
            "TRAIN(073): [ 80/196] Batch: 0.0857 (0.0816) Data: 0.0528 (0.0529) Loss: 2.0965 (2.2819)\n",
            "TRAIN(073): [ 90/196] Batch: 0.0742 (0.0812) Data: 0.0501 (0.0519) Loss: 2.2764 (2.2822)\n",
            "TRAIN(073): [100/196] Batch: 0.0863 (0.0809) Data: 0.0426 (0.0512) Loss: 2.4751 (2.2881)\n",
            "TRAIN(073): [110/196] Batch: 0.0807 (0.0806) Data: 0.0488 (0.0506) Loss: 2.2422 (2.2894)\n",
            "TRAIN(073): [120/196] Batch: 0.0751 (0.0803) Data: 0.0482 (0.0500) Loss: 2.4662 (2.2920)\n",
            "TRAIN(073): [130/196] Batch: 0.0850 (0.0800) Data: 0.0430 (0.0495) Loss: 2.3676 (2.2926)\n",
            "TRAIN(073): [140/196] Batch: 0.0796 (0.0797) Data: 0.0517 (0.0495) Loss: 2.3282 (2.2909)\n",
            "TRAIN(073): [150/196] Batch: 0.0831 (0.0795) Data: 0.0566 (0.0496) Loss: 2.3705 (2.2891)\n",
            "TRAIN(073): [160/196] Batch: 0.0660 (0.0792) Data: 0.0612 (0.0497) Loss: 2.3769 (2.2885)\n",
            "TRAIN(073): [170/196] Batch: 0.0742 (0.0791) Data: 0.0567 (0.0497) Loss: 2.3453 (2.2838)\n",
            "TRAIN(073): [180/196] Batch: 0.0657 (0.0789) Data: 0.0632 (0.0496) Loss: 2.2391 (2.2831)\n",
            "TRAIN(073): [190/196] Batch: 0.0764 (0.0788) Data: 0.0610 (0.0498) Loss: 2.3583 (2.2832)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(73)         0:00:15         0:00:09         0:00:05          2.2840\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(074): [ 10/196] Batch: 0.0697 (0.1166) Data: 0.0613 (0.0855) Loss: 2.1022 (2.2422)\n",
            "TRAIN(074): [ 20/196] Batch: 0.0797 (0.0974) Data: 0.0491 (0.0693) Loss: 2.3683 (2.2687)\n",
            "TRAIN(074): [ 30/196] Batch: 0.0678 (0.0902) Data: 0.0554 (0.0634) Loss: 2.3080 (2.2709)\n",
            "TRAIN(074): [ 40/196] Batch: 0.0737 (0.0866) Data: 0.0576 (0.0599) Loss: 2.2181 (2.2845)\n",
            "TRAIN(074): [ 50/196] Batch: 0.0812 (0.0844) Data: 0.0598 (0.0581) Loss: 2.3320 (2.2865)\n",
            "TRAIN(074): [ 60/196] Batch: 0.0802 (0.0832) Data: 0.0500 (0.0563) Loss: 2.1728 (2.2828)\n",
            "TRAIN(074): [ 70/196] Batch: 0.0891 (0.0825) Data: 0.0408 (0.0548) Loss: 2.2645 (2.2801)\n",
            "TRAIN(074): [ 80/196] Batch: 0.0752 (0.0817) Data: 0.0517 (0.0534) Loss: 2.1927 (2.2782)\n",
            "TRAIN(074): [ 90/196] Batch: 0.0818 (0.0813) Data: 0.0442 (0.0526) Loss: 2.3203 (2.2867)\n",
            "TRAIN(074): [100/196] Batch: 0.0751 (0.0809) Data: 0.0503 (0.0518) Loss: 2.2856 (2.2869)\n",
            "TRAIN(074): [110/196] Batch: 0.0706 (0.0805) Data: 0.0526 (0.0511) Loss: 2.3422 (2.2842)\n",
            "TRAIN(074): [120/196] Batch: 0.0737 (0.0801) Data: 0.0600 (0.0509) Loss: 2.2005 (2.2826)\n",
            "TRAIN(074): [130/196] Batch: 0.0756 (0.0799) Data: 0.0542 (0.0507) Loss: 2.0619 (2.2797)\n",
            "TRAIN(074): [140/196] Batch: 0.0763 (0.0796) Data: 0.0618 (0.0509) Loss: 2.2379 (2.2801)\n",
            "TRAIN(074): [150/196] Batch: 0.0719 (0.0794) Data: 0.0567 (0.0508) Loss: 2.2412 (2.2792)\n",
            "TRAIN(074): [160/196] Batch: 0.0880 (0.0792) Data: 0.0529 (0.0508) Loss: 2.2610 (2.2810)\n",
            "TRAIN(074): [170/196] Batch: 0.0833 (0.0790) Data: 0.0495 (0.0507) Loss: 2.2691 (2.2818)\n",
            "TRAIN(074): [180/196] Batch: 0.0695 (0.0789) Data: 0.0565 (0.0507) Loss: 2.2810 (2.2806)\n",
            "TRAIN(074): [190/196] Batch: 0.0746 (0.0787) Data: 0.0614 (0.0508) Loss: 2.0603 (2.2796)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(74)         0:00:15         0:00:09         0:00:05          2.2796\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(075): [ 10/196] Batch: 0.0776 (0.1226) Data: 0.0461 (0.0861) Loss: 2.2165 (2.2739)\n",
            "TRAIN(075): [ 20/196] Batch: 0.0782 (0.0994) Data: 0.0539 (0.0695) Loss: 2.3971 (2.2845)\n",
            "TRAIN(075): [ 30/196] Batch: 0.0749 (0.0916) Data: 0.0556 (0.0629) Loss: 2.2537 (2.2819)\n",
            "TRAIN(075): [ 40/196] Batch: 0.0792 (0.0879) Data: 0.0498 (0.0594) Loss: 1.9958 (2.2844)\n",
            "TRAIN(075): [ 50/196] Batch: 0.0675 (0.0856) Data: 0.0496 (0.0563) Loss: 2.4444 (2.2874)\n",
            "TRAIN(075): [ 60/196] Batch: 0.0954 (0.0846) Data: 0.0520 (0.0552) Loss: 2.1665 (2.2777)\n",
            "TRAIN(075): [ 70/196] Batch: 0.0765 (0.0837) Data: 0.0484 (0.0543) Loss: 2.1980 (2.2804)\n",
            "TRAIN(075): [ 80/196] Batch: 0.0745 (0.0826) Data: 0.0631 (0.0539) Loss: 2.3657 (2.2782)\n",
            "TRAIN(075): [ 90/196] Batch: 0.0827 (0.0822) Data: 0.0479 (0.0529) Loss: 2.2786 (2.2796)\n",
            "TRAIN(075): [100/196] Batch: 0.0822 (0.0815) Data: 0.0548 (0.0525) Loss: 2.2679 (2.2782)\n",
            "TRAIN(075): [110/196] Batch: 0.0652 (0.0810) Data: 0.0609 (0.0523) Loss: 2.1763 (2.2725)\n",
            "TRAIN(075): [120/196] Batch: 0.0708 (0.0807) Data: 0.0565 (0.0521) Loss: 2.1670 (2.2769)\n",
            "TRAIN(075): [130/196] Batch: 0.0821 (0.0804) Data: 0.0521 (0.0517) Loss: 2.3010 (2.2777)\n",
            "TRAIN(075): [140/196] Batch: 0.0782 (0.0801) Data: 0.0504 (0.0515) Loss: 2.3622 (2.2750)\n",
            "TRAIN(075): [150/196] Batch: 0.0715 (0.0798) Data: 0.0615 (0.0515) Loss: 2.1430 (2.2697)\n",
            "TRAIN(075): [160/196] Batch: 0.0726 (0.0796) Data: 0.0555 (0.0514) Loss: 2.2314 (2.2687)\n",
            "TRAIN(075): [170/196] Batch: 0.0692 (0.0794) Data: 0.0583 (0.0512) Loss: 2.2909 (2.2699)\n",
            "TRAIN(075): [180/196] Batch: 0.0686 (0.0792) Data: 0.0552 (0.0511) Loss: 2.3493 (2.2718)\n",
            "TRAIN(075): [190/196] Batch: 0.0750 (0.0791) Data: 0.0620 (0.0511) Loss: 2.1826 (2.2727)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(75)         0:00:15         0:00:10         0:00:05          2.2706\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(076): [ 10/196] Batch: 0.0789 (0.1180) Data: 0.0499 (0.0826) Loss: 2.1980 (2.2631)\n",
            "TRAIN(076): [ 20/196] Batch: 0.0726 (0.0976) Data: 0.0495 (0.0661) Loss: 2.4078 (2.2403)\n",
            "TRAIN(076): [ 30/196] Batch: 0.0733 (0.0908) Data: 0.0495 (0.0591) Loss: 2.0815 (2.2303)\n",
            "TRAIN(076): [ 40/196] Batch: 0.0769 (0.0874) Data: 0.0598 (0.0556) Loss: 2.3037 (2.2501)\n",
            "TRAIN(076): [ 50/196] Batch: 0.0803 (0.0854) Data: 0.0482 (0.0533) Loss: 2.2875 (2.2572)\n",
            "TRAIN(076): [ 60/196] Batch: 0.0739 (0.0840) Data: 0.0467 (0.0519) Loss: 2.3533 (2.2595)\n",
            "TRAIN(076): [ 70/196] Batch: 0.0824 (0.0830) Data: 0.0490 (0.0509) Loss: 2.1378 (2.2594)\n",
            "TRAIN(076): [ 80/196] Batch: 0.0802 (0.0822) Data: 0.0501 (0.0507) Loss: 2.1965 (2.2635)\n",
            "TRAIN(076): [ 90/196] Batch: 0.0810 (0.0815) Data: 0.0512 (0.0506) Loss: 2.1571 (2.2678)\n",
            "TRAIN(076): [100/196] Batch: 0.0758 (0.0810) Data: 0.0524 (0.0503) Loss: 2.2731 (2.2648)\n",
            "TRAIN(076): [110/196] Batch: 0.0819 (0.0806) Data: 0.0507 (0.0502) Loss: 2.1202 (2.2665)\n",
            "TRAIN(076): [120/196] Batch: 0.0803 (0.0802) Data: 0.0523 (0.0500) Loss: 2.2237 (2.2662)\n",
            "TRAIN(076): [130/196] Batch: 0.0704 (0.0799) Data: 0.0582 (0.0500) Loss: 2.1150 (2.2619)\n",
            "TRAIN(076): [140/196] Batch: 0.0711 (0.0796) Data: 0.0536 (0.0499) Loss: 2.2270 (2.2574)\n",
            "TRAIN(076): [150/196] Batch: 0.0791 (0.0794) Data: 0.0506 (0.0500) Loss: 2.2930 (2.2584)\n",
            "TRAIN(076): [160/196] Batch: 0.0718 (0.0792) Data: 0.0562 (0.0499) Loss: 2.2913 (2.2589)\n",
            "TRAIN(076): [170/196] Batch: 0.0678 (0.0790) Data: 0.0608 (0.0500) Loss: 2.2058 (2.2564)\n",
            "TRAIN(076): [180/196] Batch: 0.0815 (0.0789) Data: 0.0562 (0.0500) Loss: 2.3495 (2.2571)\n",
            "TRAIN(076): [190/196] Batch: 0.0755 (0.0787) Data: 0.0629 (0.0501) Loss: 2.2033 (2.2563)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(76)         0:00:15         0:00:09         0:00:05          2.2562\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(077): [ 10/196] Batch: 0.0658 (0.1365) Data: 0.0339 (0.0795) Loss: 2.2342 (2.2014)\n",
            "TRAIN(077): [ 20/196] Batch: 0.0744 (0.1067) Data: 0.0529 (0.0613) Loss: 2.2800 (2.1968)\n",
            "TRAIN(077): [ 30/196] Batch: 0.0814 (0.0973) Data: 0.0491 (0.0560) Loss: 2.3424 (2.2076)\n",
            "TRAIN(077): [ 40/196] Batch: 0.0620 (0.0924) Data: 0.0515 (0.0521) Loss: 2.1914 (2.2121)\n",
            "TRAIN(077): [ 50/196] Batch: 0.0831 (0.0892) Data: 0.0506 (0.0512) Loss: 2.1419 (2.2128)\n",
            "TRAIN(077): [ 60/196] Batch: 0.0661 (0.0868) Data: 0.0606 (0.0510) Loss: 2.1406 (2.2167)\n",
            "TRAIN(077): [ 70/196] Batch: 0.0763 (0.0854) Data: 0.0550 (0.0512) Loss: 2.2359 (2.2196)\n",
            "TRAIN(077): [ 80/196] Batch: 0.0848 (0.0843) Data: 0.0506 (0.0508) Loss: 2.3166 (2.2238)\n",
            "TRAIN(077): [ 90/196] Batch: 0.0811 (0.0834) Data: 0.0487 (0.0508) Loss: 2.1473 (2.2255)\n",
            "TRAIN(077): [100/196] Batch: 0.0756 (0.0826) Data: 0.0554 (0.0507) Loss: 2.2567 (2.2315)\n",
            "TRAIN(077): [110/196] Batch: 0.0858 (0.0821) Data: 0.0516 (0.0506) Loss: 2.2927 (2.2341)\n",
            "TRAIN(077): [120/196] Batch: 0.0858 (0.0816) Data: 0.0508 (0.0505) Loss: 2.3448 (2.2365)\n",
            "TRAIN(077): [130/196] Batch: 0.0818 (0.0812) Data: 0.0557 (0.0504) Loss: 1.9889 (2.2348)\n",
            "TRAIN(077): [140/196] Batch: 0.0767 (0.0808) Data: 0.0556 (0.0503) Loss: 2.2612 (2.2354)\n",
            "TRAIN(077): [150/196] Batch: 0.0762 (0.0805) Data: 0.0608 (0.0504) Loss: 2.1230 (2.2376)\n",
            "TRAIN(077): [160/196] Batch: 0.0871 (0.0803) Data: 0.0495 (0.0504) Loss: 2.1707 (2.2381)\n",
            "TRAIN(077): [170/196] Batch: 0.0686 (0.0800) Data: 0.0609 (0.0504) Loss: 2.1702 (2.2424)\n",
            "TRAIN(077): [180/196] Batch: 0.0821 (0.0799) Data: 0.0475 (0.0503) Loss: 2.2442 (2.2439)\n",
            "TRAIN(077): [190/196] Batch: 0.0752 (0.0797) Data: 0.0603 (0.0502) Loss: 2.1677 (2.2399)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(77)         0:00:15         0:00:09         0:00:05          2.2414\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(078): [ 10/196] Batch: 0.0834 (0.1411) Data: 0.0462 (0.1063) Loss: 2.3943 (2.2742)\n",
            "TRAIN(078): [ 20/196] Batch: 0.0802 (0.1087) Data: 0.0505 (0.0787) Loss: 2.2424 (2.2457)\n",
            "TRAIN(078): [ 30/196] Batch: 0.0717 (0.0978) Data: 0.0549 (0.0697) Loss: 2.2661 (2.2480)\n",
            "TRAIN(078): [ 40/196] Batch: 0.0730 (0.0923) Data: 0.0582 (0.0646) Loss: 2.3151 (2.2578)\n",
            "TRAIN(078): [ 50/196] Batch: 0.0867 (0.0891) Data: 0.0524 (0.0616) Loss: 2.1843 (2.2693)\n",
            "TRAIN(078): [ 60/196] Batch: 0.0702 (0.0868) Data: 0.0605 (0.0596) Loss: 2.1885 (2.2646)\n",
            "TRAIN(078): [ 70/196] Batch: 0.0752 (0.0854) Data: 0.0545 (0.0586) Loss: 2.3668 (2.2617)\n",
            "TRAIN(078): [ 80/196] Batch: 0.0767 (0.0843) Data: 0.0536 (0.0573) Loss: 2.3934 (2.2631)\n",
            "TRAIN(078): [ 90/196] Batch: 0.0702 (0.0833) Data: 0.0578 (0.0567) Loss: 2.1468 (2.2549)\n",
            "TRAIN(078): [100/196] Batch: 0.0765 (0.0826) Data: 0.0548 (0.0558) Loss: 2.0903 (2.2502)\n",
            "TRAIN(078): [110/196] Batch: 0.0786 (0.0821) Data: 0.0506 (0.0551) Loss: 2.0904 (2.2481)\n",
            "TRAIN(078): [120/196] Batch: 0.0782 (0.0815) Data: 0.0556 (0.0545) Loss: 2.1541 (2.2477)\n",
            "TRAIN(078): [130/196] Batch: 0.0826 (0.0811) Data: 0.0478 (0.0541) Loss: 2.3598 (2.2451)\n",
            "TRAIN(078): [140/196] Batch: 0.0733 (0.0808) Data: 0.0509 (0.0536) Loss: 2.2710 (2.2456)\n",
            "TRAIN(078): [150/196] Batch: 0.0708 (0.0806) Data: 0.0484 (0.0530) Loss: 1.9761 (2.2448)\n",
            "TRAIN(078): [160/196] Batch: 0.0866 (0.0804) Data: 0.0450 (0.0526) Loss: 2.3357 (2.2459)\n",
            "TRAIN(078): [170/196] Batch: 0.0720 (0.0801) Data: 0.0563 (0.0520) Loss: 2.1991 (2.2453)\n",
            "TRAIN(078): [180/196] Batch: 0.0627 (0.0801) Data: 0.0415 (0.0512) Loss: 2.1407 (2.2449)\n",
            "TRAIN(078): [190/196] Batch: 0.0724 (0.0799) Data: 0.0559 (0.0509) Loss: 2.3121 (2.2443)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(78)         0:00:15         0:00:09         0:00:05          2.2442\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(079): [ 10/196] Batch: 0.0748 (0.1223) Data: 0.0464 (0.0830) Loss: 2.3434 (2.2556)\n",
            "TRAIN(079): [ 20/196] Batch: 0.0610 (0.0987) Data: 0.0596 (0.0659) Loss: 2.4231 (2.2465)\n",
            "TRAIN(079): [ 30/196] Batch: 0.0867 (0.0917) Data: 0.0495 (0.0607) Loss: 2.0610 (2.2506)\n",
            "TRAIN(079): [ 40/196] Batch: 0.0663 (0.0874) Data: 0.0619 (0.0576) Loss: 2.2185 (2.2487)\n",
            "TRAIN(079): [ 50/196] Batch: 0.0669 (0.0851) Data: 0.0596 (0.0556) Loss: 2.1454 (2.2468)\n",
            "TRAIN(079): [ 60/196] Batch: 0.0803 (0.0838) Data: 0.0494 (0.0541) Loss: 2.1959 (2.2454)\n",
            "TRAIN(079): [ 70/196] Batch: 0.0695 (0.0826) Data: 0.0548 (0.0533) Loss: 2.3580 (2.2421)\n",
            "TRAIN(079): [ 80/196] Batch: 0.0699 (0.0817) Data: 0.0615 (0.0527) Loss: 2.2450 (2.2380)\n",
            "TRAIN(079): [ 90/196] Batch: 0.0818 (0.0812) Data: 0.0507 (0.0520) Loss: 1.8991 (2.2315)\n",
            "TRAIN(079): [100/196] Batch: 0.0821 (0.0807) Data: 0.0490 (0.0516) Loss: 2.1403 (2.2354)\n",
            "TRAIN(079): [110/196] Batch: 0.0869 (0.0803) Data: 0.0492 (0.0514) Loss: 2.2133 (2.2347)\n",
            "TRAIN(079): [120/196] Batch: 0.0745 (0.0799) Data: 0.0515 (0.0511) Loss: 2.1606 (2.2298)\n",
            "TRAIN(079): [130/196] Batch: 0.0781 (0.0798) Data: 0.0474 (0.0503) Loss: 2.1962 (2.2283)\n",
            "TRAIN(079): [140/196] Batch: 0.0699 (0.0796) Data: 0.0541 (0.0499) Loss: 2.2888 (2.2290)\n",
            "TRAIN(079): [150/196] Batch: 0.0942 (0.0799) Data: 0.0439 (0.0493) Loss: 2.3069 (2.2256)\n",
            "TRAIN(079): [160/196] Batch: 0.0918 (0.0801) Data: 0.0400 (0.0485) Loss: 2.3006 (2.2265)\n",
            "TRAIN(079): [170/196] Batch: 0.0767 (0.0801) Data: 0.0523 (0.0480) Loss: 2.2301 (2.2238)\n",
            "TRAIN(079): [180/196] Batch: 0.0786 (0.0803) Data: 0.0458 (0.0476) Loss: 2.1327 (2.2238)\n",
            "TRAIN(079): [190/196] Batch: 0.0737 (0.0800) Data: 0.0591 (0.0477) Loss: 2.1583 (2.2245)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(79)         0:00:15         0:00:09         0:00:06          2.2258\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(080): [ 10/196] Batch: 0.0628 (0.1164) Data: 0.0606 (0.0876) Loss: 2.1999 (2.2286)\n",
            "TRAIN(080): [ 20/196] Batch: 0.0869 (0.0972) Data: 0.0495 (0.0707) Loss: 2.1854 (2.2450)\n",
            "TRAIN(080): [ 30/196] Batch: 0.0774 (0.0902) Data: 0.0510 (0.0636) Loss: 2.2555 (2.2331)\n",
            "TRAIN(080): [ 40/196] Batch: 0.0748 (0.0865) Data: 0.0582 (0.0609) Loss: 2.3316 (2.2216)\n",
            "TRAIN(080): [ 50/196] Batch: 0.0780 (0.0845) Data: 0.0533 (0.0586) Loss: 2.2105 (2.2328)\n",
            "TRAIN(080): [ 60/196] Batch: 0.0695 (0.0831) Data: 0.0565 (0.0574) Loss: 2.0482 (2.2263)\n",
            "TRAIN(080): [ 70/196] Batch: 0.0701 (0.0820) Data: 0.0604 (0.0564) Loss: 2.2879 (2.2278)\n",
            "TRAIN(080): [ 80/196] Batch: 0.0707 (0.0814) Data: 0.0548 (0.0555) Loss: 2.3147 (2.2173)\n",
            "TRAIN(080): [ 90/196] Batch: 0.0855 (0.0808) Data: 0.0520 (0.0549) Loss: 2.1225 (2.2166)\n",
            "TRAIN(080): [100/196] Batch: 0.0758 (0.0803) Data: 0.0538 (0.0543) Loss: 2.2035 (2.2117)\n",
            "TRAIN(080): [110/196] Batch: 0.0760 (0.0802) Data: 0.0522 (0.0537) Loss: 2.3644 (2.2086)\n",
            "TRAIN(080): [120/196] Batch: 0.0747 (0.0799) Data: 0.0592 (0.0531) Loss: 2.0511 (2.2069)\n",
            "TRAIN(080): [130/196] Batch: 0.0683 (0.0797) Data: 0.0583 (0.0525) Loss: 2.4068 (2.2071)\n",
            "TRAIN(080): [140/196] Batch: 0.0747 (0.0796) Data: 0.0493 (0.0520) Loss: 2.2284 (2.2089)\n",
            "TRAIN(080): [150/196] Batch: 0.0771 (0.0795) Data: 0.0483 (0.0515) Loss: 2.1090 (2.2076)\n",
            "TRAIN(080): [160/196] Batch: 0.0745 (0.0792) Data: 0.0581 (0.0513) Loss: 2.0061 (2.2005)\n",
            "TRAIN(080): [170/196] Batch: 0.0819 (0.0791) Data: 0.0500 (0.0513) Loss: 2.2094 (2.2013)\n",
            "TRAIN(080): [180/196] Batch: 0.0764 (0.0789) Data: 0.0550 (0.0510) Loss: 2.0152 (2.1989)\n",
            "TRAIN(080): [190/196] Batch: 0.0747 (0.0787) Data: 0.0620 (0.0511) Loss: 2.2454 (2.1994)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(80)         0:00:15         0:00:10         0:00:05          2.1999\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(081): [ 10/196] Batch: 0.0752 (0.1195) Data: 0.0472 (0.0822) Loss: 2.2433 (2.1761)\n",
            "TRAIN(081): [ 20/196] Batch: 0.0678 (0.0977) Data: 0.0547 (0.0668) Loss: 2.1585 (2.1892)\n",
            "TRAIN(081): [ 30/196] Batch: 0.0803 (0.0908) Data: 0.0505 (0.0613) Loss: 2.0955 (2.1824)\n",
            "TRAIN(081): [ 40/196] Batch: 0.0807 (0.0871) Data: 0.0518 (0.0580) Loss: 2.3872 (2.2036)\n",
            "TRAIN(081): [ 50/196] Batch: 0.0804 (0.0849) Data: 0.0523 (0.0566) Loss: 2.1692 (2.2067)\n",
            "TRAIN(081): [ 60/196] Batch: 0.0680 (0.0833) Data: 0.0571 (0.0554) Loss: 2.2618 (2.2080)\n",
            "TRAIN(081): [ 70/196] Batch: 0.0786 (0.0824) Data: 0.0518 (0.0546) Loss: 2.3389 (2.2086)\n",
            "TRAIN(081): [ 80/196] Batch: 0.0817 (0.0816) Data: 0.0563 (0.0539) Loss: 2.3201 (2.2070)\n",
            "TRAIN(081): [ 90/196] Batch: 0.0734 (0.0811) Data: 0.0546 (0.0529) Loss: 2.2346 (2.2040)\n",
            "TRAIN(081): [100/196] Batch: 0.0675 (0.0808) Data: 0.0485 (0.0517) Loss: 2.0617 (2.2038)\n",
            "TRAIN(081): [110/196] Batch: 0.0722 (0.0805) Data: 0.0504 (0.0508) Loss: 2.1025 (2.2122)\n",
            "TRAIN(081): [120/196] Batch: 0.0769 (0.0802) Data: 0.0494 (0.0503) Loss: 2.0896 (2.2092)\n",
            "TRAIN(081): [130/196] Batch: 0.0727 (0.0798) Data: 0.0558 (0.0498) Loss: 2.1202 (2.2053)\n",
            "TRAIN(081): [140/196] Batch: 0.0901 (0.0796) Data: 0.0500 (0.0498) Loss: 2.1514 (2.2040)\n",
            "TRAIN(081): [150/196] Batch: 0.0743 (0.0793) Data: 0.0560 (0.0498) Loss: 2.1901 (2.2054)\n",
            "TRAIN(081): [160/196] Batch: 0.0697 (0.0791) Data: 0.0622 (0.0499) Loss: 2.1568 (2.2056)\n",
            "TRAIN(081): [170/196] Batch: 0.0791 (0.0790) Data: 0.0538 (0.0500) Loss: 2.2112 (2.2088)\n",
            "TRAIN(081): [180/196] Batch: 0.0793 (0.0789) Data: 0.0523 (0.0500) Loss: 2.3372 (2.2081)\n",
            "TRAIN(081): [190/196] Batch: 0.0754 (0.0787) Data: 0.0619 (0.0501) Loss: 2.1725 (2.2062)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(81)         0:00:15         0:00:09         0:00:05          2.2052\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(082): [ 10/196] Batch: 0.0764 (0.1185) Data: 0.0493 (0.0826) Loss: 2.2512 (2.1625)\n",
            "TRAIN(082): [ 20/196] Batch: 0.0860 (0.0978) Data: 0.0481 (0.0666) Loss: 2.1008 (2.1718)\n",
            "TRAIN(082): [ 30/196] Batch: 0.0760 (0.0905) Data: 0.0533 (0.0615) Loss: 2.1834 (2.1811)\n",
            "TRAIN(082): [ 40/196] Batch: 0.0732 (0.0867) Data: 0.0623 (0.0584) Loss: 2.2190 (2.1802)\n",
            "TRAIN(082): [ 50/196] Batch: 0.0751 (0.0847) Data: 0.0506 (0.0566) Loss: 2.2632 (2.1861)\n",
            "TRAIN(082): [ 60/196] Batch: 0.0733 (0.0833) Data: 0.0507 (0.0552) Loss: 2.0757 (2.1875)\n",
            "TRAIN(082): [ 70/196] Batch: 0.0784 (0.0825) Data: 0.0480 (0.0539) Loss: 2.3091 (2.1895)\n",
            "TRAIN(082): [ 80/196] Batch: 0.0671 (0.0817) Data: 0.0626 (0.0529) Loss: 2.0862 (2.1896)\n",
            "TRAIN(082): [ 90/196] Batch: 0.0647 (0.0812) Data: 0.0574 (0.0525) Loss: 2.2107 (2.1908)\n",
            "TRAIN(082): [100/196] Batch: 0.0865 (0.0809) Data: 0.0455 (0.0519) Loss: 2.2249 (2.1977)\n",
            "TRAIN(082): [110/196] Batch: 0.0744 (0.0805) Data: 0.0492 (0.0512) Loss: 2.1529 (2.1919)\n",
            "TRAIN(082): [120/196] Batch: 0.0667 (0.0801) Data: 0.0631 (0.0511) Loss: 2.4515 (2.1880)\n",
            "TRAIN(082): [130/196] Batch: 0.0740 (0.0798) Data: 0.0568 (0.0509) Loss: 2.1468 (2.1905)\n",
            "TRAIN(082): [140/196] Batch: 0.0719 (0.0795) Data: 0.0620 (0.0508) Loss: 2.0860 (2.1899)\n",
            "TRAIN(082): [150/196] Batch: 0.0711 (0.0794) Data: 0.0550 (0.0507) Loss: 2.3944 (2.1903)\n",
            "TRAIN(082): [160/196] Batch: 0.0826 (0.0792) Data: 0.0506 (0.0505) Loss: 2.1599 (2.1877)\n",
            "TRAIN(082): [170/196] Batch: 0.0754 (0.0789) Data: 0.0618 (0.0505) Loss: 2.1318 (2.1854)\n",
            "TRAIN(082): [180/196] Batch: 0.0631 (0.0788) Data: 0.0613 (0.0504) Loss: 2.1097 (2.1848)\n",
            "TRAIN(082): [190/196] Batch: 0.0757 (0.0787) Data: 0.0613 (0.0505) Loss: 2.0650 (2.1855)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(82)         0:00:15         0:00:09         0:00:05          2.1860\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(083): [ 10/196] Batch: 0.0725 (0.1212) Data: 0.0493 (0.0840) Loss: 2.1582 (2.1726)\n",
            "TRAIN(083): [ 20/196] Batch: 0.0680 (0.0985) Data: 0.0563 (0.0674) Loss: 2.2576 (2.1639)\n",
            "TRAIN(083): [ 30/196] Batch: 0.0696 (0.0910) Data: 0.0616 (0.0622) Loss: 2.1022 (2.1629)\n",
            "TRAIN(083): [ 40/196] Batch: 0.0741 (0.0876) Data: 0.0487 (0.0589) Loss: 2.2257 (2.1903)\n",
            "TRAIN(083): [ 50/196] Batch: 0.0751 (0.0854) Data: 0.0494 (0.0562) Loss: 2.2738 (2.1896)\n",
            "TRAIN(083): [ 60/196] Batch: 0.0660 (0.0840) Data: 0.0558 (0.0544) Loss: 2.3435 (2.1925)\n",
            "TRAIN(083): [ 70/196] Batch: 0.0775 (0.0831) Data: 0.0510 (0.0528) Loss: 2.0493 (2.1875)\n",
            "TRAIN(083): [ 80/196] Batch: 0.0762 (0.0823) Data: 0.0500 (0.0518) Loss: 2.1343 (2.1983)\n",
            "TRAIN(083): [ 90/196] Batch: 0.0715 (0.0817) Data: 0.0486 (0.0510) Loss: 2.1223 (2.1985)\n",
            "TRAIN(083): [100/196] Batch: 0.0680 (0.0811) Data: 0.0540 (0.0507) Loss: 2.3103 (2.1988)\n",
            "TRAIN(083): [110/196] Batch: 0.0710 (0.0806) Data: 0.0560 (0.0507) Loss: 2.1715 (2.1984)\n",
            "TRAIN(083): [120/196] Batch: 0.0696 (0.0802) Data: 0.0613 (0.0507) Loss: 2.1531 (2.1980)\n",
            "TRAIN(083): [130/196] Batch: 0.0691 (0.0799) Data: 0.0557 (0.0506) Loss: 2.2230 (2.1961)\n",
            "TRAIN(083): [140/196] Batch: 0.0688 (0.0796) Data: 0.0607 (0.0506) Loss: 1.9980 (2.1944)\n",
            "TRAIN(083): [150/196] Batch: 0.0682 (0.0794) Data: 0.0586 (0.0506) Loss: 2.0849 (2.1914)\n",
            "TRAIN(083): [160/196] Batch: 0.0768 (0.0793) Data: 0.0549 (0.0504) Loss: 2.0485 (2.1949)\n",
            "TRAIN(083): [170/196] Batch: 0.0857 (0.0791) Data: 0.0497 (0.0503) Loss: 2.1920 (2.1945)\n",
            "TRAIN(083): [180/196] Batch: 0.0685 (0.0789) Data: 0.0569 (0.0503) Loss: 2.2577 (2.1931)\n",
            "TRAIN(083): [190/196] Batch: 0.0754 (0.0787) Data: 0.0623 (0.0504) Loss: 2.1936 (2.1917)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(83)         0:00:15         0:00:09         0:00:05          2.1909\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(084): [ 10/196] Batch: 0.0763 (0.1189) Data: 0.0463 (0.0830) Loss: 2.1776 (2.2093)\n",
            "TRAIN(084): [ 20/196] Batch: 0.0747 (0.0978) Data: 0.0482 (0.0664) Loss: 2.0644 (2.1608)\n",
            "TRAIN(084): [ 30/196] Batch: 0.0742 (0.0909) Data: 0.0493 (0.0593) Loss: 2.2008 (2.1779)\n",
            "TRAIN(084): [ 40/196] Batch: 0.0771 (0.0874) Data: 0.0525 (0.0557) Loss: 2.0015 (2.1759)\n",
            "TRAIN(084): [ 50/196] Batch: 0.0765 (0.0856) Data: 0.0483 (0.0535) Loss: 2.3113 (2.1687)\n",
            "TRAIN(084): [ 60/196] Batch: 0.0855 (0.0843) Data: 0.0449 (0.0523) Loss: 2.1936 (2.1720)\n",
            "TRAIN(084): [ 70/196] Batch: 0.0760 (0.0829) Data: 0.0610 (0.0519) Loss: 2.1430 (2.1756)\n",
            "TRAIN(084): [ 80/196] Batch: 0.0727 (0.0821) Data: 0.0562 (0.0516) Loss: 2.2032 (2.1808)\n",
            "TRAIN(084): [ 90/196] Batch: 0.0794 (0.0815) Data: 0.0517 (0.0513) Loss: 2.2263 (2.1782)\n",
            "TRAIN(084): [100/196] Batch: 0.0780 (0.0810) Data: 0.0553 (0.0511) Loss: 2.3102 (2.1797)\n",
            "TRAIN(084): [110/196] Batch: 0.0657 (0.0805) Data: 0.0608 (0.0510) Loss: 2.2243 (2.1790)\n",
            "TRAIN(084): [120/196] Batch: 0.0704 (0.0802) Data: 0.0557 (0.0509) Loss: 2.1438 (2.1789)\n",
            "TRAIN(084): [130/196] Batch: 0.0766 (0.0799) Data: 0.0567 (0.0507) Loss: 2.1256 (2.1780)\n",
            "TRAIN(084): [140/196] Batch: 0.0776 (0.0796) Data: 0.0514 (0.0506) Loss: 2.1505 (2.1790)\n",
            "TRAIN(084): [150/196] Batch: 0.0749 (0.0794) Data: 0.0565 (0.0506) Loss: 2.1049 (2.1742)\n",
            "TRAIN(084): [160/196] Batch: 0.0695 (0.0792) Data: 0.0557 (0.0505) Loss: 2.1262 (2.1743)\n",
            "TRAIN(084): [170/196] Batch: 0.0876 (0.0790) Data: 0.0492 (0.0505) Loss: 2.1192 (2.1732)\n",
            "TRAIN(084): [180/196] Batch: 0.0721 (0.0788) Data: 0.0553 (0.0505) Loss: 2.3693 (2.1748)\n",
            "TRAIN(084): [190/196] Batch: 0.0772 (0.0787) Data: 0.0613 (0.0506) Loss: 2.1864 (2.1737)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(84)         0:00:15         0:00:09         0:00:05          2.1711\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(085): [ 10/196] Batch: 0.0427 (0.1311) Data: 0.0437 (0.0816) Loss: 2.2238 (2.1166)\n",
            "TRAIN(085): [ 20/196] Batch: 0.0749 (0.1060) Data: 0.0500 (0.0641) Loss: 2.2526 (2.1472)\n",
            "TRAIN(085): [ 30/196] Batch: 0.0836 (0.0969) Data: 0.0391 (0.0566) Loss: 2.2302 (2.1512)\n",
            "TRAIN(085): [ 40/196] Batch: 0.0689 (0.0920) Data: 0.0487 (0.0534) Loss: 2.2321 (2.1480)\n",
            "TRAIN(085): [ 50/196] Batch: 0.0630 (0.0888) Data: 0.0623 (0.0519) Loss: 2.2478 (2.1503)\n",
            "TRAIN(085): [ 60/196] Batch: 0.0664 (0.0868) Data: 0.0559 (0.0513) Loss: 2.2373 (2.1501)\n",
            "TRAIN(085): [ 70/196] Batch: 0.0686 (0.0853) Data: 0.0560 (0.0511) Loss: 2.2187 (2.1537)\n",
            "TRAIN(085): [ 80/196] Batch: 0.0791 (0.0842) Data: 0.0513 (0.0511) Loss: 2.1026 (2.1540)\n",
            "TRAIN(085): [ 90/196] Batch: 0.0821 (0.0834) Data: 0.0505 (0.0510) Loss: 2.0818 (2.1589)\n",
            "TRAIN(085): [100/196] Batch: 0.0753 (0.0826) Data: 0.0535 (0.0508) Loss: 2.1565 (2.1620)\n",
            "TRAIN(085): [110/196] Batch: 0.0818 (0.0820) Data: 0.0567 (0.0509) Loss: 2.0449 (2.1613)\n",
            "TRAIN(085): [120/196] Batch: 0.0832 (0.0816) Data: 0.0557 (0.0509) Loss: 2.0881 (2.1605)\n",
            "TRAIN(085): [130/196] Batch: 0.0779 (0.0812) Data: 0.0545 (0.0509) Loss: 2.1813 (2.1614)\n",
            "TRAIN(085): [140/196] Batch: 0.0735 (0.0808) Data: 0.0527 (0.0508) Loss: 2.1129 (2.1607)\n",
            "TRAIN(085): [150/196] Batch: 0.0817 (0.0805) Data: 0.0508 (0.0507) Loss: 2.2685 (2.1590)\n",
            "TRAIN(085): [160/196] Batch: 0.0877 (0.0803) Data: 0.0506 (0.0507) Loss: 1.9540 (2.1574)\n",
            "TRAIN(085): [170/196] Batch: 0.0746 (0.0800) Data: 0.0552 (0.0505) Loss: 2.0612 (2.1558)\n",
            "TRAIN(085): [180/196] Batch: 0.0738 (0.0798) Data: 0.0579 (0.0504) Loss: 2.0987 (2.1547)\n",
            "TRAIN(085): [190/196] Batch: 0.0760 (0.0796) Data: 0.0596 (0.0502) Loss: 2.0500 (2.1538)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(85)         0:00:15         0:00:09         0:00:05          2.1544\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(086): [ 10/196] Batch: 0.0847 (0.1265) Data: 0.0430 (0.0874) Loss: 2.2887 (2.1418)\n",
            "TRAIN(086): [ 20/196] Batch: 0.0752 (0.1019) Data: 0.0491 (0.0659) Loss: 2.2921 (2.1536)\n",
            "TRAIN(086): [ 30/196] Batch: 0.0824 (0.0933) Data: 0.0538 (0.0605) Loss: 1.9669 (2.1528)\n",
            "TRAIN(086): [ 40/196] Batch: 0.0802 (0.0891) Data: 0.0505 (0.0577) Loss: 2.1672 (2.1471)\n",
            "TRAIN(086): [ 50/196] Batch: 0.0766 (0.0864) Data: 0.0557 (0.0562) Loss: 2.1508 (2.1477)\n",
            "TRAIN(086): [ 60/196] Batch: 0.0654 (0.0846) Data: 0.0612 (0.0550) Loss: 2.1626 (2.1477)\n",
            "TRAIN(086): [ 70/196] Batch: 0.0858 (0.0835) Data: 0.0524 (0.0539) Loss: 2.2843 (2.1557)\n",
            "TRAIN(086): [ 80/196] Batch: 0.0685 (0.0826) Data: 0.0606 (0.0536) Loss: 2.1238 (2.1540)\n",
            "TRAIN(086): [ 90/196] Batch: 0.0702 (0.0819) Data: 0.0606 (0.0533) Loss: 2.3416 (2.1566)\n",
            "TRAIN(086): [100/196] Batch: 0.0868 (0.0814) Data: 0.0511 (0.0529) Loss: 2.1177 (2.1545)\n",
            "TRAIN(086): [110/196] Batch: 0.0707 (0.0809) Data: 0.0572 (0.0526) Loss: 2.0458 (2.1565)\n",
            "TRAIN(086): [120/196] Batch: 0.0788 (0.0805) Data: 0.0544 (0.0525) Loss: 2.1240 (2.1575)\n",
            "TRAIN(086): [130/196] Batch: 0.0688 (0.0802) Data: 0.0555 (0.0522) Loss: 2.0391 (2.1576)\n",
            "TRAIN(086): [140/196] Batch: 0.0723 (0.0799) Data: 0.0570 (0.0521) Loss: 2.0855 (2.1555)\n",
            "TRAIN(086): [150/196] Batch: 0.0705 (0.0797) Data: 0.0528 (0.0519) Loss: 2.2498 (2.1541)\n",
            "TRAIN(086): [160/196] Batch: 0.0589 (0.0796) Data: 0.0564 (0.0517) Loss: 2.0465 (2.1507)\n",
            "TRAIN(086): [170/196] Batch: 0.0760 (0.0795) Data: 0.0583 (0.0516) Loss: 2.1089 (2.1505)\n",
            "TRAIN(086): [180/196] Batch: 0.0664 (0.0794) Data: 0.0469 (0.0513) Loss: 2.1746 (2.1487)\n",
            "TRAIN(086): [190/196] Batch: 0.0735 (0.0792) Data: 0.0600 (0.0510) Loss: 2.0200 (2.1456)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(86)         0:00:15         0:00:09         0:00:05          2.1437\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(087): [ 10/196] Batch: 0.0623 (0.1212) Data: 0.0611 (0.0826) Loss: 2.2229 (2.1654)\n",
            "TRAIN(087): [ 20/196] Batch: 0.0739 (0.0995) Data: 0.0521 (0.0665) Loss: 2.2680 (2.1671)\n",
            "TRAIN(087): [ 30/196] Batch: 0.0744 (0.0918) Data: 0.0543 (0.0609) Loss: 2.2211 (2.1630)\n",
            "TRAIN(087): [ 40/196] Batch: 0.0694 (0.0879) Data: 0.0570 (0.0580) Loss: 2.3894 (2.1725)\n",
            "TRAIN(087): [ 50/196] Batch: 0.0869 (0.0856) Data: 0.0508 (0.0564) Loss: 2.1878 (2.1675)\n",
            "TRAIN(087): [ 60/196] Batch: 0.0825 (0.0840) Data: 0.0549 (0.0556) Loss: 2.0433 (2.1666)\n",
            "TRAIN(087): [ 70/196] Batch: 0.0722 (0.0829) Data: 0.0547 (0.0551) Loss: 1.9690 (2.1630)\n",
            "TRAIN(087): [ 80/196] Batch: 0.0870 (0.0822) Data: 0.0509 (0.0545) Loss: 2.2895 (2.1616)\n",
            "TRAIN(087): [ 90/196] Batch: 0.0758 (0.0814) Data: 0.0619 (0.0539) Loss: 2.2331 (2.1596)\n",
            "TRAIN(087): [100/196] Batch: 0.0767 (0.0809) Data: 0.0606 (0.0536) Loss: 2.0971 (2.1582)\n",
            "TRAIN(087): [110/196] Batch: 0.0874 (0.0806) Data: 0.0489 (0.0532) Loss: 1.9669 (2.1587)\n",
            "TRAIN(087): [120/196] Batch: 0.0854 (0.0802) Data: 0.0534 (0.0529) Loss: 2.1892 (2.1574)\n",
            "TRAIN(087): [130/196] Batch: 0.0772 (0.0799) Data: 0.0550 (0.0527) Loss: 2.0764 (2.1512)\n",
            "TRAIN(087): [140/196] Batch: 0.0744 (0.0797) Data: 0.0512 (0.0521) Loss: 2.0028 (2.1481)\n",
            "TRAIN(087): [150/196] Batch: 0.0753 (0.0795) Data: 0.0507 (0.0517) Loss: 2.1578 (2.1473)\n",
            "TRAIN(087): [160/196] Batch: 0.0863 (0.0795) Data: 0.0383 (0.0512) Loss: 2.2660 (2.1470)\n",
            "TRAIN(087): [170/196] Batch: 0.0947 (0.0794) Data: 0.0310 (0.0510) Loss: 2.1632 (2.1467)\n",
            "TRAIN(087): [180/196] Batch: 0.0723 (0.0792) Data: 0.0556 (0.0504) Loss: 2.2040 (2.1500)\n",
            "TRAIN(087): [190/196] Batch: 0.0754 (0.0790) Data: 0.0621 (0.0504) Loss: 2.1282 (2.1469)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(87)         0:00:15         0:00:09         0:00:05          2.1476\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(088): [ 10/196] Batch: 0.0753 (0.1188) Data: 0.0487 (0.0820) Loss: 2.0905 (2.1162)\n",
            "TRAIN(088): [ 20/196] Batch: 0.0717 (0.0973) Data: 0.0607 (0.0663) Loss: 2.1484 (2.1197)\n",
            "TRAIN(088): [ 30/196] Batch: 0.0752 (0.0906) Data: 0.0537 (0.0608) Loss: 2.0142 (2.1226)\n",
            "TRAIN(088): [ 40/196] Batch: 0.0804 (0.0871) Data: 0.0522 (0.0578) Loss: 2.1228 (2.1091)\n",
            "TRAIN(088): [ 50/196] Batch: 0.0809 (0.0849) Data: 0.0510 (0.0559) Loss: 2.2730 (2.1163)\n",
            "TRAIN(088): [ 60/196] Batch: 0.0705 (0.0834) Data: 0.0549 (0.0547) Loss: 2.1599 (2.1140)\n",
            "TRAIN(088): [ 70/196] Batch: 0.0807 (0.0824) Data: 0.0531 (0.0540) Loss: 2.0798 (2.1138)\n",
            "TRAIN(088): [ 80/196] Batch: 0.0729 (0.0815) Data: 0.0618 (0.0537) Loss: 2.1095 (2.1112)\n",
            "TRAIN(088): [ 90/196] Batch: 0.0873 (0.0810) Data: 0.0507 (0.0532) Loss: 2.1258 (2.1165)\n",
            "TRAIN(088): [100/196] Batch: 0.0746 (0.0804) Data: 0.0610 (0.0530) Loss: 2.2131 (2.1178)\n",
            "TRAIN(088): [110/196] Batch: 0.0752 (0.0802) Data: 0.0475 (0.0522) Loss: 2.2101 (2.1207)\n",
            "TRAIN(088): [120/196] Batch: 0.0979 (0.0801) Data: 0.0494 (0.0518) Loss: 2.0883 (2.1213)\n",
            "TRAIN(088): [130/196] Batch: 0.0743 (0.0798) Data: 0.0525 (0.0511) Loss: 2.2249 (2.1232)\n",
            "TRAIN(088): [140/196] Batch: 0.0727 (0.0797) Data: 0.0539 (0.0508) Loss: 1.9670 (2.1221)\n",
            "TRAIN(088): [150/196] Batch: 0.0808 (0.0795) Data: 0.0476 (0.0504) Loss: 2.0635 (2.1237)\n",
            "TRAIN(088): [160/196] Batch: 0.0704 (0.0793) Data: 0.0559 (0.0504) Loss: 2.1599 (2.1241)\n",
            "TRAIN(088): [170/196] Batch: 0.0743 (0.0791) Data: 0.0555 (0.0504) Loss: 2.2147 (2.1267)\n",
            "TRAIN(088): [180/196] Batch: 0.0828 (0.0790) Data: 0.0480 (0.0503) Loss: 2.1891 (2.1253)\n",
            "TRAIN(088): [190/196] Batch: 0.0740 (0.0788) Data: 0.0632 (0.0503) Loss: 2.1484 (2.1242)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(88)         0:00:15         0:00:09         0:00:05          2.1249\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(089): [ 10/196] Batch: 0.0763 (0.1191) Data: 0.0484 (0.0845) Loss: 2.0780 (2.1038)\n",
            "TRAIN(089): [ 20/196] Batch: 0.0754 (0.0975) Data: 0.0549 (0.0683) Loss: 2.0601 (2.0924)\n",
            "TRAIN(089): [ 30/196] Batch: 0.0700 (0.0904) Data: 0.0554 (0.0621) Loss: 1.9993 (2.0905)\n",
            "TRAIN(089): [ 40/196] Batch: 0.0715 (0.0869) Data: 0.0566 (0.0590) Loss: 2.0462 (2.0859)\n",
            "TRAIN(089): [ 50/196] Batch: 0.0749 (0.0849) Data: 0.0521 (0.0572) Loss: 2.1243 (2.0934)\n",
            "TRAIN(089): [ 60/196] Batch: 0.0701 (0.0833) Data: 0.0608 (0.0559) Loss: 2.0080 (2.0814)\n",
            "TRAIN(089): [ 70/196] Batch: 0.0821 (0.0825) Data: 0.0484 (0.0548) Loss: 2.1531 (2.0948)\n",
            "TRAIN(089): [ 80/196] Batch: 0.0728 (0.0816) Data: 0.0616 (0.0541) Loss: 2.1265 (2.0963)\n",
            "TRAIN(089): [ 90/196] Batch: 0.0745 (0.0811) Data: 0.0488 (0.0533) Loss: 2.1302 (2.1036)\n",
            "TRAIN(089): [100/196] Batch: 0.0747 (0.0806) Data: 0.0506 (0.0524) Loss: 2.3351 (2.1029)\n",
            "TRAIN(089): [110/196] Batch: 0.0746 (0.0804) Data: 0.0493 (0.0517) Loss: 2.1230 (2.1015)\n",
            "TRAIN(089): [120/196] Batch: 0.0738 (0.0802) Data: 0.0476 (0.0512) Loss: 2.2047 (2.1008)\n",
            "TRAIN(089): [130/196] Batch: 0.0620 (0.0800) Data: 0.0490 (0.0506) Loss: 2.1096 (2.1030)\n",
            "TRAIN(089): [140/196] Batch: 0.0743 (0.0797) Data: 0.0501 (0.0502) Loss: 2.0799 (2.1077)\n",
            "TRAIN(089): [150/196] Batch: 0.0755 (0.0795) Data: 0.0560 (0.0503) Loss: 2.1261 (2.1063)\n",
            "TRAIN(089): [160/196] Batch: 0.0816 (0.0793) Data: 0.0512 (0.0503) Loss: 2.2087 (2.1078)\n",
            "TRAIN(089): [170/196] Batch: 0.0697 (0.0790) Data: 0.0614 (0.0503) Loss: 2.0533 (2.1063)\n",
            "TRAIN(089): [180/196] Batch: 0.0717 (0.0789) Data: 0.0563 (0.0503) Loss: 2.0479 (2.1042)\n",
            "TRAIN(089): [190/196] Batch: 0.0758 (0.0787) Data: 0.0610 (0.0503) Loss: 2.0713 (2.1042)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(89)         0:00:15         0:00:09         0:00:05          2.1046\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(090): [ 10/196] Batch: 0.0735 (0.1202) Data: 0.0498 (0.0814) Loss: 2.0552 (2.1160)\n",
            "TRAIN(090): [ 20/196] Batch: 0.0789 (0.0986) Data: 0.0497 (0.0662) Loss: 2.1032 (2.1172)\n",
            "TRAIN(090): [ 30/196] Batch: 0.0687 (0.0911) Data: 0.0577 (0.0609) Loss: 1.8299 (2.0991)\n",
            "TRAIN(090): [ 40/196] Batch: 0.0729 (0.0873) Data: 0.0621 (0.0581) Loss: 1.9717 (2.0976)\n",
            "TRAIN(090): [ 50/196] Batch: 0.0797 (0.0852) Data: 0.0497 (0.0561) Loss: 2.1619 (2.1046)\n",
            "TRAIN(090): [ 60/196] Batch: 0.0865 (0.0838) Data: 0.0507 (0.0552) Loss: 2.0501 (2.1029)\n",
            "TRAIN(090): [ 70/196] Batch: 0.0780 (0.0828) Data: 0.0484 (0.0541) Loss: 2.1737 (2.1000)\n",
            "TRAIN(090): [ 80/196] Batch: 0.0683 (0.0820) Data: 0.0563 (0.0529) Loss: 1.8889 (2.0943)\n",
            "TRAIN(090): [ 90/196] Batch: 0.0914 (0.0817) Data: 0.0390 (0.0523) Loss: 2.4388 (2.1026)\n",
            "TRAIN(090): [100/196] Batch: 0.0763 (0.0812) Data: 0.0499 (0.0516) Loss: 1.9629 (2.0991)\n",
            "TRAIN(090): [110/196] Batch: 0.0855 (0.0809) Data: 0.0490 (0.0510) Loss: 2.0655 (2.0982)\n",
            "TRAIN(090): [120/196] Batch: 0.0779 (0.0807) Data: 0.0478 (0.0505) Loss: 1.9746 (2.0993)\n",
            "TRAIN(090): [130/196] Batch: 0.0665 (0.0802) Data: 0.0625 (0.0505) Loss: 2.3166 (2.0985)\n",
            "TRAIN(090): [140/196] Batch: 0.0706 (0.0800) Data: 0.0548 (0.0504) Loss: 2.0663 (2.0986)\n",
            "TRAIN(090): [150/196] Batch: 0.0869 (0.0798) Data: 0.0492 (0.0503) Loss: 2.1998 (2.0936)\n",
            "TRAIN(090): [160/196] Batch: 0.0888 (0.0796) Data: 0.0493 (0.0503) Loss: 2.1903 (2.0907)\n",
            "TRAIN(090): [170/196] Batch: 0.0743 (0.0794) Data: 0.0556 (0.0502) Loss: 2.0715 (2.0893)\n",
            "TRAIN(090): [180/196] Batch: 0.0716 (0.0792) Data: 0.0530 (0.0500) Loss: 2.0982 (2.0933)\n",
            "TRAIN(090): [190/196] Batch: 0.0764 (0.0790) Data: 0.0611 (0.0500) Loss: 2.2256 (2.0927)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(90)         0:00:15         0:00:09         0:00:05          2.0928\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(091): [ 10/196] Batch: 0.0744 (0.1198) Data: 0.0490 (0.0839) Loss: 2.1597 (2.1546)\n",
            "TRAIN(091): [ 20/196] Batch: 0.0697 (0.0979) Data: 0.0543 (0.0669) Loss: 2.0744 (2.1127)\n",
            "TRAIN(091): [ 30/196] Batch: 0.0667 (0.0907) Data: 0.0600 (0.0614) Loss: 2.0407 (2.1036)\n",
            "TRAIN(091): [ 40/196] Batch: 0.0834 (0.0872) Data: 0.0557 (0.0588) Loss: 2.0464 (2.1112)\n",
            "TRAIN(091): [ 50/196] Batch: 0.0751 (0.0849) Data: 0.0583 (0.0565) Loss: 2.0562 (2.1027)\n",
            "TRAIN(091): [ 60/196] Batch: 0.0708 (0.0838) Data: 0.0486 (0.0548) Loss: 1.9356 (2.1019)\n",
            "TRAIN(091): [ 70/196] Batch: 0.0760 (0.0828) Data: 0.0516 (0.0536) Loss: 1.9700 (2.1024)\n",
            "TRAIN(091): [ 80/196] Batch: 0.0682 (0.0822) Data: 0.0494 (0.0522) Loss: 1.8685 (2.0962)\n",
            "TRAIN(091): [ 90/196] Batch: 0.0725 (0.0816) Data: 0.0583 (0.0512) Loss: 2.1489 (2.0964)\n",
            "TRAIN(091): [100/196] Batch: 0.0888 (0.0813) Data: 0.0493 (0.0509) Loss: 2.1624 (2.0952)\n",
            "TRAIN(091): [110/196] Batch: 0.0846 (0.0809) Data: 0.0502 (0.0507) Loss: 2.1119 (2.0925)\n",
            "TRAIN(091): [120/196] Batch: 0.0849 (0.0805) Data: 0.0525 (0.0505) Loss: 2.1658 (2.0932)\n",
            "TRAIN(091): [130/196] Batch: 0.0798 (0.0801) Data: 0.0550 (0.0504) Loss: 2.1485 (2.0966)\n",
            "TRAIN(091): [140/196] Batch: 0.0801 (0.0799) Data: 0.0509 (0.0502) Loss: 2.2261 (2.0969)\n",
            "TRAIN(091): [150/196] Batch: 0.0835 (0.0796) Data: 0.0552 (0.0503) Loss: 2.1101 (2.0973)\n",
            "TRAIN(091): [160/196] Batch: 0.0819 (0.0794) Data: 0.0559 (0.0503) Loss: 1.9251 (2.0980)\n",
            "TRAIN(091): [170/196] Batch: 0.0877 (0.0792) Data: 0.0511 (0.0502) Loss: 2.2103 (2.0978)\n",
            "TRAIN(091): [180/196] Batch: 0.0897 (0.0791) Data: 0.0503 (0.0502) Loss: 2.2268 (2.0972)\n",
            "TRAIN(091): [190/196] Batch: 0.0757 (0.0789) Data: 0.0609 (0.0502) Loss: 2.1578 (2.0969)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(91)         0:00:15         0:00:09         0:00:05          2.0969\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(092): [ 10/196] Batch: 0.0786 (0.1182) Data: 0.0493 (0.0818) Loss: 2.0605 (2.0444)\n",
            "TRAIN(092): [ 20/196] Batch: 0.0823 (0.0977) Data: 0.0493 (0.0660) Loss: 1.8974 (2.0571)\n",
            "TRAIN(092): [ 30/196] Batch: 0.0759 (0.0906) Data: 0.0517 (0.0600) Loss: 2.1665 (2.0591)\n",
            "TRAIN(092): [ 40/196] Batch: 0.0835 (0.0872) Data: 0.0502 (0.0562) Loss: 2.1073 (2.0755)\n",
            "TRAIN(092): [ 50/196] Batch: 0.0759 (0.0853) Data: 0.0509 (0.0543) Loss: 2.0328 (2.0815)\n",
            "TRAIN(092): [ 60/196] Batch: 0.0718 (0.0838) Data: 0.0576 (0.0531) Loss: 2.1385 (2.0845)\n",
            "TRAIN(092): [ 70/196] Batch: 0.0852 (0.0831) Data: 0.0415 (0.0525) Loss: 2.2369 (2.0850)\n",
            "TRAIN(092): [ 80/196] Batch: 0.0831 (0.0824) Data: 0.0500 (0.0519) Loss: 2.1181 (2.0878)\n",
            "TRAIN(092): [ 90/196] Batch: 0.0756 (0.0817) Data: 0.0508 (0.0516) Loss: 2.0561 (2.0858)\n",
            "TRAIN(092): [100/196] Batch: 0.0863 (0.0812) Data: 0.0494 (0.0514) Loss: 1.9558 (2.0839)\n",
            "TRAIN(092): [110/196] Batch: 0.0704 (0.0807) Data: 0.0573 (0.0514) Loss: 2.0068 (2.0898)\n",
            "TRAIN(092): [120/196] Batch: 0.0730 (0.0804) Data: 0.0554 (0.0511) Loss: 2.0703 (2.0923)\n",
            "TRAIN(092): [130/196] Batch: 0.0822 (0.0801) Data: 0.0489 (0.0509) Loss: 1.9065 (2.0887)\n",
            "TRAIN(092): [140/196] Batch: 0.0701 (0.0797) Data: 0.0625 (0.0509) Loss: 2.1422 (2.0901)\n",
            "TRAIN(092): [150/196] Batch: 0.0774 (0.0795) Data: 0.0620 (0.0510) Loss: 2.0238 (2.0853)\n",
            "TRAIN(092): [160/196] Batch: 0.0785 (0.0794) Data: 0.0538 (0.0507) Loss: 2.0247 (2.0816)\n",
            "TRAIN(092): [170/196] Batch: 0.0813 (0.0792) Data: 0.0508 (0.0506) Loss: 2.0780 (2.0801)\n",
            "TRAIN(092): [180/196] Batch: 0.0768 (0.0790) Data: 0.0505 (0.0506) Loss: 1.9766 (2.0815)\n",
            "TRAIN(092): [190/196] Batch: 0.0755 (0.0788) Data: 0.0627 (0.0506) Loss: 2.1171 (2.0780)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(92)         0:00:15         0:00:09         0:00:05          2.0805\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(093): [ 10/196] Batch: 0.0690 (0.1266) Data: 0.0493 (0.0779) Loss: 2.0329 (2.0723)\n",
            "TRAIN(093): [ 20/196] Batch: 0.0750 (0.1023) Data: 0.0502 (0.0627) Loss: 1.9943 (2.0733)\n",
            "TRAIN(093): [ 30/196] Batch: 0.0689 (0.0941) Data: 0.0490 (0.0568) Loss: 2.2127 (2.0716)\n",
            "TRAIN(093): [ 40/196] Batch: 0.0719 (0.0895) Data: 0.0567 (0.0543) Loss: 2.1709 (2.0726)\n",
            "TRAIN(093): [ 50/196] Batch: 0.0779 (0.0873) Data: 0.0485 (0.0530) Loss: 2.0949 (2.0667)\n",
            "TRAIN(093): [ 60/196] Batch: 0.0801 (0.0853) Data: 0.0571 (0.0523) Loss: 2.0277 (2.0647)\n",
            "TRAIN(093): [ 70/196] Batch: 0.0850 (0.0841) Data: 0.0502 (0.0523) Loss: 1.9562 (2.0685)\n",
            "TRAIN(093): [ 80/196] Batch: 0.0772 (0.0831) Data: 0.0555 (0.0519) Loss: 2.1555 (2.0693)\n",
            "TRAIN(093): [ 90/196] Batch: 0.0712 (0.0824) Data: 0.0560 (0.0517) Loss: 2.0039 (2.0719)\n",
            "TRAIN(093): [100/196] Batch: 0.0886 (0.0818) Data: 0.0501 (0.0515) Loss: 2.0990 (2.0718)\n",
            "TRAIN(093): [110/196] Batch: 0.0721 (0.0813) Data: 0.0543 (0.0511) Loss: 1.9966 (2.0692)\n",
            "TRAIN(093): [120/196] Batch: 0.0823 (0.0809) Data: 0.0487 (0.0509) Loss: 2.0124 (2.0722)\n",
            "TRAIN(093): [130/196] Batch: 0.0744 (0.0805) Data: 0.0546 (0.0507) Loss: 1.9612 (2.0726)\n",
            "TRAIN(093): [140/196] Batch: 0.0828 (0.0802) Data: 0.0563 (0.0507) Loss: 1.8689 (2.0731)\n",
            "TRAIN(093): [150/196] Batch: 0.0861 (0.0800) Data: 0.0512 (0.0508) Loss: 1.8996 (2.0716)\n",
            "TRAIN(093): [160/196] Batch: 0.0866 (0.0798) Data: 0.0495 (0.0507) Loss: 2.0857 (2.0686)\n",
            "TRAIN(093): [170/196] Batch: 0.0802 (0.0795) Data: 0.0503 (0.0507) Loss: 1.8810 (2.0668)\n",
            "TRAIN(093): [180/196] Batch: 0.0706 (0.0793) Data: 0.0576 (0.0506) Loss: 2.0157 (2.0641)\n",
            "TRAIN(093): [190/196] Batch: 0.0798 (0.0792) Data: 0.0564 (0.0504) Loss: 1.9095 (2.0648)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(93)         0:00:15         0:00:09         0:00:05          2.0658\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(094): [ 10/196] Batch: 0.0925 (0.1404) Data: 0.0347 (0.1007) Loss: 1.9622 (2.0944)\n",
            "TRAIN(094): [ 20/196] Batch: 0.0713 (0.1099) Data: 0.0514 (0.0727) Loss: 2.0631 (2.0820)\n",
            "TRAIN(094): [ 30/196] Batch: 0.0863 (0.0988) Data: 0.0505 (0.0654) Loss: 1.9372 (2.0659)\n",
            "TRAIN(094): [ 40/196] Batch: 0.0703 (0.0930) Data: 0.0583 (0.0614) Loss: 2.0199 (2.0677)\n",
            "TRAIN(094): [ 50/196] Batch: 0.0672 (0.0896) Data: 0.0574 (0.0589) Loss: 2.1446 (2.0705)\n",
            "TRAIN(094): [ 60/196] Batch: 0.0838 (0.0874) Data: 0.0542 (0.0576) Loss: 1.9776 (2.0672)\n",
            "TRAIN(094): [ 70/196] Batch: 0.0806 (0.0859) Data: 0.0503 (0.0566) Loss: 2.1585 (2.0594)\n",
            "TRAIN(094): [ 80/196] Batch: 0.0654 (0.0845) Data: 0.0617 (0.0559) Loss: 2.0336 (2.0654)\n",
            "TRAIN(094): [ 90/196] Batch: 0.0760 (0.0837) Data: 0.0565 (0.0553) Loss: 2.0836 (2.0638)\n",
            "TRAIN(094): [100/196] Batch: 0.0804 (0.0830) Data: 0.0505 (0.0547) Loss: 2.1745 (2.0662)\n",
            "TRAIN(094): [110/196] Batch: 0.0771 (0.0823) Data: 0.0599 (0.0544) Loss: 2.0452 (2.0654)\n",
            "TRAIN(094): [120/196] Batch: 0.0717 (0.0819) Data: 0.0554 (0.0539) Loss: 1.9877 (2.0618)\n",
            "TRAIN(094): [130/196] Batch: 0.0835 (0.0814) Data: 0.0551 (0.0536) Loss: 1.9819 (2.0599)\n",
            "TRAIN(094): [140/196] Batch: 0.0724 (0.0811) Data: 0.0524 (0.0534) Loss: 2.2019 (2.0630)\n",
            "TRAIN(094): [150/196] Batch: 0.0624 (0.0807) Data: 0.0619 (0.0530) Loss: 2.0584 (2.0624)\n",
            "TRAIN(094): [160/196] Batch: 0.0717 (0.0805) Data: 0.0524 (0.0526) Loss: 2.1260 (2.0656)\n",
            "TRAIN(094): [170/196] Batch: 0.0861 (0.0804) Data: 0.0430 (0.0522) Loss: 2.0327 (2.0655)\n",
            "TRAIN(094): [180/196] Batch: 0.0800 (0.0802) Data: 0.0502 (0.0518) Loss: 2.2568 (2.0660)\n",
            "TRAIN(094): [190/196] Batch: 0.0745 (0.0799) Data: 0.0602 (0.0516) Loss: 2.0245 (2.0625)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(94)         0:00:15         0:00:10         0:00:05          2.0601\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(095): [ 10/196] Batch: 0.0732 (0.1179) Data: 0.0499 (0.0803) Loss: 2.1479 (2.0376)\n",
            "TRAIN(095): [ 20/196] Batch: 0.0663 (0.0970) Data: 0.0587 (0.0667) Loss: 1.9932 (2.0481)\n",
            "TRAIN(095): [ 30/196] Batch: 0.0714 (0.0903) Data: 0.0543 (0.0608) Loss: 2.0691 (2.0566)\n",
            "TRAIN(095): [ 40/196] Batch: 0.0831 (0.0869) Data: 0.0495 (0.0574) Loss: 2.0754 (2.0571)\n",
            "TRAIN(095): [ 50/196] Batch: 0.0658 (0.0845) Data: 0.0620 (0.0559) Loss: 2.0159 (2.0579)\n",
            "TRAIN(095): [ 60/196] Batch: 0.0673 (0.0831) Data: 0.0604 (0.0552) Loss: 2.1718 (2.0468)\n",
            "TRAIN(095): [ 70/196] Batch: 0.0863 (0.0824) Data: 0.0505 (0.0544) Loss: 2.0367 (2.0412)\n",
            "TRAIN(095): [ 80/196] Batch: 0.0732 (0.0815) Data: 0.0563 (0.0536) Loss: 1.9791 (2.0373)\n",
            "TRAIN(095): [ 90/196] Batch: 0.0697 (0.0809) Data: 0.0563 (0.0531) Loss: 2.1813 (2.0442)\n",
            "TRAIN(095): [100/196] Batch: 0.0833 (0.0805) Data: 0.0543 (0.0528) Loss: 1.9109 (2.0440)\n",
            "TRAIN(095): [110/196] Batch: 0.0765 (0.0800) Data: 0.0615 (0.0526) Loss: 2.1463 (2.0459)\n",
            "TRAIN(095): [120/196] Batch: 0.0667 (0.0797) Data: 0.0616 (0.0525) Loss: 2.0701 (2.0486)\n",
            "TRAIN(095): [130/196] Batch: 0.0795 (0.0796) Data: 0.0493 (0.0522) Loss: 2.0505 (2.0525)\n",
            "TRAIN(095): [140/196] Batch: 0.0739 (0.0794) Data: 0.0479 (0.0517) Loss: 2.1941 (2.0580)\n",
            "TRAIN(095): [150/196] Batch: 0.0783 (0.0792) Data: 0.0524 (0.0513) Loss: 2.0326 (2.0547)\n",
            "TRAIN(095): [160/196] Batch: 0.0746 (0.0790) Data: 0.0517 (0.0510) Loss: 1.9945 (2.0550)\n",
            "TRAIN(095): [170/196] Batch: 0.0659 (0.0789) Data: 0.0570 (0.0508) Loss: 2.2226 (2.0568)\n",
            "TRAIN(095): [180/196] Batch: 0.0766 (0.0789) Data: 0.0522 (0.0504) Loss: 1.7842 (2.0550)\n",
            "TRAIN(095): [190/196] Batch: 0.0749 (0.0787) Data: 0.0616 (0.0504) Loss: 2.1558 (2.0598)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(95)         0:00:15         0:00:09         0:00:05          2.0591\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(096): [ 10/196] Batch: 0.0843 (0.1381) Data: 0.0379 (0.0891) Loss: 2.1522 (2.0375)\n",
            "TRAIN(096): [ 20/196] Batch: 0.0889 (0.1089) Data: 0.0405 (0.0660) Loss: 2.0320 (2.0029)\n",
            "TRAIN(096): [ 30/196] Batch: 0.0778 (0.0976) Data: 0.0547 (0.0610) Loss: 2.0571 (2.0150)\n",
            "TRAIN(096): [ 40/196] Batch: 0.0811 (0.0924) Data: 0.0559 (0.0585) Loss: 2.0128 (2.0256)\n",
            "TRAIN(096): [ 50/196] Batch: 0.0817 (0.0892) Data: 0.0520 (0.0568) Loss: 1.9836 (2.0305)\n",
            "TRAIN(096): [ 60/196] Batch: 0.0677 (0.0869) Data: 0.0591 (0.0553) Loss: 1.9459 (2.0349)\n",
            "TRAIN(096): [ 70/196] Batch: 0.0791 (0.0856) Data: 0.0499 (0.0547) Loss: 1.9874 (2.0281)\n",
            "TRAIN(096): [ 80/196] Batch: 0.0708 (0.0844) Data: 0.0544 (0.0539) Loss: 1.9631 (2.0327)\n",
            "TRAIN(096): [ 90/196] Batch: 0.0875 (0.0835) Data: 0.0499 (0.0536) Loss: 1.9928 (2.0293)\n",
            "TRAIN(096): [100/196] Batch: 0.0807 (0.0828) Data: 0.0505 (0.0533) Loss: 2.2929 (2.0345)\n",
            "TRAIN(096): [110/196] Batch: 0.0711 (0.0822) Data: 0.0516 (0.0525) Loss: 1.9603 (2.0316)\n",
            "TRAIN(096): [120/196] Batch: 0.0811 (0.0818) Data: 0.0521 (0.0517) Loss: 2.0513 (2.0348)\n",
            "TRAIN(096): [130/196] Batch: 0.0721 (0.0814) Data: 0.0523 (0.0516) Loss: 1.8404 (2.0373)\n",
            "TRAIN(096): [140/196] Batch: 0.0732 (0.0813) Data: 0.0545 (0.0511) Loss: 1.9234 (2.0357)\n",
            "TRAIN(096): [150/196] Batch: 0.0904 (0.0812) Data: 0.0381 (0.0504) Loss: 2.0491 (2.0357)\n",
            "TRAIN(096): [160/196] Batch: 0.0696 (0.0808) Data: 0.0612 (0.0503) Loss: 1.9889 (2.0333)\n",
            "TRAIN(096): [170/196] Batch: 0.0834 (0.0806) Data: 0.0521 (0.0502) Loss: 2.0589 (2.0343)\n",
            "TRAIN(096): [180/196] Batch: 0.0657 (0.0803) Data: 0.0613 (0.0501) Loss: 2.0606 (2.0353)\n",
            "TRAIN(096): [190/196] Batch: 0.0763 (0.0801) Data: 0.0614 (0.0502) Loss: 2.1003 (2.0347)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(96)         0:00:15         0:00:09         0:00:05          2.0351\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(097): [ 10/196] Batch: 0.0692 (0.1155) Data: 0.0562 (0.0816) Loss: 1.9190 (2.0304)\n",
            "TRAIN(097): [ 20/196] Batch: 0.0844 (0.0967) Data: 0.0516 (0.0679) Loss: 2.0117 (2.0281)\n",
            "TRAIN(097): [ 30/196] Batch: 0.0771 (0.0898) Data: 0.0551 (0.0622) Loss: 2.1063 (2.0304)\n",
            "TRAIN(097): [ 40/196] Batch: 0.0888 (0.0866) Data: 0.0506 (0.0587) Loss: 2.0564 (2.0407)\n",
            "TRAIN(097): [ 50/196] Batch: 0.0784 (0.0845) Data: 0.0553 (0.0569) Loss: 2.0615 (2.0268)\n",
            "TRAIN(097): [ 60/196] Batch: 0.0809 (0.0832) Data: 0.0529 (0.0561) Loss: 2.0599 (2.0287)\n",
            "TRAIN(097): [ 70/196] Batch: 0.0691 (0.0821) Data: 0.0580 (0.0551) Loss: 1.9935 (2.0279)\n",
            "TRAIN(097): [ 80/196] Batch: 0.0730 (0.0815) Data: 0.0545 (0.0546) Loss: 1.9964 (2.0281)\n",
            "TRAIN(097): [ 90/196] Batch: 0.0667 (0.0810) Data: 0.0525 (0.0538) Loss: 2.2384 (2.0291)\n",
            "TRAIN(097): [100/196] Batch: 0.0833 (0.0806) Data: 0.0501 (0.0531) Loss: 2.2128 (2.0325)\n",
            "TRAIN(097): [110/196] Batch: 0.0763 (0.0802) Data: 0.0533 (0.0525) Loss: 2.1177 (2.0344)\n",
            "TRAIN(097): [120/196] Batch: 0.0756 (0.0800) Data: 0.0512 (0.0519) Loss: 2.0864 (2.0321)\n",
            "TRAIN(097): [130/196] Batch: 0.0802 (0.0797) Data: 0.0571 (0.0516) Loss: 2.0609 (2.0342)\n",
            "TRAIN(097): [140/196] Batch: 0.0743 (0.0795) Data: 0.0563 (0.0516) Loss: 2.0042 (2.0382)\n",
            "TRAIN(097): [150/196] Batch: 0.0739 (0.0793) Data: 0.0564 (0.0516) Loss: 1.9953 (2.0371)\n",
            "TRAIN(097): [160/196] Batch: 0.0844 (0.0792) Data: 0.0478 (0.0513) Loss: 1.9641 (2.0388)\n",
            "TRAIN(097): [170/196] Batch: 0.0636 (0.0789) Data: 0.0616 (0.0512) Loss: 1.9234 (2.0364)\n",
            "TRAIN(097): [180/196] Batch: 0.0645 (0.0788) Data: 0.0603 (0.0511) Loss: 1.8317 (2.0374)\n",
            "TRAIN(097): [190/196] Batch: 0.0742 (0.0786) Data: 0.0622 (0.0512) Loss: 2.2043 (2.0356)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(97)         0:00:15         0:00:10         0:00:05          2.0343\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(098): [ 10/196] Batch: 0.0756 (0.1197) Data: 0.0502 (0.0840) Loss: 1.7258 (2.0626)\n",
            "TRAIN(098): [ 20/196] Batch: 0.0769 (0.0981) Data: 0.0518 (0.0675) Loss: 1.9886 (2.0466)\n",
            "TRAIN(098): [ 30/196] Batch: 0.0741 (0.0906) Data: 0.0610 (0.0625) Loss: 1.9939 (2.0335)\n",
            "TRAIN(098): [ 40/196] Batch: 0.0701 (0.0869) Data: 0.0624 (0.0596) Loss: 2.0778 (2.0151)\n",
            "TRAIN(098): [ 50/196] Batch: 0.0850 (0.0849) Data: 0.0532 (0.0577) Loss: 1.8839 (2.0296)\n",
            "TRAIN(098): [ 60/196] Batch: 0.0778 (0.0834) Data: 0.0542 (0.0562) Loss: 2.1341 (2.0301)\n",
            "TRAIN(098): [ 70/196] Batch: 0.0837 (0.0826) Data: 0.0460 (0.0548) Loss: 2.0238 (2.0200)\n",
            "TRAIN(098): [ 80/196] Batch: 0.0773 (0.0818) Data: 0.0521 (0.0536) Loss: 1.9340 (2.0177)\n",
            "TRAIN(098): [ 90/196] Batch: 0.0742 (0.0813) Data: 0.0533 (0.0524) Loss: 2.1042 (2.0163)\n",
            "TRAIN(098): [100/196] Batch: 0.0776 (0.0809) Data: 0.0474 (0.0516) Loss: 2.2060 (2.0173)\n",
            "TRAIN(098): [110/196] Batch: 0.0821 (0.0806) Data: 0.0440 (0.0509) Loss: 1.9210 (2.0177)\n",
            "TRAIN(098): [120/196] Batch: 0.0834 (0.0802) Data: 0.0480 (0.0508) Loss: 1.8395 (2.0158)\n",
            "TRAIN(098): [130/196] Batch: 0.0738 (0.0799) Data: 0.0535 (0.0506) Loss: 2.1502 (2.0164)\n",
            "TRAIN(098): [140/196] Batch: 0.0869 (0.0797) Data: 0.0509 (0.0507) Loss: 1.8987 (2.0164)\n",
            "TRAIN(098): [150/196] Batch: 0.0780 (0.0794) Data: 0.0515 (0.0507) Loss: 1.9473 (2.0123)\n",
            "TRAIN(098): [160/196] Batch: 0.0820 (0.0793) Data: 0.0499 (0.0507) Loss: 2.1732 (2.0131)\n",
            "TRAIN(098): [170/196] Batch: 0.0686 (0.0790) Data: 0.0595 (0.0505) Loss: 2.0264 (2.0141)\n",
            "TRAIN(098): [180/196] Batch: 0.0743 (0.0789) Data: 0.0629 (0.0506) Loss: 1.9502 (2.0114)\n",
            "TRAIN(098): [190/196] Batch: 0.0764 (0.0787) Data: 0.0616 (0.0507) Loss: 2.1630 (2.0133)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(98)         0:00:15         0:00:09         0:00:05          2.0136\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(099): [ 10/196] Batch: 0.0853 (0.1172) Data: 0.0479 (0.0828) Loss: 1.9744 (2.0305)\n",
            "TRAIN(099): [ 20/196] Batch: 0.0717 (0.0968) Data: 0.0537 (0.0671) Loss: 2.0991 (2.0554)\n",
            "TRAIN(099): [ 30/196] Batch: 0.0800 (0.0902) Data: 0.0519 (0.0617) Loss: 2.0063 (2.0447)\n",
            "TRAIN(099): [ 40/196] Batch: 0.0641 (0.0864) Data: 0.0634 (0.0590) Loss: 2.0651 (2.0514)\n",
            "TRAIN(099): [ 50/196] Batch: 0.0822 (0.0848) Data: 0.0482 (0.0566) Loss: 2.0144 (2.0379)\n",
            "TRAIN(099): [ 60/196] Batch: 0.0924 (0.0835) Data: 0.0433 (0.0551) Loss: 1.9999 (2.0406)\n",
            "TRAIN(099): [ 70/196] Batch: 0.0938 (0.0826) Data: 0.0520 (0.0546) Loss: 1.7782 (2.0337)\n",
            "TRAIN(099): [ 80/196] Batch: 0.0726 (0.0819) Data: 0.0577 (0.0538) Loss: 1.9830 (2.0321)\n",
            "TRAIN(099): [ 90/196] Batch: 0.0759 (0.0814) Data: 0.0502 (0.0526) Loss: 2.0543 (2.0280)\n",
            "TRAIN(099): [100/196] Batch: 0.0797 (0.0809) Data: 0.0510 (0.0524) Loss: 1.9819 (2.0277)\n",
            "TRAIN(099): [110/196] Batch: 0.0837 (0.0805) Data: 0.0542 (0.0521) Loss: 1.9141 (2.0308)\n",
            "TRAIN(099): [120/196] Batch: 0.0770 (0.0801) Data: 0.0569 (0.0519) Loss: 2.0333 (2.0294)\n",
            "TRAIN(099): [130/196] Batch: 0.0667 (0.0797) Data: 0.0613 (0.0519) Loss: 1.8838 (2.0252)\n",
            "TRAIN(099): [140/196] Batch: 0.0665 (0.0795) Data: 0.0607 (0.0518) Loss: 2.0291 (2.0290)\n",
            "TRAIN(099): [150/196] Batch: 0.0810 (0.0793) Data: 0.0562 (0.0516) Loss: 1.9516 (2.0287)\n",
            "TRAIN(099): [160/196] Batch: 0.0901 (0.0792) Data: 0.0469 (0.0515) Loss: 2.2151 (2.0263)\n",
            "TRAIN(099): [170/196] Batch: 0.0652 (0.0790) Data: 0.0595 (0.0513) Loss: 1.9099 (2.0247)\n",
            "TRAIN(099): [180/196] Batch: 0.0699 (0.0788) Data: 0.0557 (0.0512) Loss: 1.9130 (2.0227)\n",
            "TRAIN(099): [190/196] Batch: 0.0758 (0.0787) Data: 0.0612 (0.0512) Loss: 1.9934 (2.0206)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "       TRAIN(99)         0:00:15         0:00:10         0:00:05          2.0208\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(100): [ 10/196] Batch: 0.0747 (0.1154) Data: 0.0518 (0.0805) Loss: 2.0552 (1.9799)\n",
            "TRAIN(100): [ 20/196] Batch: 0.0740 (0.0959) Data: 0.0513 (0.0662) Loss: 2.0836 (1.9821)\n",
            "TRAIN(100): [ 30/196] Batch: 0.0764 (0.0896) Data: 0.0499 (0.0601) Loss: 2.0420 (1.9760)\n",
            "TRAIN(100): [ 40/196] Batch: 0.0790 (0.0862) Data: 0.0568 (0.0574) Loss: 1.9764 (1.9745)\n",
            "TRAIN(100): [ 50/196] Batch: 0.0876 (0.0846) Data: 0.0422 (0.0545) Loss: 2.1722 (1.9829)\n",
            "TRAIN(100): [ 60/196] Batch: 0.0761 (0.0835) Data: 0.0486 (0.0524) Loss: 1.9458 (1.9853)\n",
            "TRAIN(100): [ 70/196] Batch: 0.0659 (0.0826) Data: 0.0491 (0.0513) Loss: 2.0337 (1.9899)\n",
            "TRAIN(100): [ 80/196] Batch: 0.0721 (0.0817) Data: 0.0549 (0.0511) Loss: 1.9708 (1.9895)\n",
            "TRAIN(100): [ 90/196] Batch: 0.0702 (0.0811) Data: 0.0559 (0.0509) Loss: 2.0433 (1.9823)\n",
            "TRAIN(100): [100/196] Batch: 0.0697 (0.0806) Data: 0.0550 (0.0505) Loss: 2.0736 (1.9855)\n",
            "TRAIN(100): [110/196] Batch: 0.0828 (0.0803) Data: 0.0493 (0.0503) Loss: 2.0621 (1.9905)\n",
            "TRAIN(100): [120/196] Batch: 0.0687 (0.0799) Data: 0.0569 (0.0500) Loss: 1.9387 (1.9884)\n",
            "TRAIN(100): [130/196] Batch: 0.0766 (0.0797) Data: 0.0536 (0.0500) Loss: 2.0022 (1.9921)\n",
            "TRAIN(100): [140/196] Batch: 0.0639 (0.0794) Data: 0.0622 (0.0500) Loss: 1.9137 (1.9926)\n",
            "TRAIN(100): [150/196] Batch: 0.0782 (0.0793) Data: 0.0462 (0.0499) Loss: 2.0183 (1.9894)\n",
            "TRAIN(100): [160/196] Batch: 0.0805 (0.0790) Data: 0.0564 (0.0499) Loss: 2.1112 (1.9927)\n",
            "TRAIN(100): [170/196] Batch: 0.0828 (0.0789) Data: 0.0558 (0.0500) Loss: 2.0268 (1.9941)\n",
            "TRAIN(100): [180/196] Batch: 0.0723 (0.0787) Data: 0.0549 (0.0500) Loss: 1.8586 (1.9984)\n",
            "TRAIN(100): [190/196] Batch: 0.0775 (0.0786) Data: 0.0619 (0.0501) Loss: 1.8076 (1.9976)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(100)         0:00:15         0:00:09         0:00:05          2.0013\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(101): [ 10/196] Batch: 0.0782 (0.1305) Data: 0.0378 (0.0837) Loss: 2.0202 (1.9538)\n",
            "TRAIN(101): [ 20/196] Batch: 0.0756 (0.1032) Data: 0.0526 (0.0654) Loss: 2.1812 (1.9906)\n",
            "TRAIN(101): [ 30/196] Batch: 0.0800 (0.0949) Data: 0.0465 (0.0585) Loss: 2.0146 (2.0070)\n",
            "TRAIN(101): [ 40/196] Batch: 0.0769 (0.0905) Data: 0.0492 (0.0548) Loss: 2.0013 (2.0183)\n",
            "TRAIN(101): [ 50/196] Batch: 0.0816 (0.0880) Data: 0.0441 (0.0528) Loss: 1.9564 (2.0160)\n",
            "TRAIN(101): [ 60/196] Batch: 0.0847 (0.0860) Data: 0.0527 (0.0523) Loss: 1.8885 (2.0178)\n",
            "TRAIN(101): [ 70/196] Batch: 0.0764 (0.0846) Data: 0.0554 (0.0518) Loss: 1.9833 (2.0139)\n",
            "TRAIN(101): [ 80/196] Batch: 0.0782 (0.0835) Data: 0.0558 (0.0514) Loss: 1.7949 (2.0054)\n",
            "TRAIN(101): [ 90/196] Batch: 0.0703 (0.0827) Data: 0.0559 (0.0511) Loss: 1.9300 (2.0062)\n",
            "TRAIN(101): [100/196] Batch: 0.0643 (0.0820) Data: 0.0605 (0.0511) Loss: 2.1237 (2.0084)\n",
            "TRAIN(101): [110/196] Batch: 0.0818 (0.0816) Data: 0.0504 (0.0510) Loss: 2.1305 (2.0081)\n",
            "TRAIN(101): [120/196] Batch: 0.0744 (0.0811) Data: 0.0535 (0.0509) Loss: 2.0095 (2.0077)\n",
            "TRAIN(101): [130/196] Batch: 0.0661 (0.0807) Data: 0.0563 (0.0507) Loss: 1.9357 (2.0092)\n",
            "TRAIN(101): [140/196] Batch: 0.0735 (0.0804) Data: 0.0591 (0.0507) Loss: 1.9852 (2.0067)\n",
            "TRAIN(101): [150/196] Batch: 0.0735 (0.0801) Data: 0.0530 (0.0506) Loss: 1.9025 (2.0046)\n",
            "TRAIN(101): [160/196] Batch: 0.0846 (0.0799) Data: 0.0516 (0.0506) Loss: 2.0172 (2.0052)\n",
            "TRAIN(101): [170/196] Batch: 0.0872 (0.0797) Data: 0.0513 (0.0505) Loss: 1.8900 (2.0058)\n",
            "TRAIN(101): [180/196] Batch: 0.0756 (0.0794) Data: 0.0621 (0.0505) Loss: 1.9114 (2.0026)\n",
            "TRAIN(101): [190/196] Batch: 0.0735 (0.0793) Data: 0.0583 (0.0504) Loss: 1.9686 (2.0022)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(101)         0:00:15         0:00:09         0:00:05          2.0013\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(102): [ 10/196] Batch: 0.1186 (0.1303) Data: 0.0468 (0.0911) Loss: 2.0269 (1.9988)\n",
            "TRAIN(102): [ 20/196] Batch: 0.0761 (0.1044) Data: 0.0492 (0.0676) Loss: 1.9149 (1.9993)\n",
            "TRAIN(102): [ 30/196] Batch: 0.0680 (0.0949) Data: 0.0583 (0.0615) Loss: 1.9827 (2.0035)\n",
            "TRAIN(102): [ 40/196] Batch: 0.0841 (0.0904) Data: 0.0511 (0.0585) Loss: 1.7941 (2.0066)\n",
            "TRAIN(102): [ 50/196] Batch: 0.0749 (0.0875) Data: 0.0527 (0.0571) Loss: 2.0024 (1.9977)\n",
            "TRAIN(102): [ 60/196] Batch: 0.0780 (0.0856) Data: 0.0540 (0.0556) Loss: 1.9421 (1.9953)\n",
            "TRAIN(102): [ 70/196] Batch: 0.0880 (0.0844) Data: 0.0478 (0.0548) Loss: 1.9343 (1.9945)\n",
            "TRAIN(102): [ 80/196] Batch: 0.0712 (0.0833) Data: 0.0554 (0.0539) Loss: 1.8475 (1.9875)\n",
            "TRAIN(102): [ 90/196] Batch: 0.0808 (0.0825) Data: 0.0510 (0.0533) Loss: 2.0309 (1.9883)\n",
            "TRAIN(102): [100/196] Batch: 0.0841 (0.0819) Data: 0.0492 (0.0528) Loss: 1.9068 (1.9872)\n",
            "TRAIN(102): [110/196] Batch: 0.0866 (0.0814) Data: 0.0509 (0.0524) Loss: 2.1579 (1.9886)\n",
            "TRAIN(102): [120/196] Batch: 0.0749 (0.0809) Data: 0.0621 (0.0523) Loss: 1.7526 (1.9864)\n",
            "TRAIN(102): [130/196] Batch: 0.0699 (0.0805) Data: 0.0617 (0.0522) Loss: 1.9302 (1.9848)\n",
            "TRAIN(102): [140/196] Batch: 0.0722 (0.0802) Data: 0.0616 (0.0521) Loss: 1.9877 (1.9828)\n",
            "TRAIN(102): [150/196] Batch: 0.0813 (0.0800) Data: 0.0514 (0.0519) Loss: 1.9854 (1.9830)\n",
            "TRAIN(102): [160/196] Batch: 0.0795 (0.0798) Data: 0.0491 (0.0515) Loss: 1.8397 (1.9810)\n",
            "TRAIN(102): [170/196] Batch: 0.0743 (0.0797) Data: 0.0545 (0.0512) Loss: 1.9761 (1.9827)\n",
            "TRAIN(102): [180/196] Batch: 0.0746 (0.0795) Data: 0.0511 (0.0509) Loss: 1.8467 (1.9817)\n",
            "TRAIN(102): [190/196] Batch: 0.0747 (0.0794) Data: 0.0625 (0.0508) Loss: 1.9468 (1.9833)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(102)         0:00:15         0:00:09         0:00:05          1.9832\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(103): [ 10/196] Batch: 0.0756 (0.1183) Data: 0.0498 (0.0817) Loss: 1.8701 (1.9696)\n",
            "TRAIN(103): [ 20/196] Batch: 0.0803 (0.0976) Data: 0.0484 (0.0659) Loss: 2.0259 (1.9793)\n",
            "TRAIN(103): [ 30/196] Batch: 0.0795 (0.0905) Data: 0.0504 (0.0604) Loss: 1.9003 (1.9887)\n",
            "TRAIN(103): [ 40/196] Batch: 0.0768 (0.0868) Data: 0.0567 (0.0578) Loss: 2.0106 (1.9945)\n",
            "TRAIN(103): [ 50/196] Batch: 0.0826 (0.0848) Data: 0.0511 (0.0561) Loss: 2.0432 (1.9959)\n",
            "TRAIN(103): [ 60/196] Batch: 0.0811 (0.0834) Data: 0.0484 (0.0548) Loss: 2.0793 (1.9901)\n",
            "TRAIN(103): [ 70/196] Batch: 0.0726 (0.0823) Data: 0.0532 (0.0541) Loss: 1.8918 (1.9927)\n",
            "TRAIN(103): [ 80/196] Batch: 0.0847 (0.0816) Data: 0.0541 (0.0537) Loss: 1.9041 (1.9845)\n",
            "TRAIN(103): [ 90/196] Batch: 0.0765 (0.0810) Data: 0.0529 (0.0533) Loss: 2.0302 (1.9816)\n",
            "TRAIN(103): [100/196] Batch: 0.0855 (0.0805) Data: 0.0517 (0.0530) Loss: 2.0452 (1.9855)\n",
            "TRAIN(103): [110/196] Batch: 0.0715 (0.0801) Data: 0.0567 (0.0525) Loss: 1.9011 (1.9808)\n",
            "TRAIN(103): [120/196] Batch: 0.0772 (0.0798) Data: 0.0549 (0.0524) Loss: 2.0463 (1.9817)\n",
            "TRAIN(103): [130/196] Batch: 0.0790 (0.0796) Data: 0.0464 (0.0520) Loss: 2.0301 (1.9819)\n",
            "TRAIN(103): [140/196] Batch: 0.0688 (0.0794) Data: 0.0567 (0.0514) Loss: 1.9536 (1.9790)\n",
            "TRAIN(103): [150/196] Batch: 0.0625 (0.0793) Data: 0.0574 (0.0511) Loss: 1.8702 (1.9782)\n",
            "TRAIN(103): [160/196] Batch: 0.0759 (0.0792) Data: 0.0496 (0.0507) Loss: 1.9647 (1.9778)\n",
            "TRAIN(103): [170/196] Batch: 0.0610 (0.0790) Data: 0.0529 (0.0503) Loss: 2.1162 (1.9786)\n",
            "TRAIN(103): [180/196] Batch: 0.0795 (0.0789) Data: 0.0519 (0.0502) Loss: 2.2251 (1.9787)\n",
            "TRAIN(103): [190/196] Batch: 0.0780 (0.0787) Data: 0.0614 (0.0503) Loss: 1.8855 (1.9765)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(103)         0:00:15         0:00:09         0:00:05          1.9768\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(104): [ 10/196] Batch: 0.0699 (0.1199) Data: 0.0553 (0.0822) Loss: 1.9712 (1.9188)\n",
            "TRAIN(104): [ 20/196] Batch: 0.0828 (0.0983) Data: 0.0540 (0.0668) Loss: 1.8501 (1.9263)\n",
            "TRAIN(104): [ 30/196] Batch: 0.0700 (0.0910) Data: 0.0576 (0.0618) Loss: 2.0370 (1.9095)\n",
            "TRAIN(104): [ 40/196] Batch: 0.0738 (0.0872) Data: 0.0624 (0.0592) Loss: 2.1444 (1.9236)\n",
            "TRAIN(104): [ 50/196] Batch: 0.0816 (0.0852) Data: 0.0515 (0.0573) Loss: 2.0040 (1.9307)\n",
            "TRAIN(104): [ 60/196] Batch: 0.0813 (0.0838) Data: 0.0504 (0.0560) Loss: 1.9707 (1.9332)\n",
            "TRAIN(104): [ 70/196] Batch: 0.0782 (0.0827) Data: 0.0518 (0.0552) Loss: 2.0182 (1.9465)\n",
            "TRAIN(104): [ 80/196] Batch: 0.0757 (0.0818) Data: 0.0619 (0.0547) Loss: 2.0793 (1.9505)\n",
            "TRAIN(104): [ 90/196] Batch: 0.0714 (0.0812) Data: 0.0619 (0.0541) Loss: 2.1037 (1.9477)\n",
            "TRAIN(104): [100/196] Batch: 0.0871 (0.0808) Data: 0.0496 (0.0535) Loss: 1.8718 (1.9500)\n",
            "TRAIN(104): [110/196] Batch: 0.0769 (0.0803) Data: 0.0568 (0.0531) Loss: 1.9626 (1.9520)\n",
            "TRAIN(104): [120/196] Batch: 0.0931 (0.0803) Data: 0.0364 (0.0525) Loss: 2.0122 (1.9514)\n",
            "TRAIN(104): [130/196] Batch: 0.0754 (0.0801) Data: 0.0585 (0.0521) Loss: 1.9647 (1.9547)\n",
            "TRAIN(104): [140/196] Batch: 0.0614 (0.0800) Data: 0.0522 (0.0518) Loss: 2.0823 (1.9574)\n",
            "TRAIN(104): [150/196] Batch: 0.0750 (0.0798) Data: 0.0542 (0.0515) Loss: 1.9929 (1.9569)\n",
            "TRAIN(104): [160/196] Batch: 0.0780 (0.0796) Data: 0.0587 (0.0514) Loss: 1.9202 (1.9569)\n",
            "TRAIN(104): [170/196] Batch: 0.0806 (0.0795) Data: 0.0540 (0.0513) Loss: 1.9672 (1.9536)\n",
            "TRAIN(104): [180/196] Batch: 0.0735 (0.0792) Data: 0.0611 (0.0512) Loss: 1.8637 (1.9547)\n",
            "TRAIN(104): [190/196] Batch: 0.0764 (0.0791) Data: 0.0618 (0.0513) Loss: 2.0388 (1.9578)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(104)         0:00:15         0:00:10         0:00:05          1.9555\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(105): [ 10/196] Batch: 0.0705 (0.1185) Data: 0.0548 (0.0824) Loss: 1.9987 (1.9343)\n",
            "TRAIN(105): [ 20/196] Batch: 0.0655 (0.0974) Data: 0.0584 (0.0681) Loss: 1.9559 (1.9425)\n",
            "TRAIN(105): [ 30/196] Batch: 0.0820 (0.0908) Data: 0.0488 (0.0622) Loss: 1.9654 (1.9364)\n",
            "TRAIN(105): [ 40/196] Batch: 0.0806 (0.0872) Data: 0.0505 (0.0592) Loss: 2.0172 (1.9361)\n",
            "TRAIN(105): [ 50/196] Batch: 0.0870 (0.0849) Data: 0.0544 (0.0575) Loss: 2.0917 (1.9459)\n",
            "TRAIN(105): [ 60/196] Batch: 0.0751 (0.0833) Data: 0.0613 (0.0564) Loss: 2.0015 (1.9484)\n",
            "TRAIN(105): [ 70/196] Batch: 0.0793 (0.0825) Data: 0.0510 (0.0556) Loss: 1.9372 (1.9479)\n",
            "TRAIN(105): [ 80/196] Batch: 0.0858 (0.0818) Data: 0.0511 (0.0547) Loss: 1.9736 (1.9480)\n",
            "TRAIN(105): [ 90/196] Batch: 0.0769 (0.0812) Data: 0.0502 (0.0538) Loss: 2.0356 (1.9491)\n",
            "TRAIN(105): [100/196] Batch: 0.0628 (0.0808) Data: 0.0520 (0.0531) Loss: 1.9438 (1.9498)\n",
            "TRAIN(105): [110/196] Batch: 0.0763 (0.0805) Data: 0.0537 (0.0525) Loss: 1.9648 (1.9549)\n",
            "TRAIN(105): [120/196] Batch: 0.0758 (0.0802) Data: 0.0572 (0.0520) Loss: 1.9713 (1.9578)\n",
            "TRAIN(105): [130/196] Batch: 0.0854 (0.0800) Data: 0.0488 (0.0515) Loss: 1.8079 (1.9571)\n",
            "TRAIN(105): [140/196] Batch: 0.0753 (0.0797) Data: 0.0622 (0.0514) Loss: 1.8355 (1.9575)\n",
            "TRAIN(105): [150/196] Batch: 0.0796 (0.0795) Data: 0.0544 (0.0514) Loss: 1.9351 (1.9574)\n",
            "TRAIN(105): [160/196] Batch: 0.0663 (0.0793) Data: 0.0620 (0.0513) Loss: 1.9464 (1.9570)\n",
            "TRAIN(105): [170/196] Batch: 0.0800 (0.0791) Data: 0.0501 (0.0513) Loss: 1.7272 (1.9528)\n",
            "TRAIN(105): [180/196] Batch: 0.0768 (0.0790) Data: 0.0553 (0.0512) Loss: 1.8488 (1.9510)\n",
            "TRAIN(105): [190/196] Batch: 0.0761 (0.0788) Data: 0.0629 (0.0512) Loss: 2.0939 (1.9559)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(105)         0:00:15         0:00:10         0:00:05          1.9561\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(106): [ 10/196] Batch: 0.0702 (0.1160) Data: 0.0532 (0.0786) Loss: 2.0180 (2.0490)\n",
            "TRAIN(106): [ 20/196] Batch: 0.0780 (0.0967) Data: 0.0494 (0.0647) Loss: 2.0532 (2.0085)\n",
            "TRAIN(106): [ 30/196] Batch: 0.0807 (0.0898) Data: 0.0546 (0.0605) Loss: 2.1089 (2.0074)\n",
            "TRAIN(106): [ 40/196] Batch: 0.0775 (0.0864) Data: 0.0563 (0.0579) Loss: 2.2313 (2.0008)\n",
            "TRAIN(106): [ 50/196] Batch: 0.0724 (0.0843) Data: 0.0562 (0.0562) Loss: 1.7779 (1.9862)\n",
            "TRAIN(106): [ 60/196] Batch: 0.0828 (0.0830) Data: 0.0539 (0.0553) Loss: 1.8849 (1.9828)\n",
            "TRAIN(106): [ 70/196] Batch: 0.0747 (0.0822) Data: 0.0499 (0.0539) Loss: 1.9055 (1.9730)\n",
            "TRAIN(106): [ 80/196] Batch: 0.0752 (0.0816) Data: 0.0528 (0.0526) Loss: 1.9861 (1.9621)\n",
            "TRAIN(106): [ 90/196] Batch: 0.0590 (0.0812) Data: 0.0535 (0.0515) Loss: 1.7710 (1.9613)\n",
            "TRAIN(106): [100/196] Batch: 0.0763 (0.0809) Data: 0.0517 (0.0509) Loss: 2.1260 (1.9624)\n",
            "TRAIN(106): [110/196] Batch: 0.0675 (0.0806) Data: 0.0489 (0.0503) Loss: 1.9459 (1.9576)\n",
            "TRAIN(106): [120/196] Batch: 0.0638 (0.0801) Data: 0.0612 (0.0500) Loss: 1.8821 (1.9530)\n",
            "TRAIN(106): [130/196] Batch: 0.0664 (0.0798) Data: 0.0623 (0.0502) Loss: 1.9692 (1.9533)\n",
            "TRAIN(106): [140/196] Batch: 0.0758 (0.0796) Data: 0.0550 (0.0500) Loss: 1.8710 (1.9490)\n",
            "TRAIN(106): [150/196] Batch: 0.0800 (0.0794) Data: 0.0519 (0.0501) Loss: 1.9218 (1.9495)\n",
            "TRAIN(106): [160/196] Batch: 0.0763 (0.0792) Data: 0.0554 (0.0502) Loss: 1.9443 (1.9475)\n",
            "TRAIN(106): [170/196] Batch: 0.0773 (0.0791) Data: 0.0504 (0.0499) Loss: 1.7537 (1.9454)\n",
            "TRAIN(106): [180/196] Batch: 0.0782 (0.0789) Data: 0.0553 (0.0500) Loss: 1.8695 (1.9438)\n",
            "TRAIN(106): [190/196] Batch: 0.0767 (0.0787) Data: 0.0614 (0.0501) Loss: 1.9412 (1.9417)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(106)         0:00:15         0:00:09         0:00:05          1.9405\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(107): [ 10/196] Batch: 0.0584 (0.1164) Data: 0.0597 (0.0851) Loss: 2.0268 (1.9250)\n",
            "TRAIN(107): [ 20/196] Batch: 0.0724 (0.0967) Data: 0.0565 (0.0691) Loss: 1.9878 (1.9157)\n",
            "TRAIN(107): [ 30/196] Batch: 0.0774 (0.0901) Data: 0.0548 (0.0638) Loss: 2.0357 (1.9306)\n",
            "TRAIN(107): [ 40/196] Batch: 0.0747 (0.0864) Data: 0.0618 (0.0609) Loss: 1.9916 (1.9426)\n",
            "TRAIN(107): [ 50/196] Batch: 0.0724 (0.0846) Data: 0.0524 (0.0581) Loss: 1.9694 (1.9513)\n",
            "TRAIN(107): [ 60/196] Batch: 0.0783 (0.0834) Data: 0.0483 (0.0557) Loss: 1.9301 (1.9544)\n",
            "TRAIN(107): [ 70/196] Batch: 0.0746 (0.0824) Data: 0.0504 (0.0545) Loss: 2.0654 (1.9505)\n",
            "TRAIN(107): [ 80/196] Batch: 0.0751 (0.0815) Data: 0.0576 (0.0537) Loss: 2.0668 (1.9522)\n",
            "TRAIN(107): [ 90/196] Batch: 0.0943 (0.0813) Data: 0.0409 (0.0532) Loss: 1.9172 (1.9566)\n",
            "TRAIN(107): [100/196] Batch: 0.0700 (0.0806) Data: 0.0612 (0.0527) Loss: 2.0589 (1.9604)\n",
            "TRAIN(107): [110/196] Batch: 0.0706 (0.0803) Data: 0.0550 (0.0523) Loss: 2.0062 (1.9580)\n",
            "TRAIN(107): [120/196] Batch: 0.0791 (0.0800) Data: 0.0501 (0.0518) Loss: 2.1456 (1.9566)\n",
            "TRAIN(107): [130/196] Batch: 0.0694 (0.0796) Data: 0.0571 (0.0517) Loss: 2.0513 (1.9511)\n",
            "TRAIN(107): [140/196] Batch: 0.0788 (0.0794) Data: 0.0501 (0.0516) Loss: 1.8383 (1.9491)\n",
            "TRAIN(107): [150/196] Batch: 0.0713 (0.0792) Data: 0.0540 (0.0514) Loss: 2.1180 (1.9491)\n",
            "TRAIN(107): [160/196] Batch: 0.0774 (0.0789) Data: 0.0615 (0.0514) Loss: 1.8014 (1.9486)\n",
            "TRAIN(107): [170/196] Batch: 0.0775 (0.0788) Data: 0.0556 (0.0513) Loss: 1.9047 (1.9490)\n",
            "TRAIN(107): [180/196] Batch: 0.0809 (0.0787) Data: 0.0569 (0.0514) Loss: 1.8905 (1.9467)\n",
            "TRAIN(107): [190/196] Batch: 0.0755 (0.0785) Data: 0.0614 (0.0515) Loss: 2.1042 (1.9472)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(107)         0:00:15         0:00:10         0:00:05          1.9456\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(108): [ 10/196] Batch: 0.0734 (0.1195) Data: 0.0517 (0.0841) Loss: 1.7489 (1.9957)\n",
            "TRAIN(108): [ 20/196] Batch: 0.0689 (0.0980) Data: 0.0546 (0.0674) Loss: 1.7315 (1.9300)\n",
            "TRAIN(108): [ 30/196] Batch: 0.0733 (0.0910) Data: 0.0565 (0.0617) Loss: 1.8402 (1.9081)\n",
            "TRAIN(108): [ 40/196] Batch: 0.0666 (0.0878) Data: 0.0535 (0.0568) Loss: 1.8443 (1.9007)\n",
            "TRAIN(108): [ 50/196] Batch: 0.0874 (0.0860) Data: 0.0431 (0.0544) Loss: 2.0458 (1.9087)\n",
            "TRAIN(108): [ 60/196] Batch: 0.0752 (0.0844) Data: 0.0498 (0.0525) Loss: 1.8540 (1.9120)\n",
            "TRAIN(108): [ 70/196] Batch: 0.0781 (0.0834) Data: 0.0466 (0.0514) Loss: 1.9975 (1.9138)\n",
            "TRAIN(108): [ 80/196] Batch: 0.0797 (0.0825) Data: 0.0506 (0.0510) Loss: 1.9693 (1.9211)\n",
            "TRAIN(108): [ 90/196] Batch: 0.0720 (0.0817) Data: 0.0563 (0.0508) Loss: 1.8392 (1.9240)\n",
            "TRAIN(108): [100/196] Batch: 0.0704 (0.0812) Data: 0.0552 (0.0503) Loss: 1.9942 (1.9251)\n",
            "TRAIN(108): [110/196] Batch: 0.0815 (0.0807) Data: 0.0537 (0.0502) Loss: 1.9482 (1.9256)\n",
            "TRAIN(108): [120/196] Batch: 0.0813 (0.0804) Data: 0.0506 (0.0500) Loss: 1.8362 (1.9267)\n",
            "TRAIN(108): [130/196] Batch: 0.0854 (0.0800) Data: 0.0499 (0.0498) Loss: 1.8039 (1.9272)\n",
            "TRAIN(108): [140/196] Batch: 0.0759 (0.0797) Data: 0.0570 (0.0497) Loss: 1.8253 (1.9270)\n",
            "TRAIN(108): [150/196] Batch: 0.0762 (0.0795) Data: 0.0621 (0.0497) Loss: 2.0401 (1.9292)\n",
            "TRAIN(108): [160/196] Batch: 0.0730 (0.0793) Data: 0.0515 (0.0497) Loss: 1.8093 (1.9307)\n",
            "TRAIN(108): [170/196] Batch: 0.0694 (0.0791) Data: 0.0627 (0.0498) Loss: 1.8466 (1.9301)\n",
            "TRAIN(108): [180/196] Batch: 0.0658 (0.0789) Data: 0.0618 (0.0498) Loss: 1.9332 (1.9284)\n",
            "TRAIN(108): [190/196] Batch: 0.0754 (0.0788) Data: 0.0615 (0.0500) Loss: 1.8043 (1.9268)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(108)         0:00:15         0:00:09         0:00:05          1.9262\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(109): [ 10/196] Batch: 0.0870 (0.1231) Data: 0.0377 (0.0825) Loss: 1.7178 (1.9354)\n",
            "TRAIN(109): [ 20/196] Batch: 0.0622 (0.0998) Data: 0.0528 (0.0626) Loss: 1.9332 (1.9299)\n",
            "TRAIN(109): [ 30/196] Batch: 0.0770 (0.0923) Data: 0.0515 (0.0570) Loss: 1.8841 (1.9190)\n",
            "TRAIN(109): [ 40/196] Batch: 0.0756 (0.0886) Data: 0.0489 (0.0538) Loss: 1.9935 (1.9177)\n",
            "TRAIN(109): [ 50/196] Batch: 0.0782 (0.0862) Data: 0.0487 (0.0523) Loss: 2.0629 (1.9185)\n",
            "TRAIN(109): [ 60/196] Batch: 0.0658 (0.0844) Data: 0.0557 (0.0518) Loss: 1.9928 (1.9215)\n",
            "TRAIN(109): [ 70/196] Batch: 0.0760 (0.0832) Data: 0.0625 (0.0514) Loss: 1.9859 (1.9280)\n",
            "TRAIN(109): [ 80/196] Batch: 0.0757 (0.0824) Data: 0.0550 (0.0512) Loss: 2.1306 (1.9276)\n",
            "TRAIN(109): [ 90/196] Batch: 0.0874 (0.0818) Data: 0.0453 (0.0510) Loss: 1.9625 (1.9285)\n",
            "TRAIN(109): [100/196] Batch: 0.0770 (0.0811) Data: 0.0545 (0.0509) Loss: 2.0261 (1.9325)\n",
            "TRAIN(109): [110/196] Batch: 0.0760 (0.0806) Data: 0.0567 (0.0506) Loss: 2.0195 (1.9334)\n",
            "TRAIN(109): [120/196] Batch: 0.0762 (0.0803) Data: 0.0549 (0.0506) Loss: 1.9235 (1.9315)\n",
            "TRAIN(109): [130/196] Batch: 0.0712 (0.0800) Data: 0.0550 (0.0505) Loss: 1.9156 (1.9310)\n",
            "TRAIN(109): [140/196] Batch: 0.0758 (0.0796) Data: 0.0626 (0.0505) Loss: 1.9846 (1.9299)\n",
            "TRAIN(109): [150/196] Batch: 0.0736 (0.0794) Data: 0.0598 (0.0506) Loss: 1.8661 (1.9298)\n",
            "TRAIN(109): [160/196] Batch: 0.0703 (0.0793) Data: 0.0550 (0.0505) Loss: 1.8333 (1.9291)\n",
            "TRAIN(109): [170/196] Batch: 0.0652 (0.0790) Data: 0.0626 (0.0504) Loss: 2.0233 (1.9304)\n",
            "TRAIN(109): [180/196] Batch: 0.0840 (0.0789) Data: 0.0534 (0.0504) Loss: 1.9695 (1.9333)\n",
            "TRAIN(109): [190/196] Batch: 0.0755 (0.0787) Data: 0.0598 (0.0504) Loss: 1.9566 (1.9295)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(109)         0:00:15         0:00:09         0:00:05          1.9306\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(110): [ 10/196] Batch: 0.0621 (0.1400) Data: 0.0523 (0.1025) Loss: 1.7960 (1.9534)\n",
            "TRAIN(110): [ 20/196] Batch: 0.0725 (0.1101) Data: 0.0498 (0.0751) Loss: 2.1474 (1.9592)\n",
            "TRAIN(110): [ 30/196] Batch: 0.0843 (0.0986) Data: 0.0559 (0.0669) Loss: 2.0533 (1.9526)\n",
            "TRAIN(110): [ 40/196] Batch: 0.0680 (0.0929) Data: 0.0548 (0.0628) Loss: 1.9654 (1.9466)\n",
            "TRAIN(110): [ 50/196] Batch: 0.0756 (0.0898) Data: 0.0511 (0.0600) Loss: 1.9483 (1.9461)\n",
            "TRAIN(110): [ 60/196] Batch: 0.0814 (0.0874) Data: 0.0548 (0.0587) Loss: 1.8436 (1.9339)\n",
            "TRAIN(110): [ 70/196] Batch: 0.0817 (0.0859) Data: 0.0502 (0.0572) Loss: 1.9253 (1.9299)\n",
            "TRAIN(110): [ 80/196] Batch: 0.0796 (0.0846) Data: 0.0565 (0.0562) Loss: 2.0281 (1.9394)\n",
            "TRAIN(110): [ 90/196] Batch: 0.0620 (0.0835) Data: 0.0615 (0.0556) Loss: 1.8299 (1.9345)\n",
            "TRAIN(110): [100/196] Batch: 0.0822 (0.0829) Data: 0.0565 (0.0553) Loss: 1.8593 (1.9314)\n",
            "TRAIN(110): [110/196] Batch: 0.0712 (0.0823) Data: 0.0544 (0.0548) Loss: 1.8586 (1.9234)\n",
            "TRAIN(110): [120/196] Batch: 0.0815 (0.0818) Data: 0.0502 (0.0543) Loss: 2.0175 (1.9276)\n",
            "TRAIN(110): [130/196] Batch: 0.0736 (0.0813) Data: 0.0568 (0.0541) Loss: 1.9105 (1.9263)\n",
            "TRAIN(110): [140/196] Batch: 0.0775 (0.0809) Data: 0.0607 (0.0541) Loss: 1.8894 (1.9260)\n",
            "TRAIN(110): [150/196] Batch: 0.0876 (0.0807) Data: 0.0481 (0.0540) Loss: 2.0961 (1.9265)\n",
            "TRAIN(110): [160/196] Batch: 0.0832 (0.0805) Data: 0.0466 (0.0534) Loss: 1.8576 (1.9287)\n",
            "TRAIN(110): [170/196] Batch: 0.0635 (0.0803) Data: 0.0489 (0.0530) Loss: 1.8922 (1.9284)\n",
            "TRAIN(110): [180/196] Batch: 0.0793 (0.0802) Data: 0.0520 (0.0528) Loss: 1.9729 (1.9307)\n",
            "TRAIN(110): [190/196] Batch: 0.0739 (0.0799) Data: 0.0632 (0.0526) Loss: 1.9977 (1.9331)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(110)         0:00:15         0:00:10         0:00:05          1.9330\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(111): [ 10/196] Batch: 0.0688 (0.1156) Data: 0.0563 (0.0811) Loss: 1.9574 (1.9586)\n",
            "TRAIN(111): [ 20/196] Batch: 0.0804 (0.0964) Data: 0.0502 (0.0665) Loss: 1.9686 (1.9334)\n",
            "TRAIN(111): [ 30/196] Batch: 0.0801 (0.0897) Data: 0.0541 (0.0617) Loss: 1.8443 (1.9250)\n",
            "TRAIN(111): [ 40/196] Batch: 0.0754 (0.0861) Data: 0.0633 (0.0597) Loss: 2.1670 (1.9312)\n",
            "TRAIN(111): [ 50/196] Batch: 0.0753 (0.0843) Data: 0.0512 (0.0574) Loss: 1.9483 (1.9398)\n",
            "TRAIN(111): [ 60/196] Batch: 0.0802 (0.0829) Data: 0.0555 (0.0562) Loss: 1.8956 (1.9359)\n",
            "TRAIN(111): [ 70/196] Batch: 0.0805 (0.0820) Data: 0.0511 (0.0552) Loss: 1.9580 (1.9320)\n",
            "TRAIN(111): [ 80/196] Batch: 0.0782 (0.0812) Data: 0.0536 (0.0545) Loss: 2.0222 (1.9288)\n",
            "TRAIN(111): [ 90/196] Batch: 0.0884 (0.0807) Data: 0.0494 (0.0541) Loss: 2.0241 (1.9267)\n",
            "TRAIN(111): [100/196] Batch: 0.0730 (0.0802) Data: 0.0579 (0.0536) Loss: 1.7976 (1.9316)\n",
            "TRAIN(111): [110/196] Batch: 0.0719 (0.0798) Data: 0.0580 (0.0535) Loss: 1.8906 (1.9260)\n",
            "TRAIN(111): [120/196] Batch: 0.0828 (0.0796) Data: 0.0561 (0.0532) Loss: 2.0073 (1.9254)\n",
            "TRAIN(111): [130/196] Batch: 0.0748 (0.0793) Data: 0.0592 (0.0528) Loss: 1.8873 (1.9252)\n",
            "TRAIN(111): [140/196] Batch: 0.0688 (0.0792) Data: 0.0516 (0.0521) Loss: 1.9338 (1.9224)\n",
            "TRAIN(111): [150/196] Batch: 0.0723 (0.0791) Data: 0.0512 (0.0516) Loss: 1.7487 (1.9178)\n",
            "TRAIN(111): [160/196] Batch: 0.0748 (0.0789) Data: 0.0506 (0.0513) Loss: 1.7394 (1.9171)\n",
            "TRAIN(111): [170/196] Batch: 0.0744 (0.0787) Data: 0.0515 (0.0510) Loss: 1.8656 (1.9150)\n",
            "TRAIN(111): [180/196] Batch: 0.0760 (0.0786) Data: 0.0571 (0.0508) Loss: 2.2662 (1.9172)\n",
            "TRAIN(111): [190/196] Batch: 0.0779 (0.0784) Data: 0.0590 (0.0508) Loss: 1.6818 (1.9116)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(111)         0:00:15         0:00:09         0:00:05          1.9111\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(112): [ 10/196] Batch: 0.0807 (0.1204) Data: 0.0426 (0.0823) Loss: 1.6674 (1.8854)\n",
            "TRAIN(112): [ 20/196] Batch: 0.0750 (0.0980) Data: 0.0559 (0.0670) Loss: 1.8870 (1.8901)\n",
            "TRAIN(112): [ 30/196] Batch: 0.0761 (0.0911) Data: 0.0499 (0.0610) Loss: 1.9261 (1.9014)\n",
            "TRAIN(112): [ 40/196] Batch: 0.0749 (0.0872) Data: 0.0595 (0.0586) Loss: 1.8800 (1.9055)\n",
            "TRAIN(112): [ 50/196] Batch: 0.0591 (0.0852) Data: 0.0507 (0.0557) Loss: 2.0403 (1.9037)\n",
            "TRAIN(112): [ 60/196] Batch: 0.0842 (0.0840) Data: 0.0426 (0.0538) Loss: 2.0230 (1.9092)\n",
            "TRAIN(112): [ 70/196] Batch: 0.0847 (0.0833) Data: 0.0392 (0.0527) Loss: 1.9022 (1.9125)\n",
            "TRAIN(112): [ 80/196] Batch: 0.0708 (0.0823) Data: 0.0543 (0.0519) Loss: 1.7329 (1.9151)\n",
            "TRAIN(112): [ 90/196] Batch: 0.0704 (0.0817) Data: 0.0561 (0.0516) Loss: 1.7631 (1.9123)\n",
            "TRAIN(112): [100/196] Batch: 0.0801 (0.0811) Data: 0.0503 (0.0511) Loss: 2.0068 (1.9162)\n",
            "TRAIN(112): [110/196] Batch: 0.0743 (0.0807) Data: 0.0511 (0.0507) Loss: 1.9431 (1.9159)\n",
            "TRAIN(112): [120/196] Batch: 0.0745 (0.0803) Data: 0.0535 (0.0502) Loss: 1.7866 (1.9188)\n",
            "TRAIN(112): [130/196] Batch: 0.0752 (0.0801) Data: 0.0508 (0.0498) Loss: 1.7394 (1.9148)\n",
            "TRAIN(112): [140/196] Batch: 0.0727 (0.0798) Data: 0.0559 (0.0497) Loss: 1.9798 (1.9139)\n",
            "TRAIN(112): [150/196] Batch: 0.0916 (0.0800) Data: 0.0311 (0.0494) Loss: 1.9136 (1.9113)\n",
            "TRAIN(112): [160/196] Batch: 0.0770 (0.0796) Data: 0.0572 (0.0493) Loss: 1.9422 (1.9111)\n",
            "TRAIN(112): [170/196] Batch: 0.0762 (0.0794) Data: 0.0514 (0.0493) Loss: 2.0822 (1.9104)\n",
            "TRAIN(112): [180/196] Batch: 0.0835 (0.0792) Data: 0.0540 (0.0494) Loss: 2.0878 (1.9124)\n",
            "TRAIN(112): [190/196] Batch: 0.0752 (0.0790) Data: 0.0613 (0.0495) Loss: 2.0046 (1.9119)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(112)         0:00:15         0:00:09         0:00:05          1.9117\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(113): [ 10/196] Batch: 0.0836 (0.1204) Data: 0.0473 (0.0823) Loss: 1.7909 (1.8525)\n",
            "TRAIN(113): [ 20/196] Batch: 0.0796 (0.0984) Data: 0.0504 (0.0664) Loss: 1.9570 (1.8676)\n",
            "TRAIN(113): [ 30/196] Batch: 0.0683 (0.0909) Data: 0.0589 (0.0615) Loss: 2.0363 (1.8798)\n",
            "TRAIN(113): [ 40/196] Batch: 0.0729 (0.0871) Data: 0.0631 (0.0587) Loss: 1.8596 (1.8779)\n",
            "TRAIN(113): [ 50/196] Batch: 0.0723 (0.0850) Data: 0.0562 (0.0569) Loss: 1.7646 (1.8804)\n",
            "TRAIN(113): [ 60/196] Batch: 0.0673 (0.0836) Data: 0.0541 (0.0557) Loss: 1.9959 (1.8828)\n",
            "TRAIN(113): [ 70/196] Batch: 0.0659 (0.0825) Data: 0.0616 (0.0549) Loss: 1.8317 (1.8849)\n",
            "TRAIN(113): [ 80/196] Batch: 0.0652 (0.0817) Data: 0.0634 (0.0544) Loss: 2.0706 (1.8927)\n",
            "TRAIN(113): [ 90/196] Batch: 0.0748 (0.0812) Data: 0.0506 (0.0535) Loss: 1.9483 (1.8908)\n",
            "TRAIN(113): [100/196] Batch: 0.0758 (0.0807) Data: 0.0500 (0.0528) Loss: 1.8498 (1.8895)\n",
            "TRAIN(113): [110/196] Batch: 0.0749 (0.0803) Data: 0.0542 (0.0522) Loss: 1.8306 (1.8896)\n",
            "TRAIN(113): [120/196] Batch: 0.0595 (0.0802) Data: 0.0522 (0.0517) Loss: 1.9273 (1.8918)\n",
            "TRAIN(113): [130/196] Batch: 0.0732 (0.0800) Data: 0.0507 (0.0512) Loss: 1.8921 (1.8884)\n",
            "TRAIN(113): [140/196] Batch: 0.0868 (0.0797) Data: 0.0511 (0.0512) Loss: 2.1528 (1.8902)\n",
            "TRAIN(113): [150/196] Batch: 0.0714 (0.0795) Data: 0.0568 (0.0511) Loss: 1.8483 (1.8891)\n",
            "TRAIN(113): [160/196] Batch: 0.0776 (0.0793) Data: 0.0562 (0.0510) Loss: 2.0079 (1.8906)\n",
            "TRAIN(113): [170/196] Batch: 0.0705 (0.0791) Data: 0.0624 (0.0510) Loss: 2.0157 (1.8911)\n",
            "TRAIN(113): [180/196] Batch: 0.0724 (0.0789) Data: 0.0567 (0.0509) Loss: 1.9104 (1.8893)\n",
            "TRAIN(113): [190/196] Batch: 0.0757 (0.0788) Data: 0.0614 (0.0510) Loss: 1.9177 (1.8906)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(113)         0:00:15         0:00:09         0:00:05          1.8911\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(114): [ 10/196] Batch: 0.0851 (0.1187) Data: 0.0502 (0.0828) Loss: 1.8337 (1.8707)\n",
            "TRAIN(114): [ 20/196] Batch: 0.0746 (0.0975) Data: 0.0555 (0.0669) Loss: 2.0309 (1.9078)\n",
            "TRAIN(114): [ 30/196] Batch: 0.0852 (0.0906) Data: 0.0517 (0.0620) Loss: 1.8006 (1.8910)\n",
            "TRAIN(114): [ 40/196] Batch: 0.0738 (0.0868) Data: 0.0586 (0.0595) Loss: 1.9518 (1.8825)\n",
            "TRAIN(114): [ 50/196] Batch: 0.0650 (0.0846) Data: 0.0639 (0.0577) Loss: 1.7959 (1.8928)\n",
            "TRAIN(114): [ 60/196] Batch: 0.0766 (0.0832) Data: 0.0555 (0.0564) Loss: 1.8793 (1.8996)\n",
            "TRAIN(114): [ 70/196] Batch: 0.0740 (0.0824) Data: 0.0500 (0.0552) Loss: 1.7893 (1.8873)\n",
            "TRAIN(114): [ 80/196] Batch: 0.0976 (0.0820) Data: 0.0342 (0.0535) Loss: 1.8926 (1.8883)\n",
            "TRAIN(114): [ 90/196] Batch: 0.0707 (0.0815) Data: 0.0544 (0.0523) Loss: 1.6864 (1.8803)\n",
            "TRAIN(114): [100/196] Batch: 0.0787 (0.0811) Data: 0.0508 (0.0516) Loss: 1.9408 (1.8820)\n",
            "TRAIN(114): [110/196] Batch: 0.0755 (0.0806) Data: 0.0481 (0.0510) Loss: 1.7685 (1.8815)\n",
            "TRAIN(114): [120/196] Batch: 0.0864 (0.0803) Data: 0.0516 (0.0510) Loss: 1.9855 (1.8843)\n",
            "TRAIN(114): [130/196] Batch: 0.0778 (0.0799) Data: 0.0571 (0.0509) Loss: 1.8734 (1.8803)\n",
            "TRAIN(114): [140/196] Batch: 0.0766 (0.0796) Data: 0.0587 (0.0510) Loss: 1.9075 (1.8815)\n",
            "TRAIN(114): [150/196] Batch: 0.0710 (0.0794) Data: 0.0621 (0.0510) Loss: 1.9633 (1.8831)\n",
            "TRAIN(114): [160/196] Batch: 0.0797 (0.0793) Data: 0.0513 (0.0510) Loss: 1.8792 (1.8836)\n",
            "TRAIN(114): [170/196] Batch: 0.0799 (0.0791) Data: 0.0546 (0.0509) Loss: 1.7735 (1.8844)\n",
            "TRAIN(114): [180/196] Batch: 0.0651 (0.0789) Data: 0.0615 (0.0509) Loss: 1.9999 (1.8840)\n",
            "TRAIN(114): [190/196] Batch: 0.0759 (0.0788) Data: 0.0627 (0.0510) Loss: 2.0767 (1.8855)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(114)         0:00:15         0:00:09         0:00:05          1.8851\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(115): [ 10/196] Batch: 0.0812 (0.1226) Data: 0.0490 (0.0862) Loss: 1.8607 (1.8955)\n",
            "TRAIN(115): [ 20/196] Batch: 0.0772 (0.0996) Data: 0.0532 (0.0691) Loss: 1.8187 (1.9198)\n",
            "TRAIN(115): [ 30/196] Batch: 0.0800 (0.0919) Data: 0.0515 (0.0631) Loss: 1.9874 (1.9040)\n",
            "TRAIN(115): [ 40/196] Batch: 0.0699 (0.0878) Data: 0.0610 (0.0600) Loss: 1.8077 (1.8847)\n",
            "TRAIN(115): [ 50/196] Batch: 0.0759 (0.0856) Data: 0.0526 (0.0573) Loss: 1.7821 (1.8795)\n",
            "TRAIN(115): [ 60/196] Batch: 0.0714 (0.0841) Data: 0.0531 (0.0553) Loss: 1.8590 (1.8835)\n",
            "TRAIN(115): [ 70/196] Batch: 0.0535 (0.0832) Data: 0.0570 (0.0537) Loss: 1.9055 (1.8829)\n",
            "TRAIN(115): [ 80/196] Batch: 0.0769 (0.0824) Data: 0.0591 (0.0533) Loss: 1.9375 (1.8780)\n",
            "TRAIN(115): [ 90/196] Batch: 0.0742 (0.0819) Data: 0.0481 (0.0524) Loss: 1.7956 (1.8760)\n",
            "TRAIN(115): [100/196] Batch: 0.0823 (0.0813) Data: 0.0500 (0.0522) Loss: 1.7388 (1.8738)\n",
            "TRAIN(115): [110/196] Batch: 0.0777 (0.0808) Data: 0.0542 (0.0521) Loss: 1.8914 (1.8726)\n",
            "TRAIN(115): [120/196] Batch: 0.0783 (0.0805) Data: 0.0556 (0.0519) Loss: 1.8441 (1.8708)\n",
            "TRAIN(115): [130/196] Batch: 0.0775 (0.0801) Data: 0.0559 (0.0518) Loss: 1.7140 (1.8695)\n",
            "TRAIN(115): [140/196] Batch: 0.0796 (0.0799) Data: 0.0508 (0.0516) Loss: 2.0545 (1.8733)\n",
            "TRAIN(115): [150/196] Batch: 0.0694 (0.0796) Data: 0.0565 (0.0516) Loss: 1.9175 (1.8740)\n",
            "TRAIN(115): [160/196] Batch: 0.0856 (0.0795) Data: 0.0522 (0.0515) Loss: 2.1795 (1.8734)\n",
            "TRAIN(115): [170/196] Batch: 0.0719 (0.0793) Data: 0.0567 (0.0514) Loss: 1.7458 (1.8707)\n",
            "TRAIN(115): [180/196] Batch: 0.0789 (0.0791) Data: 0.0527 (0.0514) Loss: 1.9173 (1.8693)\n",
            "TRAIN(115): [190/196] Batch: 0.0741 (0.0789) Data: 0.0609 (0.0513) Loss: 1.8804 (1.8675)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(115)         0:00:15         0:00:10         0:00:05          1.8687\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(116): [ 10/196] Batch: 0.0621 (0.1217) Data: 0.0612 (0.0893) Loss: 1.6743 (1.9249)\n",
            "TRAIN(116): [ 20/196] Batch: 0.0670 (0.0995) Data: 0.0572 (0.0709) Loss: 1.9699 (1.9172)\n",
            "TRAIN(116): [ 30/196] Batch: 0.0686 (0.0924) Data: 0.0495 (0.0632) Loss: 1.7559 (1.9061)\n",
            "TRAIN(116): [ 40/196] Batch: 0.0833 (0.0889) Data: 0.0449 (0.0585) Loss: 1.8520 (1.8954)\n",
            "TRAIN(116): [ 50/196] Batch: 0.0769 (0.0864) Data: 0.0509 (0.0559) Loss: 1.9029 (1.8919)\n",
            "TRAIN(116): [ 60/196] Batch: 0.0752 (0.0850) Data: 0.0534 (0.0544) Loss: 1.7511 (1.8855)\n",
            "TRAIN(116): [ 70/196] Batch: 0.0673 (0.0837) Data: 0.0562 (0.0535) Loss: 1.8448 (1.8810)\n",
            "TRAIN(116): [ 80/196] Batch: 0.0880 (0.0829) Data: 0.0516 (0.0530) Loss: 1.7703 (1.8726)\n",
            "TRAIN(116): [ 90/196] Batch: 0.0805 (0.0821) Data: 0.0522 (0.0526) Loss: 1.7342 (1.8752)\n",
            "TRAIN(116): [100/196] Batch: 0.0810 (0.0815) Data: 0.0559 (0.0523) Loss: 1.8540 (1.8762)\n",
            "TRAIN(116): [110/196] Batch: 0.0661 (0.0810) Data: 0.0622 (0.0521) Loss: 1.7866 (1.8770)\n",
            "TRAIN(116): [120/196] Batch: 0.0728 (0.0807) Data: 0.0546 (0.0520) Loss: 1.8572 (1.8715)\n",
            "TRAIN(116): [130/196] Batch: 0.0725 (0.0803) Data: 0.0559 (0.0519) Loss: 1.8331 (1.8724)\n",
            "TRAIN(116): [140/196] Batch: 0.0874 (0.0801) Data: 0.0517 (0.0516) Loss: 1.9177 (1.8705)\n",
            "TRAIN(116): [150/196] Batch: 0.0699 (0.0798) Data: 0.0549 (0.0514) Loss: 1.6902 (1.8677)\n",
            "TRAIN(116): [160/196] Batch: 0.0855 (0.0796) Data: 0.0503 (0.0512) Loss: 1.8223 (1.8699)\n",
            "TRAIN(116): [170/196] Batch: 0.0771 (0.0794) Data: 0.0560 (0.0511) Loss: 1.9675 (1.8718)\n",
            "TRAIN(116): [180/196] Batch: 0.0851 (0.0792) Data: 0.0507 (0.0510) Loss: 1.9540 (1.8747)\n",
            "TRAIN(116): [190/196] Batch: 0.0743 (0.0790) Data: 0.0621 (0.0511) Loss: 1.7182 (1.8733)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(116)         0:00:15         0:00:10         0:00:05          1.8724\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(117): [ 10/196] Batch: 0.0776 (0.1248) Data: 0.0465 (0.0796) Loss: 1.8446 (1.8881)\n",
            "TRAIN(117): [ 20/196] Batch: 0.0671 (0.1033) Data: 0.0445 (0.0573) Loss: 1.9402 (1.8827)\n",
            "TRAIN(117): [ 30/196] Batch: 0.0776 (0.0951) Data: 0.0474 (0.0529) Loss: 2.0085 (1.8764)\n",
            "TRAIN(117): [ 40/196] Batch: 0.0729 (0.0907) Data: 0.0526 (0.0510) Loss: 1.9413 (1.8648)\n",
            "TRAIN(117): [ 50/196] Batch: 0.0753 (0.0879) Data: 0.0506 (0.0492) Loss: 2.0683 (1.8766)\n",
            "TRAIN(117): [ 60/196] Batch: 0.0689 (0.0858) Data: 0.0628 (0.0495) Loss: 1.8719 (1.8749)\n",
            "TRAIN(117): [ 70/196] Batch: 0.0768 (0.0845) Data: 0.0548 (0.0494) Loss: 1.8557 (1.8698)\n",
            "TRAIN(117): [ 80/196] Batch: 0.0778 (0.0835) Data: 0.0528 (0.0495) Loss: 2.0059 (1.8720)\n",
            "TRAIN(117): [ 90/196] Batch: 0.0723 (0.0826) Data: 0.0633 (0.0496) Loss: 2.0554 (1.8709)\n",
            "TRAIN(117): [100/196] Batch: 0.0822 (0.0821) Data: 0.0499 (0.0497) Loss: 1.8195 (1.8652)\n",
            "TRAIN(117): [110/196] Batch: 0.0756 (0.0815) Data: 0.0552 (0.0498) Loss: 1.8343 (1.8676)\n",
            "TRAIN(117): [120/196] Batch: 0.0793 (0.0811) Data: 0.0524 (0.0497) Loss: 1.7926 (1.8672)\n",
            "TRAIN(117): [130/196] Batch: 0.0728 (0.0807) Data: 0.0552 (0.0495) Loss: 1.9385 (1.8674)\n",
            "TRAIN(117): [140/196] Batch: 0.0779 (0.0804) Data: 0.0530 (0.0495) Loss: 1.6585 (1.8632)\n",
            "TRAIN(117): [150/196] Batch: 0.0654 (0.0801) Data: 0.0615 (0.0495) Loss: 1.9766 (1.8620)\n",
            "TRAIN(117): [160/196] Batch: 0.0709 (0.0799) Data: 0.0505 (0.0494) Loss: 1.6198 (1.8630)\n",
            "TRAIN(117): [170/196] Batch: 0.0876 (0.0797) Data: 0.0498 (0.0495) Loss: 1.6056 (1.8632)\n",
            "TRAIN(117): [180/196] Batch: 0.0848 (0.0795) Data: 0.0537 (0.0496) Loss: 1.7362 (1.8642)\n",
            "TRAIN(117): [190/196] Batch: 0.0763 (0.0793) Data: 0.0603 (0.0495) Loss: 1.7309 (1.8642)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(117)         0:00:15         0:00:09         0:00:05          1.8636\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(118): [ 10/196] Batch: 0.0579 (0.1250) Data: 0.0462 (0.0779) Loss: 1.7977 (1.8780)\n",
            "TRAIN(118): [ 20/196] Batch: 0.0782 (0.1019) Data: 0.0499 (0.0599) Loss: 1.8062 (1.8804)\n",
            "TRAIN(118): [ 30/196] Batch: 0.0687 (0.0931) Data: 0.0598 (0.0574) Loss: 1.8843 (1.8917)\n",
            "TRAIN(118): [ 40/196] Batch: 0.0784 (0.0891) Data: 0.0550 (0.0555) Loss: 2.0128 (1.8971)\n",
            "TRAIN(118): [ 50/196] Batch: 0.0709 (0.0863) Data: 0.0615 (0.0543) Loss: 1.8790 (1.8917)\n",
            "TRAIN(118): [ 60/196] Batch: 0.0684 (0.0848) Data: 0.0530 (0.0533) Loss: 1.9061 (1.8932)\n",
            "TRAIN(118): [ 70/196] Batch: 0.0692 (0.0835) Data: 0.0570 (0.0526) Loss: 1.7731 (1.8858)\n",
            "TRAIN(118): [ 80/196] Batch: 0.0707 (0.0826) Data: 0.0631 (0.0523) Loss: 1.9363 (1.8780)\n",
            "TRAIN(118): [ 90/196] Batch: 0.0756 (0.0819) Data: 0.0617 (0.0523) Loss: 1.9534 (1.8821)\n",
            "TRAIN(118): [100/196] Batch: 0.0713 (0.0813) Data: 0.0626 (0.0521) Loss: 1.7874 (1.8781)\n",
            "TRAIN(118): [110/196] Batch: 0.0703 (0.0809) Data: 0.0553 (0.0519) Loss: 1.8259 (1.8730)\n",
            "TRAIN(118): [120/196] Batch: 0.0694 (0.0805) Data: 0.0617 (0.0517) Loss: 1.8680 (1.8710)\n",
            "TRAIN(118): [130/196] Batch: 0.0719 (0.0802) Data: 0.0555 (0.0515) Loss: 1.8462 (1.8651)\n",
            "TRAIN(118): [140/196] Batch: 0.0801 (0.0800) Data: 0.0526 (0.0515) Loss: 1.8775 (1.8622)\n",
            "TRAIN(118): [150/196] Batch: 0.0735 (0.0797) Data: 0.0498 (0.0513) Loss: 1.8306 (1.8603)\n",
            "TRAIN(118): [160/196] Batch: 0.0711 (0.0795) Data: 0.0577 (0.0510) Loss: 1.8839 (1.8610)\n",
            "TRAIN(118): [170/196] Batch: 0.0658 (0.0793) Data: 0.0614 (0.0506) Loss: 1.9239 (1.8576)\n",
            "TRAIN(118): [180/196] Batch: 0.0799 (0.0793) Data: 0.0476 (0.0505) Loss: 1.8492 (1.8575)\n",
            "TRAIN(118): [190/196] Batch: 0.0758 (0.0790) Data: 0.0617 (0.0502) Loss: 1.6426 (1.8538)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(118)         0:00:15         0:00:09         0:00:05          1.8534\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(119): [ 10/196] Batch: 0.0789 (0.1192) Data: 0.0490 (0.0814) Loss: 1.7833 (1.8349)\n",
            "TRAIN(119): [ 20/196] Batch: 0.0829 (0.0978) Data: 0.0524 (0.0669) Loss: 1.6574 (1.8324)\n",
            "TRAIN(119): [ 30/196] Batch: 0.0690 (0.0906) Data: 0.0578 (0.0614) Loss: 1.8863 (1.8403)\n",
            "TRAIN(119): [ 40/196] Batch: 0.0743 (0.0869) Data: 0.0635 (0.0591) Loss: 1.7949 (1.8309)\n",
            "TRAIN(119): [ 50/196] Batch: 0.0767 (0.0849) Data: 0.0538 (0.0575) Loss: 1.8766 (1.8322)\n",
            "TRAIN(119): [ 60/196] Batch: 0.0788 (0.0835) Data: 0.0495 (0.0561) Loss: 1.8367 (1.8349)\n",
            "TRAIN(119): [ 70/196] Batch: 0.0854 (0.0825) Data: 0.0517 (0.0554) Loss: 1.9407 (1.8354)\n",
            "TRAIN(119): [ 80/196] Batch: 0.0826 (0.0817) Data: 0.0560 (0.0546) Loss: 1.8638 (1.8376)\n",
            "TRAIN(119): [ 90/196] Batch: 0.0755 (0.0810) Data: 0.0619 (0.0542) Loss: 1.9284 (1.8376)\n",
            "TRAIN(119): [100/196] Batch: 0.0679 (0.0806) Data: 0.0575 (0.0538) Loss: 1.9564 (1.8370)\n",
            "TRAIN(119): [110/196] Batch: 0.0811 (0.0802) Data: 0.0559 (0.0536) Loss: 1.8062 (1.8363)\n",
            "TRAIN(119): [120/196] Batch: 0.0689 (0.0799) Data: 0.0570 (0.0533) Loss: 1.9607 (1.8336)\n",
            "TRAIN(119): [130/196] Batch: 0.0673 (0.0796) Data: 0.0585 (0.0528) Loss: 1.8147 (1.8329)\n",
            "TRAIN(119): [140/196] Batch: 0.0739 (0.0794) Data: 0.0566 (0.0524) Loss: 1.8699 (1.8336)\n",
            "TRAIN(119): [150/196] Batch: 0.0482 (0.0794) Data: 0.0629 (0.0521) Loss: 1.8378 (1.8316)\n",
            "TRAIN(119): [160/196] Batch: 0.0782 (0.0793) Data: 0.0569 (0.0516) Loss: 1.7898 (1.8338)\n",
            "TRAIN(119): [170/196] Batch: 0.0751 (0.0792) Data: 0.0494 (0.0513) Loss: 1.6586 (1.8342)\n",
            "TRAIN(119): [180/196] Batch: 0.0732 (0.0791) Data: 0.0573 (0.0513) Loss: 1.7534 (1.8333)\n",
            "TRAIN(119): [190/196] Batch: 0.0755 (0.0789) Data: 0.0603 (0.0513) Loss: 1.6906 (1.8332)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(119)         0:00:15         0:00:10         0:00:05          1.8336\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(120): [ 10/196] Batch: 0.0680 (0.1189) Data: 0.0450 (0.0809) Loss: 1.7451 (1.7973)\n",
            "TRAIN(120): [ 20/196] Batch: 0.0630 (0.0973) Data: 0.0609 (0.0672) Loss: 1.7303 (1.8081)\n",
            "TRAIN(120): [ 30/196] Batch: 0.0776 (0.0907) Data: 0.0517 (0.0621) Loss: 1.8614 (1.8363)\n",
            "TRAIN(120): [ 40/196] Batch: 0.0770 (0.0870) Data: 0.0621 (0.0593) Loss: 1.8504 (1.8537)\n",
            "TRAIN(120): [ 50/196] Batch: 0.0693 (0.0847) Data: 0.0638 (0.0577) Loss: 1.7654 (1.8524)\n",
            "TRAIN(120): [ 60/196] Batch: 0.0813 (0.0834) Data: 0.0572 (0.0563) Loss: 1.8534 (1.8512)\n",
            "TRAIN(120): [ 70/196] Batch: 0.0784 (0.0825) Data: 0.0545 (0.0555) Loss: 1.9337 (1.8478)\n",
            "TRAIN(120): [ 80/196] Batch: 0.0633 (0.0817) Data: 0.0598 (0.0548) Loss: 1.8134 (1.8492)\n",
            "TRAIN(120): [ 90/196] Batch: 0.0669 (0.0811) Data: 0.0611 (0.0544) Loss: 1.9323 (1.8481)\n",
            "TRAIN(120): [100/196] Batch: 0.0873 (0.0807) Data: 0.0501 (0.0540) Loss: 1.7926 (1.8473)\n",
            "TRAIN(120): [110/196] Batch: 0.0820 (0.0804) Data: 0.0468 (0.0534) Loss: 1.9316 (1.8440)\n",
            "TRAIN(120): [120/196] Batch: 0.0656 (0.0800) Data: 0.0524 (0.0529) Loss: 1.9087 (1.8415)\n",
            "TRAIN(120): [130/196] Batch: 0.0759 (0.0798) Data: 0.0531 (0.0523) Loss: 1.9304 (1.8433)\n",
            "TRAIN(120): [140/196] Batch: 0.0651 (0.0796) Data: 0.0543 (0.0520) Loss: 1.6688 (1.8452)\n",
            "TRAIN(120): [150/196] Batch: 0.0771 (0.0794) Data: 0.0526 (0.0517) Loss: 1.8346 (1.8457)\n",
            "TRAIN(120): [160/196] Batch: 0.0742 (0.0792) Data: 0.0632 (0.0516) Loss: 1.8044 (1.8448)\n",
            "TRAIN(120): [170/196] Batch: 0.0725 (0.0790) Data: 0.0558 (0.0515) Loss: 1.8036 (1.8435)\n",
            "TRAIN(120): [180/196] Batch: 0.0713 (0.0789) Data: 0.0539 (0.0513) Loss: 1.7632 (1.8421)\n",
            "TRAIN(120): [190/196] Batch: 0.0764 (0.0787) Data: 0.0625 (0.0513) Loss: 1.8218 (1.8405)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(120)         0:00:15         0:00:10         0:00:05          1.8412\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(121): [ 10/196] Batch: 0.0744 (0.1179) Data: 0.0495 (0.0830) Loss: 1.8896 (1.7876)\n",
            "TRAIN(121): [ 20/196] Batch: 0.0811 (0.0973) Data: 0.0499 (0.0673) Loss: 1.8382 (1.7987)\n",
            "TRAIN(121): [ 30/196] Batch: 0.0826 (0.0904) Data: 0.0493 (0.0614) Loss: 1.8240 (1.8196)\n",
            "TRAIN(121): [ 40/196] Batch: 0.0881 (0.0868) Data: 0.0516 (0.0587) Loss: 1.6279 (1.8122)\n",
            "TRAIN(121): [ 50/196] Batch: 0.0745 (0.0845) Data: 0.0563 (0.0569) Loss: 1.7275 (1.8229)\n",
            "TRAIN(121): [ 60/196] Batch: 0.0708 (0.0832) Data: 0.0571 (0.0555) Loss: 1.8710 (1.8276)\n",
            "TRAIN(121): [ 70/196] Batch: 0.0715 (0.0823) Data: 0.0528 (0.0547) Loss: 1.8343 (1.8289)\n",
            "TRAIN(121): [ 80/196] Batch: 0.0734 (0.0816) Data: 0.0552 (0.0541) Loss: 1.8142 (1.8249)\n",
            "TRAIN(121): [ 90/196] Batch: 0.0779 (0.0811) Data: 0.0489 (0.0530) Loss: 1.6085 (1.8244)\n",
            "TRAIN(121): [100/196] Batch: 0.0759 (0.0807) Data: 0.0501 (0.0524) Loss: 1.8764 (1.8226)\n",
            "TRAIN(121): [110/196] Batch: 0.0749 (0.0803) Data: 0.0511 (0.0516) Loss: 1.9424 (1.8264)\n",
            "TRAIN(121): [120/196] Batch: 0.0807 (0.0801) Data: 0.0429 (0.0511) Loss: 1.8011 (1.8310)\n",
            "TRAIN(121): [130/196] Batch: 0.0883 (0.0798) Data: 0.0509 (0.0509) Loss: 1.9262 (1.8319)\n",
            "TRAIN(121): [140/196] Batch: 0.0741 (0.0796) Data: 0.0497 (0.0507) Loss: 1.7979 (1.8354)\n",
            "TRAIN(121): [150/196] Batch: 0.0704 (0.0793) Data: 0.0613 (0.0508) Loss: 1.8297 (1.8342)\n",
            "TRAIN(121): [160/196] Batch: 0.0804 (0.0792) Data: 0.0515 (0.0506) Loss: 1.7860 (1.8332)\n",
            "TRAIN(121): [170/196] Batch: 0.0767 (0.0790) Data: 0.0556 (0.0506) Loss: 1.6913 (1.8323)\n",
            "TRAIN(121): [180/196] Batch: 0.0808 (0.0788) Data: 0.0577 (0.0506) Loss: 1.7917 (1.8316)\n",
            "TRAIN(121): [190/196] Batch: 0.0760 (0.0787) Data: 0.0615 (0.0507) Loss: 1.7222 (1.8305)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(121)         0:00:15         0:00:09         0:00:05          1.8302\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(122): [ 10/196] Batch: 0.0749 (0.1219) Data: 0.0490 (0.0867) Loss: 2.0726 (1.8532)\n",
            "TRAIN(122): [ 20/196] Batch: 0.0789 (0.0994) Data: 0.0493 (0.0685) Loss: 1.5990 (1.8017)\n",
            "TRAIN(122): [ 30/196] Batch: 0.0727 (0.0915) Data: 0.0555 (0.0626) Loss: 1.7431 (1.7976)\n",
            "TRAIN(122): [ 40/196] Batch: 0.0784 (0.0878) Data: 0.0500 (0.0589) Loss: 1.7770 (1.8085)\n",
            "TRAIN(122): [ 50/196] Batch: 0.0818 (0.0853) Data: 0.0553 (0.0571) Loss: 1.9621 (1.8180)\n",
            "TRAIN(122): [ 60/196] Batch: 0.0794 (0.0839) Data: 0.0515 (0.0558) Loss: 2.0108 (1.8168)\n",
            "TRAIN(122): [ 70/196] Batch: 0.0822 (0.0831) Data: 0.0438 (0.0548) Loss: 1.8306 (1.8149)\n",
            "TRAIN(122): [ 80/196] Batch: 0.0819 (0.0821) Data: 0.0535 (0.0541) Loss: 1.9973 (1.8237)\n",
            "TRAIN(122): [ 90/196] Batch: 0.0873 (0.0818) Data: 0.0455 (0.0536) Loss: 2.0155 (1.8282)\n",
            "TRAIN(122): [100/196] Batch: 0.0866 (0.0813) Data: 0.0386 (0.0527) Loss: 1.8292 (1.8305)\n",
            "TRAIN(122): [110/196] Batch: 0.0670 (0.0807) Data: 0.0625 (0.0524) Loss: 1.8032 (1.8290)\n",
            "TRAIN(122): [120/196] Batch: 0.0806 (0.0804) Data: 0.0511 (0.0522) Loss: 1.8050 (1.8277)\n",
            "TRAIN(122): [130/196] Batch: 0.0790 (0.0801) Data: 0.0525 (0.0520) Loss: 1.6914 (1.8283)\n",
            "TRAIN(122): [140/196] Batch: 0.0761 (0.0798) Data: 0.0548 (0.0518) Loss: 1.8712 (1.8285)\n",
            "TRAIN(122): [150/196] Batch: 0.0787 (0.0795) Data: 0.0551 (0.0518) Loss: 1.8517 (1.8282)\n",
            "TRAIN(122): [160/196] Batch: 0.0754 (0.0793) Data: 0.0571 (0.0517) Loss: 1.8258 (1.8302)\n",
            "TRAIN(122): [170/196] Batch: 0.0824 (0.0791) Data: 0.0565 (0.0516) Loss: 1.8218 (1.8289)\n",
            "TRAIN(122): [180/196] Batch: 0.0795 (0.0790) Data: 0.0513 (0.0515) Loss: 1.8049 (1.8304)\n",
            "TRAIN(122): [190/196] Batch: 0.0765 (0.0788) Data: 0.0608 (0.0515) Loss: 1.9325 (1.8308)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(122)         0:00:15         0:00:10         0:00:05          1.8318\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(123): [ 10/196] Batch: 0.0745 (0.1200) Data: 0.0480 (0.0824) Loss: 1.9266 (1.8364)\n",
            "TRAIN(123): [ 20/196] Batch: 0.0769 (0.0980) Data: 0.0552 (0.0668) Loss: 1.8368 (1.8357)\n",
            "TRAIN(123): [ 30/196] Batch: 0.0805 (0.0908) Data: 0.0549 (0.0623) Loss: 1.6388 (1.8176)\n",
            "TRAIN(123): [ 40/196] Batch: 0.0775 (0.0873) Data: 0.0506 (0.0587) Loss: 1.8003 (1.8173)\n",
            "TRAIN(123): [ 50/196] Batch: 0.0739 (0.0849) Data: 0.0583 (0.0564) Loss: 1.9521 (1.8139)\n",
            "TRAIN(123): [ 60/196] Batch: 0.0681 (0.0839) Data: 0.0478 (0.0548) Loss: 1.8270 (1.8150)\n",
            "TRAIN(123): [ 70/196] Batch: 0.0715 (0.0828) Data: 0.0556 (0.0536) Loss: 1.6780 (1.8158)\n",
            "TRAIN(123): [ 80/196] Batch: 0.0774 (0.0822) Data: 0.0490 (0.0528) Loss: 1.6218 (1.8145)\n",
            "TRAIN(123): [ 90/196] Batch: 0.0764 (0.0815) Data: 0.0617 (0.0524) Loss: 2.0022 (1.8195)\n",
            "TRAIN(123): [100/196] Batch: 0.0749 (0.0809) Data: 0.0620 (0.0525) Loss: 1.7762 (1.8192)\n",
            "TRAIN(123): [110/196] Batch: 0.0667 (0.0805) Data: 0.0611 (0.0524) Loss: 1.7348 (1.8236)\n",
            "TRAIN(123): [120/196] Batch: 0.0796 (0.0802) Data: 0.0517 (0.0522) Loss: 1.7863 (1.8255)\n",
            "TRAIN(123): [130/196] Batch: 0.0674 (0.0798) Data: 0.0612 (0.0519) Loss: 1.7565 (1.8228)\n",
            "TRAIN(123): [140/196] Batch: 0.0644 (0.0795) Data: 0.0625 (0.0518) Loss: 1.8241 (1.8208)\n",
            "TRAIN(123): [150/196] Batch: 0.0813 (0.0794) Data: 0.0518 (0.0516) Loss: 2.0197 (1.8247)\n",
            "TRAIN(123): [160/196] Batch: 0.0755 (0.0791) Data: 0.0614 (0.0517) Loss: 1.8724 (1.8243)\n",
            "TRAIN(123): [170/196] Batch: 0.0803 (0.0790) Data: 0.0524 (0.0515) Loss: 1.8363 (1.8234)\n",
            "TRAIN(123): [180/196] Batch: 0.0783 (0.0789) Data: 0.0535 (0.0513) Loss: 1.9022 (1.8250)\n",
            "TRAIN(123): [190/196] Batch: 0.0746 (0.0787) Data: 0.0622 (0.0513) Loss: 1.7786 (1.8230)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(123)         0:00:15         0:00:10         0:00:05          1.8238\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(124): [ 10/196] Batch: 0.0640 (0.1156) Data: 0.0607 (0.0840) Loss: 1.9345 (1.8450)\n",
            "TRAIN(124): [ 20/196] Batch: 0.0744 (0.0968) Data: 0.0503 (0.0668) Loss: 1.6267 (1.8219)\n",
            "TRAIN(124): [ 30/196] Batch: 0.1075 (0.0908) Data: 0.0400 (0.0596) Loss: 1.7118 (1.8131)\n",
            "TRAIN(124): [ 40/196] Batch: 0.0645 (0.0873) Data: 0.0520 (0.0566) Loss: 1.8031 (1.8241)\n",
            "TRAIN(124): [ 50/196] Batch: 0.0755 (0.0855) Data: 0.0506 (0.0550) Loss: 1.7752 (1.8188)\n",
            "TRAIN(124): [ 60/196] Batch: 0.0736 (0.0839) Data: 0.0515 (0.0534) Loss: 1.9109 (1.8195)\n",
            "TRAIN(124): [ 70/196] Batch: 0.0765 (0.0827) Data: 0.0617 (0.0530) Loss: 1.7440 (1.8127)\n",
            "TRAIN(124): [ 80/196] Batch: 0.0812 (0.0819) Data: 0.0565 (0.0529) Loss: 1.8929 (1.8107)\n",
            "TRAIN(124): [ 90/196] Batch: 0.0790 (0.0813) Data: 0.0546 (0.0526) Loss: 1.7115 (1.8062)\n",
            "TRAIN(124): [100/196] Batch: 0.0741 (0.0808) Data: 0.0577 (0.0521) Loss: 1.8266 (1.8059)\n",
            "TRAIN(124): [110/196] Batch: 0.0737 (0.0804) Data: 0.0537 (0.0520) Loss: 1.8285 (1.8036)\n",
            "TRAIN(124): [120/196] Batch: 0.0682 (0.0800) Data: 0.0552 (0.0519) Loss: 1.7034 (1.8021)\n",
            "TRAIN(124): [130/196] Batch: 0.0813 (0.0797) Data: 0.0518 (0.0517) Loss: 1.7333 (1.7983)\n",
            "TRAIN(124): [140/196] Batch: 0.0692 (0.0794) Data: 0.0571 (0.0516) Loss: 1.7986 (1.7965)\n",
            "TRAIN(124): [150/196] Batch: 0.0738 (0.0792) Data: 0.0553 (0.0515) Loss: 1.7095 (1.7991)\n",
            "TRAIN(124): [160/196] Batch: 0.0792 (0.0790) Data: 0.0541 (0.0515) Loss: 1.8940 (1.7993)\n",
            "TRAIN(124): [170/196] Batch: 0.0695 (0.0788) Data: 0.0632 (0.0514) Loss: 1.5288 (1.7969)\n",
            "TRAIN(124): [180/196] Batch: 0.0721 (0.0787) Data: 0.0547 (0.0513) Loss: 1.9082 (1.7987)\n",
            "TRAIN(124): [190/196] Batch: 0.0757 (0.0785) Data: 0.0621 (0.0512) Loss: 1.6852 (1.7993)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(124)         0:00:15         0:00:10         0:00:05          1.8013\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(125): [ 10/196] Batch: 0.0677 (0.1375) Data: 0.0491 (0.0975) Loss: 2.0557 (1.8102)\n",
            "TRAIN(125): [ 20/196] Batch: 0.0754 (0.1073) Data: 0.0500 (0.0705) Loss: 1.9279 (1.7962)\n",
            "TRAIN(125): [ 30/196] Batch: 0.0879 (0.0977) Data: 0.0418 (0.0618) Loss: 1.7679 (1.8154)\n",
            "TRAIN(125): [ 40/196] Batch: 0.0804 (0.0920) Data: 0.0519 (0.0580) Loss: 1.8946 (1.8201)\n",
            "TRAIN(125): [ 50/196] Batch: 0.0871 (0.0888) Data: 0.0505 (0.0564) Loss: 2.0760 (1.8220)\n",
            "TRAIN(125): [ 60/196] Batch: 0.0798 (0.0867) Data: 0.0492 (0.0551) Loss: 1.6843 (1.8161)\n",
            "TRAIN(125): [ 70/196] Batch: 0.0727 (0.0852) Data: 0.0548 (0.0544) Loss: 1.8161 (1.8136)\n",
            "TRAIN(125): [ 80/196] Batch: 0.0715 (0.0841) Data: 0.0564 (0.0539) Loss: 1.7956 (1.8142)\n",
            "TRAIN(125): [ 90/196] Batch: 0.0717 (0.0832) Data: 0.0549 (0.0534) Loss: 1.8055 (1.8099)\n",
            "TRAIN(125): [100/196] Batch: 0.0754 (0.0825) Data: 0.0507 (0.0530) Loss: 1.9876 (1.8099)\n",
            "TRAIN(125): [110/196] Batch: 0.0704 (0.0819) Data: 0.0558 (0.0526) Loss: 1.8906 (1.8136)\n",
            "TRAIN(125): [120/196] Batch: 0.0772 (0.0814) Data: 0.0529 (0.0524) Loss: 1.7803 (1.8136)\n",
            "TRAIN(125): [130/196] Batch: 0.0809 (0.0810) Data: 0.0536 (0.0523) Loss: 1.7371 (1.8092)\n",
            "TRAIN(125): [140/196] Batch: 0.0678 (0.0806) Data: 0.0578 (0.0520) Loss: 1.7599 (1.8080)\n",
            "TRAIN(125): [150/196] Batch: 0.0825 (0.0804) Data: 0.0485 (0.0518) Loss: 2.0611 (1.8103)\n",
            "TRAIN(125): [160/196] Batch: 0.0697 (0.0801) Data: 0.0588 (0.0517) Loss: 1.6848 (1.8102)\n",
            "TRAIN(125): [170/196] Batch: 0.0670 (0.0799) Data: 0.0503 (0.0514) Loss: 1.7582 (1.8103)\n",
            "TRAIN(125): [180/196] Batch: 0.0757 (0.0797) Data: 0.0513 (0.0511) Loss: 1.8050 (1.8098)\n",
            "TRAIN(125): [190/196] Batch: 0.0746 (0.0795) Data: 0.0632 (0.0508) Loss: 1.9073 (1.8114)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(125)         0:00:15         0:00:09         0:00:05          1.8106\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(126): [ 10/196] Batch: 0.0730 (0.1243) Data: 0.0476 (0.0854) Loss: 1.7938 (1.7989)\n",
            "TRAIN(126): [ 20/196] Batch: 0.0758 (0.1001) Data: 0.0554 (0.0685) Loss: 1.8028 (1.8030)\n",
            "TRAIN(126): [ 30/196] Batch: 0.0810 (0.0925) Data: 0.0457 (0.0627) Loss: 1.9095 (1.7892)\n",
            "TRAIN(126): [ 40/196] Batch: 0.0774 (0.0881) Data: 0.0599 (0.0590) Loss: 1.8679 (1.7819)\n",
            "TRAIN(126): [ 50/196] Batch: 0.0880 (0.0858) Data: 0.0512 (0.0570) Loss: 1.7558 (1.7878)\n",
            "TRAIN(126): [ 60/196] Batch: 0.0855 (0.0843) Data: 0.0499 (0.0557) Loss: 1.7312 (1.7836)\n",
            "TRAIN(126): [ 70/196] Batch: 0.0743 (0.0831) Data: 0.0536 (0.0550) Loss: 1.6629 (1.7825)\n",
            "TRAIN(126): [ 80/196] Batch: 0.0873 (0.0823) Data: 0.0505 (0.0546) Loss: 1.7399 (1.7842)\n",
            "TRAIN(126): [ 90/196] Batch: 0.0856 (0.0816) Data: 0.0511 (0.0542) Loss: 1.7620 (1.7818)\n",
            "TRAIN(126): [100/196] Batch: 0.0871 (0.0811) Data: 0.0507 (0.0537) Loss: 1.7170 (1.7831)\n",
            "TRAIN(126): [110/196] Batch: 0.0817 (0.0806) Data: 0.0548 (0.0535) Loss: 1.8488 (1.7862)\n",
            "TRAIN(126): [120/196] Batch: 0.0748 (0.0802) Data: 0.0627 (0.0534) Loss: 1.5947 (1.7852)\n",
            "TRAIN(126): [130/196] Batch: 0.0708 (0.0798) Data: 0.0620 (0.0532) Loss: 1.7649 (1.7843)\n",
            "TRAIN(126): [140/196] Batch: 0.0747 (0.0796) Data: 0.0538 (0.0530) Loss: 1.7402 (1.7864)\n",
            "TRAIN(126): [150/196] Batch: 0.0711 (0.0795) Data: 0.0508 (0.0525) Loss: 1.8237 (1.7896)\n",
            "TRAIN(126): [160/196] Batch: 0.0800 (0.0794) Data: 0.0390 (0.0518) Loss: 1.8726 (1.7895)\n",
            "TRAIN(126): [170/196] Batch: 0.0751 (0.0792) Data: 0.0504 (0.0514) Loss: 1.8474 (1.7901)\n",
            "TRAIN(126): [180/196] Batch: 0.0740 (0.0790) Data: 0.0575 (0.0513) Loss: 1.8334 (1.7946)\n",
            "TRAIN(126): [190/196] Batch: 0.0778 (0.0788) Data: 0.0610 (0.0512) Loss: 2.1388 (1.7969)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(126)         0:00:15         0:00:10         0:00:05          1.7965\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(127): [ 10/196] Batch: 0.0856 (0.1161) Data: 0.0502 (0.0811) Loss: 1.7339 (1.8122)\n",
            "TRAIN(127): [ 20/196] Batch: 0.0877 (0.0965) Data: 0.0488 (0.0671) Loss: 1.6891 (1.7927)\n",
            "TRAIN(127): [ 30/196] Batch: 0.0772 (0.0896) Data: 0.0541 (0.0617) Loss: 2.0858 (1.8083)\n",
            "TRAIN(127): [ 40/196] Batch: 0.0738 (0.0861) Data: 0.0599 (0.0593) Loss: 1.7836 (1.8115)\n",
            "TRAIN(127): [ 50/196] Batch: 0.0745 (0.0840) Data: 0.0624 (0.0577) Loss: 1.7177 (1.8131)\n",
            "TRAIN(127): [ 60/196] Batch: 0.0787 (0.0829) Data: 0.0512 (0.0564) Loss: 1.7378 (1.8089)\n",
            "TRAIN(127): [ 70/196] Batch: 0.0703 (0.0819) Data: 0.0614 (0.0558) Loss: 1.8166 (1.8122)\n",
            "TRAIN(127): [ 80/196] Batch: 0.0877 (0.0813) Data: 0.0503 (0.0551) Loss: 1.7661 (1.8063)\n",
            "TRAIN(127): [ 90/196] Batch: 0.0823 (0.0807) Data: 0.0500 (0.0545) Loss: 1.8178 (1.8066)\n",
            "TRAIN(127): [100/196] Batch: 0.0679 (0.0802) Data: 0.0627 (0.0541) Loss: 1.7219 (1.8044)\n",
            "TRAIN(127): [110/196] Batch: 0.0756 (0.0800) Data: 0.0490 (0.0536) Loss: 1.9566 (1.7970)\n",
            "TRAIN(127): [120/196] Batch: 0.0834 (0.0796) Data: 0.0495 (0.0531) Loss: 1.7417 (1.7921)\n",
            "TRAIN(127): [130/196] Batch: 0.0741 (0.0794) Data: 0.0488 (0.0525) Loss: 2.0139 (1.7936)\n",
            "TRAIN(127): [140/196] Batch: 0.0754 (0.0792) Data: 0.0498 (0.0520) Loss: 1.7880 (1.7942)\n",
            "TRAIN(127): [150/196] Batch: 0.0753 (0.0791) Data: 0.0496 (0.0515) Loss: 1.9718 (1.7933)\n",
            "TRAIN(127): [160/196] Batch: 0.0758 (0.0789) Data: 0.0483 (0.0512) Loss: 1.8275 (1.7927)\n",
            "TRAIN(127): [170/196] Batch: 0.0741 (0.0787) Data: 0.0640 (0.0510) Loss: 1.7292 (1.7940)\n",
            "TRAIN(127): [180/196] Batch: 0.0707 (0.0786) Data: 0.0563 (0.0509) Loss: 1.7246 (1.7959)\n",
            "TRAIN(127): [190/196] Batch: 0.0754 (0.0785) Data: 0.0618 (0.0509) Loss: 1.7602 (1.7941)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(127)         0:00:15         0:00:09         0:00:05          1.7939\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(128): [ 10/196] Batch: 0.0850 (0.1190) Data: 0.0499 (0.0856) Loss: 1.7600 (1.7766)\n",
            "TRAIN(128): [ 20/196] Batch: 0.0797 (0.0980) Data: 0.0489 (0.0676) Loss: 1.6429 (1.7684)\n",
            "TRAIN(128): [ 30/196] Batch: 0.0812 (0.0907) Data: 0.0514 (0.0611) Loss: 1.7319 (1.7800)\n",
            "TRAIN(128): [ 40/196] Batch: 0.0692 (0.0869) Data: 0.0574 (0.0584) Loss: 1.9762 (1.7813)\n",
            "TRAIN(128): [ 50/196] Batch: 0.0818 (0.0849) Data: 0.0505 (0.0563) Loss: 1.7524 (1.7788)\n",
            "TRAIN(128): [ 60/196] Batch: 0.0723 (0.0833) Data: 0.0561 (0.0552) Loss: 1.6723 (1.7855)\n",
            "TRAIN(128): [ 70/196] Batch: 0.0743 (0.0822) Data: 0.0624 (0.0549) Loss: 1.8732 (1.7866)\n",
            "TRAIN(128): [ 80/196] Batch: 0.0819 (0.0816) Data: 0.0504 (0.0543) Loss: 1.8835 (1.7859)\n",
            "TRAIN(128): [ 90/196] Batch: 0.0813 (0.0810) Data: 0.0524 (0.0538) Loss: 1.7385 (1.7850)\n",
            "TRAIN(128): [100/196] Batch: 0.0744 (0.0806) Data: 0.0478 (0.0530) Loss: 1.8126 (1.7864)\n",
            "TRAIN(128): [110/196] Batch: 0.0794 (0.0803) Data: 0.0502 (0.0526) Loss: 1.7787 (1.7885)\n",
            "TRAIN(128): [120/196] Batch: 0.0647 (0.0800) Data: 0.0522 (0.0520) Loss: 1.8408 (1.7852)\n",
            "TRAIN(128): [130/196] Batch: 0.0904 (0.0801) Data: 0.0371 (0.0509) Loss: 1.7719 (1.7823)\n",
            "TRAIN(128): [140/196] Batch: 0.0747 (0.0805) Data: 0.0345 (0.0495) Loss: 1.8583 (1.7850)\n",
            "TRAIN(128): [150/196] Batch: 0.0860 (0.0807) Data: 0.0501 (0.0487) Loss: 1.8158 (1.7832)\n",
            "TRAIN(128): [160/196] Batch: 0.0790 (0.0808) Data: 0.0453 (0.0481) Loss: 1.9334 (1.7849)\n",
            "TRAIN(128): [170/196] Batch: 0.0683 (0.0805) Data: 0.0547 (0.0475) Loss: 1.9845 (1.7859)\n",
            "TRAIN(128): [180/196] Batch: 0.0724 (0.0803) Data: 0.0610 (0.0474) Loss: 1.8005 (1.7843)\n",
            "TRAIN(128): [190/196] Batch: 0.0752 (0.0801) Data: 0.0620 (0.0477) Loss: 1.7094 (1.7828)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(128)         0:00:15         0:00:09         0:00:06          1.7820\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(129): [ 10/196] Batch: 0.0745 (0.1197) Data: 0.0498 (0.0809) Loss: 1.7990 (1.7940)\n",
            "TRAIN(129): [ 20/196] Batch: 0.0715 (0.0980) Data: 0.0558 (0.0660) Loss: 1.8271 (1.7852)\n",
            "TRAIN(129): [ 30/196] Batch: 0.0714 (0.0908) Data: 0.0590 (0.0611) Loss: 1.8145 (1.7871)\n",
            "TRAIN(129): [ 40/196] Batch: 0.0743 (0.0873) Data: 0.0545 (0.0582) Loss: 1.7076 (1.7857)\n",
            "TRAIN(129): [ 50/196] Batch: 0.0728 (0.0850) Data: 0.0569 (0.0563) Loss: 1.6956 (1.7827)\n",
            "TRAIN(129): [ 60/196] Batch: 0.0789 (0.0837) Data: 0.0524 (0.0550) Loss: 1.7578 (1.7859)\n",
            "TRAIN(129): [ 70/196] Batch: 0.0777 (0.0825) Data: 0.0583 (0.0543) Loss: 1.7632 (1.7865)\n",
            "TRAIN(129): [ 80/196] Batch: 0.0737 (0.0818) Data: 0.0514 (0.0537) Loss: 1.7937 (1.7926)\n",
            "TRAIN(129): [ 90/196] Batch: 0.0861 (0.0815) Data: 0.0402 (0.0527) Loss: 1.6519 (1.7811)\n",
            "TRAIN(129): [100/196] Batch: 0.0922 (0.0812) Data: 0.0337 (0.0516) Loss: 1.7288 (1.7839)\n",
            "TRAIN(129): [110/196] Batch: 0.1011 (0.0809) Data: 0.0448 (0.0517) Loss: 1.7617 (1.7821)\n",
            "TRAIN(129): [120/196] Batch: 0.0756 (0.0806) Data: 0.0543 (0.0516) Loss: 1.7555 (1.7817)\n",
            "TRAIN(129): [130/196] Batch: 0.0766 (0.0803) Data: 0.0522 (0.0512) Loss: 1.6840 (1.7791)\n",
            "TRAIN(129): [140/196] Batch: 0.0840 (0.0800) Data: 0.0528 (0.0512) Loss: 1.8080 (1.7774)\n",
            "TRAIN(129): [150/196] Batch: 0.0771 (0.0798) Data: 0.0543 (0.0512) Loss: 1.6359 (1.7789)\n",
            "TRAIN(129): [160/196] Batch: 0.0825 (0.0796) Data: 0.0518 (0.0512) Loss: 1.7002 (1.7774)\n",
            "TRAIN(129): [170/196] Batch: 0.0780 (0.0794) Data: 0.0546 (0.0511) Loss: 1.7587 (1.7747)\n",
            "TRAIN(129): [180/196] Batch: 0.0701 (0.0792) Data: 0.0617 (0.0510) Loss: 1.7697 (1.7761)\n",
            "TRAIN(129): [190/196] Batch: 0.0770 (0.0790) Data: 0.0610 (0.0511) Loss: 1.9474 (1.7770)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(129)         0:00:15         0:00:10         0:00:05          1.7763\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(130): [ 10/196] Batch: 0.0815 (0.1176) Data: 0.0435 (0.0817) Loss: 1.9039 (1.7987)\n",
            "TRAIN(130): [ 20/196] Batch: 0.0858 (0.0970) Data: 0.0498 (0.0657) Loss: 1.5798 (1.7742)\n",
            "TRAIN(130): [ 30/196] Batch: 0.0703 (0.0902) Data: 0.0547 (0.0608) Loss: 1.6507 (1.7856)\n",
            "TRAIN(130): [ 40/196] Batch: 0.0792 (0.0868) Data: 0.0545 (0.0580) Loss: 1.8177 (1.7799)\n",
            "TRAIN(130): [ 50/196] Batch: 0.0709 (0.0846) Data: 0.0581 (0.0564) Loss: 1.7803 (1.7847)\n",
            "TRAIN(130): [ 60/196] Batch: 0.0794 (0.0833) Data: 0.0520 (0.0552) Loss: 1.7795 (1.7828)\n",
            "TRAIN(130): [ 70/196] Batch: 0.0724 (0.0824) Data: 0.0574 (0.0542) Loss: 1.8342 (1.7843)\n",
            "TRAIN(130): [ 80/196] Batch: 0.0740 (0.0817) Data: 0.0557 (0.0538) Loss: 1.9689 (1.7772)\n",
            "TRAIN(130): [ 90/196] Batch: 0.0761 (0.0814) Data: 0.0450 (0.0527) Loss: 1.7433 (1.7754)\n",
            "TRAIN(130): [100/196] Batch: 0.0804 (0.0808) Data: 0.0522 (0.0524) Loss: 1.5908 (1.7738)\n",
            "TRAIN(130): [110/196] Batch: 0.0873 (0.0806) Data: 0.0519 (0.0518) Loss: 1.7867 (1.7770)\n",
            "TRAIN(130): [120/196] Batch: 0.0778 (0.0802) Data: 0.0510 (0.0515) Loss: 1.6083 (1.7800)\n",
            "TRAIN(130): [130/196] Batch: 0.0884 (0.0799) Data: 0.0502 (0.0514) Loss: 1.6353 (1.7816)\n",
            "TRAIN(130): [140/196] Batch: 0.0740 (0.0796) Data: 0.0549 (0.0512) Loss: 1.8511 (1.7814)\n",
            "TRAIN(130): [150/196] Batch: 0.0721 (0.0794) Data: 0.0617 (0.0512) Loss: 1.8841 (1.7785)\n",
            "TRAIN(130): [160/196] Batch: 0.0820 (0.0793) Data: 0.0489 (0.0512) Loss: 1.8084 (1.7794)\n",
            "TRAIN(130): [170/196] Batch: 0.0809 (0.0791) Data: 0.0528 (0.0511) Loss: 1.8145 (1.7805)\n",
            "TRAIN(130): [180/196] Batch: 0.0850 (0.0789) Data: 0.0520 (0.0511) Loss: 1.9052 (1.7824)\n",
            "TRAIN(130): [190/196] Batch: 0.0760 (0.0787) Data: 0.0621 (0.0511) Loss: 1.6460 (1.7813)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(130)         0:00:15         0:00:10         0:00:05          1.7800\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(131): [ 10/196] Batch: 0.0710 (0.1187) Data: 0.0607 (0.0854) Loss: 1.7649 (1.7218)\n",
            "TRAIN(131): [ 20/196] Batch: 0.0739 (0.0979) Data: 0.0563 (0.0698) Loss: 1.7651 (1.7650)\n",
            "TRAIN(131): [ 30/196] Batch: 0.0745 (0.0907) Data: 0.0590 (0.0641) Loss: 1.6997 (1.7658)\n",
            "TRAIN(131): [ 40/196] Batch: 0.0754 (0.0874) Data: 0.0500 (0.0596) Loss: 1.8501 (1.7669)\n",
            "TRAIN(131): [ 50/196] Batch: 0.0764 (0.0854) Data: 0.0535 (0.0572) Loss: 1.6084 (1.7647)\n",
            "TRAIN(131): [ 60/196] Batch: 0.0882 (0.0842) Data: 0.0407 (0.0554) Loss: 1.6603 (1.7583)\n",
            "TRAIN(131): [ 70/196] Batch: 0.0737 (0.0832) Data: 0.0481 (0.0540) Loss: 1.7741 (1.7629)\n",
            "TRAIN(131): [ 80/196] Batch: 0.0752 (0.0824) Data: 0.0526 (0.0531) Loss: 1.7950 (1.7649)\n",
            "TRAIN(131): [ 90/196] Batch: 0.0828 (0.0817) Data: 0.0528 (0.0525) Loss: 1.6564 (1.7666)\n",
            "TRAIN(131): [100/196] Batch: 0.0726 (0.0812) Data: 0.0548 (0.0523) Loss: 1.7130 (1.7630)\n",
            "TRAIN(131): [110/196] Batch: 0.0653 (0.0806) Data: 0.0604 (0.0520) Loss: 1.8209 (1.7626)\n",
            "TRAIN(131): [120/196] Batch: 0.0782 (0.0803) Data: 0.0539 (0.0518) Loss: 1.9236 (1.7662)\n",
            "TRAIN(131): [130/196] Batch: 0.0783 (0.0800) Data: 0.0525 (0.0516) Loss: 1.5434 (1.7622)\n",
            "TRAIN(131): [140/196] Batch: 0.0709 (0.0797) Data: 0.0560 (0.0515) Loss: 1.8006 (1.7608)\n",
            "TRAIN(131): [150/196] Batch: 0.0797 (0.0795) Data: 0.0512 (0.0512) Loss: 1.7920 (1.7559)\n",
            "TRAIN(131): [160/196] Batch: 0.0746 (0.0793) Data: 0.0522 (0.0512) Loss: 1.8479 (1.7604)\n",
            "TRAIN(131): [170/196] Batch: 0.0768 (0.0792) Data: 0.0549 (0.0511) Loss: 1.9206 (1.7588)\n",
            "TRAIN(131): [180/196] Batch: 0.0722 (0.0790) Data: 0.0554 (0.0510) Loss: 1.6199 (1.7547)\n",
            "TRAIN(131): [190/196] Batch: 0.0759 (0.0788) Data: 0.0622 (0.0510) Loss: 1.6996 (1.7533)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(131)         0:00:15         0:00:09         0:00:05          1.7521\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(132): [ 10/196] Batch: 0.0726 (0.1207) Data: 0.0495 (0.0838) Loss: 1.8586 (1.7543)\n",
            "TRAIN(132): [ 20/196] Batch: 0.0756 (0.0989) Data: 0.0496 (0.0669) Loss: 1.9270 (1.7672)\n",
            "TRAIN(132): [ 30/196] Batch: 0.0824 (0.0916) Data: 0.0431 (0.0601) Loss: 1.6886 (1.7466)\n",
            "TRAIN(132): [ 40/196] Batch: 0.0763 (0.0879) Data: 0.0517 (0.0563) Loss: 1.8773 (1.7459)\n",
            "TRAIN(132): [ 50/196] Batch: 0.0746 (0.0857) Data: 0.0443 (0.0540) Loss: 1.7963 (1.7596)\n",
            "TRAIN(132): [ 60/196] Batch: 0.0712 (0.0841) Data: 0.0490 (0.0523) Loss: 1.7655 (1.7563)\n",
            "TRAIN(132): [ 70/196] Batch: 0.0824 (0.0830) Data: 0.0553 (0.0516) Loss: 1.6713 (1.7618)\n",
            "TRAIN(132): [ 80/196] Batch: 0.0743 (0.0820) Data: 0.0625 (0.0516) Loss: 1.8116 (1.7583)\n",
            "TRAIN(132): [ 90/196] Batch: 0.0760 (0.0814) Data: 0.0619 (0.0516) Loss: 1.8299 (1.7575)\n",
            "TRAIN(132): [100/196] Batch: 0.0688 (0.0808) Data: 0.0612 (0.0515) Loss: 1.6967 (1.7503)\n",
            "TRAIN(132): [110/196] Batch: 0.0813 (0.0805) Data: 0.0486 (0.0514) Loss: 1.7369 (1.7494)\n",
            "TRAIN(132): [120/196] Batch: 0.0732 (0.0801) Data: 0.0555 (0.0512) Loss: 1.6700 (1.7501)\n",
            "TRAIN(132): [130/196] Batch: 0.0702 (0.0798) Data: 0.0578 (0.0512) Loss: 1.6695 (1.7502)\n",
            "TRAIN(132): [140/196] Batch: 0.0846 (0.0796) Data: 0.0538 (0.0513) Loss: 1.6632 (1.7522)\n",
            "TRAIN(132): [150/196] Batch: 0.0802 (0.0794) Data: 0.0520 (0.0514) Loss: 1.7128 (1.7525)\n",
            "TRAIN(132): [160/196] Batch: 0.0679 (0.0791) Data: 0.0591 (0.0515) Loss: 1.6700 (1.7499)\n",
            "TRAIN(132): [170/196] Batch: 0.0706 (0.0790) Data: 0.0572 (0.0514) Loss: 1.7345 (1.7483)\n",
            "TRAIN(132): [180/196] Batch: 0.0708 (0.0788) Data: 0.0564 (0.0514) Loss: 1.8244 (1.7516)\n",
            "TRAIN(132): [190/196] Batch: 0.0765 (0.0787) Data: 0.0605 (0.0514) Loss: 1.6624 (1.7536)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(132)         0:00:15         0:00:10         0:00:05          1.7536\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(133): [ 10/196] Batch: 0.0899 (0.1295) Data: 0.0315 (0.0813) Loss: 1.6699 (1.7531)\n",
            "TRAIN(133): [ 20/196] Batch: 0.0694 (0.1030) Data: 0.0510 (0.0634) Loss: 1.8777 (1.7533)\n",
            "TRAIN(133): [ 30/196] Batch: 0.0794 (0.0949) Data: 0.0529 (0.0580) Loss: 1.7423 (1.7579)\n",
            "TRAIN(133): [ 40/196] Batch: 0.0868 (0.0903) Data: 0.0508 (0.0549) Loss: 1.7859 (1.7587)\n",
            "TRAIN(133): [ 50/196] Batch: 0.0823 (0.0873) Data: 0.0558 (0.0539) Loss: 1.6995 (1.7645)\n",
            "TRAIN(133): [ 60/196] Batch: 0.0822 (0.0855) Data: 0.0558 (0.0530) Loss: 1.8272 (1.7584)\n",
            "TRAIN(133): [ 70/196] Batch: 0.0710 (0.0842) Data: 0.0559 (0.0525) Loss: 1.8246 (1.7631)\n",
            "TRAIN(133): [ 80/196] Batch: 0.0718 (0.0832) Data: 0.0546 (0.0519) Loss: 1.8542 (1.7630)\n",
            "TRAIN(133): [ 90/196] Batch: 0.0672 (0.0824) Data: 0.0578 (0.0517) Loss: 1.6917 (1.7567)\n",
            "TRAIN(133): [100/196] Batch: 0.0689 (0.0818) Data: 0.0569 (0.0518) Loss: 1.7660 (1.7569)\n",
            "TRAIN(133): [110/196] Batch: 0.0816 (0.0813) Data: 0.0505 (0.0515) Loss: 1.7608 (1.7536)\n",
            "TRAIN(133): [120/196] Batch: 0.0696 (0.0808) Data: 0.0575 (0.0513) Loss: 1.9582 (1.7535)\n",
            "TRAIN(133): [130/196] Batch: 0.0756 (0.0805) Data: 0.0542 (0.0511) Loss: 1.7473 (1.7558)\n",
            "TRAIN(133): [140/196] Batch: 0.0791 (0.0802) Data: 0.0540 (0.0510) Loss: 1.6672 (1.7510)\n",
            "TRAIN(133): [150/196] Batch: 0.0710 (0.0799) Data: 0.0541 (0.0508) Loss: 1.7841 (1.7488)\n",
            "TRAIN(133): [160/196] Batch: 0.0714 (0.0797) Data: 0.0530 (0.0507) Loss: 1.8286 (1.7531)\n",
            "TRAIN(133): [170/196] Batch: 0.0728 (0.0795) Data: 0.0547 (0.0506) Loss: 1.7925 (1.7527)\n",
            "TRAIN(133): [180/196] Batch: 0.0747 (0.0793) Data: 0.0511 (0.0504) Loss: 1.7883 (1.7511)\n",
            "TRAIN(133): [190/196] Batch: 0.0743 (0.0791) Data: 0.0600 (0.0503) Loss: 1.6645 (1.7509)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(133)         0:00:15         0:00:09         0:00:05          1.7497\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(134): [ 10/196] Batch: 0.0888 (0.1351) Data: 0.0386 (0.0963) Loss: 1.7414 (1.7423)\n",
            "TRAIN(134): [ 20/196] Batch: 0.0703 (0.1049) Data: 0.0555 (0.0737) Loss: 1.8738 (1.7326)\n",
            "TRAIN(134): [ 30/196] Batch: 0.0813 (0.0954) Data: 0.0546 (0.0659) Loss: 1.8918 (1.7439)\n",
            "TRAIN(134): [ 40/196] Batch: 0.0715 (0.0905) Data: 0.0579 (0.0623) Loss: 1.7195 (1.7472)\n",
            "TRAIN(134): [ 50/196] Batch: 0.0863 (0.0877) Data: 0.0521 (0.0597) Loss: 1.6691 (1.7384)\n",
            "TRAIN(134): [ 60/196] Batch: 0.0790 (0.0858) Data: 0.0502 (0.0578) Loss: 1.7063 (1.7396)\n",
            "TRAIN(134): [ 70/196] Batch: 0.0772 (0.0843) Data: 0.0538 (0.0565) Loss: 1.7133 (1.7354)\n",
            "TRAIN(134): [ 80/196] Batch: 0.0803 (0.0834) Data: 0.0510 (0.0556) Loss: 1.8097 (1.7460)\n",
            "TRAIN(134): [ 90/196] Batch: 0.0862 (0.0825) Data: 0.0522 (0.0548) Loss: 1.8734 (1.7489)\n",
            "TRAIN(134): [100/196] Batch: 0.0783 (0.0819) Data: 0.0540 (0.0542) Loss: 1.7350 (1.7455)\n",
            "TRAIN(134): [110/196] Batch: 0.0784 (0.0814) Data: 0.0523 (0.0537) Loss: 1.8275 (1.7470)\n",
            "TRAIN(134): [120/196] Batch: 0.0862 (0.0810) Data: 0.0512 (0.0534) Loss: 1.7389 (1.7482)\n",
            "TRAIN(134): [130/196] Batch: 0.0846 (0.0806) Data: 0.0521 (0.0532) Loss: 1.7698 (1.7488)\n",
            "TRAIN(134): [140/196] Batch: 0.0838 (0.0803) Data: 0.0536 (0.0529) Loss: 1.7371 (1.7491)\n",
            "TRAIN(134): [150/196] Batch: 0.0747 (0.0800) Data: 0.0494 (0.0524) Loss: 1.6677 (1.7453)\n",
            "TRAIN(134): [160/196] Batch: 0.0863 (0.0799) Data: 0.0451 (0.0519) Loss: 1.6875 (1.7468)\n",
            "TRAIN(134): [170/196] Batch: 0.0744 (0.0796) Data: 0.0589 (0.0519) Loss: 1.8167 (1.7456)\n",
            "TRAIN(134): [180/196] Batch: 0.0883 (0.0796) Data: 0.0412 (0.0514) Loss: 1.7528 (1.7465)\n",
            "TRAIN(134): [190/196] Batch: 0.0729 (0.0793) Data: 0.0604 (0.0513) Loss: 1.8665 (1.7482)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(134)         0:00:15         0:00:10         0:00:05          1.7487\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(135): [ 10/196] Batch: 0.0771 (0.1209) Data: 0.0511 (0.0873) Loss: 1.8342 (1.7730)\n",
            "TRAIN(135): [ 20/196] Batch: 0.0793 (0.0988) Data: 0.0495 (0.0683) Loss: 1.8825 (1.7537)\n",
            "TRAIN(135): [ 30/196] Batch: 0.0758 (0.0909) Data: 0.0603 (0.0629) Loss: 1.5829 (1.7446)\n",
            "TRAIN(135): [ 40/196] Batch: 0.0820 (0.0875) Data: 0.0500 (0.0596) Loss: 1.8542 (1.7606)\n",
            "TRAIN(135): [ 50/196] Batch: 0.0715 (0.0850) Data: 0.0560 (0.0573) Loss: 1.6806 (1.7461)\n",
            "TRAIN(135): [ 60/196] Batch: 0.0826 (0.0836) Data: 0.0486 (0.0558) Loss: 1.6241 (1.7473)\n",
            "TRAIN(135): [ 70/196] Batch: 0.0728 (0.0825) Data: 0.0541 (0.0548) Loss: 1.9411 (1.7584)\n",
            "TRAIN(135): [ 80/196] Batch: 0.0743 (0.0817) Data: 0.0589 (0.0544) Loss: 1.8160 (1.7624)\n",
            "TRAIN(135): [ 90/196] Batch: 0.0872 (0.0811) Data: 0.0509 (0.0543) Loss: 1.7435 (1.7633)\n",
            "TRAIN(135): [100/196] Batch: 0.0719 (0.0806) Data: 0.0578 (0.0539) Loss: 1.8918 (1.7581)\n",
            "TRAIN(135): [110/196] Batch: 0.0744 (0.0802) Data: 0.0565 (0.0534) Loss: 1.7579 (1.7604)\n",
            "TRAIN(135): [120/196] Batch: 0.0739 (0.0799) Data: 0.0526 (0.0532) Loss: 1.6275 (1.7608)\n",
            "TRAIN(135): [130/196] Batch: 0.0617 (0.0797) Data: 0.0531 (0.0529) Loss: 1.7701 (1.7567)\n",
            "TRAIN(135): [140/196] Batch: 0.0797 (0.0795) Data: 0.0578 (0.0527) Loss: 1.7297 (1.7536)\n",
            "TRAIN(135): [150/196] Batch: 0.1057 (0.0794) Data: 0.0349 (0.0523) Loss: 2.0981 (1.7533)\n",
            "TRAIN(135): [160/196] Batch: 0.0771 (0.0792) Data: 0.0500 (0.0518) Loss: 1.5606 (1.7527)\n",
            "TRAIN(135): [170/196] Batch: 0.0637 (0.0790) Data: 0.0603 (0.0515) Loss: 1.6152 (1.7516)\n",
            "TRAIN(135): [180/196] Batch: 0.0741 (0.0789) Data: 0.0556 (0.0514) Loss: 1.6251 (1.7510)\n",
            "TRAIN(135): [190/196] Batch: 0.0786 (0.0787) Data: 0.0598 (0.0513) Loss: 1.6414 (1.7495)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(135)         0:00:15         0:00:10         0:00:05          1.7481\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(136): [ 10/196] Batch: 0.0695 (0.1179) Data: 0.0548 (0.0834) Loss: 1.6715 (1.7246)\n",
            "TRAIN(136): [ 20/196] Batch: 0.0821 (0.0973) Data: 0.0546 (0.0683) Loss: 1.7053 (1.7289)\n",
            "TRAIN(136): [ 30/196] Batch: 0.0824 (0.0903) Data: 0.0540 (0.0624) Loss: 1.6862 (1.7098)\n",
            "TRAIN(136): [ 40/196] Batch: 0.0777 (0.0868) Data: 0.0557 (0.0595) Loss: 1.8128 (1.7213)\n",
            "TRAIN(136): [ 50/196] Batch: 0.0758 (0.0846) Data: 0.0586 (0.0577) Loss: 1.8210 (1.7357)\n",
            "TRAIN(136): [ 60/196] Batch: 0.0742 (0.0832) Data: 0.0559 (0.0562) Loss: 1.8505 (1.7415)\n",
            "TRAIN(136): [ 70/196] Batch: 0.0703 (0.0822) Data: 0.0608 (0.0555) Loss: 1.6346 (1.7393)\n",
            "TRAIN(136): [ 80/196] Batch: 0.0685 (0.0814) Data: 0.0616 (0.0552) Loss: 1.6721 (1.7356)\n",
            "TRAIN(136): [ 90/196] Batch: 0.0768 (0.0808) Data: 0.0615 (0.0546) Loss: 1.9092 (1.7420)\n",
            "TRAIN(136): [100/196] Batch: 0.0774 (0.0805) Data: 0.0499 (0.0538) Loss: 1.7231 (1.7426)\n",
            "TRAIN(136): [110/196] Batch: 0.0746 (0.0802) Data: 0.0490 (0.0528) Loss: 1.9147 (1.7439)\n",
            "TRAIN(136): [120/196] Batch: 0.0982 (0.0800) Data: 0.0384 (0.0522) Loss: 1.6683 (1.7459)\n",
            "TRAIN(136): [130/196] Batch: 0.0721 (0.0797) Data: 0.0488 (0.0515) Loss: 1.6398 (1.7433)\n",
            "TRAIN(136): [140/196] Batch: 0.0863 (0.0795) Data: 0.0540 (0.0515) Loss: 1.7878 (1.7440)\n",
            "TRAIN(136): [150/196] Batch: 0.0725 (0.0793) Data: 0.0547 (0.0512) Loss: 1.7117 (1.7449)\n",
            "TRAIN(136): [160/196] Batch: 0.0814 (0.0791) Data: 0.0566 (0.0512) Loss: 1.7297 (1.7419)\n",
            "TRAIN(136): [170/196] Batch: 0.0704 (0.0789) Data: 0.0551 (0.0510) Loss: 1.8557 (1.7409)\n",
            "TRAIN(136): [180/196] Batch: 0.0733 (0.0788) Data: 0.0566 (0.0510) Loss: 1.6654 (1.7407)\n",
            "TRAIN(136): [190/196] Batch: 0.0762 (0.0786) Data: 0.0636 (0.0511) Loss: 1.7207 (1.7402)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(136)         0:00:15         0:00:10         0:00:05          1.7412\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(137): [ 10/196] Batch: 0.0721 (0.1177) Data: 0.0515 (0.0837) Loss: 1.6287 (1.7101)\n",
            "TRAIN(137): [ 20/196] Batch: 0.0685 (0.0971) Data: 0.0560 (0.0682) Loss: 1.7978 (1.7248)\n",
            "TRAIN(137): [ 30/196] Batch: 0.0772 (0.0905) Data: 0.0500 (0.0617) Loss: 1.6692 (1.7233)\n",
            "TRAIN(137): [ 40/196] Batch: 0.0805 (0.0869) Data: 0.0530 (0.0589) Loss: 1.8291 (1.7156)\n",
            "TRAIN(137): [ 50/196] Batch: 0.0762 (0.0846) Data: 0.0578 (0.0572) Loss: 1.7227 (1.7155)\n",
            "TRAIN(137): [ 60/196] Batch: 0.0688 (0.0831) Data: 0.0605 (0.0558) Loss: 1.6927 (1.7170)\n",
            "TRAIN(137): [ 70/196] Batch: 0.0802 (0.0822) Data: 0.0573 (0.0554) Loss: 1.7540 (1.7169)\n",
            "TRAIN(137): [ 80/196] Batch: 0.0788 (0.0816) Data: 0.0485 (0.0543) Loss: 1.7346 (1.7202)\n",
            "TRAIN(137): [ 90/196] Batch: 0.0660 (0.0811) Data: 0.0503 (0.0532) Loss: 1.9579 (1.7230)\n",
            "TRAIN(137): [100/196] Batch: 0.0770 (0.0805) Data: 0.0599 (0.0531) Loss: 1.8169 (1.7250)\n",
            "TRAIN(137): [110/196] Batch: 0.0582 (0.0804) Data: 0.0530 (0.0521) Loss: 1.7329 (1.7263)\n",
            "TRAIN(137): [120/196] Batch: 0.0687 (0.0804) Data: 0.0540 (0.0515) Loss: 1.7316 (1.7277)\n",
            "TRAIN(137): [130/196] Batch: 0.0665 (0.0801) Data: 0.0617 (0.0513) Loss: 1.6615 (1.7287)\n",
            "TRAIN(137): [140/196] Batch: 0.0681 (0.0798) Data: 0.0584 (0.0512) Loss: 1.5712 (1.7266)\n",
            "TRAIN(137): [150/196] Batch: 0.0801 (0.0797) Data: 0.0516 (0.0511) Loss: 1.4034 (1.7207)\n",
            "TRAIN(137): [160/196] Batch: 0.0792 (0.0794) Data: 0.0519 (0.0510) Loss: 1.7990 (1.7218)\n",
            "TRAIN(137): [170/196] Batch: 0.0802 (0.0793) Data: 0.0501 (0.0508) Loss: 1.7118 (1.7243)\n",
            "TRAIN(137): [180/196] Batch: 0.0671 (0.0791) Data: 0.0558 (0.0506) Loss: 1.6663 (1.7261)\n",
            "TRAIN(137): [190/196] Batch: 0.0776 (0.0789) Data: 0.0610 (0.0507) Loss: 1.6974 (1.7239)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(137)         0:00:15         0:00:09         0:00:05          1.7251\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(138): [ 10/196] Batch: 0.0639 (0.1166) Data: 0.0610 (0.0838) Loss: 1.6535 (1.6890)\n",
            "TRAIN(138): [ 20/196] Batch: 0.0636 (0.0968) Data: 0.0620 (0.0686) Loss: 1.8072 (1.7103)\n",
            "TRAIN(138): [ 30/196] Batch: 0.0838 (0.0905) Data: 0.0484 (0.0630) Loss: 1.9435 (1.7208)\n",
            "TRAIN(138): [ 40/196] Batch: 0.0764 (0.0867) Data: 0.0628 (0.0603) Loss: 1.6892 (1.7204)\n",
            "TRAIN(138): [ 50/196] Batch: 0.0836 (0.0847) Data: 0.0560 (0.0580) Loss: 1.6967 (1.7325)\n",
            "TRAIN(138): [ 60/196] Batch: 0.0729 (0.0834) Data: 0.0593 (0.0565) Loss: 1.7923 (1.7276)\n",
            "TRAIN(138): [ 70/196] Batch: 0.0750 (0.0825) Data: 0.0542 (0.0555) Loss: 1.8238 (1.7314)\n",
            "TRAIN(138): [ 80/196] Batch: 0.0758 (0.0819) Data: 0.0619 (0.0545) Loss: 1.7636 (1.7301)\n",
            "TRAIN(138): [ 90/196] Batch: 0.0765 (0.0815) Data: 0.0487 (0.0533) Loss: 1.7417 (1.7318)\n",
            "TRAIN(138): [100/196] Batch: 0.0794 (0.0811) Data: 0.0476 (0.0525) Loss: 1.6999 (1.7325)\n",
            "TRAIN(138): [110/196] Batch: 0.0681 (0.0806) Data: 0.0609 (0.0521) Loss: 1.7980 (1.7322)\n",
            "TRAIN(138): [120/196] Batch: 0.0734 (0.0802) Data: 0.0563 (0.0519) Loss: 1.8367 (1.7344)\n",
            "TRAIN(138): [130/196] Batch: 0.0841 (0.0800) Data: 0.0472 (0.0517) Loss: 1.7282 (1.7307)\n",
            "TRAIN(138): [140/196] Batch: 0.0702 (0.0797) Data: 0.0623 (0.0516) Loss: 1.6424 (1.7283)\n",
            "TRAIN(138): [150/196] Batch: 0.0791 (0.0795) Data: 0.0522 (0.0513) Loss: 1.6865 (1.7303)\n",
            "TRAIN(138): [160/196] Batch: 0.0735 (0.0792) Data: 0.0597 (0.0514) Loss: 1.5415 (1.7292)\n",
            "TRAIN(138): [170/196] Batch: 0.0825 (0.0791) Data: 0.0569 (0.0514) Loss: 1.6327 (1.7300)\n",
            "TRAIN(138): [180/196] Batch: 0.0714 (0.0789) Data: 0.0560 (0.0513) Loss: 1.7680 (1.7308)\n",
            "TRAIN(138): [190/196] Batch: 0.0746 (0.0788) Data: 0.0623 (0.0514) Loss: 1.7877 (1.7290)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(138)         0:00:15         0:00:10         0:00:05          1.7292\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(139): [ 10/196] Batch: 0.0717 (0.1191) Data: 0.0525 (0.0822) Loss: 1.7337 (1.7303)\n",
            "TRAIN(139): [ 20/196] Batch: 0.0707 (0.0975) Data: 0.0608 (0.0668) Loss: 1.5854 (1.7203)\n",
            "TRAIN(139): [ 30/196] Batch: 0.0746 (0.0907) Data: 0.0547 (0.0613) Loss: 1.8227 (1.7330)\n",
            "TRAIN(139): [ 40/196] Batch: 0.0682 (0.0872) Data: 0.0515 (0.0574) Loss: 1.6669 (1.7103)\n",
            "TRAIN(139): [ 50/196] Batch: 0.0779 (0.0849) Data: 0.0600 (0.0552) Loss: 1.7198 (1.7061)\n",
            "TRAIN(139): [ 60/196] Batch: 0.0699 (0.0837) Data: 0.0554 (0.0535) Loss: 1.7809 (1.7142)\n",
            "TRAIN(139): [ 70/196] Batch: 0.0844 (0.0830) Data: 0.0435 (0.0525) Loss: 1.7770 (1.7164)\n",
            "TRAIN(139): [ 80/196] Batch: 0.0851 (0.0823) Data: 0.0453 (0.0516) Loss: 1.5558 (1.7141)\n",
            "TRAIN(139): [ 90/196] Batch: 0.0715 (0.0815) Data: 0.0554 (0.0511) Loss: 1.6966 (1.7144)\n",
            "TRAIN(139): [100/196] Batch: 0.0637 (0.0809) Data: 0.0583 (0.0511) Loss: 1.7480 (1.7179)\n",
            "TRAIN(139): [110/196] Batch: 0.0664 (0.0805) Data: 0.0552 (0.0509) Loss: 1.7751 (1.7171)\n",
            "TRAIN(139): [120/196] Batch: 0.0721 (0.0802) Data: 0.0566 (0.0508) Loss: 1.7375 (1.7174)\n",
            "TRAIN(139): [130/196] Batch: 0.0784 (0.0799) Data: 0.0559 (0.0506) Loss: 1.6910 (1.7162)\n",
            "TRAIN(139): [140/196] Batch: 0.0807 (0.0797) Data: 0.0491 (0.0503) Loss: 1.8465 (1.7145)\n",
            "TRAIN(139): [150/196] Batch: 0.0760 (0.0794) Data: 0.0564 (0.0502) Loss: 1.6574 (1.7146)\n",
            "TRAIN(139): [160/196] Batch: 0.0778 (0.0792) Data: 0.0530 (0.0501) Loss: 1.7246 (1.7158)\n",
            "TRAIN(139): [170/196] Batch: 0.0869 (0.0791) Data: 0.0485 (0.0500) Loss: 1.6447 (1.7149)\n",
            "TRAIN(139): [180/196] Batch: 0.0727 (0.0789) Data: 0.0561 (0.0499) Loss: 1.6349 (1.7139)\n",
            "TRAIN(139): [190/196] Batch: 0.0777 (0.0787) Data: 0.0590 (0.0500) Loss: 1.8379 (1.7130)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(139)         0:00:15         0:00:09         0:00:05          1.7154\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(140): [ 10/196] Batch: 0.0730 (0.1187) Data: 0.0535 (0.0791) Loss: 1.6626 (1.6914)\n",
            "TRAIN(140): [ 20/196] Batch: 0.0737 (0.0976) Data: 0.0566 (0.0642) Loss: 1.6711 (1.6716)\n",
            "TRAIN(140): [ 30/196] Batch: 0.0688 (0.0912) Data: 0.0525 (0.0589) Loss: 1.6882 (1.6907)\n",
            "TRAIN(140): [ 40/196] Batch: 0.0850 (0.0878) Data: 0.0439 (0.0558) Loss: 1.6391 (1.7009)\n",
            "TRAIN(140): [ 50/196] Batch: 0.0759 (0.0854) Data: 0.0517 (0.0532) Loss: 1.7292 (1.6955)\n",
            "TRAIN(140): [ 60/196] Batch: 0.0795 (0.0840) Data: 0.0507 (0.0520) Loss: 1.6522 (1.6988)\n",
            "TRAIN(140): [ 70/196] Batch: 0.0834 (0.0828) Data: 0.0523 (0.0515) Loss: 1.5778 (1.6984)\n",
            "TRAIN(140): [ 80/196] Batch: 0.0783 (0.0820) Data: 0.0540 (0.0515) Loss: 1.7861 (1.6908)\n",
            "TRAIN(140): [ 90/196] Batch: 0.0810 (0.0813) Data: 0.0559 (0.0514) Loss: 1.5891 (1.6887)\n",
            "TRAIN(140): [100/196] Batch: 0.0746 (0.0808) Data: 0.0529 (0.0513) Loss: 1.6957 (1.6894)\n",
            "TRAIN(140): [110/196] Batch: 0.0786 (0.0804) Data: 0.0542 (0.0512) Loss: 1.5793 (1.6921)\n",
            "TRAIN(140): [120/196] Batch: 0.0758 (0.0800) Data: 0.0562 (0.0511) Loss: 1.6992 (1.6951)\n",
            "TRAIN(140): [130/196] Batch: 0.0762 (0.0797) Data: 0.0617 (0.0512) Loss: 1.7297 (1.6967)\n",
            "TRAIN(140): [140/196] Batch: 0.0831 (0.0795) Data: 0.0491 (0.0512) Loss: 1.6494 (1.6983)\n",
            "TRAIN(140): [150/196] Batch: 0.0643 (0.0792) Data: 0.0600 (0.0510) Loss: 1.6754 (1.6987)\n",
            "TRAIN(140): [160/196] Batch: 0.0722 (0.0791) Data: 0.0555 (0.0509) Loss: 1.6815 (1.6970)\n",
            "TRAIN(140): [170/196] Batch: 0.0711 (0.0789) Data: 0.0564 (0.0510) Loss: 1.8085 (1.6962)\n",
            "TRAIN(140): [180/196] Batch: 0.0733 (0.0787) Data: 0.0551 (0.0510) Loss: 1.8188 (1.6966)\n",
            "TRAIN(140): [190/196] Batch: 0.0753 (0.0786) Data: 0.0617 (0.0510) Loss: 1.6452 (1.6977)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(140)         0:00:15         0:00:09         0:00:05          1.6985\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(141): [ 10/196] Batch: 0.0905 (0.1343) Data: 0.0391 (0.0848) Loss: 1.7241 (1.7161)\n",
            "TRAIN(141): [ 20/196] Batch: 0.0715 (0.1059) Data: 0.0562 (0.0667) Loss: 1.6773 (1.7146)\n",
            "TRAIN(141): [ 30/196] Batch: 0.0659 (0.0968) Data: 0.0513 (0.0590) Loss: 1.8326 (1.7181)\n",
            "TRAIN(141): [ 40/196] Batch: 0.0701 (0.0920) Data: 0.0576 (0.0557) Loss: 1.8687 (1.7085)\n",
            "TRAIN(141): [ 50/196] Batch: 0.0702 (0.0888) Data: 0.0563 (0.0545) Loss: 1.7559 (1.7086)\n",
            "TRAIN(141): [ 60/196] Batch: 0.0810 (0.0868) Data: 0.0495 (0.0534) Loss: 1.8968 (1.7100)\n",
            "TRAIN(141): [ 70/196] Batch: 0.0770 (0.0851) Data: 0.0605 (0.0532) Loss: 1.6792 (1.7098)\n",
            "TRAIN(141): [ 80/196] Batch: 0.0742 (0.0840) Data: 0.0552 (0.0526) Loss: 1.7085 (1.7119)\n",
            "TRAIN(141): [ 90/196] Batch: 0.0803 (0.0832) Data: 0.0512 (0.0523) Loss: 1.5842 (1.7134)\n",
            "TRAIN(141): [100/196] Batch: 0.0668 (0.0824) Data: 0.0624 (0.0520) Loss: 1.6960 (1.7113)\n",
            "TRAIN(141): [110/196] Batch: 0.0736 (0.0818) Data: 0.0556 (0.0519) Loss: 1.5967 (1.7128)\n",
            "TRAIN(141): [120/196] Batch: 0.0794 (0.0814) Data: 0.0515 (0.0519) Loss: 1.8809 (1.7132)\n",
            "TRAIN(141): [130/196] Batch: 0.0701 (0.0809) Data: 0.0551 (0.0515) Loss: 1.8792 (1.7130)\n",
            "TRAIN(141): [140/196] Batch: 0.0671 (0.0806) Data: 0.0617 (0.0515) Loss: 1.8412 (1.7142)\n",
            "TRAIN(141): [150/196] Batch: 0.0689 (0.0803) Data: 0.0592 (0.0515) Loss: 1.7799 (1.7125)\n",
            "TRAIN(141): [160/196] Batch: 0.0803 (0.0800) Data: 0.0511 (0.0514) Loss: 1.6085 (1.7099)\n",
            "TRAIN(141): [170/196] Batch: 0.0792 (0.0798) Data: 0.0480 (0.0513) Loss: 1.7534 (1.7099)\n",
            "TRAIN(141): [180/196] Batch: 0.0768 (0.0797) Data: 0.0571 (0.0512) Loss: 1.6247 (1.7105)\n",
            "TRAIN(141): [190/196] Batch: 0.0729 (0.0794) Data: 0.0597 (0.0509) Loss: 1.5838 (1.7117)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(141)         0:00:15         0:00:09         0:00:05          1.7113\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(142): [ 10/196] Batch: 0.0750 (0.1357) Data: 0.0484 (0.0888) Loss: 1.6435 (1.7190)\n",
            "TRAIN(142): [ 20/196] Batch: 0.0802 (0.1058) Data: 0.0505 (0.0706) Loss: 1.6831 (1.6923)\n",
            "TRAIN(142): [ 30/196] Batch: 0.0768 (0.0958) Data: 0.0553 (0.0637) Loss: 1.7856 (1.6958)\n",
            "TRAIN(142): [ 40/196] Batch: 0.0674 (0.0907) Data: 0.0589 (0.0601) Loss: 1.6460 (1.6923)\n",
            "TRAIN(142): [ 50/196] Batch: 0.0691 (0.0878) Data: 0.0557 (0.0579) Loss: 1.9159 (1.6887)\n",
            "TRAIN(142): [ 60/196] Batch: 0.0763 (0.0859) Data: 0.0539 (0.0563) Loss: 1.7707 (1.6904)\n",
            "TRAIN(142): [ 70/196] Batch: 0.0831 (0.0845) Data: 0.0551 (0.0554) Loss: 1.6438 (1.6872)\n",
            "TRAIN(142): [ 80/196] Batch: 0.0637 (0.0834) Data: 0.0623 (0.0546) Loss: 1.5756 (1.6824)\n",
            "TRAIN(142): [ 90/196] Batch: 0.0714 (0.0827) Data: 0.0555 (0.0540) Loss: 1.6708 (1.6795)\n",
            "TRAIN(142): [100/196] Batch: 0.0716 (0.0820) Data: 0.0544 (0.0535) Loss: 1.6844 (1.6805)\n",
            "TRAIN(142): [110/196] Batch: 0.0686 (0.0815) Data: 0.0563 (0.0532) Loss: 1.6599 (1.6780)\n",
            "TRAIN(142): [120/196] Batch: 0.0755 (0.0810) Data: 0.0615 (0.0529) Loss: 1.6599 (1.6741)\n",
            "TRAIN(142): [130/196] Batch: 0.0641 (0.0806) Data: 0.0617 (0.0528) Loss: 1.7679 (1.6755)\n",
            "TRAIN(142): [140/196] Batch: 0.0838 (0.0804) Data: 0.0500 (0.0525) Loss: 1.5269 (1.6805)\n",
            "TRAIN(142): [150/196] Batch: 0.0771 (0.0801) Data: 0.0499 (0.0520) Loss: 1.6083 (1.6776)\n",
            "TRAIN(142): [160/196] Batch: 0.0801 (0.0800) Data: 0.0496 (0.0516) Loss: 1.5710 (1.6796)\n",
            "TRAIN(142): [170/196] Batch: 0.0824 (0.0798) Data: 0.0522 (0.0515) Loss: 1.7334 (1.6767)\n",
            "TRAIN(142): [180/196] Batch: 0.0764 (0.0796) Data: 0.0566 (0.0514) Loss: 1.5943 (1.6794)\n",
            "TRAIN(142): [190/196] Batch: 0.0751 (0.0794) Data: 0.0612 (0.0510) Loss: 1.7343 (1.6789)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(142)         0:00:15         0:00:09         0:00:05          1.6787\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(143): [ 10/196] Batch: 0.0743 (0.1198) Data: 0.0499 (0.0854) Loss: 1.7678 (1.6850)\n",
            "TRAIN(143): [ 20/196] Batch: 0.0792 (0.0985) Data: 0.0487 (0.0691) Loss: 1.6553 (1.6703)\n",
            "TRAIN(143): [ 30/196] Batch: 0.0693 (0.0909) Data: 0.0577 (0.0628) Loss: 1.5599 (1.6837)\n",
            "TRAIN(143): [ 40/196] Batch: 0.0763 (0.0873) Data: 0.0559 (0.0596) Loss: 1.8598 (1.6794)\n",
            "TRAIN(143): [ 50/196] Batch: 0.0804 (0.0851) Data: 0.0509 (0.0572) Loss: 1.6082 (1.6791)\n",
            "TRAIN(143): [ 60/196] Batch: 0.0667 (0.0835) Data: 0.0596 (0.0563) Loss: 1.6313 (1.6824)\n",
            "TRAIN(143): [ 70/196] Batch: 0.0821 (0.0826) Data: 0.0476 (0.0554) Loss: 1.6346 (1.6772)\n",
            "TRAIN(143): [ 80/196] Batch: 0.0812 (0.0817) Data: 0.0556 (0.0548) Loss: 1.7230 (1.6816)\n",
            "TRAIN(143): [ 90/196] Batch: 0.0792 (0.0811) Data: 0.0545 (0.0542) Loss: 1.7692 (1.6894)\n",
            "TRAIN(143): [100/196] Batch: 0.0798 (0.0806) Data: 0.0512 (0.0540) Loss: 1.6949 (1.6870)\n",
            "TRAIN(143): [110/196] Batch: 0.0723 (0.0801) Data: 0.0599 (0.0536) Loss: 1.7130 (1.6835)\n",
            "TRAIN(143): [120/196] Batch: 0.0758 (0.0799) Data: 0.0546 (0.0533) Loss: 1.7559 (1.6828)\n",
            "TRAIN(143): [130/196] Batch: 0.0724 (0.0796) Data: 0.0647 (0.0529) Loss: 1.6571 (1.6824)\n",
            "TRAIN(143): [140/196] Batch: 0.0657 (0.0795) Data: 0.0468 (0.0525) Loss: 1.5118 (1.6855)\n",
            "TRAIN(143): [150/196] Batch: 0.0908 (0.0795) Data: 0.0420 (0.0518) Loss: 1.5867 (1.6875)\n",
            "TRAIN(143): [160/196] Batch: 0.0980 (0.0795) Data: 0.0377 (0.0511) Loss: 1.5651 (1.6893)\n",
            "TRAIN(143): [170/196] Batch: 0.0711 (0.0792) Data: 0.0573 (0.0508) Loss: 1.6214 (1.6850)\n",
            "TRAIN(143): [180/196] Batch: 0.0802 (0.0790) Data: 0.0516 (0.0506) Loss: 1.6732 (1.6872)\n",
            "TRAIN(143): [190/196] Batch: 0.0756 (0.0788) Data: 0.0607 (0.0507) Loss: 1.4376 (1.6869)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(143)         0:00:15         0:00:09         0:00:05          1.6869\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(144): [ 10/196] Batch: 0.0742 (0.1175) Data: 0.0504 (0.0851) Loss: 1.7733 (1.6771)\n",
            "TRAIN(144): [ 20/196] Batch: 0.0793 (0.0973) Data: 0.0511 (0.0689) Loss: 1.4425 (1.6626)\n",
            "TRAIN(144): [ 30/196] Batch: 0.0794 (0.0901) Data: 0.0566 (0.0634) Loss: 1.7121 (1.6692)\n",
            "TRAIN(144): [ 40/196] Batch: 0.0788 (0.0866) Data: 0.0532 (0.0605) Loss: 1.5395 (1.6665)\n",
            "TRAIN(144): [ 50/196] Batch: 0.0706 (0.0844) Data: 0.0586 (0.0583) Loss: 1.8626 (1.6694)\n",
            "TRAIN(144): [ 60/196] Batch: 0.0715 (0.0831) Data: 0.0545 (0.0570) Loss: 1.7821 (1.6759)\n",
            "TRAIN(144): [ 70/196] Batch: 0.0837 (0.0821) Data: 0.0534 (0.0560) Loss: 1.7013 (1.6814)\n",
            "TRAIN(144): [ 80/196] Batch: 0.0709 (0.0813) Data: 0.0609 (0.0556) Loss: 1.5895 (1.6790)\n",
            "TRAIN(144): [ 90/196] Batch: 0.0822 (0.0808) Data: 0.0507 (0.0550) Loss: 1.7845 (1.6770)\n",
            "TRAIN(144): [100/196] Batch: 0.0788 (0.0803) Data: 0.0511 (0.0543) Loss: 1.6790 (1.6778)\n",
            "TRAIN(144): [110/196] Batch: 0.0749 (0.0800) Data: 0.0550 (0.0537) Loss: 1.7208 (1.6773)\n",
            "TRAIN(144): [120/196] Batch: 0.0751 (0.0797) Data: 0.0512 (0.0529) Loss: 1.7959 (1.6781)\n",
            "TRAIN(144): [130/196] Batch: 0.1085 (0.0797) Data: 0.0364 (0.0525) Loss: 1.6260 (1.6750)\n",
            "TRAIN(144): [140/196] Batch: 0.0793 (0.0795) Data: 0.0419 (0.0519) Loss: 1.6966 (1.6773)\n",
            "TRAIN(144): [150/196] Batch: 0.0692 (0.0792) Data: 0.0582 (0.0516) Loss: 1.6390 (1.6792)\n",
            "TRAIN(144): [160/196] Batch: 0.0749 (0.0790) Data: 0.0623 (0.0514) Loss: 1.7173 (1.6776)\n",
            "TRAIN(144): [170/196] Batch: 0.0744 (0.0789) Data: 0.0519 (0.0513) Loss: 1.7487 (1.6798)\n",
            "TRAIN(144): [180/196] Batch: 0.0796 (0.0788) Data: 0.0518 (0.0512) Loss: 1.7340 (1.6814)\n",
            "TRAIN(144): [190/196] Batch: 0.0775 (0.0786) Data: 0.0599 (0.0512) Loss: 1.7618 (1.6811)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(144)         0:00:15         0:00:10         0:00:05          1.6799\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(145): [ 10/196] Batch: 0.0652 (0.1303) Data: 0.0531 (0.0809) Loss: 1.6136 (1.6968)\n",
            "TRAIN(145): [ 20/196] Batch: 0.0738 (0.1054) Data: 0.0405 (0.0586) Loss: 1.7934 (1.7021)\n",
            "TRAIN(145): [ 30/196] Batch: 0.0874 (0.0965) Data: 0.0481 (0.0535) Loss: 1.6079 (1.6973)\n",
            "TRAIN(145): [ 40/196] Batch: 0.0742 (0.0911) Data: 0.0556 (0.0526) Loss: 1.5740 (1.6983)\n",
            "TRAIN(145): [ 50/196] Batch: 0.0719 (0.0881) Data: 0.0543 (0.0518) Loss: 1.5917 (1.6844)\n",
            "TRAIN(145): [ 60/196] Batch: 0.0827 (0.0861) Data: 0.0545 (0.0513) Loss: 1.5748 (1.6763)\n",
            "TRAIN(145): [ 70/196] Batch: 0.0745 (0.0847) Data: 0.0515 (0.0515) Loss: 1.6666 (1.6772)\n",
            "TRAIN(145): [ 80/196] Batch: 0.0728 (0.0836) Data: 0.0617 (0.0509) Loss: 1.6917 (1.6725)\n",
            "TRAIN(145): [ 90/196] Batch: 0.0741 (0.0829) Data: 0.0509 (0.0505) Loss: 1.8894 (1.6784)\n",
            "TRAIN(145): [100/196] Batch: 0.0711 (0.0823) Data: 0.0495 (0.0500) Loss: 1.6379 (1.6739)\n",
            "TRAIN(145): [110/196] Batch: 0.0762 (0.0818) Data: 0.0497 (0.0493) Loss: 1.5625 (1.6697)\n",
            "TRAIN(145): [120/196] Batch: 0.0852 (0.0813) Data: 0.0496 (0.0490) Loss: 1.6136 (1.6680)\n",
            "TRAIN(145): [130/196] Batch: 0.0701 (0.0809) Data: 0.0564 (0.0489) Loss: 1.6596 (1.6706)\n",
            "TRAIN(145): [140/196] Batch: 0.0862 (0.0806) Data: 0.0503 (0.0490) Loss: 1.6241 (1.6704)\n",
            "TRAIN(145): [150/196] Batch: 0.0869 (0.0803) Data: 0.0508 (0.0490) Loss: 1.5771 (1.6678)\n",
            "TRAIN(145): [160/196] Batch: 0.0642 (0.0799) Data: 0.0622 (0.0491) Loss: 1.5631 (1.6662)\n",
            "TRAIN(145): [170/196] Batch: 0.0788 (0.0798) Data: 0.0551 (0.0492) Loss: 1.7449 (1.6651)\n",
            "TRAIN(145): [180/196] Batch: 0.0760 (0.0796) Data: 0.0550 (0.0493) Loss: 1.6096 (1.6676)\n",
            "TRAIN(145): [190/196] Batch: 0.0749 (0.0794) Data: 0.0624 (0.0494) Loss: 1.6047 (1.6675)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(145)         0:00:15         0:00:09         0:00:05          1.6682\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(146): [ 10/196] Batch: 0.0790 (0.1172) Data: 0.0504 (0.0810) Loss: 1.6884 (1.6393)\n",
            "TRAIN(146): [ 20/196] Batch: 0.0738 (0.0963) Data: 0.0623 (0.0673) Loss: 1.7625 (1.6585)\n",
            "TRAIN(146): [ 30/196] Batch: 0.0842 (0.0900) Data: 0.0528 (0.0615) Loss: 1.6014 (1.6623)\n",
            "TRAIN(146): [ 40/196] Batch: 0.0822 (0.0864) Data: 0.0566 (0.0587) Loss: 1.8370 (1.6604)\n",
            "TRAIN(146): [ 50/196] Batch: 0.0855 (0.0844) Data: 0.0513 (0.0570) Loss: 1.7414 (1.6755)\n",
            "TRAIN(146): [ 60/196] Batch: 0.0748 (0.0830) Data: 0.0542 (0.0557) Loss: 1.6857 (1.6791)\n",
            "TRAIN(146): [ 70/196] Batch: 0.0778 (0.0821) Data: 0.0570 (0.0546) Loss: 1.7623 (1.6858)\n",
            "TRAIN(146): [ 80/196] Batch: 0.0649 (0.0817) Data: 0.0511 (0.0532) Loss: 1.6303 (1.6823)\n",
            "TRAIN(146): [ 90/196] Batch: 0.0782 (0.0811) Data: 0.0483 (0.0524) Loss: 1.6799 (1.6782)\n",
            "TRAIN(146): [100/196] Batch: 0.0791 (0.0807) Data: 0.0477 (0.0516) Loss: 1.5980 (1.6754)\n",
            "TRAIN(146): [110/196] Batch: 0.0763 (0.0802) Data: 0.0562 (0.0514) Loss: 1.6720 (1.6759)\n",
            "TRAIN(146): [120/196] Batch: 0.0703 (0.0799) Data: 0.0596 (0.0514) Loss: 1.7133 (1.6751)\n",
            "TRAIN(146): [130/196] Batch: 0.0727 (0.0796) Data: 0.0621 (0.0514) Loss: 1.5544 (1.6722)\n",
            "TRAIN(146): [140/196] Batch: 0.0753 (0.0793) Data: 0.0614 (0.0513) Loss: 1.6097 (1.6681)\n",
            "TRAIN(146): [150/196] Batch: 0.0726 (0.0792) Data: 0.0550 (0.0512) Loss: 1.6394 (1.6710)\n",
            "TRAIN(146): [160/196] Batch: 0.0815 (0.0790) Data: 0.0500 (0.0511) Loss: 1.6539 (1.6697)\n",
            "TRAIN(146): [170/196] Batch: 0.0757 (0.0788) Data: 0.0606 (0.0511) Loss: 1.5860 (1.6683)\n",
            "TRAIN(146): [180/196] Batch: 0.0791 (0.0787) Data: 0.0530 (0.0510) Loss: 1.6051 (1.6691)\n",
            "TRAIN(146): [190/196] Batch: 0.0755 (0.0785) Data: 0.0623 (0.0510) Loss: 1.7683 (1.6702)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(146)         0:00:15         0:00:09         0:00:05          1.6705\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(147): [ 10/196] Batch: 0.0764 (0.1198) Data: 0.0501 (0.0804) Loss: 1.7092 (1.6452)\n",
            "TRAIN(147): [ 20/196] Batch: 0.0685 (0.0977) Data: 0.0604 (0.0662) Loss: 1.7423 (1.6331)\n",
            "TRAIN(147): [ 30/196] Batch: 0.0819 (0.0911) Data: 0.0489 (0.0608) Loss: 1.7330 (1.6510)\n",
            "TRAIN(147): [ 40/196] Batch: 0.0772 (0.0875) Data: 0.0505 (0.0573) Loss: 1.6525 (1.6665)\n",
            "TRAIN(147): [ 50/196] Batch: 0.0818 (0.0856) Data: 0.0467 (0.0550) Loss: 1.7245 (1.6612)\n",
            "TRAIN(147): [ 60/196] Batch: 0.0742 (0.0840) Data: 0.0547 (0.0539) Loss: 1.8371 (1.6677)\n",
            "TRAIN(147): [ 70/196] Batch: 0.0828 (0.0833) Data: 0.0431 (0.0522) Loss: 1.7401 (1.6688)\n",
            "TRAIN(147): [ 80/196] Batch: 0.0759 (0.0825) Data: 0.0490 (0.0511) Loss: 1.5707 (1.6654)\n",
            "TRAIN(147): [ 90/196] Batch: 0.0712 (0.0817) Data: 0.0630 (0.0511) Loss: 1.6411 (1.6682)\n",
            "TRAIN(147): [100/196] Batch: 0.0714 (0.0811) Data: 0.0607 (0.0508) Loss: 1.4672 (1.6661)\n",
            "TRAIN(147): [110/196] Batch: 0.0746 (0.0807) Data: 0.0565 (0.0507) Loss: 1.6249 (1.6627)\n",
            "TRAIN(147): [120/196] Batch: 0.0882 (0.0804) Data: 0.0495 (0.0506) Loss: 1.5230 (1.6563)\n",
            "TRAIN(147): [130/196] Batch: 0.0786 (0.0801) Data: 0.0537 (0.0505) Loss: 1.6877 (1.6545)\n",
            "TRAIN(147): [140/196] Batch: 0.0802 (0.0798) Data: 0.0518 (0.0504) Loss: 1.5811 (1.6505)\n",
            "TRAIN(147): [150/196] Batch: 0.0710 (0.0795) Data: 0.0573 (0.0503) Loss: 1.7018 (1.6543)\n",
            "TRAIN(147): [160/196] Batch: 0.0723 (0.0793) Data: 0.0611 (0.0504) Loss: 1.8152 (1.6562)\n",
            "TRAIN(147): [170/196] Batch: 0.0765 (0.0791) Data: 0.0612 (0.0505) Loss: 1.5882 (1.6559)\n",
            "TRAIN(147): [180/196] Batch: 0.0786 (0.0790) Data: 0.0525 (0.0505) Loss: 1.6188 (1.6525)\n",
            "TRAIN(147): [190/196] Batch: 0.0753 (0.0788) Data: 0.0623 (0.0505) Loss: 1.6786 (1.6543)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(147)         0:00:15         0:00:09         0:00:05          1.6529\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(148): [ 10/196] Batch: 0.0854 (0.1194) Data: 0.0459 (0.0793) Loss: 1.6042 (1.6353)\n",
            "TRAIN(148): [ 20/196] Batch: 0.0720 (0.0978) Data: 0.0553 (0.0635) Loss: 1.6485 (1.6362)\n",
            "TRAIN(148): [ 30/196] Batch: 0.0688 (0.0908) Data: 0.0597 (0.0583) Loss: 1.7114 (1.6318)\n",
            "TRAIN(148): [ 40/196] Batch: 0.0765 (0.0876) Data: 0.0586 (0.0568) Loss: 1.5465 (1.6381)\n",
            "TRAIN(148): [ 50/196] Batch: 0.0735 (0.0858) Data: 0.0570 (0.0550) Loss: 1.7554 (1.6474)\n",
            "TRAIN(148): [ 60/196] Batch: 0.0737 (0.0845) Data: 0.0516 (0.0538) Loss: 1.6876 (1.6546)\n",
            "TRAIN(148): [ 70/196] Batch: 0.0805 (0.0833) Data: 0.0557 (0.0534) Loss: 1.8321 (1.6596)\n",
            "TRAIN(148): [ 80/196] Batch: 0.0762 (0.0824) Data: 0.0544 (0.0528) Loss: 1.6701 (1.6600)\n",
            "TRAIN(148): [ 90/196] Batch: 0.0716 (0.0817) Data: 0.0553 (0.0523) Loss: 1.5365 (1.6620)\n",
            "TRAIN(148): [100/196] Batch: 0.0731 (0.0812) Data: 0.0563 (0.0521) Loss: 1.6854 (1.6612)\n",
            "TRAIN(148): [110/196] Batch: 0.0707 (0.0807) Data: 0.0583 (0.0519) Loss: 1.7244 (1.6617)\n",
            "TRAIN(148): [120/196] Batch: 0.0871 (0.0804) Data: 0.0514 (0.0518) Loss: 1.5314 (1.6558)\n",
            "TRAIN(148): [130/196] Batch: 0.0822 (0.0800) Data: 0.0557 (0.0520) Loss: 1.8719 (1.6549)\n",
            "TRAIN(148): [140/196] Batch: 0.0777 (0.0798) Data: 0.0561 (0.0519) Loss: 1.7899 (1.6562)\n",
            "TRAIN(148): [150/196] Batch: 0.0796 (0.0796) Data: 0.0515 (0.0517) Loss: 1.6819 (1.6575)\n",
            "TRAIN(148): [160/196] Batch: 0.0735 (0.0794) Data: 0.0585 (0.0516) Loss: 1.8126 (1.6569)\n",
            "TRAIN(148): [170/196] Batch: 0.0694 (0.0792) Data: 0.0570 (0.0516) Loss: 1.5020 (1.6540)\n",
            "TRAIN(148): [180/196] Batch: 0.0738 (0.0790) Data: 0.0621 (0.0516) Loss: 1.6949 (1.6546)\n",
            "TRAIN(148): [190/196] Batch: 0.0794 (0.0789) Data: 0.0595 (0.0516) Loss: 1.6324 (1.6528)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(148)         0:00:15         0:00:10         0:00:05          1.6523\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(149): [ 10/196] Batch: 0.0752 (0.1303) Data: 0.0417 (0.0858) Loss: 1.6852 (1.6740)\n",
            "TRAIN(149): [ 20/196] Batch: 0.0741 (0.1041) Data: 0.0543 (0.0644) Loss: 1.6348 (1.6707)\n",
            "TRAIN(149): [ 30/196] Batch: 0.0804 (0.0955) Data: 0.0516 (0.0590) Loss: 1.5022 (1.6574)\n",
            "TRAIN(149): [ 40/196] Batch: 0.0765 (0.0907) Data: 0.0550 (0.0575) Loss: 1.7963 (1.6557)\n",
            "TRAIN(149): [ 50/196] Batch: 0.0803 (0.0878) Data: 0.0530 (0.0558) Loss: 1.7599 (1.6567)\n",
            "TRAIN(149): [ 60/196] Batch: 0.0722 (0.0858) Data: 0.0576 (0.0551) Loss: 1.7004 (1.6527)\n",
            "TRAIN(149): [ 70/196] Batch: 0.0872 (0.0846) Data: 0.0491 (0.0546) Loss: 1.7187 (1.6564)\n",
            "TRAIN(149): [ 80/196] Batch: 0.0758 (0.0834) Data: 0.0571 (0.0541) Loss: 1.5386 (1.6546)\n",
            "TRAIN(149): [ 90/196] Batch: 0.0709 (0.0826) Data: 0.0560 (0.0535) Loss: 1.7629 (1.6597)\n",
            "TRAIN(149): [100/196] Batch: 0.0717 (0.0820) Data: 0.0564 (0.0531) Loss: 1.5966 (1.6550)\n",
            "TRAIN(149): [110/196] Batch: 0.0694 (0.0815) Data: 0.0559 (0.0528) Loss: 1.6738 (1.6620)\n",
            "TRAIN(149): [120/196] Batch: 0.0810 (0.0811) Data: 0.0494 (0.0525) Loss: 1.8081 (1.6625)\n",
            "TRAIN(149): [130/196] Batch: 0.0737 (0.0808) Data: 0.0522 (0.0522) Loss: 1.5405 (1.6578)\n",
            "TRAIN(149): [140/196] Batch: 0.0639 (0.0804) Data: 0.0603 (0.0520) Loss: 1.5463 (1.6540)\n",
            "TRAIN(149): [150/196] Batch: 0.0774 (0.0802) Data: 0.0556 (0.0520) Loss: 1.6335 (1.6537)\n",
            "TRAIN(149): [160/196] Batch: 0.0754 (0.0800) Data: 0.0514 (0.0516) Loss: 1.7648 (1.6571)\n",
            "TRAIN(149): [170/196] Batch: 0.0692 (0.0798) Data: 0.0458 (0.0512) Loss: 1.7326 (1.6550)\n",
            "TRAIN(149): [180/196] Batch: 0.0757 (0.0796) Data: 0.0573 (0.0511) Loss: 1.6201 (1.6539)\n",
            "TRAIN(149): [190/196] Batch: 0.0741 (0.0795) Data: 0.0571 (0.0510) Loss: 1.8486 (1.6560)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(149)         0:00:15         0:00:09         0:00:05          1.6564\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(150): [ 10/196] Batch: 0.0743 (0.1240) Data: 0.0493 (0.0881) Loss: 1.5180 (1.6613)\n",
            "TRAIN(150): [ 20/196] Batch: 0.0791 (0.1004) Data: 0.0504 (0.0705) Loss: 1.6639 (1.6423)\n",
            "TRAIN(150): [ 30/196] Batch: 0.0764 (0.0923) Data: 0.0568 (0.0640) Loss: 1.5859 (1.6349)\n",
            "TRAIN(150): [ 40/196] Batch: 0.0871 (0.0884) Data: 0.0506 (0.0606) Loss: 1.5527 (1.6303)\n",
            "TRAIN(150): [ 50/196] Batch: 0.0742 (0.0859) Data: 0.0547 (0.0583) Loss: 1.6487 (1.6443)\n",
            "TRAIN(150): [ 60/196] Batch: 0.0710 (0.0843) Data: 0.0568 (0.0568) Loss: 1.6626 (1.6402)\n",
            "TRAIN(150): [ 70/196] Batch: 0.0656 (0.0831) Data: 0.0620 (0.0561) Loss: 1.6535 (1.6423)\n",
            "TRAIN(150): [ 80/196] Batch: 0.0664 (0.0823) Data: 0.0620 (0.0554) Loss: 1.8031 (1.6446)\n",
            "TRAIN(150): [ 90/196] Batch: 0.0650 (0.0816) Data: 0.0621 (0.0548) Loss: 1.7660 (1.6445)\n",
            "TRAIN(150): [100/196] Batch: 0.0753 (0.0811) Data: 0.0561 (0.0542) Loss: 1.7776 (1.6499)\n",
            "TRAIN(150): [110/196] Batch: 0.0722 (0.0806) Data: 0.0614 (0.0540) Loss: 1.6028 (1.6504)\n",
            "TRAIN(150): [120/196] Batch: 0.0806 (0.0803) Data: 0.0514 (0.0536) Loss: 1.5893 (1.6497)\n",
            "TRAIN(150): [130/196] Batch: 0.0848 (0.0800) Data: 0.0516 (0.0533) Loss: 1.5351 (1.6490)\n",
            "TRAIN(150): [140/196] Batch: 0.0654 (0.0798) Data: 0.0505 (0.0526) Loss: 1.7205 (1.6488)\n",
            "TRAIN(150): [150/196] Batch: 0.0752 (0.0795) Data: 0.0583 (0.0525) Loss: 1.6715 (1.6507)\n",
            "TRAIN(150): [160/196] Batch: 0.0802 (0.0794) Data: 0.0500 (0.0522) Loss: 1.6424 (1.6498)\n",
            "TRAIN(150): [170/196] Batch: 0.0814 (0.0793) Data: 0.0492 (0.0520) Loss: 1.5758 (1.6473)\n",
            "TRAIN(150): [180/196] Batch: 0.0664 (0.0792) Data: 0.0501 (0.0515) Loss: 1.5688 (1.6458)\n",
            "TRAIN(150): [190/196] Batch: 0.0756 (0.0789) Data: 0.0619 (0.0514) Loss: 1.7168 (1.6437)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(150)         0:00:15         0:00:10         0:00:05          1.6450\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(151): [ 10/196] Batch: 0.0743 (0.1176) Data: 0.0502 (0.0829) Loss: 1.6321 (1.6392)\n",
            "TRAIN(151): [ 20/196] Batch: 0.0700 (0.0970) Data: 0.0545 (0.0675) Loss: 1.4764 (1.6363)\n",
            "TRAIN(151): [ 30/196] Batch: 0.0668 (0.0901) Data: 0.0581 (0.0619) Loss: 1.8269 (1.6449)\n",
            "TRAIN(151): [ 40/196] Batch: 0.0737 (0.0866) Data: 0.0607 (0.0594) Loss: 1.6535 (1.6479)\n",
            "TRAIN(151): [ 50/196] Batch: 0.0702 (0.0844) Data: 0.0625 (0.0575) Loss: 1.6475 (1.6370)\n",
            "TRAIN(151): [ 60/196] Batch: 0.0784 (0.0833) Data: 0.0515 (0.0559) Loss: 1.5536 (1.6347)\n",
            "TRAIN(151): [ 70/196] Batch: 0.0837 (0.0823) Data: 0.0546 (0.0554) Loss: 1.7508 (1.6343)\n",
            "TRAIN(151): [ 80/196] Batch: 0.0777 (0.0815) Data: 0.0562 (0.0547) Loss: 1.5496 (1.6342)\n",
            "TRAIN(151): [ 90/196] Batch: 0.0736 (0.0808) Data: 0.0617 (0.0543) Loss: 1.7196 (1.6369)\n",
            "TRAIN(151): [100/196] Batch: 0.0739 (0.0804) Data: 0.0572 (0.0539) Loss: 1.8003 (1.6395)\n",
            "TRAIN(151): [110/196] Batch: 0.0776 (0.0801) Data: 0.0488 (0.0536) Loss: 1.5653 (1.6353)\n",
            "TRAIN(151): [120/196] Batch: 0.0758 (0.0798) Data: 0.0509 (0.0530) Loss: 1.5249 (1.6339)\n",
            "TRAIN(151): [130/196] Batch: 0.0744 (0.0796) Data: 0.0524 (0.0526) Loss: 1.8355 (1.6402)\n",
            "TRAIN(151): [140/196] Batch: 0.0744 (0.0794) Data: 0.0516 (0.0522) Loss: 1.6382 (1.6396)\n",
            "TRAIN(151): [150/196] Batch: 0.0839 (0.0793) Data: 0.0455 (0.0517) Loss: 1.6454 (1.6420)\n",
            "TRAIN(151): [160/196] Batch: 0.0826 (0.0791) Data: 0.0504 (0.0513) Loss: 1.6721 (1.6412)\n",
            "TRAIN(151): [170/196] Batch: 0.0753 (0.0789) Data: 0.0563 (0.0512) Loss: 1.6675 (1.6417)\n",
            "TRAIN(151): [180/196] Batch: 0.0651 (0.0788) Data: 0.0611 (0.0511) Loss: 1.6053 (1.6419)\n",
            "TRAIN(151): [190/196] Batch: 0.0769 (0.0786) Data: 0.0611 (0.0512) Loss: 1.5716 (1.6428)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(151)         0:00:15         0:00:10         0:00:05          1.6425\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(152): [ 10/196] Batch: 0.0792 (0.1149) Data: 0.0443 (0.0830) Loss: 1.5358 (1.6399)\n",
            "TRAIN(152): [ 20/196] Batch: 0.0839 (0.0954) Data: 0.0519 (0.0681) Loss: 1.6817 (1.6541)\n",
            "TRAIN(152): [ 30/196] Batch: 0.0741 (0.0891) Data: 0.0516 (0.0619) Loss: 1.6365 (1.6416)\n",
            "TRAIN(152): [ 40/196] Batch: 0.0786 (0.0856) Data: 0.0622 (0.0590) Loss: 1.6296 (1.6455)\n",
            "TRAIN(152): [ 50/196] Batch: 0.0734 (0.0837) Data: 0.0606 (0.0567) Loss: 1.4774 (1.6433)\n",
            "TRAIN(152): [ 60/196] Batch: 0.0751 (0.0825) Data: 0.0549 (0.0553) Loss: 1.6386 (1.6435)\n",
            "TRAIN(152): [ 70/196] Batch: 0.0713 (0.0816) Data: 0.0549 (0.0547) Loss: 1.5590 (1.6413)\n",
            "TRAIN(152): [ 80/196] Batch: 0.0721 (0.0810) Data: 0.0557 (0.0540) Loss: 1.5265 (1.6422)\n",
            "TRAIN(152): [ 90/196] Batch: 0.0765 (0.0805) Data: 0.0482 (0.0532) Loss: 1.7675 (1.6481)\n",
            "TRAIN(152): [100/196] Batch: 0.0745 (0.0801) Data: 0.0508 (0.0524) Loss: 1.6066 (1.6472)\n",
            "TRAIN(152): [110/196] Batch: 0.0616 (0.0798) Data: 0.0548 (0.0519) Loss: 1.7804 (1.6441)\n",
            "TRAIN(152): [120/196] Batch: 0.0733 (0.0795) Data: 0.0513 (0.0515) Loss: 1.6180 (1.6445)\n",
            "TRAIN(152): [130/196] Batch: 0.0775 (0.0793) Data: 0.0476 (0.0508) Loss: 1.6979 (1.6452)\n",
            "TRAIN(152): [140/196] Batch: 0.0648 (0.0791) Data: 0.0626 (0.0506) Loss: 1.5314 (1.6471)\n",
            "TRAIN(152): [150/196] Batch: 0.0848 (0.0789) Data: 0.0527 (0.0506) Loss: 1.6321 (1.6433)\n",
            "TRAIN(152): [160/196] Batch: 0.0758 (0.0788) Data: 0.0548 (0.0507) Loss: 1.6619 (1.6421)\n",
            "TRAIN(152): [170/196] Batch: 0.0823 (0.0786) Data: 0.0554 (0.0508) Loss: 1.6479 (1.6422)\n",
            "TRAIN(152): [180/196] Batch: 0.0788 (0.0785) Data: 0.0545 (0.0506) Loss: 1.6294 (1.6405)\n",
            "TRAIN(152): [190/196] Batch: 0.0745 (0.0783) Data: 0.0622 (0.0507) Loss: 1.6109 (1.6394)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(152)         0:00:15         0:00:09         0:00:05          1.6374\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(153): [ 10/196] Batch: 0.0758 (0.1188) Data: 0.0494 (0.0820) Loss: 1.5260 (1.6135)\n",
            "TRAIN(153): [ 20/196] Batch: 0.0770 (0.0977) Data: 0.0530 (0.0675) Loss: 1.5252 (1.6097)\n",
            "TRAIN(153): [ 30/196] Batch: 0.0827 (0.0907) Data: 0.0496 (0.0619) Loss: 1.7665 (1.6180)\n",
            "TRAIN(153): [ 40/196] Batch: 0.0690 (0.0869) Data: 0.0566 (0.0588) Loss: 1.5849 (1.6009)\n",
            "TRAIN(153): [ 50/196] Batch: 0.0820 (0.0848) Data: 0.0512 (0.0565) Loss: 1.4419 (1.5984)\n",
            "TRAIN(153): [ 60/196] Batch: 0.0710 (0.0833) Data: 0.0595 (0.0554) Loss: 1.6328 (1.6020)\n",
            "TRAIN(153): [ 70/196] Batch: 0.0757 (0.0823) Data: 0.0537 (0.0543) Loss: 1.6206 (1.6045)\n",
            "TRAIN(153): [ 80/196] Batch: 0.0814 (0.0818) Data: 0.0518 (0.0536) Loss: 1.4826 (1.6062)\n",
            "TRAIN(153): [ 90/196] Batch: 0.1067 (0.0816) Data: 0.0374 (0.0531) Loss: 1.6530 (1.6085)\n",
            "TRAIN(153): [100/196] Batch: 0.0793 (0.0811) Data: 0.0453 (0.0523) Loss: 1.8024 (1.6187)\n",
            "TRAIN(153): [110/196] Batch: 0.0775 (0.0808) Data: 0.0469 (0.0516) Loss: 1.5750 (1.6177)\n",
            "TRAIN(153): [120/196] Batch: 0.0681 (0.0803) Data: 0.0618 (0.0513) Loss: 1.5124 (1.6150)\n",
            "TRAIN(153): [130/196] Batch: 0.0802 (0.0801) Data: 0.0573 (0.0512) Loss: 1.6752 (1.6138)\n",
            "TRAIN(153): [140/196] Batch: 0.0747 (0.0798) Data: 0.0541 (0.0511) Loss: 1.6707 (1.6178)\n",
            "TRAIN(153): [150/196] Batch: 0.0744 (0.0796) Data: 0.0576 (0.0510) Loss: 1.6363 (1.6172)\n",
            "TRAIN(153): [160/196] Batch: 0.0827 (0.0794) Data: 0.0512 (0.0510) Loss: 1.7880 (1.6196)\n",
            "TRAIN(153): [170/196] Batch: 0.0750 (0.0792) Data: 0.0556 (0.0509) Loss: 1.5786 (1.6208)\n",
            "TRAIN(153): [180/196] Batch: 0.0797 (0.0791) Data: 0.0496 (0.0507) Loss: 1.6221 (1.6225)\n",
            "TRAIN(153): [190/196] Batch: 0.0780 (0.0789) Data: 0.0600 (0.0507) Loss: 1.7179 (1.6251)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(153)         0:00:15         0:00:09         0:00:05          1.6266\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(154): [ 10/196] Batch: 0.0737 (0.1167) Data: 0.0583 (0.0853) Loss: 1.7948 (1.6515)\n",
            "TRAIN(154): [ 20/196] Batch: 0.0734 (0.0966) Data: 0.0599 (0.0691) Loss: 1.6178 (1.6311)\n",
            "TRAIN(154): [ 30/196] Batch: 0.0797 (0.0903) Data: 0.0496 (0.0629) Loss: 1.8016 (1.6295)\n",
            "TRAIN(154): [ 40/196] Batch: 0.0806 (0.0868) Data: 0.0523 (0.0595) Loss: 1.4925 (1.6338)\n",
            "TRAIN(154): [ 50/196] Batch: 0.0844 (0.0849) Data: 0.0438 (0.0570) Loss: 1.5044 (1.6367)\n",
            "TRAIN(154): [ 60/196] Batch: 0.0946 (0.0837) Data: 0.0330 (0.0543) Loss: 1.6595 (1.6349)\n",
            "TRAIN(154): [ 70/196] Batch: 0.0810 (0.0826) Data: 0.0443 (0.0529) Loss: 1.6955 (1.6357)\n",
            "TRAIN(154): [ 80/196] Batch: 0.0736 (0.0819) Data: 0.0537 (0.0524) Loss: 1.4585 (1.6316)\n",
            "TRAIN(154): [ 90/196] Batch: 0.0690 (0.0813) Data: 0.0544 (0.0520) Loss: 1.6590 (1.6362)\n",
            "TRAIN(154): [100/196] Batch: 0.0764 (0.0808) Data: 0.0562 (0.0515) Loss: 1.5735 (1.6335)\n",
            "TRAIN(154): [110/196] Batch: 0.0834 (0.0804) Data: 0.0554 (0.0513) Loss: 1.6211 (1.6301)\n",
            "TRAIN(154): [120/196] Batch: 0.0793 (0.0801) Data: 0.0506 (0.0513) Loss: 1.6016 (1.6316)\n",
            "TRAIN(154): [130/196] Batch: 0.0786 (0.0798) Data: 0.0535 (0.0513) Loss: 1.8138 (1.6321)\n",
            "TRAIN(154): [140/196] Batch: 0.0765 (0.0795) Data: 0.0549 (0.0511) Loss: 1.7245 (1.6321)\n",
            "TRAIN(154): [150/196] Batch: 0.0700 (0.0793) Data: 0.0563 (0.0509) Loss: 1.7096 (1.6308)\n",
            "TRAIN(154): [160/196] Batch: 0.0776 (0.0791) Data: 0.0520 (0.0508) Loss: 1.7815 (1.6332)\n",
            "TRAIN(154): [170/196] Batch: 0.0762 (0.0789) Data: 0.0509 (0.0506) Loss: 1.5766 (1.6296)\n",
            "TRAIN(154): [180/196] Batch: 0.0653 (0.0787) Data: 0.0615 (0.0508) Loss: 1.5689 (1.6288)\n",
            "TRAIN(154): [190/196] Batch: 0.0765 (0.0786) Data: 0.0616 (0.0508) Loss: 1.6222 (1.6285)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(154)         0:00:15         0:00:09         0:00:05          1.6283\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(155): [ 10/196] Batch: 0.0699 (0.1158) Data: 0.0515 (0.0809) Loss: 1.6603 (1.6664)\n",
            "TRAIN(155): [ 20/196] Batch: 0.0794 (0.0964) Data: 0.0497 (0.0664) Loss: 1.5923 (1.6495)\n",
            "TRAIN(155): [ 30/196] Batch: 0.0720 (0.0899) Data: 0.0493 (0.0593) Loss: 1.5906 (1.6489)\n",
            "TRAIN(155): [ 40/196] Batch: 0.0849 (0.0866) Data: 0.0515 (0.0559) Loss: 1.5177 (1.6375)\n",
            "TRAIN(155): [ 50/196] Batch: 0.0913 (0.0847) Data: 0.0455 (0.0541) Loss: 1.5211 (1.6252)\n",
            "TRAIN(155): [ 60/196] Batch: 0.0763 (0.0835) Data: 0.0471 (0.0527) Loss: 1.5184 (1.6261)\n",
            "TRAIN(155): [ 70/196] Batch: 0.0688 (0.0823) Data: 0.0578 (0.0518) Loss: 1.4713 (1.6172)\n",
            "TRAIN(155): [ 80/196] Batch: 0.0779 (0.0816) Data: 0.0517 (0.0513) Loss: 1.5954 (1.6145)\n",
            "TRAIN(155): [ 90/196] Batch: 0.0804 (0.0810) Data: 0.0521 (0.0511) Loss: 1.4625 (1.6151)\n",
            "TRAIN(155): [100/196] Batch: 0.0720 (0.0805) Data: 0.0549 (0.0509) Loss: 1.5945 (1.6175)\n",
            "TRAIN(155): [110/196] Batch: 0.0805 (0.0802) Data: 0.0510 (0.0509) Loss: 1.5820 (1.6160)\n",
            "TRAIN(155): [120/196] Batch: 0.0687 (0.0798) Data: 0.0578 (0.0506) Loss: 1.4956 (1.6153)\n",
            "TRAIN(155): [130/196] Batch: 0.0840 (0.0796) Data: 0.0453 (0.0503) Loss: 1.6504 (1.6166)\n",
            "TRAIN(155): [140/196] Batch: 0.0794 (0.0793) Data: 0.0522 (0.0500) Loss: 1.6414 (1.6167)\n",
            "TRAIN(155): [150/196] Batch: 0.0755 (0.0790) Data: 0.0548 (0.0500) Loss: 1.6046 (1.6151)\n",
            "TRAIN(155): [160/196] Batch: 0.0804 (0.0789) Data: 0.0507 (0.0501) Loss: 1.7816 (1.6163)\n",
            "TRAIN(155): [170/196] Batch: 0.0838 (0.0787) Data: 0.0490 (0.0500) Loss: 1.7731 (1.6183)\n",
            "TRAIN(155): [180/196] Batch: 0.0772 (0.0785) Data: 0.0572 (0.0500) Loss: 1.5607 (1.6183)\n",
            "TRAIN(155): [190/196] Batch: 0.0762 (0.0784) Data: 0.0633 (0.0500) Loss: 1.5037 (1.6173)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(155)         0:00:15         0:00:09         0:00:05          1.6161\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(156): [ 10/196] Batch: 0.0687 (0.1214) Data: 0.0473 (0.0809) Loss: 1.8512 (1.6693)\n",
            "TRAIN(156): [ 20/196] Batch: 0.0703 (0.0991) Data: 0.0516 (0.0643) Loss: 1.6660 (1.6455)\n",
            "TRAIN(156): [ 30/196] Batch: 0.0735 (0.0914) Data: 0.0575 (0.0598) Loss: 1.6089 (1.6335)\n",
            "TRAIN(156): [ 40/196] Batch: 0.0657 (0.0884) Data: 0.0453 (0.0556) Loss: 1.7411 (1.6367)\n",
            "TRAIN(156): [ 50/196] Batch: 0.0695 (0.0866) Data: 0.0477 (0.0521) Loss: 1.4672 (1.6276)\n",
            "TRAIN(156): [ 60/196] Batch: 0.0656 (0.0847) Data: 0.0623 (0.0512) Loss: 1.4867 (1.6177)\n",
            "TRAIN(156): [ 70/196] Batch: 0.0860 (0.0836) Data: 0.0484 (0.0510) Loss: 1.5839 (1.6220)\n",
            "TRAIN(156): [ 80/196] Batch: 0.0705 (0.0826) Data: 0.0621 (0.0504) Loss: 1.4891 (1.6171)\n",
            "TRAIN(156): [ 90/196] Batch: 0.0720 (0.0818) Data: 0.0626 (0.0504) Loss: 1.8374 (1.6209)\n",
            "TRAIN(156): [100/196] Batch: 0.0751 (0.0813) Data: 0.0553 (0.0504) Loss: 1.5947 (1.6180)\n",
            "TRAIN(156): [110/196] Batch: 0.0707 (0.0808) Data: 0.0569 (0.0506) Loss: 1.4362 (1.6181)\n",
            "TRAIN(156): [120/196] Batch: 0.0799 (0.0805) Data: 0.0520 (0.0505) Loss: 1.6588 (1.6192)\n",
            "TRAIN(156): [130/196] Batch: 0.0711 (0.0800) Data: 0.0614 (0.0505) Loss: 1.5977 (1.6129)\n",
            "TRAIN(156): [140/196] Batch: 0.0773 (0.0798) Data: 0.0551 (0.0503) Loss: 1.6109 (1.6126)\n",
            "TRAIN(156): [150/196] Batch: 0.0711 (0.0795) Data: 0.0558 (0.0503) Loss: 1.5655 (1.6130)\n",
            "TRAIN(156): [160/196] Batch: 0.0688 (0.0793) Data: 0.0582 (0.0504) Loss: 1.5691 (1.6094)\n",
            "TRAIN(156): [170/196] Batch: 0.0698 (0.0791) Data: 0.0622 (0.0503) Loss: 1.6236 (1.6099)\n",
            "TRAIN(156): [180/196] Batch: 0.0740 (0.0790) Data: 0.0556 (0.0502) Loss: 1.6755 (1.6069)\n",
            "TRAIN(156): [190/196] Batch: 0.0686 (0.0788) Data: 0.0607 (0.0501) Loss: 1.6334 (1.6033)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(156)         0:00:15         0:00:09         0:00:05          1.6039\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(157): [ 10/196] Batch: 0.0784 (0.1352) Data: 0.0385 (0.0895) Loss: 1.6189 (1.6154)\n",
            "TRAIN(157): [ 20/196] Batch: 0.0934 (0.1075) Data: 0.0395 (0.0677) Loss: 1.4903 (1.5859)\n",
            "TRAIN(157): [ 30/196] Batch: 0.0795 (0.0975) Data: 0.0539 (0.0608) Loss: 1.5681 (1.5999)\n",
            "TRAIN(157): [ 40/196] Batch: 0.0710 (0.0922) Data: 0.0546 (0.0579) Loss: 1.7103 (1.6155)\n",
            "TRAIN(157): [ 50/196] Batch: 0.0720 (0.0890) Data: 0.0544 (0.0561) Loss: 1.5636 (1.6126)\n",
            "TRAIN(157): [ 60/196] Batch: 0.0790 (0.0869) Data: 0.0488 (0.0546) Loss: 1.6544 (1.6059)\n",
            "TRAIN(157): [ 70/196] Batch: 0.0804 (0.0854) Data: 0.0476 (0.0534) Loss: 1.6413 (1.6017)\n",
            "TRAIN(157): [ 80/196] Batch: 0.0885 (0.0842) Data: 0.0495 (0.0526) Loss: 1.4225 (1.6035)\n",
            "TRAIN(157): [ 90/196] Batch: 0.0877 (0.0833) Data: 0.0483 (0.0523) Loss: 1.4864 (1.6049)\n",
            "TRAIN(157): [100/196] Batch: 0.0678 (0.0825) Data: 0.0588 (0.0519) Loss: 1.7828 (1.6112)\n",
            "TRAIN(157): [110/196] Batch: 0.0657 (0.0819) Data: 0.0614 (0.0517) Loss: 1.6604 (1.6173)\n",
            "TRAIN(157): [120/196] Batch: 0.0778 (0.0815) Data: 0.0557 (0.0516) Loss: 1.5645 (1.6179)\n",
            "TRAIN(157): [130/196] Batch: 0.0726 (0.0810) Data: 0.0544 (0.0512) Loss: 1.5197 (1.6145)\n",
            "TRAIN(157): [140/196] Batch: 0.0721 (0.0807) Data: 0.0561 (0.0511) Loss: 1.4698 (1.6132)\n",
            "TRAIN(157): [150/196] Batch: 0.0870 (0.0804) Data: 0.0512 (0.0511) Loss: 1.7076 (1.6168)\n",
            "TRAIN(157): [160/196] Batch: 0.0684 (0.0801) Data: 0.0493 (0.0508) Loss: 1.5600 (1.6178)\n",
            "TRAIN(157): [170/196] Batch: 0.0846 (0.0800) Data: 0.0445 (0.0504) Loss: 1.6017 (1.6183)\n",
            "TRAIN(157): [180/196] Batch: 0.0657 (0.0798) Data: 0.0529 (0.0499) Loss: 1.7314 (1.6163)\n",
            "TRAIN(157): [190/196] Batch: 0.0759 (0.0796) Data: 0.0591 (0.0498) Loss: 1.6805 (1.6160)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(157)         0:00:15         0:00:09         0:00:05          1.6155\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(158): [ 10/196] Batch: 0.0622 (0.1224) Data: 0.0608 (0.0866) Loss: 1.4323 (1.6244)\n",
            "TRAIN(158): [ 20/196] Batch: 0.0716 (0.0998) Data: 0.0511 (0.0680) Loss: 1.7143 (1.6315)\n",
            "TRAIN(158): [ 30/196] Batch: 0.0643 (0.0917) Data: 0.0596 (0.0618) Loss: 1.6698 (1.6341)\n",
            "TRAIN(158): [ 40/196] Batch: 0.0767 (0.0880) Data: 0.0541 (0.0584) Loss: 1.7134 (1.6283)\n",
            "TRAIN(158): [ 50/196] Batch: 0.0742 (0.0856) Data: 0.0507 (0.0561) Loss: 1.6681 (1.6219)\n",
            "TRAIN(158): [ 60/196] Batch: 0.0756 (0.0841) Data: 0.0474 (0.0547) Loss: 1.5986 (1.6126)\n",
            "TRAIN(158): [ 70/196] Batch: 0.0919 (0.0830) Data: 0.0457 (0.0537) Loss: 1.6119 (1.6146)\n",
            "TRAIN(158): [ 80/196] Batch: 0.0751 (0.0822) Data: 0.0460 (0.0525) Loss: 1.5895 (1.6134)\n",
            "TRAIN(158): [ 90/196] Batch: 0.0797 (0.0815) Data: 0.0458 (0.0515) Loss: 1.6104 (1.6091)\n",
            "TRAIN(158): [100/196] Batch: 0.0639 (0.0809) Data: 0.0609 (0.0511) Loss: 1.7081 (1.6098)\n",
            "TRAIN(158): [110/196] Batch: 0.0753 (0.0806) Data: 0.0471 (0.0506) Loss: 1.5624 (1.6076)\n",
            "TRAIN(158): [120/196] Batch: 0.0788 (0.0802) Data: 0.0468 (0.0502) Loss: 1.6862 (1.6070)\n",
            "TRAIN(158): [130/196] Batch: 0.0721 (0.0799) Data: 0.0485 (0.0499) Loss: 1.5044 (1.6050)\n",
            "TRAIN(158): [140/196] Batch: 0.0922 (0.0799) Data: 0.0314 (0.0492) Loss: 1.6898 (1.6044)\n",
            "TRAIN(158): [150/196] Batch: 0.0672 (0.0800) Data: 0.0417 (0.0486) Loss: 1.5984 (1.6043)\n",
            "TRAIN(158): [160/196] Batch: 0.0645 (0.0799) Data: 0.0537 (0.0481) Loss: 1.5665 (1.6024)\n",
            "TRAIN(158): [170/196] Batch: 0.0849 (0.0799) Data: 0.0423 (0.0473) Loss: 1.4939 (1.6021)\n",
            "TRAIN(158): [180/196] Batch: 0.0762 (0.0800) Data: 0.0338 (0.0469) Loss: 1.5993 (1.6025)\n",
            "TRAIN(158): [190/196] Batch: 0.0746 (0.0798) Data: 0.0615 (0.0468) Loss: 1.6354 (1.6025)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(158)         0:00:15         0:00:09         0:00:06          1.6022\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(159): [ 10/196] Batch: 0.0755 (0.1214) Data: 0.0481 (0.0838) Loss: 1.7050 (1.5783)\n",
            "TRAIN(159): [ 20/196] Batch: 0.0700 (0.0986) Data: 0.0550 (0.0675) Loss: 1.6698 (1.5802)\n",
            "TRAIN(159): [ 30/196] Batch: 0.0819 (0.0912) Data: 0.0540 (0.0617) Loss: 1.6735 (1.5875)\n",
            "TRAIN(159): [ 40/196] Batch: 0.0724 (0.0874) Data: 0.0581 (0.0588) Loss: 1.6531 (1.5985)\n",
            "TRAIN(159): [ 50/196] Batch: 0.0756 (0.0851) Data: 0.0557 (0.0568) Loss: 1.6565 (1.5963)\n",
            "TRAIN(159): [ 60/196] Batch: 0.0785 (0.0837) Data: 0.0524 (0.0558) Loss: 1.5043 (1.5898)\n",
            "TRAIN(159): [ 70/196] Batch: 0.0740 (0.0826) Data: 0.0529 (0.0550) Loss: 1.6002 (1.5872)\n",
            "TRAIN(159): [ 80/196] Batch: 0.0824 (0.0818) Data: 0.0552 (0.0543) Loss: 1.4643 (1.5859)\n",
            "TRAIN(159): [ 90/196] Batch: 0.0670 (0.0811) Data: 0.0635 (0.0538) Loss: 1.5186 (1.5855)\n",
            "TRAIN(159): [100/196] Batch: 0.0716 (0.0807) Data: 0.0543 (0.0535) Loss: 1.6110 (1.5842)\n",
            "TRAIN(159): [110/196] Batch: 0.0673 (0.0802) Data: 0.0613 (0.0533) Loss: 1.5489 (1.5845)\n",
            "TRAIN(159): [120/196] Batch: 0.0729 (0.0800) Data: 0.0524 (0.0526) Loss: 1.6306 (1.5852)\n",
            "TRAIN(159): [130/196] Batch: 0.0704 (0.0798) Data: 0.0446 (0.0517) Loss: 1.6300 (1.5887)\n",
            "TRAIN(159): [140/196] Batch: 0.0786 (0.0797) Data: 0.0502 (0.0510) Loss: 1.5866 (1.5939)\n",
            "TRAIN(159): [150/196] Batch: 0.0722 (0.0795) Data: 0.0501 (0.0506) Loss: 1.4014 (1.5912)\n",
            "TRAIN(159): [160/196] Batch: 0.0741 (0.0794) Data: 0.0524 (0.0502) Loss: 1.5740 (1.5941)\n",
            "TRAIN(159): [170/196] Batch: 0.0762 (0.0792) Data: 0.0504 (0.0499) Loss: 1.4609 (1.5931)\n",
            "TRAIN(159): [180/196] Batch: 0.0792 (0.0790) Data: 0.0516 (0.0498) Loss: 1.4642 (1.5951)\n",
            "TRAIN(159): [190/196] Batch: 0.0753 (0.0788) Data: 0.0626 (0.0500) Loss: 1.5604 (1.5929)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(159)         0:00:15         0:00:09         0:00:05          1.5934\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(160): [ 10/196] Batch: 0.0701 (0.1150) Data: 0.0541 (0.0831) Loss: 1.4815 (1.5860)\n",
            "TRAIN(160): [ 20/196] Batch: 0.0649 (0.0957) Data: 0.0600 (0.0672) Loss: 1.6074 (1.5712)\n",
            "TRAIN(160): [ 30/196] Batch: 0.0852 (0.0896) Data: 0.0505 (0.0628) Loss: 1.6606 (1.5912)\n",
            "TRAIN(160): [ 40/196] Batch: 0.0715 (0.0861) Data: 0.0586 (0.0601) Loss: 1.6464 (1.5883)\n",
            "TRAIN(160): [ 50/196] Batch: 0.0715 (0.0840) Data: 0.0625 (0.0585) Loss: 1.5458 (1.5847)\n",
            "TRAIN(160): [ 60/196] Batch: 0.0694 (0.0828) Data: 0.0573 (0.0567) Loss: 1.7314 (1.5874)\n",
            "TRAIN(160): [ 70/196] Batch: 0.0810 (0.0820) Data: 0.0495 (0.0558) Loss: 1.6049 (1.5845)\n",
            "TRAIN(160): [ 80/196] Batch: 0.0700 (0.0812) Data: 0.0575 (0.0550) Loss: 1.6782 (1.5822)\n",
            "TRAIN(160): [ 90/196] Batch: 0.0724 (0.0807) Data: 0.0565 (0.0543) Loss: 1.8604 (1.5859)\n",
            "TRAIN(160): [100/196] Batch: 0.0754 (0.0803) Data: 0.0484 (0.0539) Loss: 1.5584 (1.5860)\n",
            "TRAIN(160): [110/196] Batch: 0.0603 (0.0799) Data: 0.0573 (0.0532) Loss: 1.4339 (1.5878)\n",
            "TRAIN(160): [120/196] Batch: 0.0726 (0.0796) Data: 0.0532 (0.0526) Loss: 1.5898 (1.5882)\n",
            "TRAIN(160): [130/196] Batch: 0.0667 (0.0794) Data: 0.0569 (0.0520) Loss: 1.6586 (1.5853)\n",
            "TRAIN(160): [140/196] Batch: 0.0762 (0.0793) Data: 0.0581 (0.0515) Loss: 1.6151 (1.5880)\n",
            "TRAIN(160): [150/196] Batch: 0.0868 (0.0793) Data: 0.0502 (0.0512) Loss: 1.5345 (1.5875)\n",
            "TRAIN(160): [160/196] Batch: 0.0694 (0.0791) Data: 0.0561 (0.0512) Loss: 1.5718 (1.5891)\n",
            "TRAIN(160): [170/196] Batch: 0.0842 (0.0790) Data: 0.0546 (0.0511) Loss: 1.3913 (1.5903)\n",
            "TRAIN(160): [180/196] Batch: 0.0861 (0.0788) Data: 0.0536 (0.0511) Loss: 1.3715 (1.5878)\n",
            "TRAIN(160): [190/196] Batch: 0.0756 (0.0786) Data: 0.0632 (0.0511) Loss: 1.6080 (1.5890)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(160)         0:00:15         0:00:10         0:00:05          1.5884\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(161): [ 10/196] Batch: 0.0751 (0.1174) Data: 0.0497 (0.0785) Loss: 1.6279 (1.5989)\n",
            "TRAIN(161): [ 20/196] Batch: 0.0746 (0.0964) Data: 0.0605 (0.0655) Loss: 1.4969 (1.5615)\n",
            "TRAIN(161): [ 30/196] Batch: 0.0806 (0.0902) Data: 0.0493 (0.0602) Loss: 1.4942 (1.5591)\n",
            "TRAIN(161): [ 40/196] Batch: 0.0751 (0.0865) Data: 0.0573 (0.0578) Loss: 1.6719 (1.5694)\n",
            "TRAIN(161): [ 50/196] Batch: 0.0669 (0.0843) Data: 0.0626 (0.0563) Loss: 1.6680 (1.5655)\n",
            "TRAIN(161): [ 60/196] Batch: 0.0780 (0.0831) Data: 0.0515 (0.0554) Loss: 1.4309 (1.5595)\n",
            "TRAIN(161): [ 70/196] Batch: 0.0717 (0.0822) Data: 0.0547 (0.0543) Loss: 1.6293 (1.5612)\n",
            "TRAIN(161): [ 80/196] Batch: 0.0783 (0.0815) Data: 0.0493 (0.0534) Loss: 1.5412 (1.5667)\n",
            "TRAIN(161): [ 90/196] Batch: 0.0763 (0.0810) Data: 0.0573 (0.0526) Loss: 1.5801 (1.5617)\n",
            "TRAIN(161): [100/196] Batch: 0.0784 (0.0807) Data: 0.0463 (0.0522) Loss: 1.4888 (1.5608)\n",
            "TRAIN(161): [110/196] Batch: 0.0783 (0.0806) Data: 0.0443 (0.0509) Loss: 1.4411 (1.5612)\n",
            "TRAIN(161): [120/196] Batch: 0.1330 (0.0814) Data: 0.0400 (0.0504) Loss: 1.4604 (1.5609)\n",
            "TRAIN(161): [130/196] Batch: 0.0848 (0.0817) Data: 0.0366 (0.0497) Loss: 1.4929 (1.5613)\n",
            "TRAIN(161): [140/196] Batch: 0.0835 (0.0815) Data: 0.0405 (0.0490) Loss: 1.5513 (1.5620)\n",
            "TRAIN(161): [150/196] Batch: 0.0760 (0.0811) Data: 0.0526 (0.0487) Loss: 1.6451 (1.5638)\n",
            "TRAIN(161): [160/196] Batch: 0.0671 (0.0808) Data: 0.0615 (0.0487) Loss: 1.5299 (1.5637)\n",
            "TRAIN(161): [170/196] Batch: 0.0838 (0.0806) Data: 0.0505 (0.0487) Loss: 1.7012 (1.5645)\n",
            "TRAIN(161): [180/196] Batch: 0.0836 (0.0804) Data: 0.0472 (0.0488) Loss: 1.7860 (1.5674)\n",
            "TRAIN(161): [190/196] Batch: 0.0761 (0.0801) Data: 0.0614 (0.0489) Loss: 1.6100 (1.5656)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(161)         0:00:15         0:00:09         0:00:06          1.5645\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(162): [ 10/196] Batch: 0.0650 (0.1193) Data: 0.0600 (0.0839) Loss: 1.6485 (1.5352)\n",
            "TRAIN(162): [ 20/196] Batch: 0.0749 (0.0984) Data: 0.0541 (0.0681) Loss: 1.5580 (1.5574)\n",
            "TRAIN(162): [ 30/196] Batch: 0.0665 (0.0909) Data: 0.0607 (0.0623) Loss: 1.4859 (1.5671)\n",
            "TRAIN(162): [ 40/196] Batch: 0.0717 (0.0874) Data: 0.0557 (0.0593) Loss: 1.4717 (1.5558)\n",
            "TRAIN(162): [ 50/196] Batch: 0.0821 (0.0852) Data: 0.0493 (0.0569) Loss: 1.5061 (1.5665)\n",
            "TRAIN(162): [ 60/196] Batch: 0.0759 (0.0837) Data: 0.0631 (0.0555) Loss: 1.5581 (1.5720)\n",
            "TRAIN(162): [ 70/196] Batch: 0.0728 (0.0829) Data: 0.0499 (0.0541) Loss: 1.5983 (1.5807)\n",
            "TRAIN(162): [ 80/196] Batch: 0.0615 (0.0822) Data: 0.0543 (0.0535) Loss: 1.5645 (1.5779)\n",
            "TRAIN(162): [ 90/196] Batch: 0.0712 (0.0816) Data: 0.0522 (0.0524) Loss: 1.4889 (1.5772)\n",
            "TRAIN(162): [100/196] Batch: 0.0814 (0.0812) Data: 0.0498 (0.0519) Loss: 1.4606 (1.5748)\n",
            "TRAIN(162): [110/196] Batch: 0.0700 (0.0807) Data: 0.0598 (0.0515) Loss: 1.7209 (1.5789)\n",
            "TRAIN(162): [120/196] Batch: 0.0814 (0.0803) Data: 0.0562 (0.0515) Loss: 1.5276 (1.5799)\n",
            "TRAIN(162): [130/196] Batch: 0.0712 (0.0801) Data: 0.0577 (0.0512) Loss: 1.4005 (1.5789)\n",
            "TRAIN(162): [140/196] Batch: 0.0810 (0.0798) Data: 0.0496 (0.0510) Loss: 1.6078 (1.5783)\n",
            "TRAIN(162): [150/196] Batch: 0.0781 (0.0796) Data: 0.0528 (0.0509) Loss: 1.5700 (1.5792)\n",
            "TRAIN(162): [160/196] Batch: 0.0719 (0.0794) Data: 0.0559 (0.0507) Loss: 1.5108 (1.5764)\n",
            "TRAIN(162): [170/196] Batch: 0.0888 (0.0792) Data: 0.0494 (0.0506) Loss: 1.6167 (1.5762)\n",
            "TRAIN(162): [180/196] Batch: 0.0789 (0.0790) Data: 0.0506 (0.0505) Loss: 1.4970 (1.5753)\n",
            "TRAIN(162): [190/196] Batch: 0.0762 (0.0788) Data: 0.0618 (0.0506) Loss: 1.7001 (1.5729)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(162)         0:00:15         0:00:09         0:00:05          1.5727\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(163): [ 10/196] Batch: 0.0804 (0.1166) Data: 0.0438 (0.0809) Loss: 1.4090 (1.5907)\n",
            "TRAIN(163): [ 20/196] Batch: 0.0768 (0.0963) Data: 0.0536 (0.0665) Loss: 1.5578 (1.5684)\n",
            "TRAIN(163): [ 30/196] Batch: 0.0807 (0.0897) Data: 0.0567 (0.0611) Loss: 1.6239 (1.5853)\n",
            "TRAIN(163): [ 40/196] Batch: 0.0768 (0.0863) Data: 0.0566 (0.0579) Loss: 1.7624 (1.5952)\n",
            "TRAIN(163): [ 50/196] Batch: 0.0921 (0.0850) Data: 0.0434 (0.0562) Loss: 1.5215 (1.5871)\n",
            "TRAIN(163): [ 60/196] Batch: 0.0795 (0.0835) Data: 0.0517 (0.0550) Loss: 1.5323 (1.5913)\n",
            "TRAIN(163): [ 70/196] Batch: 0.0899 (0.0828) Data: 0.0406 (0.0537) Loss: 1.4022 (1.5839)\n",
            "TRAIN(163): [ 80/196] Batch: 0.0763 (0.0819) Data: 0.0517 (0.0524) Loss: 1.3975 (1.5772)\n",
            "TRAIN(163): [ 90/196] Batch: 0.0771 (0.0813) Data: 0.0470 (0.0516) Loss: 1.6921 (1.5714)\n",
            "TRAIN(163): [100/196] Batch: 0.0796 (0.0808) Data: 0.0506 (0.0513) Loss: 1.7010 (1.5691)\n",
            "TRAIN(163): [110/196] Batch: 0.0795 (0.0804) Data: 0.0518 (0.0511) Loss: 1.6024 (1.5710)\n",
            "TRAIN(163): [120/196] Batch: 0.0875 (0.0800) Data: 0.0514 (0.0510) Loss: 1.7305 (1.5728)\n",
            "TRAIN(163): [130/196] Batch: 0.0670 (0.0797) Data: 0.0586 (0.0508) Loss: 1.5885 (1.5739)\n",
            "TRAIN(163): [140/196] Batch: 0.0734 (0.0795) Data: 0.0572 (0.0508) Loss: 1.5446 (1.5694)\n",
            "TRAIN(163): [150/196] Batch: 0.0823 (0.0793) Data: 0.0514 (0.0507) Loss: 1.4978 (1.5663)\n",
            "TRAIN(163): [160/196] Batch: 0.0629 (0.0791) Data: 0.0623 (0.0506) Loss: 1.4653 (1.5709)\n",
            "TRAIN(163): [170/196] Batch: 0.0877 (0.0790) Data: 0.0523 (0.0505) Loss: 1.5435 (1.5689)\n",
            "TRAIN(163): [180/196] Batch: 0.0700 (0.0788) Data: 0.0620 (0.0505) Loss: 1.7269 (1.5694)\n",
            "TRAIN(163): [190/196] Batch: 0.0748 (0.0786) Data: 0.0620 (0.0505) Loss: 1.6813 (1.5721)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(163)         0:00:15         0:00:09         0:00:05          1.5723\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(164): [ 10/196] Batch: 0.0668 (0.1177) Data: 0.0614 (0.0836) Loss: 1.4782 (1.5714)\n",
            "TRAIN(164): [ 20/196] Batch: 0.0836 (0.0983) Data: 0.0418 (0.0650) Loss: 1.4866 (1.5627)\n",
            "TRAIN(164): [ 30/196] Batch: 0.0761 (0.0909) Data: 0.0491 (0.0585) Loss: 1.6791 (1.5708)\n",
            "TRAIN(164): [ 40/196] Batch: 0.0742 (0.0874) Data: 0.0542 (0.0549) Loss: 1.5939 (1.5703)\n",
            "TRAIN(164): [ 50/196] Batch: 0.0825 (0.0853) Data: 0.0527 (0.0528) Loss: 1.4935 (1.5714)\n",
            "TRAIN(164): [ 60/196] Batch: 0.0762 (0.0839) Data: 0.0490 (0.0518) Loss: 1.6360 (1.5632)\n",
            "TRAIN(164): [ 70/196] Batch: 0.0842 (0.0828) Data: 0.0522 (0.0516) Loss: 1.5990 (1.5687)\n",
            "TRAIN(164): [ 80/196] Batch: 0.0687 (0.0820) Data: 0.0577 (0.0512) Loss: 1.5281 (1.5665)\n",
            "TRAIN(164): [ 90/196] Batch: 0.0805 (0.0814) Data: 0.0516 (0.0509) Loss: 1.4939 (1.5662)\n",
            "TRAIN(164): [100/196] Batch: 0.0713 (0.0809) Data: 0.0551 (0.0507) Loss: 1.5166 (1.5598)\n",
            "TRAIN(164): [110/196] Batch: 0.0714 (0.0805) Data: 0.0563 (0.0505) Loss: 1.4165 (1.5615)\n",
            "TRAIN(164): [120/196] Batch: 0.0728 (0.0802) Data: 0.0553 (0.0506) Loss: 1.5493 (1.5612)\n",
            "TRAIN(164): [130/196] Batch: 0.0675 (0.0798) Data: 0.0623 (0.0506) Loss: 1.4446 (1.5600)\n",
            "TRAIN(164): [140/196] Batch: 0.0759 (0.0796) Data: 0.0580 (0.0506) Loss: 1.6690 (1.5592)\n",
            "TRAIN(164): [150/196] Batch: 0.0756 (0.0794) Data: 0.0558 (0.0505) Loss: 1.4898 (1.5599)\n",
            "TRAIN(164): [160/196] Batch: 0.0818 (0.0792) Data: 0.0551 (0.0505) Loss: 1.6571 (1.5645)\n",
            "TRAIN(164): [170/196] Batch: 0.0654 (0.0790) Data: 0.0612 (0.0505) Loss: 1.5779 (1.5683)\n",
            "TRAIN(164): [180/196] Batch: 0.0787 (0.0788) Data: 0.0587 (0.0505) Loss: 1.4182 (1.5685)\n",
            "TRAIN(164): [190/196] Batch: 0.0748 (0.0787) Data: 0.0618 (0.0505) Loss: 1.6386 (1.5707)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(164)         0:00:15         0:00:09         0:00:05          1.5719\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(165): [ 10/196] Batch: 0.0658 (0.1324) Data: 0.0549 (0.0900) Loss: 1.7283 (1.5789)\n",
            "TRAIN(165): [ 20/196] Batch: 0.0560 (0.1064) Data: 0.0504 (0.0664) Loss: 1.5552 (1.6041)\n",
            "TRAIN(165): [ 30/196] Batch: 0.0775 (0.0967) Data: 0.0467 (0.0597) Loss: 1.5492 (1.5825)\n",
            "TRAIN(165): [ 40/196] Batch: 0.0833 (0.0914) Data: 0.0499 (0.0569) Loss: 1.4966 (1.5756)\n",
            "TRAIN(165): [ 50/196] Batch: 0.0758 (0.0882) Data: 0.0562 (0.0554) Loss: 1.5895 (1.5744)\n",
            "TRAIN(165): [ 60/196] Batch: 0.0795 (0.0863) Data: 0.0508 (0.0546) Loss: 1.4968 (1.5776)\n",
            "TRAIN(165): [ 70/196] Batch: 0.0741 (0.0848) Data: 0.0548 (0.0540) Loss: 1.4720 (1.5737)\n",
            "TRAIN(165): [ 80/196] Batch: 0.0782 (0.0838) Data: 0.0536 (0.0535) Loss: 1.4866 (1.5762)\n",
            "TRAIN(165): [ 90/196] Batch: 0.0688 (0.0828) Data: 0.0580 (0.0531) Loss: 1.4769 (1.5774)\n",
            "TRAIN(165): [100/196] Batch: 0.0717 (0.0822) Data: 0.0566 (0.0529) Loss: 1.5369 (1.5740)\n",
            "TRAIN(165): [110/196] Batch: 0.0849 (0.0817) Data: 0.0494 (0.0526) Loss: 1.4225 (1.5704)\n",
            "TRAIN(165): [120/196] Batch: 0.0762 (0.0812) Data: 0.0616 (0.0524) Loss: 1.3848 (1.5714)\n",
            "TRAIN(165): [130/196] Batch: 0.0851 (0.0809) Data: 0.0511 (0.0521) Loss: 1.6702 (1.5732)\n",
            "TRAIN(165): [140/196] Batch: 0.0729 (0.0805) Data: 0.0567 (0.0520) Loss: 1.4402 (1.5729)\n",
            "TRAIN(165): [150/196] Batch: 0.0689 (0.0802) Data: 0.0567 (0.0519) Loss: 1.6101 (1.5717)\n",
            "TRAIN(165): [160/196] Batch: 0.0683 (0.0800) Data: 0.0623 (0.0519) Loss: 1.4978 (1.5711)\n",
            "TRAIN(165): [170/196] Batch: 0.0757 (0.0798) Data: 0.0574 (0.0517) Loss: 1.4007 (1.5691)\n",
            "TRAIN(165): [180/196] Batch: 0.0676 (0.0797) Data: 0.0531 (0.0515) Loss: 1.4012 (1.5711)\n",
            "TRAIN(165): [190/196] Batch: 0.0754 (0.0795) Data: 0.0596 (0.0514) Loss: 1.4774 (1.5704)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(165)         0:00:15         0:00:10         0:00:05          1.5694\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(166): [ 10/196] Batch: 0.0876 (0.1243) Data: 0.0430 (0.0797) Loss: 1.5766 (1.5058)\n",
            "TRAIN(166): [ 20/196] Batch: 0.0695 (0.0997) Data: 0.0567 (0.0660) Loss: 1.6217 (1.5256)\n",
            "TRAIN(166): [ 30/196] Batch: 0.0729 (0.0917) Data: 0.0619 (0.0611) Loss: 1.7378 (1.5400)\n",
            "TRAIN(166): [ 40/196] Batch: 0.0826 (0.0880) Data: 0.0548 (0.0586) Loss: 1.6778 (1.5448)\n",
            "TRAIN(166): [ 50/196] Batch: 0.0717 (0.0856) Data: 0.0561 (0.0569) Loss: 1.5796 (1.5651)\n",
            "TRAIN(166): [ 60/196] Batch: 0.0683 (0.0840) Data: 0.0564 (0.0558) Loss: 1.5273 (1.5668)\n",
            "TRAIN(166): [ 70/196] Batch: 0.0712 (0.0829) Data: 0.0618 (0.0551) Loss: 1.7850 (1.5706)\n",
            "TRAIN(166): [ 80/196] Batch: 0.0830 (0.0821) Data: 0.0554 (0.0547) Loss: 1.4021 (1.5689)\n",
            "TRAIN(166): [ 90/196] Batch: 0.0811 (0.0815) Data: 0.0503 (0.0539) Loss: 1.4871 (1.5687)\n",
            "TRAIN(166): [100/196] Batch: 0.0746 (0.0810) Data: 0.0499 (0.0534) Loss: 1.6958 (1.5732)\n",
            "TRAIN(166): [110/196] Batch: 0.0712 (0.0805) Data: 0.0606 (0.0533) Loss: 1.6239 (1.5761)\n",
            "TRAIN(166): [120/196] Batch: 0.0793 (0.0802) Data: 0.0505 (0.0531) Loss: 1.6307 (1.5764)\n",
            "TRAIN(166): [130/196] Batch: 0.0745 (0.0799) Data: 0.0567 (0.0529) Loss: 1.5107 (1.5749)\n",
            "TRAIN(166): [140/196] Batch: 0.0768 (0.0797) Data: 0.0502 (0.0525) Loss: 1.5560 (1.5707)\n",
            "TRAIN(166): [150/196] Batch: 0.0769 (0.0795) Data: 0.0490 (0.0519) Loss: 1.4820 (1.5700)\n",
            "TRAIN(166): [160/196] Batch: 0.0781 (0.0794) Data: 0.0464 (0.0515) Loss: 1.4506 (1.5691)\n",
            "TRAIN(166): [170/196] Batch: 0.0737 (0.0793) Data: 0.0507 (0.0510) Loss: 1.6013 (1.5664)\n",
            "TRAIN(166): [180/196] Batch: 0.0728 (0.0791) Data: 0.0524 (0.0508) Loss: 1.5617 (1.5673)\n",
            "TRAIN(166): [190/196] Batch: 0.0761 (0.0790) Data: 0.0608 (0.0506) Loss: 1.4464 (1.5657)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(166)         0:00:15         0:00:09         0:00:05          1.5673\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(167): [ 10/196] Batch: 0.0701 (0.1192) Data: 0.0493 (0.0832) Loss: 1.5112 (1.6010)\n",
            "TRAIN(167): [ 20/196] Batch: 0.0648 (0.0974) Data: 0.0603 (0.0674) Loss: 1.4386 (1.5755)\n",
            "TRAIN(167): [ 30/196] Batch: 0.0703 (0.0906) Data: 0.0563 (0.0614) Loss: 1.4131 (1.5807)\n",
            "TRAIN(167): [ 40/196] Batch: 0.0879 (0.0872) Data: 0.0498 (0.0583) Loss: 1.5026 (1.5764)\n",
            "TRAIN(167): [ 50/196] Batch: 0.0747 (0.0848) Data: 0.0548 (0.0562) Loss: 1.4734 (1.5745)\n",
            "TRAIN(167): [ 60/196] Batch: 0.0671 (0.0834) Data: 0.0561 (0.0549) Loss: 1.5619 (1.5730)\n",
            "TRAIN(167): [ 70/196] Batch: 0.0887 (0.0825) Data: 0.0477 (0.0541) Loss: 1.4825 (1.5781)\n",
            "TRAIN(167): [ 80/196] Batch: 0.0689 (0.0816) Data: 0.0570 (0.0532) Loss: 1.6780 (1.5702)\n",
            "TRAIN(167): [ 90/196] Batch: 0.0721 (0.0810) Data: 0.0622 (0.0528) Loss: 1.8476 (1.5736)\n",
            "TRAIN(167): [100/196] Batch: 0.0787 (0.0806) Data: 0.0526 (0.0525) Loss: 1.5555 (1.5715)\n",
            "TRAIN(167): [110/196] Batch: 0.0774 (0.0802) Data: 0.0538 (0.0524) Loss: 1.6974 (1.5731)\n",
            "TRAIN(167): [120/196] Batch: 0.0735 (0.0799) Data: 0.0521 (0.0520) Loss: 1.3212 (1.5726)\n",
            "TRAIN(167): [130/196] Batch: 0.0657 (0.0796) Data: 0.0583 (0.0516) Loss: 1.5149 (1.5724)\n",
            "TRAIN(167): [140/196] Batch: 0.0756 (0.0795) Data: 0.0571 (0.0516) Loss: 1.6006 (1.5724)\n",
            "TRAIN(167): [150/196] Batch: 0.0916 (0.0795) Data: 0.0388 (0.0510) Loss: 1.6169 (1.5716)\n",
            "TRAIN(167): [160/196] Batch: 0.0778 (0.0793) Data: 0.0496 (0.0507) Loss: 1.3263 (1.5683)\n",
            "TRAIN(167): [170/196] Batch: 0.0819 (0.0791) Data: 0.0489 (0.0505) Loss: 1.5326 (1.5661)\n",
            "TRAIN(167): [180/196] Batch: 0.0795 (0.0790) Data: 0.0519 (0.0503) Loss: 1.6320 (1.5668)\n",
            "TRAIN(167): [190/196] Batch: 0.0767 (0.0788) Data: 0.0616 (0.0505) Loss: 1.6700 (1.5652)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(167)         0:00:15         0:00:09         0:00:05          1.5643\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(168): [ 10/196] Batch: 0.0869 (0.1208) Data: 0.0481 (0.0869) Loss: 1.4232 (1.5436)\n",
            "TRAIN(168): [ 20/196] Batch: 0.0793 (0.0983) Data: 0.0556 (0.0693) Loss: 1.5501 (1.5605)\n",
            "TRAIN(168): [ 30/196] Batch: 0.0694 (0.0910) Data: 0.0582 (0.0639) Loss: 1.4692 (1.5488)\n",
            "TRAIN(168): [ 40/196] Batch: 0.0775 (0.0872) Data: 0.0622 (0.0609) Loss: 1.6371 (1.5542)\n",
            "TRAIN(168): [ 50/196] Batch: 0.0715 (0.0851) Data: 0.0575 (0.0588) Loss: 1.5031 (1.5645)\n",
            "TRAIN(168): [ 60/196] Batch: 0.0692 (0.0837) Data: 0.0554 (0.0573) Loss: 1.5924 (1.5660)\n",
            "TRAIN(168): [ 70/196] Batch: 0.0743 (0.0826) Data: 0.0615 (0.0564) Loss: 1.6126 (1.5645)\n",
            "TRAIN(168): [ 80/196] Batch: 0.0713 (0.0819) Data: 0.0549 (0.0554) Loss: 1.4714 (1.5645)\n",
            "TRAIN(168): [ 90/196] Batch: 0.0796 (0.0813) Data: 0.0541 (0.0546) Loss: 1.5503 (1.5694)\n",
            "TRAIN(168): [100/196] Batch: 0.0760 (0.0808) Data: 0.0504 (0.0536) Loss: 1.4706 (1.5729)\n",
            "TRAIN(168): [110/196] Batch: 0.0762 (0.0805) Data: 0.0497 (0.0527) Loss: 1.5768 (1.5747)\n",
            "TRAIN(168): [120/196] Batch: 0.0756 (0.0801) Data: 0.0581 (0.0522) Loss: 1.6758 (1.5734)\n",
            "TRAIN(168): [130/196] Batch: 0.0909 (0.0799) Data: 0.0479 (0.0521) Loss: 1.5550 (1.5725)\n",
            "TRAIN(168): [140/196] Batch: 0.0807 (0.0798) Data: 0.0479 (0.0516) Loss: 1.5926 (1.5736)\n",
            "TRAIN(168): [150/196] Batch: 0.0734 (0.0795) Data: 0.0524 (0.0513) Loss: 1.5120 (1.5724)\n",
            "TRAIN(168): [160/196] Batch: 0.0782 (0.0793) Data: 0.0550 (0.0512) Loss: 1.3938 (1.5719)\n",
            "TRAIN(168): [170/196] Batch: 0.0840 (0.0792) Data: 0.0536 (0.0511) Loss: 1.6368 (1.5707)\n",
            "TRAIN(168): [180/196] Batch: 0.0626 (0.0790) Data: 0.0629 (0.0511) Loss: 1.6131 (1.5682)\n",
            "TRAIN(168): [190/196] Batch: 0.0766 (0.0788) Data: 0.0618 (0.0511) Loss: 1.5738 (1.5685)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(168)         0:00:15         0:00:10         0:00:05          1.5695\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(169): [ 10/196] Batch: 0.0698 (0.1200) Data: 0.0554 (0.0861) Loss: 1.6084 (1.5311)\n",
            "TRAIN(169): [ 20/196] Batch: 0.0663 (0.0984) Data: 0.0564 (0.0685) Loss: 1.6765 (1.5434)\n",
            "TRAIN(169): [ 30/196] Batch: 0.0706 (0.0910) Data: 0.0619 (0.0630) Loss: 1.6138 (1.5552)\n",
            "TRAIN(169): [ 40/196] Batch: 0.0728 (0.0874) Data: 0.0612 (0.0602) Loss: 1.4452 (1.5593)\n",
            "TRAIN(169): [ 50/196] Batch: 0.0760 (0.0851) Data: 0.0650 (0.0586) Loss: 1.7389 (1.5649)\n",
            "TRAIN(169): [ 60/196] Batch: 0.0713 (0.0838) Data: 0.0550 (0.0570) Loss: 1.6975 (1.5642)\n",
            "TRAIN(169): [ 70/196] Batch: 0.0731 (0.0828) Data: 0.0559 (0.0560) Loss: 1.5384 (1.5673)\n",
            "TRAIN(169): [ 80/196] Batch: 0.0876 (0.0822) Data: 0.0403 (0.0549) Loss: 1.6176 (1.5658)\n",
            "TRAIN(169): [ 90/196] Batch: 0.0762 (0.0814) Data: 0.0579 (0.0539) Loss: 1.5966 (1.5675)\n",
            "TRAIN(169): [100/196] Batch: 0.0900 (0.0811) Data: 0.0457 (0.0533) Loss: 1.5775 (1.5656)\n",
            "TRAIN(169): [110/196] Batch: 0.0759 (0.0806) Data: 0.0569 (0.0527) Loss: 1.6394 (1.5669)\n",
            "TRAIN(169): [120/196] Batch: 0.0786 (0.0804) Data: 0.0480 (0.0523) Loss: 1.6561 (1.5698)\n",
            "TRAIN(169): [130/196] Batch: 0.0714 (0.0800) Data: 0.0624 (0.0524) Loss: 1.5676 (1.5683)\n",
            "TRAIN(169): [140/196] Batch: 0.0805 (0.0798) Data: 0.0503 (0.0520) Loss: 1.4169 (1.5666)\n",
            "TRAIN(169): [150/196] Batch: 0.0757 (0.0796) Data: 0.0519 (0.0518) Loss: 1.5690 (1.5662)\n",
            "TRAIN(169): [160/196] Batch: 0.0756 (0.0793) Data: 0.0612 (0.0518) Loss: 1.6102 (1.5678)\n",
            "TRAIN(169): [170/196] Batch: 0.0767 (0.0792) Data: 0.0559 (0.0518) Loss: 1.5273 (1.5666)\n",
            "TRAIN(169): [180/196] Batch: 0.0752 (0.0790) Data: 0.0615 (0.0518) Loss: 1.5734 (1.5675)\n",
            "TRAIN(169): [190/196] Batch: 0.0755 (0.0788) Data: 0.0635 (0.0518) Loss: 1.5214 (1.5650)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(169)         0:00:15         0:00:10         0:00:05          1.5656\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(170): [ 10/196] Batch: 0.0764 (0.1168) Data: 0.0527 (0.0813) Loss: 1.5057 (1.5609)\n",
            "TRAIN(170): [ 20/196] Batch: 0.0746 (0.0967) Data: 0.0561 (0.0662) Loss: 1.6167 (1.5629)\n",
            "TRAIN(170): [ 30/196] Batch: 0.0662 (0.0899) Data: 0.0618 (0.0611) Loss: 1.5856 (1.5566)\n",
            "TRAIN(170): [ 40/196] Batch: 0.0735 (0.0866) Data: 0.0592 (0.0584) Loss: 1.8295 (1.5524)\n",
            "TRAIN(170): [ 50/196] Batch: 0.0796 (0.0846) Data: 0.0512 (0.0565) Loss: 1.6097 (1.5464)\n",
            "TRAIN(170): [ 60/196] Batch: 0.0873 (0.0833) Data: 0.0544 (0.0553) Loss: 1.3542 (1.5465)\n",
            "TRAIN(170): [ 70/196] Batch: 0.0729 (0.0826) Data: 0.0503 (0.0541) Loss: 1.4149 (1.5531)\n",
            "TRAIN(170): [ 80/196] Batch: 0.0749 (0.0819) Data: 0.0556 (0.0532) Loss: 1.8113 (1.5545)\n",
            "TRAIN(170): [ 90/196] Batch: 0.0798 (0.0815) Data: 0.0481 (0.0519) Loss: 1.5201 (1.5567)\n",
            "TRAIN(170): [100/196] Batch: 0.0810 (0.0812) Data: 0.0444 (0.0514) Loss: 1.4989 (1.5548)\n",
            "TRAIN(170): [110/196] Batch: 0.0767 (0.0806) Data: 0.0606 (0.0513) Loss: 1.4601 (1.5555)\n",
            "TRAIN(170): [120/196] Batch: 0.0780 (0.0803) Data: 0.0506 (0.0512) Loss: 1.5952 (1.5566)\n",
            "TRAIN(170): [130/196] Batch: 0.0760 (0.0799) Data: 0.0625 (0.0513) Loss: 1.6830 (1.5598)\n",
            "TRAIN(170): [140/196] Batch: 0.0782 (0.0797) Data: 0.0507 (0.0510) Loss: 1.5515 (1.5594)\n",
            "TRAIN(170): [150/196] Batch: 0.0860 (0.0795) Data: 0.0510 (0.0510) Loss: 1.5577 (1.5588)\n",
            "TRAIN(170): [160/196] Batch: 0.0863 (0.0793) Data: 0.0535 (0.0511) Loss: 1.5594 (1.5569)\n",
            "TRAIN(170): [170/196] Batch: 0.0738 (0.0790) Data: 0.0624 (0.0511) Loss: 1.5706 (1.5615)\n",
            "TRAIN(170): [180/196] Batch: 0.0700 (0.0789) Data: 0.0567 (0.0510) Loss: 1.4426 (1.5597)\n",
            "TRAIN(170): [190/196] Batch: 0.0768 (0.0787) Data: 0.0624 (0.0512) Loss: 1.4999 (1.5599)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(170)         0:00:15         0:00:10         0:00:05          1.5614\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(171): [ 10/196] Batch: 0.0780 (0.1158) Data: 0.0465 (0.0819) Loss: 1.5754 (1.5917)\n",
            "TRAIN(171): [ 20/196] Batch: 0.0785 (0.0960) Data: 0.0515 (0.0671) Loss: 1.6521 (1.5982)\n",
            "TRAIN(171): [ 30/196] Batch: 0.0893 (0.0897) Data: 0.0465 (0.0613) Loss: 1.5420 (1.5948)\n",
            "TRAIN(171): [ 40/196] Batch: 0.0846 (0.0864) Data: 0.0453 (0.0575) Loss: 1.5451 (1.5869)\n",
            "TRAIN(171): [ 50/196] Batch: 0.0798 (0.0842) Data: 0.0542 (0.0550) Loss: 1.5611 (1.5823)\n",
            "TRAIN(171): [ 60/196] Batch: 0.0722 (0.0835) Data: 0.0463 (0.0530) Loss: 1.5610 (1.5748)\n",
            "TRAIN(171): [ 70/196] Batch: 0.0756 (0.0826) Data: 0.0525 (0.0520) Loss: 1.5603 (1.5798)\n",
            "TRAIN(171): [ 80/196] Batch: 0.0825 (0.0820) Data: 0.0438 (0.0516) Loss: 1.6158 (1.5752)\n",
            "TRAIN(171): [ 90/196] Batch: 0.0699 (0.0813) Data: 0.0632 (0.0513) Loss: 1.5988 (1.5761)\n",
            "TRAIN(171): [100/196] Batch: 0.0687 (0.0808) Data: 0.0578 (0.0511) Loss: 1.5724 (1.5723)\n",
            "TRAIN(171): [110/196] Batch: 0.0887 (0.0805) Data: 0.0473 (0.0509) Loss: 1.5670 (1.5697)\n",
            "TRAIN(171): [120/196] Batch: 0.0711 (0.0801) Data: 0.0561 (0.0508) Loss: 1.6330 (1.5711)\n",
            "TRAIN(171): [130/196] Batch: 0.0732 (0.0797) Data: 0.0612 (0.0508) Loss: 1.5965 (1.5673)\n",
            "TRAIN(171): [140/196] Batch: 0.0862 (0.0795) Data: 0.0533 (0.0508) Loss: 1.6187 (1.5666)\n",
            "TRAIN(171): [150/196] Batch: 0.0805 (0.0794) Data: 0.0491 (0.0507) Loss: 1.5071 (1.5653)\n",
            "TRAIN(171): [160/196] Batch: 0.0833 (0.0792) Data: 0.0481 (0.0506) Loss: 1.7055 (1.5692)\n",
            "TRAIN(171): [170/196] Batch: 0.0724 (0.0790) Data: 0.0512 (0.0504) Loss: 1.7734 (1.5734)\n",
            "TRAIN(171): [180/196] Batch: 0.0679 (0.0788) Data: 0.0600 (0.0504) Loss: 1.6074 (1.5724)\n",
            "TRAIN(171): [190/196] Batch: 0.0756 (0.0786) Data: 0.0624 (0.0505) Loss: 1.4505 (1.5731)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(171)         0:00:15         0:00:09         0:00:05          1.5733\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(172): [ 10/196] Batch: 0.0748 (0.1184) Data: 0.0494 (0.0848) Loss: 1.5481 (1.5593)\n",
            "TRAIN(172): [ 20/196] Batch: 0.0788 (0.0977) Data: 0.0478 (0.0662) Loss: 1.6032 (1.5659)\n",
            "TRAIN(172): [ 30/196] Batch: 0.0854 (0.0909) Data: 0.0475 (0.0603) Loss: 1.5795 (1.5729)\n",
            "TRAIN(172): [ 40/196] Batch: 0.0935 (0.0877) Data: 0.0400 (0.0565) Loss: 1.7540 (1.5760)\n",
            "TRAIN(172): [ 50/196] Batch: 0.0764 (0.0854) Data: 0.0516 (0.0544) Loss: 1.7084 (1.5738)\n",
            "TRAIN(172): [ 60/196] Batch: 0.0781 (0.0842) Data: 0.0508 (0.0528) Loss: 1.4611 (1.5748)\n",
            "TRAIN(172): [ 70/196] Batch: 0.0701 (0.0831) Data: 0.0538 (0.0523) Loss: 1.4040 (1.5693)\n",
            "TRAIN(172): [ 80/196] Batch: 0.0810 (0.0823) Data: 0.0517 (0.0518) Loss: 1.4023 (1.5664)\n",
            "TRAIN(172): [ 90/196] Batch: 0.0712 (0.0815) Data: 0.0586 (0.0515) Loss: 1.4524 (1.5621)\n",
            "TRAIN(172): [100/196] Batch: 0.0713 (0.0810) Data: 0.0535 (0.0513) Loss: 1.3479 (1.5629)\n",
            "TRAIN(172): [110/196] Batch: 0.0799 (0.0806) Data: 0.0531 (0.0512) Loss: 1.5721 (1.5654)\n",
            "TRAIN(172): [120/196] Batch: 0.0695 (0.0802) Data: 0.0576 (0.0511) Loss: 1.6151 (1.5660)\n",
            "TRAIN(172): [130/196] Batch: 0.0807 (0.0799) Data: 0.0520 (0.0510) Loss: 1.4251 (1.5600)\n",
            "TRAIN(172): [140/196] Batch: 0.0742 (0.0796) Data: 0.0566 (0.0509) Loss: 1.6205 (1.5628)\n",
            "TRAIN(172): [150/196] Batch: 0.0682 (0.0794) Data: 0.0573 (0.0508) Loss: 1.4195 (1.5640)\n",
            "TRAIN(172): [160/196] Batch: 0.0867 (0.0793) Data: 0.0528 (0.0508) Loss: 1.7130 (1.5627)\n",
            "TRAIN(172): [170/196] Batch: 0.0823 (0.0791) Data: 0.0555 (0.0508) Loss: 1.6188 (1.5618)\n",
            "TRAIN(172): [180/196] Batch: 0.0691 (0.0789) Data: 0.0582 (0.0506) Loss: 1.3837 (1.5625)\n",
            "TRAIN(172): [190/196] Batch: 0.0830 (0.0788) Data: 0.0561 (0.0506) Loss: 1.5105 (1.5637)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(172)         0:00:15         0:00:09         0:00:05          1.5638\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(173): [ 10/196] Batch: 0.0777 (0.1379) Data: 0.0394 (0.0949) Loss: 1.5830 (1.5352)\n",
            "TRAIN(173): [ 20/196] Batch: 0.0913 (0.1091) Data: 0.0373 (0.0699) Loss: 1.5204 (1.5478)\n",
            "TRAIN(173): [ 30/196] Batch: 0.0756 (0.0978) Data: 0.0518 (0.0620) Loss: 1.7371 (1.5656)\n",
            "TRAIN(173): [ 40/196] Batch: 0.0769 (0.0923) Data: 0.0569 (0.0589) Loss: 1.5585 (1.5676)\n",
            "TRAIN(173): [ 50/196] Batch: 0.0632 (0.0889) Data: 0.0606 (0.0571) Loss: 1.5600 (1.5698)\n",
            "TRAIN(173): [ 60/196] Batch: 0.0710 (0.0868) Data: 0.0619 (0.0558) Loss: 1.5851 (1.5635)\n",
            "TRAIN(173): [ 70/196] Batch: 0.0691 (0.0854) Data: 0.0569 (0.0550) Loss: 1.5865 (1.5662)\n",
            "TRAIN(173): [ 80/196] Batch: 0.0715 (0.0842) Data: 0.0595 (0.0545) Loss: 1.4983 (1.5652)\n",
            "TRAIN(173): [ 90/196] Batch: 0.0772 (0.0834) Data: 0.0538 (0.0538) Loss: 1.6046 (1.5691)\n",
            "TRAIN(173): [100/196] Batch: 0.0728 (0.0825) Data: 0.0629 (0.0538) Loss: 1.5021 (1.5639)\n",
            "TRAIN(173): [110/196] Batch: 0.0796 (0.0821) Data: 0.0529 (0.0534) Loss: 1.6432 (1.5638)\n",
            "TRAIN(173): [120/196] Batch: 0.0683 (0.0815) Data: 0.0597 (0.0531) Loss: 1.7598 (1.5695)\n",
            "TRAIN(173): [130/196] Batch: 0.0856 (0.0812) Data: 0.0526 (0.0529) Loss: 1.3490 (1.5646)\n",
            "TRAIN(173): [140/196] Batch: 0.0791 (0.0808) Data: 0.0518 (0.0529) Loss: 1.4138 (1.5634)\n",
            "TRAIN(173): [150/196] Batch: 0.0792 (0.0805) Data: 0.0530 (0.0528) Loss: 1.5994 (1.5613)\n",
            "TRAIN(173): [160/196] Batch: 0.0764 (0.0803) Data: 0.0500 (0.0523) Loss: 1.3953 (1.5618)\n",
            "TRAIN(173): [170/196] Batch: 0.0620 (0.0800) Data: 0.0573 (0.0518) Loss: 1.6417 (1.5637)\n",
            "TRAIN(173): [180/196] Batch: 0.0671 (0.0799) Data: 0.0571 (0.0518) Loss: 1.3526 (1.5605)\n",
            "TRAIN(173): [190/196] Batch: 0.0768 (0.0797) Data: 0.0626 (0.0516) Loss: 1.4540 (1.5606)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(173)         0:00:15         0:00:10         0:00:05          1.5613\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(174): [ 10/196] Batch: 0.0701 (0.1190) Data: 0.0546 (0.0860) Loss: 1.6883 (1.5633)\n",
            "TRAIN(174): [ 20/196] Batch: 0.0761 (0.0982) Data: 0.0500 (0.0686) Loss: 1.6924 (1.5694)\n",
            "TRAIN(174): [ 30/196] Batch: 0.0682 (0.0906) Data: 0.0619 (0.0628) Loss: 1.5468 (1.5427)\n",
            "TRAIN(174): [ 40/196] Batch: 0.0664 (0.0870) Data: 0.0610 (0.0601) Loss: 1.5244 (1.5424)\n",
            "TRAIN(174): [ 50/196] Batch: 0.0668 (0.0849) Data: 0.0607 (0.0579) Loss: 1.6106 (1.5504)\n",
            "TRAIN(174): [ 60/196] Batch: 0.0791 (0.0836) Data: 0.0506 (0.0564) Loss: 1.6108 (1.5540)\n",
            "TRAIN(174): [ 70/196] Batch: 0.0800 (0.0826) Data: 0.0506 (0.0554) Loss: 1.5608 (1.5564)\n",
            "TRAIN(174): [ 80/196] Batch: 0.0801 (0.0818) Data: 0.0521 (0.0548) Loss: 1.6077 (1.5560)\n",
            "TRAIN(174): [ 90/196] Batch: 0.0734 (0.0811) Data: 0.0590 (0.0546) Loss: 1.5459 (1.5560)\n",
            "TRAIN(174): [100/196] Batch: 0.0753 (0.0806) Data: 0.0575 (0.0543) Loss: 1.6217 (1.5619)\n",
            "TRAIN(174): [110/196] Batch: 0.0737 (0.0802) Data: 0.0617 (0.0540) Loss: 1.5076 (1.5618)\n",
            "TRAIN(174): [120/196] Batch: 0.0715 (0.0799) Data: 0.0561 (0.0537) Loss: 1.5940 (1.5646)\n",
            "TRAIN(174): [130/196] Batch: 0.0769 (0.0797) Data: 0.0532 (0.0535) Loss: 1.6310 (1.5610)\n",
            "TRAIN(174): [140/196] Batch: 0.0784 (0.0795) Data: 0.0482 (0.0530) Loss: 1.4820 (1.5597)\n",
            "TRAIN(174): [150/196] Batch: 0.0749 (0.0793) Data: 0.0528 (0.0525) Loss: 1.8439 (1.5665)\n",
            "TRAIN(174): [160/196] Batch: 0.0760 (0.0791) Data: 0.0504 (0.0522) Loss: 1.5433 (1.5652)\n",
            "TRAIN(174): [170/196] Batch: 0.0769 (0.0790) Data: 0.0498 (0.0518) Loss: 1.8133 (1.5684)\n",
            "TRAIN(174): [180/196] Batch: 0.0750 (0.0789) Data: 0.0516 (0.0515) Loss: 1.5725 (1.5672)\n",
            "TRAIN(174): [190/196] Batch: 0.0762 (0.0787) Data: 0.0625 (0.0516) Loss: 1.6166 (1.5682)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(174)         0:00:15         0:00:10         0:00:05          1.5679\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(175): [ 10/196] Batch: 0.0635 (0.1189) Data: 0.0620 (0.0873) Loss: 1.5872 (1.5807)\n",
            "TRAIN(175): [ 20/196] Batch: 0.0683 (0.0980) Data: 0.0615 (0.0714) Loss: 1.5276 (1.5687)\n",
            "TRAIN(175): [ 30/196] Batch: 0.0758 (0.0910) Data: 0.0554 (0.0651) Loss: 1.5237 (1.5465)\n",
            "TRAIN(175): [ 40/196] Batch: 0.0767 (0.0875) Data: 0.0491 (0.0605) Loss: 1.4325 (1.5403)\n",
            "TRAIN(175): [ 50/196] Batch: 0.0833 (0.0850) Data: 0.0563 (0.0587) Loss: 1.4102 (1.5507)\n",
            "TRAIN(175): [ 60/196] Batch: 0.0701 (0.0835) Data: 0.0606 (0.0577) Loss: 1.5440 (1.5549)\n",
            "TRAIN(175): [ 70/196] Batch: 0.0739 (0.0826) Data: 0.0550 (0.0566) Loss: 1.5780 (1.5618)\n",
            "TRAIN(175): [ 80/196] Batch: 0.0731 (0.0818) Data: 0.0560 (0.0556) Loss: 1.4875 (1.5565)\n",
            "TRAIN(175): [ 90/196] Batch: 0.0743 (0.0812) Data: 0.0549 (0.0548) Loss: 1.6207 (1.5566)\n",
            "TRAIN(175): [100/196] Batch: 0.0650 (0.0807) Data: 0.0605 (0.0543) Loss: 1.4893 (1.5525)\n",
            "TRAIN(175): [110/196] Batch: 0.0753 (0.0803) Data: 0.0659 (0.0540) Loss: 1.4223 (1.5510)\n",
            "TRAIN(175): [120/196] Batch: 0.0887 (0.0803) Data: 0.0421 (0.0531) Loss: 1.4809 (1.5538)\n",
            "TRAIN(175): [130/196] Batch: 0.0759 (0.0799) Data: 0.0576 (0.0527) Loss: 1.5061 (1.5536)\n",
            "TRAIN(175): [140/196] Batch: 0.0761 (0.0798) Data: 0.0541 (0.0524) Loss: 1.6376 (1.5517)\n",
            "TRAIN(175): [150/196] Batch: 0.0761 (0.0797) Data: 0.0538 (0.0520) Loss: 1.6471 (1.5530)\n",
            "TRAIN(175): [160/196] Batch: 0.0746 (0.0795) Data: 0.0557 (0.0519) Loss: 1.7088 (1.5525)\n",
            "TRAIN(175): [170/196] Batch: 0.0716 (0.0793) Data: 0.0628 (0.0518) Loss: 1.5299 (1.5527)\n",
            "TRAIN(175): [180/196] Batch: 0.0776 (0.0791) Data: 0.0612 (0.0518) Loss: 1.6085 (1.5534)\n",
            "TRAIN(175): [190/196] Batch: 0.0760 (0.0790) Data: 0.0611 (0.0517) Loss: 1.5072 (1.5529)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(175)         0:00:15         0:00:10         0:00:05          1.5520\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(176): [ 10/196] Batch: 0.0716 (0.1175) Data: 0.0516 (0.0797) Loss: 1.5117 (1.5822)\n",
            "TRAIN(176): [ 20/196] Batch: 0.0807 (0.0973) Data: 0.0504 (0.0663) Loss: 1.4604 (1.5994)\n",
            "TRAIN(176): [ 30/196] Batch: 0.0669 (0.0902) Data: 0.0579 (0.0611) Loss: 1.5388 (1.5829)\n",
            "TRAIN(176): [ 40/196] Batch: 0.0721 (0.0868) Data: 0.0562 (0.0580) Loss: 1.5040 (1.5657)\n",
            "TRAIN(176): [ 50/196] Batch: 0.0870 (0.0847) Data: 0.0510 (0.0562) Loss: 1.5705 (1.5617)\n",
            "TRAIN(176): [ 60/196] Batch: 0.0682 (0.0832) Data: 0.0563 (0.0549) Loss: 1.3719 (1.5614)\n",
            "TRAIN(176): [ 70/196] Batch: 0.0750 (0.0823) Data: 0.0521 (0.0541) Loss: 1.4609 (1.5608)\n",
            "TRAIN(176): [ 80/196] Batch: 0.0774 (0.0815) Data: 0.0618 (0.0539) Loss: 1.6007 (1.5647)\n",
            "TRAIN(176): [ 90/196] Batch: 0.0746 (0.0810) Data: 0.0527 (0.0531) Loss: 1.5973 (1.5565)\n",
            "TRAIN(176): [100/196] Batch: 0.0726 (0.0806) Data: 0.0553 (0.0523) Loss: 1.5879 (1.5563)\n",
            "TRAIN(176): [110/196] Batch: 0.0776 (0.0802) Data: 0.0514 (0.0518) Loss: 1.6295 (1.5581)\n",
            "TRAIN(176): [120/196] Batch: 0.0728 (0.0799) Data: 0.0509 (0.0512) Loss: 1.6845 (1.5591)\n",
            "TRAIN(176): [130/196] Batch: 0.0596 (0.0796) Data: 0.0569 (0.0506) Loss: 1.6520 (1.5576)\n",
            "TRAIN(176): [140/196] Batch: 0.0830 (0.0794) Data: 0.0505 (0.0503) Loss: 1.6335 (1.5591)\n",
            "TRAIN(176): [150/196] Batch: 0.0869 (0.0792) Data: 0.0508 (0.0503) Loss: 1.4241 (1.5582)\n",
            "TRAIN(176): [160/196] Batch: 0.0736 (0.0790) Data: 0.0608 (0.0505) Loss: 1.4669 (1.5575)\n",
            "TRAIN(176): [170/196] Batch: 0.0783 (0.0789) Data: 0.0536 (0.0504) Loss: 1.5451 (1.5572)\n",
            "TRAIN(176): [180/196] Batch: 0.0825 (0.0787) Data: 0.0561 (0.0505) Loss: 1.5678 (1.5595)\n",
            "TRAIN(176): [190/196] Batch: 0.0786 (0.0785) Data: 0.0609 (0.0506) Loss: 1.5451 (1.5617)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(176)         0:00:15         0:00:09         0:00:05          1.5626\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(177): [ 10/196] Batch: 0.0752 (0.1186) Data: 0.0496 (0.0823) Loss: 1.5870 (1.5455)\n",
            "TRAIN(177): [ 20/196] Batch: 0.0693 (0.0975) Data: 0.0576 (0.0682) Loss: 1.4307 (1.5600)\n",
            "TRAIN(177): [ 30/196] Batch: 0.0676 (0.0904) Data: 0.0599 (0.0626) Loss: 1.5980 (1.5581)\n",
            "TRAIN(177): [ 40/196] Batch: 0.0791 (0.0870) Data: 0.0564 (0.0589) Loss: 1.6000 (1.5548)\n",
            "TRAIN(177): [ 50/196] Batch: 0.0705 (0.0848) Data: 0.0537 (0.0566) Loss: 1.6283 (1.5612)\n",
            "TRAIN(177): [ 60/196] Batch: 0.0634 (0.0833) Data: 0.0611 (0.0555) Loss: 1.3980 (1.5596)\n",
            "TRAIN(177): [ 70/196] Batch: 0.0772 (0.0824) Data: 0.0493 (0.0545) Loss: 1.6312 (1.5526)\n",
            "TRAIN(177): [ 80/196] Batch: 0.0823 (0.0817) Data: 0.0461 (0.0533) Loss: 1.6722 (1.5573)\n",
            "TRAIN(177): [ 90/196] Batch: 0.0794 (0.0812) Data: 0.0536 (0.0527) Loss: 1.5383 (1.5548)\n",
            "TRAIN(177): [100/196] Batch: 0.0733 (0.0807) Data: 0.0519 (0.0523) Loss: 1.5884 (1.5527)\n",
            "TRAIN(177): [110/196] Batch: 0.0774 (0.0804) Data: 0.0518 (0.0516) Loss: 1.6590 (1.5571)\n",
            "TRAIN(177): [120/196] Batch: 0.0814 (0.0801) Data: 0.0502 (0.0512) Loss: 1.3138 (1.5565)\n",
            "TRAIN(177): [130/196] Batch: 0.0649 (0.0797) Data: 0.0615 (0.0511) Loss: 1.4722 (1.5565)\n",
            "TRAIN(177): [140/196] Batch: 0.0790 (0.0795) Data: 0.0517 (0.0509) Loss: 1.4029 (1.5578)\n",
            "TRAIN(177): [150/196] Batch: 0.0732 (0.0792) Data: 0.0565 (0.0509) Loss: 1.7489 (1.5599)\n",
            "TRAIN(177): [160/196] Batch: 0.0754 (0.0791) Data: 0.0532 (0.0507) Loss: 1.4902 (1.5592)\n",
            "TRAIN(177): [170/196] Batch: 0.0758 (0.0789) Data: 0.0519 (0.0506) Loss: 1.5920 (1.5570)\n",
            "TRAIN(177): [180/196] Batch: 0.0819 (0.0787) Data: 0.0552 (0.0506) Loss: 1.5923 (1.5577)\n",
            "TRAIN(177): [190/196] Batch: 0.0765 (0.0786) Data: 0.0621 (0.0508) Loss: 1.5940 (1.5587)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(177)         0:00:15         0:00:09         0:00:05          1.5592\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(178): [ 10/196] Batch: 0.0690 (0.1147) Data: 0.0562 (0.0811) Loss: 1.7660 (1.5581)\n",
            "TRAIN(178): [ 20/196] Batch: 0.0730 (0.0957) Data: 0.0575 (0.0665) Loss: 1.5804 (1.5339)\n",
            "TRAIN(178): [ 30/196] Batch: 0.0762 (0.0895) Data: 0.0509 (0.0587) Loss: 1.4487 (1.5285)\n",
            "TRAIN(178): [ 40/196] Batch: 0.0913 (0.0864) Data: 0.0531 (0.0563) Loss: 1.4527 (1.5473)\n",
            "TRAIN(178): [ 50/196] Batch: 0.1036 (0.0860) Data: 0.0209 (0.0528) Loss: 1.3024 (1.5347)\n",
            "TRAIN(178): [ 60/196] Batch: 0.0865 (0.0863) Data: 0.0447 (0.0506) Loss: 1.5750 (1.5370)\n",
            "TRAIN(178): [ 70/196] Batch: 0.0850 (0.0854) Data: 0.0409 (0.0499) Loss: 1.5502 (1.5367)\n",
            "TRAIN(178): [ 80/196] Batch: 0.1003 (0.0845) Data: 0.0405 (0.0495) Loss: 1.6425 (1.5412)\n",
            "TRAIN(178): [ 90/196] Batch: 0.0844 (0.0836) Data: 0.0459 (0.0488) Loss: 1.4805 (1.5435)\n",
            "TRAIN(178): [100/196] Batch: 0.0750 (0.0829) Data: 0.0610 (0.0491) Loss: 1.5652 (1.5441)\n",
            "TRAIN(178): [110/196] Batch: 0.0766 (0.0823) Data: 0.0557 (0.0492) Loss: 1.5074 (1.5461)\n",
            "TRAIN(178): [120/196] Batch: 0.0770 (0.0818) Data: 0.0557 (0.0493) Loss: 1.4878 (1.5399)\n",
            "TRAIN(178): [130/196] Batch: 0.0685 (0.0814) Data: 0.0577 (0.0493) Loss: 1.5195 (1.5429)\n",
            "TRAIN(178): [140/196] Batch: 0.0692 (0.0810) Data: 0.0560 (0.0493) Loss: 1.5233 (1.5464)\n",
            "TRAIN(178): [150/196] Batch: 0.0730 (0.0807) Data: 0.0581 (0.0493) Loss: 1.5871 (1.5468)\n",
            "TRAIN(178): [160/196] Batch: 0.0750 (0.0804) Data: 0.0524 (0.0493) Loss: 1.5040 (1.5443)\n",
            "TRAIN(178): [170/196] Batch: 0.0809 (0.0802) Data: 0.0504 (0.0492) Loss: 1.5442 (1.5438)\n",
            "TRAIN(178): [180/196] Batch: 0.0677 (0.0799) Data: 0.0586 (0.0492) Loss: 1.7157 (1.5449)\n",
            "TRAIN(178): [190/196] Batch: 0.0750 (0.0797) Data: 0.0612 (0.0492) Loss: 1.6422 (1.5463)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(178)         0:00:15         0:00:09         0:00:05          1.5479\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(179): [ 10/196] Batch: 0.0747 (0.1152) Data: 0.0485 (0.0801) Loss: 1.5926 (1.5284)\n",
            "TRAIN(179): [ 20/196] Batch: 0.0813 (0.0961) Data: 0.0458 (0.0651) Loss: 1.5819 (1.5634)\n",
            "TRAIN(179): [ 30/196] Batch: 0.0795 (0.0894) Data: 0.0480 (0.0600) Loss: 1.4693 (1.5503)\n",
            "TRAIN(179): [ 40/196] Batch: 0.0770 (0.0860) Data: 0.0500 (0.0561) Loss: 1.5653 (1.5569)\n",
            "TRAIN(179): [ 50/196] Batch: 0.0762 (0.0841) Data: 0.0526 (0.0544) Loss: 1.5781 (1.5580)\n",
            "TRAIN(179): [ 60/196] Batch: 0.0895 (0.0830) Data: 0.0385 (0.0527) Loss: 1.5599 (1.5556)\n",
            "TRAIN(179): [ 70/196] Batch: 0.0733 (0.0821) Data: 0.0469 (0.0520) Loss: 1.7072 (1.5560)\n",
            "TRAIN(179): [ 80/196] Batch: 0.0721 (0.0812) Data: 0.0549 (0.0516) Loss: 1.4588 (1.5576)\n",
            "TRAIN(179): [ 90/196] Batch: 0.0831 (0.0806) Data: 0.0564 (0.0516) Loss: 1.5163 (1.5586)\n",
            "TRAIN(179): [100/196] Batch: 0.0803 (0.0802) Data: 0.0494 (0.0515) Loss: 1.5012 (1.5544)\n",
            "TRAIN(179): [110/196] Batch: 0.0855 (0.0799) Data: 0.0523 (0.0514) Loss: 1.6553 (1.5533)\n",
            "TRAIN(179): [120/196] Batch: 0.0785 (0.0795) Data: 0.0563 (0.0512) Loss: 1.6918 (1.5532)\n",
            "TRAIN(179): [130/196] Batch: 0.0794 (0.0793) Data: 0.0514 (0.0511) Loss: 1.6259 (1.5558)\n",
            "TRAIN(179): [140/196] Batch: 0.0761 (0.0790) Data: 0.0549 (0.0510) Loss: 1.5374 (1.5552)\n",
            "TRAIN(179): [150/196] Batch: 0.0760 (0.0788) Data: 0.0532 (0.0509) Loss: 1.5539 (1.5550)\n",
            "TRAIN(179): [160/196] Batch: 0.0832 (0.0787) Data: 0.0502 (0.0508) Loss: 1.6907 (1.5556)\n",
            "TRAIN(179): [170/196] Batch: 0.0690 (0.0785) Data: 0.0570 (0.0507) Loss: 1.4256 (1.5546)\n",
            "TRAIN(179): [180/196] Batch: 0.0749 (0.0784) Data: 0.0562 (0.0507) Loss: 1.4752 (1.5564)\n",
            "TRAIN(179): [190/196] Batch: 0.0747 (0.0782) Data: 0.0625 (0.0507) Loss: 1.5052 (1.5537)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(179)         0:00:15         0:00:09         0:00:05          1.5538\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(180): [ 10/196] Batch: 0.0715 (0.1217) Data: 0.0437 (0.0810) Loss: 1.4927 (1.5453)\n",
            "TRAIN(180): [ 20/196] Batch: 0.0752 (0.0999) Data: 0.0486 (0.0644) Loss: 1.5070 (1.5426)\n",
            "TRAIN(180): [ 30/196] Batch: 0.0703 (0.0921) Data: 0.0550 (0.0579) Loss: 1.5364 (1.5574)\n",
            "TRAIN(180): [ 40/196] Batch: 0.1059 (0.0891) Data: 0.0306 (0.0547) Loss: 1.6285 (1.5634)\n",
            "TRAIN(180): [ 50/196] Batch: 0.0819 (0.0866) Data: 0.0419 (0.0524) Loss: 1.5453 (1.5703)\n",
            "TRAIN(180): [ 60/196] Batch: 0.0775 (0.0847) Data: 0.0565 (0.0515) Loss: 1.5108 (1.5752)\n",
            "TRAIN(180): [ 70/196] Batch: 0.0733 (0.0834) Data: 0.0622 (0.0516) Loss: 1.7156 (1.5809)\n",
            "TRAIN(180): [ 80/196] Batch: 0.0743 (0.0826) Data: 0.0549 (0.0516) Loss: 1.6419 (1.5797)\n",
            "TRAIN(180): [ 90/196] Batch: 0.0832 (0.0819) Data: 0.0568 (0.0516) Loss: 1.5173 (1.5784)\n",
            "TRAIN(180): [100/196] Batch: 0.0753 (0.0813) Data: 0.0604 (0.0515) Loss: 1.4782 (1.5703)\n",
            "TRAIN(180): [110/196] Batch: 0.0724 (0.0808) Data: 0.0630 (0.0515) Loss: 1.5866 (1.5724)\n",
            "TRAIN(180): [120/196] Batch: 0.0704 (0.0805) Data: 0.0573 (0.0514) Loss: 1.4624 (1.5708)\n",
            "TRAIN(180): [130/196] Batch: 0.0688 (0.0801) Data: 0.0628 (0.0514) Loss: 1.6210 (1.5722)\n",
            "TRAIN(180): [140/196] Batch: 0.0802 (0.0799) Data: 0.0507 (0.0513) Loss: 1.7771 (1.5745)\n",
            "TRAIN(180): [150/196] Batch: 0.0732 (0.0796) Data: 0.0607 (0.0511) Loss: 1.8706 (1.5733)\n",
            "TRAIN(180): [160/196] Batch: 0.0795 (0.0794) Data: 0.0513 (0.0510) Loss: 1.3711 (1.5745)\n",
            "TRAIN(180): [170/196] Batch: 0.0728 (0.0792) Data: 0.0613 (0.0510) Loss: 1.5207 (1.5743)\n",
            "TRAIN(180): [180/196] Batch: 0.0821 (0.0791) Data: 0.0482 (0.0509) Loss: 1.5229 (1.5743)\n",
            "TRAIN(180): [190/196] Batch: 0.0800 (0.0789) Data: 0.0577 (0.0509) Loss: 1.6009 (1.5746)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(180)         0:00:15         0:00:09         0:00:05          1.5731\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(181): [ 10/196] Batch: 0.0817 (0.1297) Data: 0.0478 (0.0866) Loss: 1.5519 (1.5377)\n",
            "TRAIN(181): [ 20/196] Batch: 0.0743 (0.1058) Data: 0.0486 (0.0631) Loss: 1.4280 (1.5538)\n",
            "TRAIN(181): [ 30/196] Batch: 0.0646 (0.0959) Data: 0.0624 (0.0574) Loss: 1.5581 (1.5498)\n",
            "TRAIN(181): [ 40/196] Batch: 0.0802 (0.0912) Data: 0.0506 (0.0556) Loss: 1.4328 (1.5465)\n",
            "TRAIN(181): [ 50/196] Batch: 0.0786 (0.0883) Data: 0.0496 (0.0540) Loss: 1.4621 (1.5372)\n",
            "TRAIN(181): [ 60/196] Batch: 0.0767 (0.0861) Data: 0.0617 (0.0534) Loss: 1.7137 (1.5381)\n",
            "TRAIN(181): [ 70/196] Batch: 0.0822 (0.0847) Data: 0.0543 (0.0529) Loss: 1.7171 (1.5421)\n",
            "TRAIN(181): [ 80/196] Batch: 0.0687 (0.0836) Data: 0.0624 (0.0527) Loss: 1.3691 (1.5434)\n",
            "TRAIN(181): [ 90/196] Batch: 0.0758 (0.0828) Data: 0.0612 (0.0526) Loss: 1.4177 (1.5496)\n",
            "TRAIN(181): [100/196] Batch: 0.0682 (0.0821) Data: 0.0577 (0.0523) Loss: 1.4448 (1.5528)\n",
            "TRAIN(181): [110/196] Batch: 0.0731 (0.0816) Data: 0.0551 (0.0520) Loss: 1.2955 (1.5505)\n",
            "TRAIN(181): [120/196] Batch: 0.0828 (0.0812) Data: 0.0483 (0.0518) Loss: 1.3969 (1.5505)\n",
            "TRAIN(181): [130/196] Batch: 0.0781 (0.0809) Data: 0.0508 (0.0517) Loss: 1.4748 (1.5484)\n",
            "TRAIN(181): [140/196] Batch: 0.0701 (0.0805) Data: 0.0608 (0.0517) Loss: 1.5016 (1.5499)\n",
            "TRAIN(181): [150/196] Batch: 0.0706 (0.0802) Data: 0.0620 (0.0516) Loss: 1.6277 (1.5526)\n",
            "TRAIN(181): [160/196] Batch: 0.0804 (0.0800) Data: 0.0494 (0.0514) Loss: 1.5475 (1.5548)\n",
            "TRAIN(181): [170/196] Batch: 0.0817 (0.0798) Data: 0.0458 (0.0512) Loss: 1.5829 (1.5556)\n",
            "TRAIN(181): [180/196] Batch: 0.0881 (0.0798) Data: 0.0468 (0.0509) Loss: 1.3658 (1.5524)\n",
            "TRAIN(181): [190/196] Batch: 0.0633 (0.0795) Data: 0.0618 (0.0508) Loss: 1.6320 (1.5526)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(181)         0:00:15         0:00:09         0:00:05          1.5542\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(182): [ 10/196] Batch: 0.0642 (0.1202) Data: 0.0613 (0.0829) Loss: 1.4564 (1.5836)\n",
            "TRAIN(182): [ 20/196] Batch: 0.0721 (0.0989) Data: 0.0542 (0.0677) Loss: 1.6185 (1.5855)\n",
            "TRAIN(182): [ 30/196] Batch: 0.0771 (0.0915) Data: 0.0521 (0.0614) Loss: 1.6462 (1.5842)\n",
            "TRAIN(182): [ 40/196] Batch: 0.0797 (0.0877) Data: 0.0518 (0.0583) Loss: 1.6642 (1.5733)\n",
            "TRAIN(182): [ 50/196] Batch: 0.0779 (0.0852) Data: 0.0565 (0.0563) Loss: 1.6057 (1.5668)\n",
            "TRAIN(182): [ 60/196] Batch: 0.0759 (0.0838) Data: 0.0541 (0.0554) Loss: 1.7150 (1.5806)\n",
            "TRAIN(182): [ 70/196] Batch: 0.0739 (0.0827) Data: 0.0578 (0.0549) Loss: 1.4516 (1.5723)\n",
            "TRAIN(182): [ 80/196] Batch: 0.0716 (0.0819) Data: 0.0620 (0.0542) Loss: 1.5459 (1.5746)\n",
            "TRAIN(182): [ 90/196] Batch: 0.0692 (0.0813) Data: 0.0586 (0.0537) Loss: 1.4283 (1.5711)\n",
            "TRAIN(182): [100/196] Batch: 0.0776 (0.0809) Data: 0.0520 (0.0532) Loss: 1.4373 (1.5745)\n",
            "TRAIN(182): [110/196] Batch: 0.0639 (0.0804) Data: 0.0613 (0.0530) Loss: 1.5333 (1.5761)\n",
            "TRAIN(182): [120/196] Batch: 0.0698 (0.0801) Data: 0.0573 (0.0528) Loss: 1.4113 (1.5716)\n",
            "TRAIN(182): [130/196] Batch: 0.0786 (0.0798) Data: 0.0503 (0.0523) Loss: 1.5574 (1.5644)\n",
            "TRAIN(182): [140/196] Batch: 0.0745 (0.0796) Data: 0.0501 (0.0518) Loss: 1.4624 (1.5650)\n",
            "TRAIN(182): [150/196] Batch: 0.0842 (0.0795) Data: 0.0442 (0.0514) Loss: 1.6281 (1.5670)\n",
            "TRAIN(182): [160/196] Batch: 0.0745 (0.0793) Data: 0.0569 (0.0511) Loss: 1.7605 (1.5690)\n",
            "TRAIN(182): [170/196] Batch: 0.0775 (0.0793) Data: 0.0511 (0.0507) Loss: 1.5508 (1.5666)\n",
            "TRAIN(182): [180/196] Batch: 0.0699 (0.0792) Data: 0.0558 (0.0504) Loss: 1.5365 (1.5677)\n",
            "TRAIN(182): [190/196] Batch: 0.0780 (0.0790) Data: 0.0621 (0.0504) Loss: 1.4716 (1.5675)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(182)         0:00:15         0:00:09         0:00:05          1.5678\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(183): [ 10/196] Batch: 0.0732 (0.1186) Data: 0.0515 (0.0752) Loss: 1.6086 (1.5292)\n",
            "TRAIN(183): [ 20/196] Batch: 0.0806 (0.0978) Data: 0.0510 (0.0639) Loss: 1.5959 (1.5426)\n",
            "TRAIN(183): [ 30/196] Batch: 0.0694 (0.0907) Data: 0.0566 (0.0599) Loss: 1.5858 (1.5407)\n",
            "TRAIN(183): [ 40/196] Batch: 0.0830 (0.0871) Data: 0.0560 (0.0578) Loss: 1.6333 (1.5340)\n",
            "TRAIN(183): [ 50/196] Batch: 0.0745 (0.0849) Data: 0.0535 (0.0562) Loss: 1.6071 (1.5359)\n",
            "TRAIN(183): [ 60/196] Batch: 0.0782 (0.0835) Data: 0.0549 (0.0551) Loss: 1.4960 (1.5369)\n",
            "TRAIN(183): [ 70/196] Batch: 0.0653 (0.0824) Data: 0.0607 (0.0543) Loss: 1.6209 (1.5405)\n",
            "TRAIN(183): [ 80/196] Batch: 0.0772 (0.0817) Data: 0.0542 (0.0539) Loss: 1.4458 (1.5449)\n",
            "TRAIN(183): [ 90/196] Batch: 0.0632 (0.0810) Data: 0.0622 (0.0532) Loss: 1.5559 (1.5483)\n",
            "TRAIN(183): [100/196] Batch: 0.0833 (0.0807) Data: 0.0516 (0.0527) Loss: 1.5124 (1.5421)\n",
            "TRAIN(183): [110/196] Batch: 0.0772 (0.0803) Data: 0.0487 (0.0523) Loss: 1.5094 (1.5415)\n",
            "TRAIN(183): [120/196] Batch: 0.0764 (0.0801) Data: 0.0507 (0.0516) Loss: 1.6643 (1.5415)\n",
            "TRAIN(183): [130/196] Batch: 0.0674 (0.0798) Data: 0.0507 (0.0512) Loss: 1.4574 (1.5416)\n",
            "TRAIN(183): [140/196] Batch: 0.0756 (0.0796) Data: 0.0516 (0.0509) Loss: 1.4942 (1.5403)\n",
            "TRAIN(183): [150/196] Batch: 0.0803 (0.0795) Data: 0.0467 (0.0504) Loss: 1.5405 (1.5392)\n",
            "TRAIN(183): [160/196] Batch: 0.0688 (0.0793) Data: 0.0568 (0.0502) Loss: 1.5571 (1.5443)\n",
            "TRAIN(183): [170/196] Batch: 0.0678 (0.0791) Data: 0.0601 (0.0502) Loss: 1.3443 (1.5441)\n",
            "TRAIN(183): [180/196] Batch: 0.0725 (0.0789) Data: 0.0561 (0.0502) Loss: 1.6742 (1.5438)\n",
            "TRAIN(183): [190/196] Batch: 0.0747 (0.0788) Data: 0.0621 (0.0502) Loss: 1.7199 (1.5463)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(183)         0:00:15         0:00:09         0:00:05          1.5468\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(184): [ 10/196] Batch: 0.0773 (0.1170) Data: 0.0471 (0.0810) Loss: 1.5558 (1.5256)\n",
            "TRAIN(184): [ 20/196] Batch: 0.0722 (0.0966) Data: 0.0533 (0.0659) Loss: 1.4299 (1.5391)\n",
            "TRAIN(184): [ 30/196] Batch: 0.0792 (0.0900) Data: 0.0509 (0.0607) Loss: 1.4611 (1.5352)\n",
            "TRAIN(184): [ 40/196] Batch: 0.0733 (0.0863) Data: 0.0638 (0.0584) Loss: 1.5053 (1.5320)\n",
            "TRAIN(184): [ 50/196] Batch: 0.0642 (0.0842) Data: 0.0614 (0.0563) Loss: 1.6286 (1.5429)\n",
            "TRAIN(184): [ 60/196] Batch: 0.0830 (0.0831) Data: 0.0528 (0.0555) Loss: 1.6901 (1.5542)\n",
            "TRAIN(184): [ 70/196] Batch: 0.0802 (0.0822) Data: 0.0497 (0.0548) Loss: 1.6697 (1.5573)\n",
            "TRAIN(184): [ 80/196] Batch: 0.0726 (0.0813) Data: 0.0629 (0.0546) Loss: 1.6516 (1.5559)\n",
            "TRAIN(184): [ 90/196] Batch: 0.0753 (0.0808) Data: 0.0558 (0.0540) Loss: 1.5414 (1.5544)\n",
            "TRAIN(184): [100/196] Batch: 0.0705 (0.0804) Data: 0.0586 (0.0532) Loss: 1.7058 (1.5544)\n",
            "TRAIN(184): [110/196] Batch: 0.0753 (0.0801) Data: 0.0512 (0.0526) Loss: 1.6431 (1.5527)\n",
            "TRAIN(184): [120/196] Batch: 0.0862 (0.0800) Data: 0.0436 (0.0520) Loss: 1.6309 (1.5541)\n",
            "TRAIN(184): [130/196] Batch: 0.0774 (0.0798) Data: 0.0496 (0.0514) Loss: 1.5414 (1.5556)\n",
            "TRAIN(184): [140/196] Batch: 0.0838 (0.0795) Data: 0.0488 (0.0510) Loss: 1.5792 (1.5547)\n",
            "TRAIN(184): [150/196] Batch: 0.0686 (0.0792) Data: 0.0607 (0.0509) Loss: 1.5434 (1.5571)\n",
            "TRAIN(184): [160/196] Batch: 0.0799 (0.0791) Data: 0.0525 (0.0507) Loss: 1.4042 (1.5597)\n",
            "TRAIN(184): [170/196] Batch: 0.0783 (0.0789) Data: 0.0554 (0.0507) Loss: 1.5479 (1.5594)\n",
            "TRAIN(184): [180/196] Batch: 0.0802 (0.0788) Data: 0.0508 (0.0506) Loss: 1.5503 (1.5587)\n",
            "TRAIN(184): [190/196] Batch: 0.0765 (0.0786) Data: 0.0612 (0.0506) Loss: 1.6460 (1.5586)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(184)         0:00:15         0:00:09         0:00:05          1.5585\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(185): [ 10/196] Batch: 0.0751 (0.1196) Data: 0.0541 (0.0839) Loss: 1.5437 (1.5398)\n",
            "TRAIN(185): [ 20/196] Batch: 0.0652 (0.0978) Data: 0.0614 (0.0691) Loss: 1.5788 (1.5506)\n",
            "TRAIN(185): [ 30/196] Batch: 0.0866 (0.0910) Data: 0.0510 (0.0627) Loss: 1.5020 (1.5415)\n",
            "TRAIN(185): [ 40/196] Batch: 0.0774 (0.0872) Data: 0.0550 (0.0596) Loss: 1.5510 (1.5388)\n",
            "TRAIN(185): [ 50/196] Batch: 0.0785 (0.0850) Data: 0.0527 (0.0574) Loss: 1.6681 (1.5512)\n",
            "TRAIN(185): [ 60/196] Batch: 0.0789 (0.0835) Data: 0.0506 (0.0560) Loss: 1.5415 (1.5630)\n",
            "TRAIN(185): [ 70/196] Batch: 0.0702 (0.0825) Data: 0.0495 (0.0548) Loss: 1.4385 (1.5586)\n",
            "TRAIN(185): [ 80/196] Batch: 0.0780 (0.0819) Data: 0.0526 (0.0535) Loss: 1.5320 (1.5533)\n",
            "TRAIN(185): [ 90/196] Batch: 0.0676 (0.0813) Data: 0.0516 (0.0524) Loss: 1.5174 (1.5545)\n",
            "TRAIN(185): [100/196] Batch: 0.0736 (0.0808) Data: 0.0580 (0.0518) Loss: 1.5470 (1.5525)\n",
            "TRAIN(185): [110/196] Batch: 0.0699 (0.0805) Data: 0.0563 (0.0514) Loss: 1.5133 (1.5520)\n",
            "TRAIN(185): [120/196] Batch: 0.0738 (0.0801) Data: 0.0607 (0.0513) Loss: 1.7173 (1.5552)\n",
            "TRAIN(185): [130/196] Batch: 0.0722 (0.0798) Data: 0.0632 (0.0512) Loss: 1.4899 (1.5522)\n",
            "TRAIN(185): [140/196] Batch: 0.0705 (0.0796) Data: 0.0594 (0.0512) Loss: 1.4049 (1.5508)\n",
            "TRAIN(185): [150/196] Batch: 0.0778 (0.0794) Data: 0.0516 (0.0512) Loss: 1.5790 (1.5497)\n",
            "TRAIN(185): [160/196] Batch: 0.0703 (0.0792) Data: 0.0563 (0.0511) Loss: 1.5817 (1.5473)\n",
            "TRAIN(185): [170/196] Batch: 0.0778 (0.0790) Data: 0.0604 (0.0512) Loss: 1.3650 (1.5483)\n",
            "TRAIN(185): [180/196] Batch: 0.0774 (0.0788) Data: 0.0546 (0.0511) Loss: 1.4979 (1.5523)\n",
            "TRAIN(185): [190/196] Batch: 0.0749 (0.0787) Data: 0.0618 (0.0511) Loss: 1.4274 (1.5500)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(185)         0:00:15         0:00:10         0:00:05          1.5494\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(186): [ 10/196] Batch: 0.0742 (0.1182) Data: 0.0495 (0.0806) Loss: 1.5927 (1.5121)\n",
            "TRAIN(186): [ 20/196] Batch: 0.0781 (0.0972) Data: 0.0509 (0.0659) Loss: 1.5110 (1.5360)\n",
            "TRAIN(186): [ 30/196] Batch: 0.0871 (0.0903) Data: 0.0498 (0.0612) Loss: 1.4639 (1.5557)\n",
            "TRAIN(186): [ 40/196] Batch: 0.0695 (0.0865) Data: 0.0562 (0.0580) Loss: 1.4689 (1.5418)\n",
            "TRAIN(186): [ 50/196] Batch: 0.0747 (0.0846) Data: 0.0507 (0.0554) Loss: 1.5370 (1.5405)\n",
            "TRAIN(186): [ 60/196] Batch: 0.0757 (0.0834) Data: 0.0486 (0.0536) Loss: 1.4962 (1.5445)\n",
            "TRAIN(186): [ 70/196] Batch: 0.0884 (0.0826) Data: 0.0406 (0.0521) Loss: 1.5291 (1.5428)\n",
            "TRAIN(186): [ 80/196] Batch: 0.0854 (0.0819) Data: 0.0463 (0.0512) Loss: 1.5917 (1.5483)\n",
            "TRAIN(186): [ 90/196] Batch: 0.0791 (0.0812) Data: 0.0489 (0.0508) Loss: 1.5021 (1.5475)\n",
            "TRAIN(186): [100/196] Batch: 0.0764 (0.0807) Data: 0.0513 (0.0507) Loss: 1.6391 (1.5504)\n",
            "TRAIN(186): [110/196] Batch: 0.0770 (0.0802) Data: 0.0560 (0.0507) Loss: 1.6257 (1.5556)\n",
            "TRAIN(186): [120/196] Batch: 0.0778 (0.0799) Data: 0.0578 (0.0506) Loss: 1.5140 (1.5589)\n",
            "TRAIN(186): [130/196] Batch: 0.0757 (0.0796) Data: 0.0618 (0.0507) Loss: 1.4725 (1.5606)\n",
            "TRAIN(186): [140/196] Batch: 0.0761 (0.0794) Data: 0.0551 (0.0506) Loss: 1.7204 (1.5595)\n",
            "TRAIN(186): [150/196] Batch: 0.0718 (0.0791) Data: 0.0612 (0.0508) Loss: 1.5630 (1.5581)\n",
            "TRAIN(186): [160/196] Batch: 0.0659 (0.0789) Data: 0.0604 (0.0506) Loss: 1.6601 (1.5585)\n",
            "TRAIN(186): [170/196] Batch: 0.0769 (0.0788) Data: 0.0561 (0.0507) Loss: 1.7049 (1.5587)\n",
            "TRAIN(186): [180/196] Batch: 0.0682 (0.0786) Data: 0.0551 (0.0506) Loss: 1.4832 (1.5588)\n",
            "TRAIN(186): [190/196] Batch: 0.0760 (0.0785) Data: 0.0616 (0.0506) Loss: 1.5836 (1.5606)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(186)         0:00:15         0:00:09         0:00:05          1.5594\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(187): [ 10/196] Batch: 0.0706 (0.1157) Data: 0.0545 (0.0789) Loss: 1.5127 (1.5764)\n",
            "TRAIN(187): [ 20/196] Batch: 0.0694 (0.0961) Data: 0.0607 (0.0658) Loss: 1.4967 (1.5743)\n",
            "TRAIN(187): [ 30/196] Batch: 0.0746 (0.0900) Data: 0.0541 (0.0594) Loss: 1.4370 (1.5744)\n",
            "TRAIN(187): [ 40/196] Batch: 0.0852 (0.0869) Data: 0.0494 (0.0562) Loss: 1.4722 (1.5595)\n",
            "TRAIN(187): [ 50/196] Batch: 0.0781 (0.0846) Data: 0.0540 (0.0544) Loss: 1.5877 (1.5623)\n",
            "TRAIN(187): [ 60/196] Batch: 0.0796 (0.0833) Data: 0.0488 (0.0531) Loss: 1.6117 (1.5574)\n",
            "TRAIN(187): [ 70/196] Batch: 0.0748 (0.0823) Data: 0.0558 (0.0521) Loss: 1.3502 (1.5573)\n",
            "TRAIN(187): [ 80/196] Batch: 0.0716 (0.0816) Data: 0.0576 (0.0516) Loss: 1.5686 (1.5517)\n",
            "TRAIN(187): [ 90/196] Batch: 0.0796 (0.0810) Data: 0.0559 (0.0516) Loss: 1.5956 (1.5568)\n",
            "TRAIN(187): [100/196] Batch: 0.0790 (0.0805) Data: 0.0506 (0.0513) Loss: 1.7126 (1.5580)\n",
            "TRAIN(187): [110/196] Batch: 0.0637 (0.0800) Data: 0.0614 (0.0511) Loss: 1.5832 (1.5571)\n",
            "TRAIN(187): [120/196] Batch: 0.0767 (0.0798) Data: 0.0550 (0.0511) Loss: 1.6454 (1.5577)\n",
            "TRAIN(187): [130/196] Batch: 0.0795 (0.0795) Data: 0.0526 (0.0510) Loss: 1.6613 (1.5559)\n",
            "TRAIN(187): [140/196] Batch: 0.0697 (0.0792) Data: 0.0612 (0.0509) Loss: 1.5480 (1.5552)\n",
            "TRAIN(187): [150/196] Batch: 0.0817 (0.0790) Data: 0.0566 (0.0508) Loss: 1.5778 (1.5566)\n",
            "TRAIN(187): [160/196] Batch: 0.0753 (0.0788) Data: 0.0572 (0.0508) Loss: 1.4614 (1.5560)\n",
            "TRAIN(187): [170/196] Batch: 0.0764 (0.0787) Data: 0.0557 (0.0507) Loss: 1.5578 (1.5571)\n",
            "TRAIN(187): [180/196] Batch: 0.0785 (0.0786) Data: 0.0528 (0.0506) Loss: 1.7492 (1.5571)\n",
            "TRAIN(187): [190/196] Batch: 0.0747 (0.0784) Data: 0.0631 (0.0506) Loss: 1.3710 (1.5574)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(187)         0:00:15         0:00:09         0:00:05          1.5566\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(188): [ 10/196] Batch: 0.0683 (0.1271) Data: 0.0488 (0.0803) Loss: 1.5199 (1.5980)\n",
            "TRAIN(188): [ 20/196] Batch: 0.0746 (0.1027) Data: 0.0528 (0.0626) Loss: 1.5536 (1.5548)\n",
            "TRAIN(188): [ 30/196] Batch: 0.0765 (0.0943) Data: 0.0509 (0.0568) Loss: 1.6062 (1.5566)\n",
            "TRAIN(188): [ 40/196] Batch: 0.0771 (0.0899) Data: 0.0559 (0.0545) Loss: 1.5468 (1.5392)\n",
            "TRAIN(188): [ 50/196] Batch: 0.0832 (0.0875) Data: 0.0459 (0.0530) Loss: 1.6265 (1.5452)\n",
            "TRAIN(188): [ 60/196] Batch: 0.0767 (0.0855) Data: 0.0562 (0.0525) Loss: 1.5942 (1.5408)\n",
            "TRAIN(188): [ 70/196] Batch: 0.0858 (0.0842) Data: 0.0502 (0.0521) Loss: 1.5220 (1.5434)\n",
            "TRAIN(188): [ 80/196] Batch: 0.0767 (0.0830) Data: 0.0628 (0.0523) Loss: 1.5802 (1.5427)\n",
            "TRAIN(188): [ 90/196] Batch: 0.0711 (0.0823) Data: 0.0571 (0.0520) Loss: 1.7213 (1.5441)\n",
            "TRAIN(188): [100/196] Batch: 0.0698 (0.0816) Data: 0.0621 (0.0521) Loss: 1.4026 (1.5437)\n",
            "TRAIN(188): [110/196] Batch: 0.0768 (0.0812) Data: 0.0544 (0.0520) Loss: 1.7456 (1.5447)\n",
            "TRAIN(188): [120/196] Batch: 0.0675 (0.0808) Data: 0.0602 (0.0518) Loss: 1.5855 (1.5478)\n",
            "TRAIN(188): [130/196] Batch: 0.0750 (0.0805) Data: 0.0510 (0.0514) Loss: 1.4174 (1.5477)\n",
            "TRAIN(188): [140/196] Batch: 0.0800 (0.0802) Data: 0.0518 (0.0511) Loss: 1.5482 (1.5499)\n",
            "TRAIN(188): [150/196] Batch: 0.0757 (0.0799) Data: 0.0556 (0.0510) Loss: 1.6167 (1.5535)\n",
            "TRAIN(188): [160/196] Batch: 0.0693 (0.0797) Data: 0.0576 (0.0509) Loss: 1.4427 (1.5540)\n",
            "TRAIN(188): [170/196] Batch: 0.0763 (0.0794) Data: 0.0610 (0.0509) Loss: 1.6332 (1.5528)\n",
            "TRAIN(188): [180/196] Batch: 0.0757 (0.0792) Data: 0.0610 (0.0510) Loss: 1.5099 (1.5520)\n",
            "TRAIN(188): [190/196] Batch: 0.0762 (0.0791) Data: 0.0628 (0.0509) Loss: 1.5924 (1.5531)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(188)         0:00:15         0:00:09         0:00:05          1.5526\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(189): [ 10/196] Batch: 0.0580 (0.1272) Data: 0.0717 (0.0927) Loss: 1.3840 (1.5405)\n",
            "TRAIN(189): [ 20/196] Batch: 0.0765 (0.1046) Data: 0.0487 (0.0688) Loss: 1.6069 (1.5177)\n",
            "TRAIN(189): [ 30/196] Batch: 0.0804 (0.0951) Data: 0.0503 (0.0622) Loss: 1.3872 (1.5222)\n",
            "TRAIN(189): [ 40/196] Batch: 0.0803 (0.0903) Data: 0.0500 (0.0590) Loss: 1.6314 (1.5259)\n",
            "TRAIN(189): [ 50/196] Batch: 0.0715 (0.0874) Data: 0.0594 (0.0573) Loss: 1.5151 (1.5223)\n",
            "TRAIN(189): [ 60/196] Batch: 0.0736 (0.0856) Data: 0.0540 (0.0557) Loss: 1.6243 (1.5282)\n",
            "TRAIN(189): [ 70/196] Batch: 0.0795 (0.0843) Data: 0.0506 (0.0548) Loss: 1.5569 (1.5321)\n",
            "TRAIN(189): [ 80/196] Batch: 0.0864 (0.0832) Data: 0.0510 (0.0541) Loss: 1.4475 (1.5333)\n",
            "TRAIN(189): [ 90/196] Batch: 0.0802 (0.0824) Data: 0.0536 (0.0537) Loss: 1.5390 (1.5350)\n",
            "TRAIN(189): [100/196] Batch: 0.0772 (0.0818) Data: 0.0565 (0.0533) Loss: 1.5857 (1.5383)\n",
            "TRAIN(189): [110/196] Batch: 0.0790 (0.0812) Data: 0.0583 (0.0533) Loss: 1.6218 (1.5392)\n",
            "TRAIN(189): [120/196] Batch: 0.0697 (0.0809) Data: 0.0567 (0.0531) Loss: 1.6208 (1.5400)\n",
            "TRAIN(189): [130/196] Batch: 0.0699 (0.0805) Data: 0.0563 (0.0528) Loss: 1.6896 (1.5409)\n",
            "TRAIN(189): [140/196] Batch: 0.0839 (0.0802) Data: 0.0527 (0.0526) Loss: 1.4459 (1.5411)\n",
            "TRAIN(189): [150/196] Batch: 0.0864 (0.0799) Data: 0.0543 (0.0525) Loss: 1.6517 (1.5422)\n",
            "TRAIN(189): [160/196] Batch: 0.0928 (0.0798) Data: 0.0444 (0.0523) Loss: 1.4834 (1.5463)\n",
            "TRAIN(189): [170/196] Batch: 0.0782 (0.0796) Data: 0.0564 (0.0522) Loss: 1.4981 (1.5476)\n",
            "TRAIN(189): [180/196] Batch: 0.0878 (0.0796) Data: 0.0403 (0.0517) Loss: 1.5818 (1.5454)\n",
            "TRAIN(189): [190/196] Batch: 0.0763 (0.0794) Data: 0.0635 (0.0515) Loss: 1.5141 (1.5434)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(189)         0:00:15         0:00:10         0:00:05          1.5445\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(190): [ 10/196] Batch: 0.0810 (0.1193) Data: 0.0498 (0.0848) Loss: 1.5691 (1.5329)\n",
            "TRAIN(190): [ 20/196] Batch: 0.0693 (0.0976) Data: 0.0614 (0.0683) Loss: 1.4404 (1.5319)\n",
            "TRAIN(190): [ 30/196] Batch: 0.0702 (0.0907) Data: 0.0538 (0.0631) Loss: 1.4924 (1.5477)\n",
            "TRAIN(190): [ 40/196] Batch: 0.0843 (0.0872) Data: 0.0522 (0.0600) Loss: 1.6051 (1.5493)\n",
            "TRAIN(190): [ 50/196] Batch: 0.0799 (0.0850) Data: 0.0509 (0.0580) Loss: 1.5866 (1.5534)\n",
            "TRAIN(190): [ 60/196] Batch: 0.0773 (0.0836) Data: 0.0494 (0.0568) Loss: 1.6718 (1.5638)\n",
            "TRAIN(190): [ 70/196] Batch: 0.0720 (0.0825) Data: 0.0546 (0.0559) Loss: 1.4396 (1.5620)\n",
            "TRAIN(190): [ 80/196] Batch: 0.0720 (0.0817) Data: 0.0559 (0.0549) Loss: 1.6923 (1.5642)\n",
            "TRAIN(190): [ 90/196] Batch: 0.0783 (0.0811) Data: 0.0565 (0.0545) Loss: 1.5811 (1.5652)\n",
            "TRAIN(190): [100/196] Batch: 0.0797 (0.0806) Data: 0.0523 (0.0540) Loss: 1.4393 (1.5604)\n",
            "TRAIN(190): [110/196] Batch: 0.0707 (0.0802) Data: 0.0555 (0.0536) Loss: 1.6116 (1.5581)\n",
            "TRAIN(190): [120/196] Batch: 0.0789 (0.0799) Data: 0.0531 (0.0533) Loss: 1.6112 (1.5646)\n",
            "TRAIN(190): [130/196] Batch: 0.0780 (0.0797) Data: 0.0503 (0.0527) Loss: 1.4645 (1.5618)\n",
            "TRAIN(190): [140/196] Batch: 0.0777 (0.0794) Data: 0.0554 (0.0523) Loss: 1.5499 (1.5636)\n",
            "TRAIN(190): [150/196] Batch: 0.0878 (0.0793) Data: 0.0431 (0.0521) Loss: 1.5061 (1.5618)\n",
            "TRAIN(190): [160/196] Batch: 0.0683 (0.0795) Data: 0.0557 (0.0515) Loss: 1.6789 (1.5655)\n",
            "TRAIN(190): [170/196] Batch: 0.0719 (0.0794) Data: 0.0556 (0.0511) Loss: 1.6255 (1.5661)\n",
            "TRAIN(190): [180/196] Batch: 0.0696 (0.0792) Data: 0.0627 (0.0510) Loss: 1.5181 (1.5655)\n",
            "TRAIN(190): [190/196] Batch: 0.0763 (0.0791) Data: 0.0609 (0.0511) Loss: 1.6324 (1.5660)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(190)         0:00:15         0:00:10         0:00:05          1.5658\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(191): [ 10/196] Batch: 0.0731 (0.1226) Data: 0.0513 (0.0841) Loss: 1.4851 (1.6111)\n",
            "TRAIN(191): [ 20/196] Batch: 0.0783 (0.0998) Data: 0.0517 (0.0681) Loss: 1.4336 (1.5711)\n",
            "TRAIN(191): [ 30/196] Batch: 0.0759 (0.0920) Data: 0.0549 (0.0625) Loss: 1.4844 (1.5443)\n",
            "TRAIN(191): [ 40/196] Batch: 0.0801 (0.0882) Data: 0.0530 (0.0594) Loss: 1.6085 (1.5370)\n",
            "TRAIN(191): [ 50/196] Batch: 0.0777 (0.0857) Data: 0.0561 (0.0575) Loss: 1.5602 (1.5482)\n",
            "TRAIN(191): [ 60/196] Batch: 0.0747 (0.0841) Data: 0.0555 (0.0562) Loss: 1.5149 (1.5457)\n",
            "TRAIN(191): [ 70/196] Batch: 0.0842 (0.0831) Data: 0.0527 (0.0557) Loss: 1.6277 (1.5540)\n",
            "TRAIN(191): [ 80/196] Batch: 0.0672 (0.0821) Data: 0.0620 (0.0551) Loss: 1.5189 (1.5515)\n",
            "TRAIN(191): [ 90/196] Batch: 0.0815 (0.0816) Data: 0.0506 (0.0546) Loss: 1.4280 (1.5469)\n",
            "TRAIN(191): [100/196] Batch: 0.0804 (0.0811) Data: 0.0504 (0.0543) Loss: 1.7444 (1.5580)\n",
            "TRAIN(191): [110/196] Batch: 0.0743 (0.0807) Data: 0.0508 (0.0537) Loss: 1.5019 (1.5548)\n",
            "TRAIN(191): [120/196] Batch: 0.0740 (0.0803) Data: 0.0517 (0.0529) Loss: 1.7574 (1.5562)\n",
            "TRAIN(191): [130/196] Batch: 0.0763 (0.0801) Data: 0.0538 (0.0524) Loss: 1.3640 (1.5549)\n",
            "TRAIN(191): [140/196] Batch: 0.0848 (0.0800) Data: 0.0432 (0.0521) Loss: 1.6716 (1.5573)\n",
            "TRAIN(191): [150/196] Batch: 0.0749 (0.0797) Data: 0.0483 (0.0517) Loss: 1.4320 (1.5574)\n",
            "TRAIN(191): [160/196] Batch: 0.0710 (0.0794) Data: 0.0609 (0.0516) Loss: 1.4128 (1.5564)\n",
            "TRAIN(191): [170/196] Batch: 0.0828 (0.0793) Data: 0.0563 (0.0516) Loss: 1.5932 (1.5576)\n",
            "TRAIN(191): [180/196] Batch: 0.0722 (0.0791) Data: 0.0556 (0.0515) Loss: 1.6157 (1.5588)\n",
            "TRAIN(191): [190/196] Batch: 0.0755 (0.0790) Data: 0.0605 (0.0515) Loss: 1.5443 (1.5580)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(191)         0:00:15         0:00:10         0:00:05          1.5579\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(192): [ 10/196] Batch: 0.0868 (0.1181) Data: 0.0477 (0.0825) Loss: 1.4509 (1.5350)\n",
            "TRAIN(192): [ 20/196] Batch: 0.0729 (0.0970) Data: 0.0603 (0.0671) Loss: 1.5955 (1.5695)\n",
            "TRAIN(192): [ 30/196] Batch: 0.0799 (0.0905) Data: 0.0559 (0.0618) Loss: 1.4745 (1.5687)\n",
            "TRAIN(192): [ 40/196] Batch: 0.0718 (0.0869) Data: 0.0583 (0.0588) Loss: 1.5408 (1.5738)\n",
            "TRAIN(192): [ 50/196] Batch: 0.0737 (0.0846) Data: 0.0627 (0.0575) Loss: 1.5271 (1.5674)\n",
            "TRAIN(192): [ 60/196] Batch: 0.0800 (0.0834) Data: 0.0493 (0.0563) Loss: 1.5846 (1.5689)\n",
            "TRAIN(192): [ 70/196] Batch: 0.0637 (0.0822) Data: 0.0619 (0.0556) Loss: 1.5995 (1.5647)\n",
            "TRAIN(192): [ 80/196] Batch: 0.0781 (0.0816) Data: 0.0536 (0.0551) Loss: 1.7145 (1.5612)\n",
            "TRAIN(192): [ 90/196] Batch: 0.0806 (0.0812) Data: 0.0399 (0.0542) Loss: 1.4373 (1.5616)\n",
            "TRAIN(192): [100/196] Batch: 0.0772 (0.0806) Data: 0.0498 (0.0532) Loss: 1.5467 (1.5618)\n",
            "TRAIN(192): [110/196] Batch: 0.0718 (0.0803) Data: 0.0528 (0.0526) Loss: 1.6214 (1.5621)\n",
            "TRAIN(192): [120/196] Batch: 0.0745 (0.0800) Data: 0.0501 (0.0520) Loss: 1.6222 (1.5650)\n",
            "TRAIN(192): [130/196] Batch: 0.0796 (0.0798) Data: 0.0479 (0.0515) Loss: 1.4585 (1.5667)\n",
            "TRAIN(192): [140/196] Batch: 0.0777 (0.0795) Data: 0.0548 (0.0514) Loss: 1.4589 (1.5661)\n",
            "TRAIN(192): [150/196] Batch: 0.0771 (0.0793) Data: 0.0550 (0.0513) Loss: 1.4937 (1.5663)\n",
            "TRAIN(192): [160/196] Batch: 0.0690 (0.0791) Data: 0.0572 (0.0512) Loss: 1.7280 (1.5664)\n",
            "TRAIN(192): [170/196] Batch: 0.0766 (0.0789) Data: 0.0615 (0.0512) Loss: 1.4579 (1.5684)\n",
            "TRAIN(192): [180/196] Batch: 0.0740 (0.0788) Data: 0.0567 (0.0511) Loss: 1.7472 (1.5677)\n",
            "TRAIN(192): [190/196] Batch: 0.0753 (0.0786) Data: 0.0620 (0.0513) Loss: 1.7171 (1.5691)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(192)         0:00:15         0:00:10         0:00:05          1.5693\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(193): [ 10/196] Batch: 0.0734 (0.1233) Data: 0.0505 (0.0877) Loss: 1.5680 (1.5668)\n",
            "TRAIN(193): [ 20/196] Batch: 0.0786 (0.1002) Data: 0.0498 (0.0692) Loss: 1.3331 (1.5576)\n",
            "TRAIN(193): [ 30/196] Batch: 0.0671 (0.0921) Data: 0.0563 (0.0628) Loss: 1.4291 (1.5514)\n",
            "TRAIN(193): [ 40/196] Batch: 0.0776 (0.0880) Data: 0.0626 (0.0602) Loss: 1.5385 (1.5524)\n",
            "TRAIN(193): [ 50/196] Batch: 0.0791 (0.0857) Data: 0.0542 (0.0582) Loss: 1.6633 (1.5557)\n",
            "TRAIN(193): [ 60/196] Batch: 0.0790 (0.0842) Data: 0.0527 (0.0566) Loss: 1.5670 (1.5630)\n",
            "TRAIN(193): [ 70/196] Batch: 0.0791 (0.0831) Data: 0.0581 (0.0552) Loss: 1.4135 (1.5644)\n",
            "TRAIN(193): [ 80/196] Batch: 0.0757 (0.0823) Data: 0.0502 (0.0541) Loss: 1.6719 (1.5680)\n",
            "TRAIN(193): [ 90/196] Batch: 0.0751 (0.0817) Data: 0.0530 (0.0532) Loss: 1.5033 (1.5661)\n",
            "TRAIN(193): [100/196] Batch: 0.0872 (0.0814) Data: 0.0410 (0.0521) Loss: 1.5195 (1.5650)\n",
            "TRAIN(193): [110/196] Batch: 0.0738 (0.0810) Data: 0.0467 (0.0512) Loss: 1.4467 (1.5624)\n",
            "TRAIN(193): [120/196] Batch: 0.0804 (0.0806) Data: 0.0511 (0.0510) Loss: 1.4854 (1.5564)\n",
            "TRAIN(193): [130/196] Batch: 0.0703 (0.0802) Data: 0.0553 (0.0508) Loss: 1.5467 (1.5575)\n",
            "TRAIN(193): [140/196] Batch: 0.0726 (0.0799) Data: 0.0538 (0.0507) Loss: 1.6359 (1.5613)\n",
            "TRAIN(193): [150/196] Batch: 0.0710 (0.0796) Data: 0.0609 (0.0508) Loss: 1.5473 (1.5586)\n",
            "TRAIN(193): [160/196] Batch: 0.0711 (0.0794) Data: 0.0617 (0.0509) Loss: 1.6161 (1.5581)\n",
            "TRAIN(193): [170/196] Batch: 0.0704 (0.0792) Data: 0.0557 (0.0507) Loss: 1.6833 (1.5586)\n",
            "TRAIN(193): [180/196] Batch: 0.0782 (0.0791) Data: 0.0518 (0.0506) Loss: 1.5605 (1.5578)\n",
            "TRAIN(193): [190/196] Batch: 0.0754 (0.0789) Data: 0.0620 (0.0506) Loss: 1.6388 (1.5583)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(193)         0:00:15         0:00:09         0:00:05          1.5563\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(194): [ 10/196] Batch: 0.0858 (0.1199) Data: 0.0479 (0.0813) Loss: 1.5317 (1.4910)\n",
            "TRAIN(194): [ 20/196] Batch: 0.0756 (0.0979) Data: 0.0543 (0.0664) Loss: 1.6663 (1.5324)\n",
            "TRAIN(194): [ 30/196] Batch: 0.0852 (0.0910) Data: 0.0482 (0.0610) Loss: 1.5513 (1.5364)\n",
            "TRAIN(194): [ 40/196] Batch: 0.0813 (0.0872) Data: 0.0530 (0.0583) Loss: 1.5202 (1.5487)\n",
            "TRAIN(194): [ 50/196] Batch: 0.0783 (0.0850) Data: 0.0512 (0.0560) Loss: 1.4682 (1.5501)\n",
            "TRAIN(194): [ 60/196] Batch: 0.0848 (0.0841) Data: 0.0410 (0.0537) Loss: 1.5787 (1.5509)\n",
            "TRAIN(194): [ 70/196] Batch: 0.0854 (0.0831) Data: 0.0465 (0.0523) Loss: 1.4651 (1.5530)\n",
            "TRAIN(194): [ 80/196] Batch: 0.0805 (0.0824) Data: 0.0501 (0.0512) Loss: 1.5289 (1.5503)\n",
            "TRAIN(194): [ 90/196] Batch: 0.0734 (0.0817) Data: 0.0507 (0.0505) Loss: 1.3725 (1.5510)\n",
            "TRAIN(194): [100/196] Batch: 0.0646 (0.0811) Data: 0.0618 (0.0505) Loss: 1.4987 (1.5504)\n",
            "TRAIN(194): [110/196] Batch: 0.0856 (0.0807) Data: 0.0520 (0.0505) Loss: 1.4876 (1.5542)\n",
            "TRAIN(194): [120/196] Batch: 0.0718 (0.0803) Data: 0.0554 (0.0504) Loss: 1.6781 (1.5559)\n",
            "TRAIN(194): [130/196] Batch: 0.0804 (0.0800) Data: 0.0515 (0.0503) Loss: 1.6481 (1.5618)\n",
            "TRAIN(194): [140/196] Batch: 0.0864 (0.0798) Data: 0.0479 (0.0503) Loss: 1.5716 (1.5615)\n",
            "TRAIN(194): [150/196] Batch: 0.0763 (0.0796) Data: 0.0518 (0.0501) Loss: 1.4824 (1.5583)\n",
            "TRAIN(194): [160/196] Batch: 0.0780 (0.0794) Data: 0.0530 (0.0498) Loss: 1.6330 (1.5601)\n",
            "TRAIN(194): [170/196] Batch: 0.0756 (0.0792) Data: 0.0500 (0.0495) Loss: 1.6164 (1.5615)\n",
            "TRAIN(194): [180/196] Batch: 0.0898 (0.0792) Data: 0.0374 (0.0492) Loss: 1.5161 (1.5625)\n",
            "TRAIN(194): [190/196] Batch: 0.0756 (0.0790) Data: 0.0625 (0.0490) Loss: 1.5830 (1.5600)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(194)         0:00:15         0:00:09         0:00:05          1.5611\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(195): [ 10/196] Batch: 0.0729 (0.1207) Data: 0.0486 (0.0812) Loss: 1.6130 (1.6121)\n",
            "TRAIN(195): [ 20/196] Batch: 0.0747 (0.0987) Data: 0.0481 (0.0650) Loss: 1.5713 (1.5754)\n",
            "TRAIN(195): [ 30/196] Batch: 0.0941 (0.0922) Data: 0.0385 (0.0579) Loss: 1.6557 (1.5899)\n",
            "TRAIN(195): [ 40/196] Batch: 0.0756 (0.0881) Data: 0.0573 (0.0555) Loss: 1.5055 (1.5941)\n",
            "TRAIN(195): [ 50/196] Batch: 0.0810 (0.0862) Data: 0.0423 (0.0532) Loss: 1.6166 (1.5771)\n",
            "TRAIN(195): [ 60/196] Batch: 0.0726 (0.0847) Data: 0.0493 (0.0524) Loss: 1.4611 (1.5668)\n",
            "TRAIN(195): [ 70/196] Batch: 0.0731 (0.0834) Data: 0.0561 (0.0519) Loss: 1.6127 (1.5658)\n",
            "TRAIN(195): [ 80/196] Batch: 0.0855 (0.0825) Data: 0.0528 (0.0515) Loss: 1.6611 (1.5666)\n",
            "TRAIN(195): [ 90/196] Batch: 0.0760 (0.0818) Data: 0.0540 (0.0512) Loss: 1.6703 (1.5735)\n",
            "TRAIN(195): [100/196] Batch: 0.0758 (0.0812) Data: 0.0622 (0.0512) Loss: 1.5395 (1.5672)\n",
            "TRAIN(195): [110/196] Batch: 0.0804 (0.0808) Data: 0.0569 (0.0509) Loss: 1.7581 (1.5677)\n",
            "TRAIN(195): [120/196] Batch: 0.0796 (0.0804) Data: 0.0509 (0.0507) Loss: 1.4982 (1.5676)\n",
            "TRAIN(195): [130/196] Batch: 0.0813 (0.0801) Data: 0.0506 (0.0508) Loss: 1.5782 (1.5669)\n",
            "TRAIN(195): [140/196] Batch: 0.0767 (0.0798) Data: 0.0556 (0.0506) Loss: 1.5780 (1.5705)\n",
            "TRAIN(195): [150/196] Batch: 0.0705 (0.0795) Data: 0.0563 (0.0505) Loss: 1.5605 (1.5696)\n",
            "TRAIN(195): [160/196] Batch: 0.0756 (0.0794) Data: 0.0521 (0.0504) Loss: 1.6421 (1.5661)\n",
            "TRAIN(195): [170/196] Batch: 0.0760 (0.0791) Data: 0.0569 (0.0503) Loss: 1.5924 (1.5703)\n",
            "TRAIN(195): [180/196] Batch: 0.0716 (0.0790) Data: 0.0543 (0.0502) Loss: 1.6231 (1.5703)\n",
            "TRAIN(195): [190/196] Batch: 0.0758 (0.0788) Data: 0.0625 (0.0503) Loss: 1.5350 (1.5714)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(195)         0:00:15         0:00:09         0:00:05          1.5707\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(196): [ 10/196] Batch: 0.0746 (0.1316) Data: 0.0490 (0.0879) Loss: 1.4914 (1.5544)\n",
            "TRAIN(196): [ 20/196] Batch: 0.0588 (0.1062) Data: 0.0537 (0.0646) Loss: 1.6205 (1.5556)\n",
            "TRAIN(196): [ 30/196] Batch: 0.0753 (0.0967) Data: 0.0485 (0.0571) Loss: 1.5333 (1.5563)\n",
            "TRAIN(196): [ 40/196] Batch: 0.0732 (0.0918) Data: 0.0488 (0.0537) Loss: 1.4233 (1.5646)\n",
            "TRAIN(196): [ 50/196] Batch: 0.0879 (0.0887) Data: 0.0527 (0.0524) Loss: 1.3346 (1.5577)\n",
            "TRAIN(196): [ 60/196] Batch: 0.0701 (0.0865) Data: 0.0548 (0.0518) Loss: 1.4999 (1.5569)\n",
            "TRAIN(196): [ 70/196] Batch: 0.0757 (0.0851) Data: 0.0609 (0.0517) Loss: 1.5849 (1.5649)\n",
            "TRAIN(196): [ 80/196] Batch: 0.0827 (0.0840) Data: 0.0548 (0.0516) Loss: 1.6599 (1.5656)\n",
            "TRAIN(196): [ 90/196] Batch: 0.0799 (0.0831) Data: 0.0584 (0.0515) Loss: 1.4635 (1.5614)\n",
            "TRAIN(196): [100/196] Batch: 0.0751 (0.0825) Data: 0.0519 (0.0512) Loss: 1.4443 (1.5589)\n",
            "TRAIN(196): [110/196] Batch: 0.0765 (0.0819) Data: 0.0547 (0.0510) Loss: 1.4133 (1.5573)\n",
            "TRAIN(196): [120/196] Batch: 0.0757 (0.0814) Data: 0.0519 (0.0507) Loss: 1.5692 (1.5582)\n",
            "TRAIN(196): [130/196] Batch: 0.0728 (0.0810) Data: 0.0565 (0.0506) Loss: 1.7744 (1.5604)\n",
            "TRAIN(196): [140/196] Batch: 0.0723 (0.0806) Data: 0.0604 (0.0507) Loss: 1.3855 (1.5568)\n",
            "TRAIN(196): [150/196] Batch: 0.0759 (0.0804) Data: 0.0570 (0.0506) Loss: 1.5317 (1.5567)\n",
            "TRAIN(196): [160/196] Batch: 0.0742 (0.0801) Data: 0.0557 (0.0505) Loss: 1.6776 (1.5587)\n",
            "TRAIN(196): [170/196] Batch: 0.0812 (0.0799) Data: 0.0488 (0.0504) Loss: 1.6478 (1.5608)\n",
            "TRAIN(196): [180/196] Batch: 0.0934 (0.0798) Data: 0.0501 (0.0501) Loss: 1.4758 (1.5638)\n",
            "TRAIN(196): [190/196] Batch: 0.0737 (0.0797) Data: 0.0612 (0.0497) Loss: 1.5788 (1.5619)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(196)         0:00:15         0:00:09         0:00:05          1.5603\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(197): [ 10/196] Batch: 0.0780 (0.1372) Data: 0.0507 (0.0953) Loss: 1.5443 (1.5633)\n",
            "TRAIN(197): [ 20/196] Batch: 0.0637 (0.1069) Data: 0.0603 (0.0722) Loss: 1.5157 (1.5513)\n",
            "TRAIN(197): [ 30/196] Batch: 0.0726 (0.0969) Data: 0.0531 (0.0646) Loss: 1.5252 (1.5549)\n",
            "TRAIN(197): [ 40/196] Batch: 0.0727 (0.0917) Data: 0.0576 (0.0611) Loss: 1.6637 (1.5515)\n",
            "TRAIN(197): [ 50/196] Batch: 0.0751 (0.0886) Data: 0.0529 (0.0589) Loss: 1.6287 (1.5561)\n",
            "TRAIN(197): [ 60/196] Batch: 0.0783 (0.0867) Data: 0.0501 (0.0575) Loss: 1.4189 (1.5490)\n",
            "TRAIN(197): [ 70/196] Batch: 0.0735 (0.0852) Data: 0.0500 (0.0559) Loss: 1.6439 (1.5525)\n",
            "TRAIN(197): [ 80/196] Batch: 0.0698 (0.0840) Data: 0.0569 (0.0550) Loss: 1.4685 (1.5485)\n",
            "TRAIN(197): [ 90/196] Batch: 0.0757 (0.0831) Data: 0.0569 (0.0544) Loss: 1.6101 (1.5386)\n",
            "TRAIN(197): [100/196] Batch: 0.0762 (0.0825) Data: 0.0508 (0.0539) Loss: 1.4650 (1.5405)\n",
            "TRAIN(197): [110/196] Batch: 0.0863 (0.0819) Data: 0.0515 (0.0535) Loss: 1.6130 (1.5437)\n",
            "TRAIN(197): [120/196] Batch: 0.0727 (0.0814) Data: 0.0631 (0.0532) Loss: 1.5545 (1.5406)\n",
            "TRAIN(197): [130/196] Batch: 0.0804 (0.0810) Data: 0.0517 (0.0529) Loss: 1.5597 (1.5380)\n",
            "TRAIN(197): [140/196] Batch: 0.0749 (0.0806) Data: 0.0614 (0.0528) Loss: 1.4966 (1.5402)\n",
            "TRAIN(197): [150/196] Batch: 0.0766 (0.0804) Data: 0.0513 (0.0525) Loss: 1.5946 (1.5404)\n",
            "TRAIN(197): [160/196] Batch: 0.0715 (0.0802) Data: 0.0539 (0.0518) Loss: 1.4833 (1.5410)\n",
            "TRAIN(197): [170/196] Batch: 0.0771 (0.0800) Data: 0.0515 (0.0515) Loss: 1.5796 (1.5400)\n",
            "TRAIN(197): [180/196] Batch: 0.0820 (0.0798) Data: 0.0476 (0.0512) Loss: 1.6630 (1.5428)\n",
            "TRAIN(197): [190/196] Batch: 0.0693 (0.0796) Data: 0.0602 (0.0512) Loss: 1.7313 (1.5437)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(197)         0:00:15         0:00:10         0:00:05          1.5445\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(198): [ 10/196] Batch: 0.0714 (0.1153) Data: 0.0517 (0.0804) Loss: 1.4386 (1.5065)\n",
            "TRAIN(198): [ 20/196] Batch: 0.0702 (0.0959) Data: 0.0548 (0.0657) Loss: 1.6090 (1.5056)\n",
            "TRAIN(198): [ 30/196] Batch: 0.0770 (0.0896) Data: 0.0508 (0.0605) Loss: 1.5214 (1.5288)\n",
            "TRAIN(198): [ 40/196] Batch: 0.0777 (0.0860) Data: 0.0608 (0.0585) Loss: 1.5908 (1.5440)\n",
            "TRAIN(198): [ 50/196] Batch: 0.0686 (0.0841) Data: 0.0536 (0.0562) Loss: 1.4720 (1.5404)\n",
            "TRAIN(198): [ 60/196] Batch: 0.0813 (0.0828) Data: 0.0536 (0.0556) Loss: 1.6058 (1.5510)\n",
            "TRAIN(198): [ 70/196] Batch: 0.0663 (0.0818) Data: 0.0608 (0.0548) Loss: 1.6633 (1.5429)\n",
            "TRAIN(198): [ 80/196] Batch: 0.0793 (0.0811) Data: 0.0598 (0.0541) Loss: 1.5114 (1.5396)\n",
            "TRAIN(198): [ 90/196] Batch: 0.0808 (0.0807) Data: 0.0520 (0.0534) Loss: 1.5407 (1.5403)\n",
            "TRAIN(198): [100/196] Batch: 0.0771 (0.0801) Data: 0.0603 (0.0531) Loss: 1.6498 (1.5456)\n",
            "TRAIN(198): [110/196] Batch: 0.0705 (0.0798) Data: 0.0575 (0.0530) Loss: 1.7019 (1.5504)\n",
            "TRAIN(198): [120/196] Batch: 0.0808 (0.0796) Data: 0.0561 (0.0528) Loss: 1.5570 (1.5545)\n",
            "TRAIN(198): [130/196] Batch: 0.0764 (0.0795) Data: 0.0505 (0.0522) Loss: 1.5520 (1.5525)\n",
            "TRAIN(198): [140/196] Batch: 0.0689 (0.0792) Data: 0.0572 (0.0519) Loss: 1.5372 (1.5540)\n",
            "TRAIN(198): [150/196] Batch: 0.0692 (0.0791) Data: 0.0483 (0.0517) Loss: 1.6745 (1.5505)\n",
            "TRAIN(198): [160/196] Batch: 0.0767 (0.0790) Data: 0.0509 (0.0514) Loss: 1.5071 (1.5491)\n",
            "TRAIN(198): [170/196] Batch: 0.0722 (0.0789) Data: 0.0479 (0.0508) Loss: 1.4501 (1.5484)\n",
            "TRAIN(198): [180/196] Batch: 0.0789 (0.0787) Data: 0.0548 (0.0507) Loss: 1.5162 (1.5477)\n",
            "TRAIN(198): [190/196] Batch: 0.0759 (0.0786) Data: 0.0624 (0.0507) Loss: 1.6992 (1.5463)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(198)         0:00:15         0:00:09         0:00:05          1.5469\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(199): [ 10/196] Batch: 0.0686 (0.1208) Data: 0.0549 (0.0817) Loss: 1.6033 (1.5361)\n",
            "TRAIN(199): [ 20/196] Batch: 0.0719 (0.0989) Data: 0.0553 (0.0665) Loss: 1.5699 (1.5264)\n",
            "TRAIN(199): [ 30/196] Batch: 0.0719 (0.0915) Data: 0.0560 (0.0614) Loss: 1.5334 (1.5400)\n",
            "TRAIN(199): [ 40/196] Batch: 0.0755 (0.0878) Data: 0.0556 (0.0586) Loss: 1.2809 (1.5468)\n",
            "TRAIN(199): [ 50/196] Batch: 0.0724 (0.0855) Data: 0.0588 (0.0570) Loss: 1.5917 (1.5498)\n",
            "TRAIN(199): [ 60/196] Batch: 0.0874 (0.0841) Data: 0.0507 (0.0562) Loss: 1.5285 (1.5569)\n",
            "TRAIN(199): [ 70/196] Batch: 0.0809 (0.0830) Data: 0.0498 (0.0552) Loss: 1.6493 (1.5660)\n",
            "TRAIN(199): [ 80/196] Batch: 0.0752 (0.0821) Data: 0.0569 (0.0546) Loss: 1.5914 (1.5647)\n",
            "TRAIN(199): [ 90/196] Batch: 0.0701 (0.0814) Data: 0.0624 (0.0541) Loss: 1.5371 (1.5648)\n",
            "TRAIN(199): [100/196] Batch: 0.0835 (0.0810) Data: 0.0533 (0.0537) Loss: 1.4641 (1.5646)\n",
            "TRAIN(199): [110/196] Batch: 0.0789 (0.0807) Data: 0.0407 (0.0530) Loss: 1.4934 (1.5609)\n",
            "TRAIN(199): [120/196] Batch: 0.0798 (0.0803) Data: 0.0539 (0.0525) Loss: 1.6432 (1.5572)\n",
            "TRAIN(199): [130/196] Batch: 0.0794 (0.0802) Data: 0.0442 (0.0519) Loss: 1.6464 (1.5581)\n",
            "TRAIN(199): [140/196] Batch: 0.0794 (0.0800) Data: 0.0553 (0.0516) Loss: 1.6090 (1.5559)\n",
            "TRAIN(199): [150/196] Batch: 0.0595 (0.0800) Data: 0.0549 (0.0512) Loss: 1.4602 (1.5530)\n",
            "TRAIN(199): [160/196] Batch: 0.0784 (0.0798) Data: 0.0558 (0.0511) Loss: 1.6676 (1.5573)\n",
            "TRAIN(199): [170/196] Batch: 0.0700 (0.0795) Data: 0.0575 (0.0510) Loss: 1.5873 (1.5570)\n",
            "TRAIN(199): [180/196] Batch: 0.0821 (0.0794) Data: 0.0561 (0.0510) Loss: 1.7040 (1.5579)\n",
            "TRAIN(199): [190/196] Batch: 0.0750 (0.0792) Data: 0.0617 (0.0511) Loss: 1.5228 (1.5578)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(199)         0:00:15         0:00:10         0:00:05          1.5571\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(200): [ 10/196] Batch: 0.0647 (0.1189) Data: 0.0618 (0.0875) Loss: 1.5177 (1.5642)\n",
            "TRAIN(200): [ 20/196] Batch: 0.0699 (0.0980) Data: 0.0559 (0.0702) Loss: 1.4839 (1.5746)\n",
            "TRAIN(200): [ 30/196] Batch: 0.0664 (0.0906) Data: 0.0622 (0.0647) Loss: 1.5350 (1.5572)\n",
            "TRAIN(200): [ 40/196] Batch: 0.0831 (0.0872) Data: 0.0576 (0.0608) Loss: 1.5927 (1.5623)\n",
            "TRAIN(200): [ 50/196] Batch: 0.0828 (0.0851) Data: 0.0511 (0.0587) Loss: 1.4494 (1.5588)\n",
            "TRAIN(200): [ 60/196] Batch: 0.0686 (0.0836) Data: 0.0558 (0.0573) Loss: 1.4827 (1.5628)\n",
            "TRAIN(200): [ 70/196] Batch: 0.0704 (0.0826) Data: 0.0560 (0.0560) Loss: 1.6627 (1.5623)\n",
            "TRAIN(200): [ 80/196] Batch: 0.0766 (0.0819) Data: 0.0530 (0.0549) Loss: 1.5041 (1.5652)\n",
            "TRAIN(200): [ 90/196] Batch: 0.0841 (0.0813) Data: 0.0478 (0.0539) Loss: 1.4738 (1.5657)\n",
            "TRAIN(200): [100/196] Batch: 0.0752 (0.0808) Data: 0.0508 (0.0531) Loss: 1.4671 (1.5646)\n",
            "TRAIN(200): [110/196] Batch: 0.0739 (0.0805) Data: 0.0517 (0.0523) Loss: 1.6357 (1.5626)\n",
            "TRAIN(200): [120/196] Batch: 0.0710 (0.0801) Data: 0.0591 (0.0520) Loss: 1.4204 (1.5656)\n",
            "TRAIN(200): [130/196] Batch: 0.0721 (0.0799) Data: 0.0528 (0.0517) Loss: 1.5722 (1.5684)\n",
            "TRAIN(200): [140/196] Batch: 0.0780 (0.0796) Data: 0.0545 (0.0515) Loss: 1.3918 (1.5656)\n",
            "TRAIN(200): [150/196] Batch: 0.0815 (0.0794) Data: 0.0558 (0.0515) Loss: 1.4764 (1.5628)\n",
            "TRAIN(200): [160/196] Batch: 0.0709 (0.0792) Data: 0.0552 (0.0513) Loss: 1.5650 (1.5626)\n",
            "TRAIN(200): [170/196] Batch: 0.0744 (0.0791) Data: 0.0576 (0.0512) Loss: 1.4768 (1.5629)\n",
            "TRAIN(200): [180/196] Batch: 0.0713 (0.0789) Data: 0.0612 (0.0513) Loss: 1.5126 (1.5603)\n",
            "TRAIN(200): [190/196] Batch: 0.0764 (0.0787) Data: 0.0627 (0.0514) Loss: 1.5501 (1.5599)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(200)         0:00:15         0:00:10         0:00:05          1.5590\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(201): [ 10/196] Batch: 0.0841 (0.1202) Data: 0.0517 (0.0861) Loss: 1.5232 (1.5723)\n",
            "TRAIN(201): [ 20/196] Batch: 0.0823 (0.0988) Data: 0.0480 (0.0690) Loss: 1.4727 (1.5616)\n",
            "TRAIN(201): [ 30/196] Batch: 0.0691 (0.0911) Data: 0.0559 (0.0625) Loss: 1.4987 (1.5685)\n",
            "TRAIN(201): [ 40/196] Batch: 0.0765 (0.0875) Data: 0.0530 (0.0593) Loss: 1.6201 (1.5717)\n",
            "TRAIN(201): [ 50/196] Batch: 0.0757 (0.0851) Data: 0.0581 (0.0576) Loss: 1.5402 (1.5722)\n",
            "TRAIN(201): [ 60/196] Batch: 0.0762 (0.0837) Data: 0.0532 (0.0565) Loss: 1.4145 (1.5630)\n",
            "TRAIN(201): [ 70/196] Batch: 0.0742 (0.0828) Data: 0.0498 (0.0549) Loss: 1.5888 (1.5650)\n",
            "TRAIN(201): [ 80/196] Batch: 0.0811 (0.0821) Data: 0.0478 (0.0538) Loss: 1.4492 (1.5626)\n",
            "TRAIN(201): [ 90/196] Batch: 0.0727 (0.0813) Data: 0.0552 (0.0528) Loss: 1.4817 (1.5621)\n",
            "TRAIN(201): [100/196] Batch: 0.0774 (0.0810) Data: 0.0493 (0.0519) Loss: 1.6184 (1.5604)\n",
            "TRAIN(201): [110/196] Batch: 0.0689 (0.0806) Data: 0.0542 (0.0514) Loss: 1.5316 (1.5613)\n",
            "TRAIN(201): [120/196] Batch: 0.0757 (0.0802) Data: 0.0601 (0.0514) Loss: 1.7104 (1.5614)\n",
            "TRAIN(201): [130/196] Batch: 0.0652 (0.0799) Data: 0.0611 (0.0513) Loss: 1.5650 (1.5598)\n",
            "TRAIN(201): [140/196] Batch: 0.0651 (0.0796) Data: 0.0625 (0.0510) Loss: 1.6461 (1.5612)\n",
            "TRAIN(201): [150/196] Batch: 0.0700 (0.0794) Data: 0.0554 (0.0509) Loss: 1.5855 (1.5607)\n",
            "TRAIN(201): [160/196] Batch: 0.0754 (0.0792) Data: 0.0611 (0.0508) Loss: 1.7093 (1.5585)\n",
            "TRAIN(201): [170/196] Batch: 0.0807 (0.0791) Data: 0.0516 (0.0507) Loss: 1.6185 (1.5580)\n",
            "TRAIN(201): [180/196] Batch: 0.0843 (0.0789) Data: 0.0553 (0.0507) Loss: 1.5915 (1.5585)\n",
            "TRAIN(201): [190/196] Batch: 0.0752 (0.0787) Data: 0.0613 (0.0507) Loss: 1.4545 (1.5603)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(201)         0:00:15         0:00:09         0:00:05          1.5612\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(202): [ 10/196] Batch: 0.0711 (0.1219) Data: 0.0552 (0.0873) Loss: 1.6018 (1.5694)\n",
            "TRAIN(202): [ 20/196] Batch: 0.0715 (0.0990) Data: 0.0604 (0.0689) Loss: 1.6218 (1.5792)\n",
            "TRAIN(202): [ 30/196] Batch: 0.0811 (0.0917) Data: 0.0509 (0.0632) Loss: 1.4346 (1.5640)\n",
            "TRAIN(202): [ 40/196] Batch: 0.0698 (0.0877) Data: 0.0574 (0.0593) Loss: 1.5278 (1.5578)\n",
            "TRAIN(202): [ 50/196] Batch: 0.0670 (0.0857) Data: 0.0518 (0.0566) Loss: 1.6600 (1.5596)\n",
            "TRAIN(202): [ 60/196] Batch: 0.0885 (0.0844) Data: 0.0451 (0.0549) Loss: 1.5149 (1.5571)\n",
            "TRAIN(202): [ 70/196] Batch: 0.0713 (0.0833) Data: 0.0524 (0.0530) Loss: 1.6583 (1.5656)\n",
            "TRAIN(202): [ 80/196] Batch: 0.0701 (0.0827) Data: 0.0475 (0.0520) Loss: 1.4640 (1.5614)\n",
            "TRAIN(202): [ 90/196] Batch: 0.0854 (0.0819) Data: 0.0516 (0.0515) Loss: 1.6275 (1.5533)\n",
            "TRAIN(202): [100/196] Batch: 0.0666 (0.0812) Data: 0.0633 (0.0516) Loss: 1.6425 (1.5549)\n",
            "TRAIN(202): [110/196] Batch: 0.0871 (0.0809) Data: 0.0493 (0.0515) Loss: 1.6849 (1.5537)\n",
            "TRAIN(202): [120/196] Batch: 0.0658 (0.0804) Data: 0.0560 (0.0513) Loss: 1.3821 (1.5557)\n",
            "TRAIN(202): [130/196] Batch: 0.0763 (0.0800) Data: 0.0628 (0.0513) Loss: 1.6484 (1.5573)\n",
            "TRAIN(202): [140/196] Batch: 0.0781 (0.0798) Data: 0.0530 (0.0510) Loss: 1.7761 (1.5606)\n",
            "TRAIN(202): [150/196] Batch: 0.0810 (0.0796) Data: 0.0493 (0.0508) Loss: 1.6980 (1.5637)\n",
            "TRAIN(202): [160/196] Batch: 0.0719 (0.0794) Data: 0.0558 (0.0508) Loss: 1.5657 (1.5642)\n",
            "TRAIN(202): [170/196] Batch: 0.0710 (0.0792) Data: 0.0541 (0.0508) Loss: 1.4806 (1.5616)\n",
            "TRAIN(202): [180/196] Batch: 0.0804 (0.0790) Data: 0.0515 (0.0507) Loss: 1.4763 (1.5592)\n",
            "TRAIN(202): [190/196] Batch: 0.0758 (0.0788) Data: 0.0603 (0.0508) Loss: 1.4899 (1.5581)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(202)         0:00:15         0:00:09         0:00:05          1.5591\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(203): [ 10/196] Batch: 0.0743 (0.1193) Data: 0.0499 (0.0842) Loss: 1.4046 (1.5113)\n",
            "TRAIN(203): [ 20/196] Batch: 0.0726 (0.0981) Data: 0.0490 (0.0673) Loss: 1.5467 (1.5387)\n",
            "TRAIN(203): [ 30/196] Batch: 0.0754 (0.0908) Data: 0.0556 (0.0610) Loss: 1.5604 (1.5541)\n",
            "TRAIN(203): [ 40/196] Batch: 0.0660 (0.0876) Data: 0.0490 (0.0567) Loss: 1.6003 (1.5573)\n",
            "TRAIN(203): [ 50/196] Batch: 0.0800 (0.0853) Data: 0.0534 (0.0545) Loss: 1.3810 (1.5574)\n",
            "TRAIN(203): [ 60/196] Batch: 0.0726 (0.0840) Data: 0.0462 (0.0536) Loss: 1.6205 (1.5530)\n",
            "TRAIN(203): [ 70/196] Batch: 0.0811 (0.0829) Data: 0.0505 (0.0522) Loss: 1.5915 (1.5586)\n",
            "TRAIN(203): [ 80/196] Batch: 0.0821 (0.0820) Data: 0.0497 (0.0519) Loss: 1.5218 (1.5606)\n",
            "TRAIN(203): [ 90/196] Batch: 0.0707 (0.0813) Data: 0.0571 (0.0516) Loss: 1.5423 (1.5625)\n",
            "TRAIN(203): [100/196] Batch: 0.0678 (0.0807) Data: 0.0602 (0.0515) Loss: 1.5791 (1.5636)\n",
            "TRAIN(203): [110/196] Batch: 0.0710 (0.0803) Data: 0.0577 (0.0513) Loss: 1.6707 (1.5649)\n",
            "TRAIN(203): [120/196] Batch: 0.0799 (0.0800) Data: 0.0496 (0.0512) Loss: 1.4475 (1.5618)\n",
            "TRAIN(203): [130/196] Batch: 0.0670 (0.0796) Data: 0.0624 (0.0511) Loss: 1.5307 (1.5618)\n",
            "TRAIN(203): [140/196] Batch: 0.0844 (0.0795) Data: 0.0501 (0.0510) Loss: 1.5994 (1.5608)\n",
            "TRAIN(203): [150/196] Batch: 0.0811 (0.0793) Data: 0.0485 (0.0508) Loss: 1.6904 (1.5635)\n",
            "TRAIN(203): [160/196] Batch: 0.0795 (0.0791) Data: 0.0505 (0.0507) Loss: 1.3337 (1.5626)\n",
            "TRAIN(203): [170/196] Batch: 0.0697 (0.0789) Data: 0.0603 (0.0506) Loss: 1.5082 (1.5619)\n",
            "TRAIN(203): [180/196] Batch: 0.0788 (0.0788) Data: 0.0509 (0.0505) Loss: 1.4933 (1.5610)\n",
            "TRAIN(203): [190/196] Batch: 0.0757 (0.0786) Data: 0.0608 (0.0505) Loss: 1.7028 (1.5597)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(203)         0:00:15         0:00:09         0:00:05          1.5585\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(204): [ 10/196] Batch: 0.0805 (0.1401) Data: 0.0487 (0.0993) Loss: 1.5572 (1.5089)\n",
            "TRAIN(204): [ 20/196] Batch: 0.0761 (0.1090) Data: 0.0600 (0.0748) Loss: 1.7767 (1.5502)\n",
            "TRAIN(204): [ 30/196] Batch: 0.0773 (0.0991) Data: 0.0502 (0.0652) Loss: 1.5539 (1.5609)\n",
            "TRAIN(204): [ 40/196] Batch: 0.0759 (0.0936) Data: 0.0470 (0.0601) Loss: 1.5378 (1.5344)\n",
            "TRAIN(204): [ 50/196] Batch: 0.0883 (0.0900) Data: 0.0501 (0.0575) Loss: 1.6536 (1.5467)\n",
            "TRAIN(204): [ 60/196] Batch: 0.0854 (0.0876) Data: 0.0499 (0.0560) Loss: 1.5769 (1.5438)\n",
            "TRAIN(204): [ 70/196] Batch: 0.0758 (0.0860) Data: 0.0507 (0.0553) Loss: 1.5889 (1.5437)\n",
            "TRAIN(204): [ 80/196] Batch: 0.0801 (0.0847) Data: 0.0510 (0.0547) Loss: 1.4707 (1.5410)\n",
            "TRAIN(204): [ 90/196] Batch: 0.0787 (0.0837) Data: 0.0583 (0.0541) Loss: 1.5625 (1.5445)\n",
            "TRAIN(204): [100/196] Batch: 0.0848 (0.0830) Data: 0.0527 (0.0537) Loss: 1.5587 (1.5471)\n",
            "TRAIN(204): [110/196] Batch: 0.0687 (0.0823) Data: 0.0611 (0.0533) Loss: 1.5914 (1.5458)\n",
            "TRAIN(204): [120/196] Batch: 0.0823 (0.0818) Data: 0.0560 (0.0530) Loss: 1.6123 (1.5479)\n",
            "TRAIN(204): [130/196] Batch: 0.0824 (0.0815) Data: 0.0496 (0.0527) Loss: 1.5303 (1.5477)\n",
            "TRAIN(204): [140/196] Batch: 0.0676 (0.0810) Data: 0.0552 (0.0524) Loss: 1.4131 (1.5471)\n",
            "TRAIN(204): [150/196] Batch: 0.0720 (0.0807) Data: 0.0556 (0.0522) Loss: 1.5799 (1.5483)\n",
            "TRAIN(204): [160/196] Batch: 0.0806 (0.0805) Data: 0.0507 (0.0520) Loss: 1.5314 (1.5482)\n",
            "TRAIN(204): [170/196] Batch: 0.0789 (0.0802) Data: 0.0524 (0.0518) Loss: 1.3984 (1.5488)\n",
            "TRAIN(204): [180/196] Batch: 0.0757 (0.0800) Data: 0.0565 (0.0516) Loss: 1.5820 (1.5457)\n",
            "TRAIN(204): [190/196] Batch: 0.0742 (0.0799) Data: 0.0619 (0.0512) Loss: 1.4753 (1.5467)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(204)         0:00:15         0:00:10         0:00:05          1.5467\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(205): [ 10/196] Batch: 0.0747 (0.1323) Data: 0.0537 (0.0914) Loss: 1.6332 (1.6039)\n",
            "TRAIN(205): [ 20/196] Batch: 0.0854 (0.1049) Data: 0.0518 (0.0712) Loss: 1.5926 (1.6048)\n",
            "TRAIN(205): [ 30/196] Batch: 0.0675 (0.0952) Data: 0.0576 (0.0641) Loss: 1.6066 (1.6012)\n",
            "TRAIN(205): [ 40/196] Batch: 0.0779 (0.0905) Data: 0.0543 (0.0608) Loss: 1.6564 (1.5861)\n",
            "TRAIN(205): [ 50/196] Batch: 0.0692 (0.0876) Data: 0.0554 (0.0581) Loss: 1.7447 (1.5853)\n",
            "TRAIN(205): [ 60/196] Batch: 0.0694 (0.0857) Data: 0.0564 (0.0565) Loss: 1.6368 (1.5825)\n",
            "TRAIN(205): [ 70/196] Batch: 0.0802 (0.0844) Data: 0.0488 (0.0554) Loss: 1.8567 (1.5801)\n",
            "TRAIN(205): [ 80/196] Batch: 0.0670 (0.0833) Data: 0.0612 (0.0548) Loss: 1.5293 (1.5840)\n",
            "TRAIN(205): [ 90/196] Batch: 0.0773 (0.0825) Data: 0.0621 (0.0543) Loss: 1.4277 (1.5774)\n",
            "TRAIN(205): [100/196] Batch: 0.0717 (0.0820) Data: 0.0528 (0.0538) Loss: 1.4597 (1.5712)\n",
            "TRAIN(205): [110/196] Batch: 0.0749 (0.0814) Data: 0.0546 (0.0532) Loss: 1.5864 (1.5694)\n",
            "TRAIN(205): [120/196] Batch: 0.0687 (0.0810) Data: 0.0568 (0.0528) Loss: 1.4845 (1.5679)\n",
            "TRAIN(205): [130/196] Batch: 0.0805 (0.0807) Data: 0.0531 (0.0527) Loss: 1.6599 (1.5623)\n",
            "TRAIN(205): [140/196] Batch: 0.0756 (0.0804) Data: 0.0510 (0.0523) Loss: 1.3935 (1.5603)\n",
            "TRAIN(205): [150/196] Batch: 0.0928 (0.0802) Data: 0.0363 (0.0517) Loss: 1.4305 (1.5577)\n",
            "TRAIN(205): [160/196] Batch: 0.0743 (0.0800) Data: 0.0497 (0.0512) Loss: 1.4498 (1.5584)\n",
            "TRAIN(205): [170/196] Batch: 0.0684 (0.0798) Data: 0.0538 (0.0508) Loss: 1.3920 (1.5538)\n",
            "TRAIN(205): [180/196] Batch: 0.0943 (0.0798) Data: 0.0430 (0.0506) Loss: 1.5230 (1.5546)\n",
            "TRAIN(205): [190/196] Batch: 0.0748 (0.0796) Data: 0.0600 (0.0505) Loss: 1.4334 (1.5551)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(205)         0:00:15         0:00:09         0:00:05          1.5554\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(206): [ 10/196] Batch: 0.0761 (0.1210) Data: 0.0498 (0.0850) Loss: 1.7431 (1.5745)\n",
            "TRAIN(206): [ 20/196] Batch: 0.0682 (0.0985) Data: 0.0565 (0.0681) Loss: 1.5814 (1.5628)\n",
            "TRAIN(206): [ 30/196] Batch: 0.0763 (0.0913) Data: 0.0506 (0.0625) Loss: 1.6639 (1.5526)\n",
            "TRAIN(206): [ 40/196] Batch: 0.0853 (0.0875) Data: 0.0529 (0.0593) Loss: 1.4396 (1.5559)\n",
            "TRAIN(206): [ 50/196] Batch: 0.0718 (0.0851) Data: 0.0555 (0.0571) Loss: 1.5383 (1.5559)\n",
            "TRAIN(206): [ 60/196] Batch: 0.0686 (0.0837) Data: 0.0552 (0.0556) Loss: 1.6371 (1.5554)\n",
            "TRAIN(206): [ 70/196] Batch: 0.0767 (0.0827) Data: 0.0490 (0.0547) Loss: 1.7077 (1.5602)\n",
            "TRAIN(206): [ 80/196] Batch: 0.0708 (0.0818) Data: 0.0630 (0.0540) Loss: 1.6324 (1.5594)\n",
            "TRAIN(206): [ 90/196] Batch: 0.0763 (0.0813) Data: 0.0522 (0.0534) Loss: 1.5575 (1.5610)\n",
            "TRAIN(206): [100/196] Batch: 0.0709 (0.0808) Data: 0.0561 (0.0532) Loss: 1.5345 (1.5644)\n",
            "TRAIN(206): [110/196] Batch: 0.0835 (0.0804) Data: 0.0490 (0.0530) Loss: 1.5636 (1.5642)\n",
            "TRAIN(206): [120/196] Batch: 0.0775 (0.0801) Data: 0.0511 (0.0525) Loss: 1.6353 (1.5621)\n",
            "TRAIN(206): [130/196] Batch: 0.0688 (0.0798) Data: 0.0505 (0.0517) Loss: 1.5279 (1.5636)\n",
            "TRAIN(206): [140/196] Batch: 0.0752 (0.0797) Data: 0.0522 (0.0511) Loss: 1.7181 (1.5650)\n",
            "TRAIN(206): [150/196] Batch: 0.0730 (0.0795) Data: 0.0482 (0.0506) Loss: 1.4521 (1.5678)\n",
            "TRAIN(206): [160/196] Batch: 0.0653 (0.0793) Data: 0.0668 (0.0505) Loss: 1.5886 (1.5728)\n",
            "TRAIN(206): [170/196] Batch: 0.0813 (0.0793) Data: 0.0461 (0.0500) Loss: 1.5398 (1.5740)\n",
            "TRAIN(206): [180/196] Batch: 0.0775 (0.0791) Data: 0.0540 (0.0499) Loss: 1.5458 (1.5752)\n",
            "TRAIN(206): [190/196] Batch: 0.0754 (0.0789) Data: 0.0603 (0.0500) Loss: 1.6287 (1.5731)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(206)         0:00:15         0:00:09         0:00:05          1.5732\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(207): [ 10/196] Batch: 0.0699 (0.1198) Data: 0.0541 (0.0849) Loss: 1.5594 (1.5984)\n",
            "TRAIN(207): [ 20/196] Batch: 0.0726 (0.0986) Data: 0.0506 (0.0674) Loss: 1.6425 (1.5623)\n",
            "TRAIN(207): [ 30/196] Batch: 0.0674 (0.0910) Data: 0.0589 (0.0616) Loss: 1.5557 (1.5433)\n",
            "TRAIN(207): [ 40/196] Batch: 0.0687 (0.0873) Data: 0.0623 (0.0589) Loss: 1.7354 (1.5547)\n",
            "TRAIN(207): [ 50/196] Batch: 0.0721 (0.0852) Data: 0.0582 (0.0569) Loss: 1.6656 (1.5534)\n",
            "TRAIN(207): [ 60/196] Batch: 0.0799 (0.0838) Data: 0.0505 (0.0553) Loss: 1.4977 (1.5578)\n",
            "TRAIN(207): [ 70/196] Batch: 0.0730 (0.0827) Data: 0.0546 (0.0547) Loss: 1.7195 (1.5613)\n",
            "TRAIN(207): [ 80/196] Batch: 0.0751 (0.0819) Data: 0.0559 (0.0540) Loss: 1.6930 (1.5595)\n",
            "TRAIN(207): [ 90/196] Batch: 0.0760 (0.0813) Data: 0.0508 (0.0534) Loss: 1.5845 (1.5610)\n",
            "TRAIN(207): [100/196] Batch: 0.0790 (0.0808) Data: 0.0527 (0.0528) Loss: 1.5232 (1.5639)\n",
            "TRAIN(207): [110/196] Batch: 0.0729 (0.0805) Data: 0.0510 (0.0523) Loss: 1.5807 (1.5639)\n",
            "TRAIN(207): [120/196] Batch: 0.0763 (0.0802) Data: 0.0506 (0.0516) Loss: 1.5492 (1.5647)\n",
            "TRAIN(207): [130/196] Batch: 0.0771 (0.0800) Data: 0.0581 (0.0512) Loss: 1.5866 (1.5642)\n",
            "TRAIN(207): [140/196] Batch: 0.0777 (0.0798) Data: 0.0510 (0.0507) Loss: 1.4286 (1.5642)\n",
            "TRAIN(207): [150/196] Batch: 0.0759 (0.0796) Data: 0.0490 (0.0501) Loss: 1.5753 (1.5627)\n",
            "TRAIN(207): [160/196] Batch: 0.0705 (0.0793) Data: 0.0609 (0.0500) Loss: 1.5693 (1.5621)\n",
            "TRAIN(207): [170/196] Batch: 0.0692 (0.0792) Data: 0.0609 (0.0500) Loss: 1.6394 (1.5621)\n",
            "TRAIN(207): [180/196] Batch: 0.0840 (0.0790) Data: 0.0509 (0.0498) Loss: 1.6757 (1.5607)\n",
            "TRAIN(207): [190/196] Batch: 0.0754 (0.0788) Data: 0.0614 (0.0499) Loss: 1.4927 (1.5610)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(207)         0:00:15         0:00:09         0:00:05          1.5604\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(208): [ 10/196] Batch: 0.0816 (0.1212) Data: 0.0481 (0.0836) Loss: 1.5394 (1.5335)\n",
            "TRAIN(208): [ 20/196] Batch: 0.0726 (0.0984) Data: 0.0602 (0.0679) Loss: 1.5488 (1.5528)\n",
            "TRAIN(208): [ 30/196] Batch: 0.0670 (0.0911) Data: 0.0587 (0.0621) Loss: 1.3605 (1.5477)\n",
            "TRAIN(208): [ 40/196] Batch: 0.0730 (0.0875) Data: 0.0554 (0.0587) Loss: 1.5157 (1.5398)\n",
            "TRAIN(208): [ 50/196] Batch: 0.0696 (0.0852) Data: 0.0561 (0.0566) Loss: 1.6302 (1.5344)\n",
            "TRAIN(208): [ 60/196] Batch: 0.0806 (0.0838) Data: 0.0491 (0.0551) Loss: 1.8038 (1.5363)\n",
            "TRAIN(208): [ 70/196] Batch: 0.0753 (0.0826) Data: 0.0611 (0.0544) Loss: 1.4205 (1.5277)\n",
            "TRAIN(208): [ 80/196] Batch: 0.0743 (0.0820) Data: 0.0458 (0.0533) Loss: 1.4700 (1.5295)\n",
            "TRAIN(208): [ 90/196] Batch: 0.0712 (0.0813) Data: 0.0533 (0.0523) Loss: 1.6364 (1.5309)\n",
            "TRAIN(208): [100/196] Batch: 0.0745 (0.0809) Data: 0.0546 (0.0516) Loss: 1.5305 (1.5331)\n",
            "TRAIN(208): [110/196] Batch: 0.0824 (0.0806) Data: 0.0411 (0.0507) Loss: 1.5114 (1.5401)\n",
            "TRAIN(208): [120/196] Batch: 0.0786 (0.0804) Data: 0.0393 (0.0500) Loss: 1.4427 (1.5392)\n",
            "TRAIN(208): [130/196] Batch: 0.0766 (0.0800) Data: 0.0523 (0.0496) Loss: 1.6337 (1.5414)\n",
            "TRAIN(208): [140/196] Batch: 0.0816 (0.0797) Data: 0.0541 (0.0496) Loss: 1.4198 (1.5457)\n",
            "TRAIN(208): [150/196] Batch: 0.0690 (0.0795) Data: 0.0555 (0.0497) Loss: 1.7436 (1.5442)\n",
            "TRAIN(208): [160/196] Batch: 0.0725 (0.0793) Data: 0.0553 (0.0496) Loss: 1.7052 (1.5447)\n",
            "TRAIN(208): [170/196] Batch: 0.0829 (0.0791) Data: 0.0564 (0.0496) Loss: 1.6350 (1.5476)\n",
            "TRAIN(208): [180/196] Batch: 0.0731 (0.0789) Data: 0.0614 (0.0496) Loss: 1.5165 (1.5471)\n",
            "TRAIN(208): [190/196] Batch: 0.0748 (0.0787) Data: 0.0626 (0.0498) Loss: 1.5793 (1.5479)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(208)         0:00:15         0:00:09         0:00:05          1.5452\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(209): [ 10/196] Batch: 0.0741 (0.1208) Data: 0.0549 (0.0867) Loss: 1.4112 (1.5477)\n",
            "TRAIN(209): [ 20/196] Batch: 0.0802 (0.0987) Data: 0.0536 (0.0694) Loss: 1.4325 (1.5298)\n",
            "TRAIN(209): [ 30/196] Batch: 0.0795 (0.0914) Data: 0.0507 (0.0626) Loss: 1.5357 (1.5482)\n",
            "TRAIN(209): [ 40/196] Batch: 0.0682 (0.0874) Data: 0.0607 (0.0592) Loss: 1.6605 (1.5607)\n",
            "TRAIN(209): [ 50/196] Batch: 0.0861 (0.0852) Data: 0.0507 (0.0573) Loss: 1.4664 (1.5556)\n",
            "TRAIN(209): [ 60/196] Batch: 0.0764 (0.0837) Data: 0.0512 (0.0560) Loss: 1.3836 (1.5519)\n",
            "TRAIN(209): [ 70/196] Batch: 0.0743 (0.0828) Data: 0.0499 (0.0547) Loss: 1.5753 (1.5526)\n",
            "TRAIN(209): [ 80/196] Batch: 0.0896 (0.0821) Data: 0.0438 (0.0536) Loss: 1.5195 (1.5570)\n",
            "TRAIN(209): [ 90/196] Batch: 0.0560 (0.0816) Data: 0.0492 (0.0525) Loss: 1.6561 (1.5657)\n",
            "TRAIN(209): [100/196] Batch: 0.0764 (0.0812) Data: 0.0492 (0.0517) Loss: 1.4111 (1.5639)\n",
            "TRAIN(209): [110/196] Batch: 0.0787 (0.0808) Data: 0.0504 (0.0512) Loss: 1.5824 (1.5637)\n",
            "TRAIN(209): [120/196] Batch: 0.0786 (0.0804) Data: 0.0532 (0.0511) Loss: 1.4662 (1.5593)\n",
            "TRAIN(209): [130/196] Batch: 0.0753 (0.0800) Data: 0.0523 (0.0509) Loss: 1.4951 (1.5629)\n",
            "TRAIN(209): [140/196] Batch: 0.0776 (0.0798) Data: 0.0550 (0.0508) Loss: 1.6670 (1.5642)\n",
            "TRAIN(209): [150/196] Batch: 0.0785 (0.0795) Data: 0.0523 (0.0507) Loss: 1.7242 (1.5689)\n",
            "TRAIN(209): [160/196] Batch: 0.0721 (0.0793) Data: 0.0545 (0.0505) Loss: 1.5894 (1.5715)\n",
            "TRAIN(209): [170/196] Batch: 0.0706 (0.0791) Data: 0.0554 (0.0504) Loss: 1.5546 (1.5720)\n",
            "TRAIN(209): [180/196] Batch: 0.0729 (0.0790) Data: 0.0531 (0.0503) Loss: 1.5902 (1.5709)\n",
            "TRAIN(209): [190/196] Batch: 0.0760 (0.0788) Data: 0.0610 (0.0503) Loss: 1.6372 (1.5691)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(209)         0:00:15         0:00:09         0:00:05          1.5676\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(210): [ 10/196] Batch: 0.0732 (0.1220) Data: 0.0487 (0.0811) Loss: 1.4871 (1.5462)\n",
            "TRAIN(210): [ 20/196] Batch: 0.0809 (0.0995) Data: 0.0485 (0.0661) Loss: 1.3160 (1.5378)\n",
            "TRAIN(210): [ 30/196] Batch: 0.0662 (0.0914) Data: 0.0587 (0.0606) Loss: 1.5059 (1.5588)\n",
            "TRAIN(210): [ 40/196] Batch: 0.0800 (0.0878) Data: 0.0471 (0.0571) Loss: 1.6799 (1.5608)\n",
            "TRAIN(210): [ 50/196] Batch: 0.0701 (0.0856) Data: 0.0537 (0.0544) Loss: 1.4597 (1.5511)\n",
            "TRAIN(210): [ 60/196] Batch: 0.0749 (0.0843) Data: 0.0482 (0.0525) Loss: 1.3775 (1.5534)\n",
            "TRAIN(210): [ 70/196] Batch: 0.0760 (0.0835) Data: 0.0425 (0.0512) Loss: 1.5338 (1.5584)\n",
            "TRAIN(210): [ 80/196] Batch: 0.0754 (0.0825) Data: 0.0619 (0.0506) Loss: 1.5811 (1.5587)\n",
            "TRAIN(210): [ 90/196] Batch: 0.0750 (0.0818) Data: 0.0619 (0.0507) Loss: 1.5859 (1.5557)\n",
            "TRAIN(210): [100/196] Batch: 0.0797 (0.0814) Data: 0.0497 (0.0505) Loss: 1.4384 (1.5592)\n",
            "TRAIN(210): [110/196] Batch: 0.0830 (0.0809) Data: 0.0467 (0.0501) Loss: 1.4961 (1.5505)\n",
            "TRAIN(210): [120/196] Batch: 0.0782 (0.0805) Data: 0.0543 (0.0499) Loss: 1.3430 (1.5499)\n",
            "TRAIN(210): [130/196] Batch: 0.0803 (0.0801) Data: 0.0507 (0.0498) Loss: 1.6681 (1.5481)\n",
            "TRAIN(210): [140/196] Batch: 0.0685 (0.0798) Data: 0.0558 (0.0497) Loss: 1.5125 (1.5476)\n",
            "TRAIN(210): [150/196] Batch: 0.0811 (0.0796) Data: 0.0466 (0.0495) Loss: 1.5965 (1.5483)\n",
            "TRAIN(210): [160/196] Batch: 0.0732 (0.0794) Data: 0.0548 (0.0494) Loss: 1.4483 (1.5457)\n",
            "TRAIN(210): [170/196] Batch: 0.0817 (0.0792) Data: 0.0484 (0.0493) Loss: 1.5268 (1.5460)\n",
            "TRAIN(210): [180/196] Batch: 0.0815 (0.0791) Data: 0.0481 (0.0491) Loss: 1.4620 (1.5476)\n",
            "TRAIN(210): [190/196] Batch: 0.0767 (0.0789) Data: 0.0607 (0.0492) Loss: 1.5187 (1.5489)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(210)         0:00:15         0:00:09         0:00:05          1.5502\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(211): [ 10/196] Batch: 0.0806 (0.1188) Data: 0.0548 (0.0803) Loss: 1.6541 (1.5380)\n",
            "TRAIN(211): [ 20/196] Batch: 0.0923 (0.0991) Data: 0.0365 (0.0619) Loss: 1.5962 (1.5491)\n",
            "TRAIN(211): [ 30/196] Batch: 0.0923 (0.0919) Data: 0.0413 (0.0568) Loss: 1.5773 (1.5506)\n",
            "TRAIN(211): [ 40/196] Batch: 0.0809 (0.0882) Data: 0.0444 (0.0539) Loss: 1.6026 (1.5511)\n",
            "TRAIN(211): [ 50/196] Batch: 0.0704 (0.0857) Data: 0.0548 (0.0518) Loss: 1.5289 (1.5540)\n",
            "TRAIN(211): [ 60/196] Batch: 0.0894 (0.0844) Data: 0.0404 (0.0505) Loss: 1.6343 (1.5540)\n",
            "TRAIN(211): [ 70/196] Batch: 0.0674 (0.0834) Data: 0.0472 (0.0496) Loss: 1.6854 (1.5574)\n",
            "TRAIN(211): [ 80/196] Batch: 0.0650 (0.0831) Data: 0.0520 (0.0483) Loss: 1.5442 (1.5535)\n",
            "TRAIN(211): [ 90/196] Batch: 0.0971 (0.0826) Data: 0.0369 (0.0477) Loss: 1.4514 (1.5477)\n",
            "TRAIN(211): [100/196] Batch: 0.0734 (0.0818) Data: 0.0545 (0.0476) Loss: 1.6457 (1.5523)\n",
            "TRAIN(211): [110/196] Batch: 0.0705 (0.0812) Data: 0.0618 (0.0478) Loss: 1.6093 (1.5559)\n",
            "TRAIN(211): [120/196] Batch: 0.0624 (0.0808) Data: 0.0612 (0.0481) Loss: 1.7036 (1.5569)\n",
            "TRAIN(211): [130/196] Batch: 0.0839 (0.0805) Data: 0.0502 (0.0481) Loss: 1.5885 (1.5540)\n",
            "TRAIN(211): [140/196] Batch: 0.0770 (0.0801) Data: 0.0552 (0.0483) Loss: 1.5470 (1.5524)\n",
            "TRAIN(211): [150/196] Batch: 0.0738 (0.0799) Data: 0.0507 (0.0483) Loss: 1.5299 (1.5538)\n",
            "TRAIN(211): [160/196] Batch: 0.0829 (0.0797) Data: 0.0488 (0.0482) Loss: 1.6581 (1.5571)\n",
            "TRAIN(211): [170/196] Batch: 0.0714 (0.0794) Data: 0.0552 (0.0482) Loss: 1.5393 (1.5565)\n",
            "TRAIN(211): [180/196] Batch: 0.0756 (0.0793) Data: 0.0519 (0.0482) Loss: 1.7394 (1.5567)\n",
            "TRAIN(211): [190/196] Batch: 0.0672 (0.0791) Data: 0.0568 (0.0481) Loss: 1.3741 (1.5554)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(211)         0:00:15         0:00:09         0:00:06          1.5549\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(212): [ 10/196] Batch: 0.0535 (0.1353) Data: 0.0582 (0.0893) Loss: 1.5956 (1.5377)\n",
            "TRAIN(212): [ 20/196] Batch: 0.0725 (0.1060) Data: 0.0547 (0.0686) Loss: 1.6455 (1.5683)\n",
            "TRAIN(212): [ 30/196] Batch: 0.0682 (0.0959) Data: 0.0573 (0.0618) Loss: 1.5459 (1.5542)\n",
            "TRAIN(212): [ 40/196] Batch: 0.0685 (0.0909) Data: 0.0575 (0.0584) Loss: 1.5926 (1.5612)\n",
            "TRAIN(212): [ 50/196] Batch: 0.0880 (0.0880) Data: 0.0510 (0.0564) Loss: 1.5311 (1.5603)\n",
            "TRAIN(212): [ 60/196] Batch: 0.0700 (0.0860) Data: 0.0533 (0.0550) Loss: 1.6635 (1.5623)\n",
            "TRAIN(212): [ 70/196] Batch: 0.0852 (0.0847) Data: 0.0504 (0.0541) Loss: 1.4762 (1.5588)\n",
            "TRAIN(212): [ 80/196] Batch: 0.0628 (0.0835) Data: 0.0607 (0.0536) Loss: 1.5611 (1.5544)\n",
            "TRAIN(212): [ 90/196] Batch: 0.0801 (0.0828) Data: 0.0500 (0.0530) Loss: 1.5515 (1.5492)\n",
            "TRAIN(212): [100/196] Batch: 0.0851 (0.0822) Data: 0.0448 (0.0523) Loss: 1.5538 (1.5507)\n",
            "TRAIN(212): [110/196] Batch: 0.0716 (0.0815) Data: 0.0558 (0.0518) Loss: 1.4037 (1.5490)\n",
            "TRAIN(212): [120/196] Batch: 0.0713 (0.0811) Data: 0.0543 (0.0515) Loss: 1.4652 (1.5493)\n",
            "TRAIN(212): [130/196] Batch: 0.0657 (0.0807) Data: 0.0576 (0.0512) Loss: 1.5654 (1.5516)\n",
            "TRAIN(212): [140/196] Batch: 0.0855 (0.0804) Data: 0.0504 (0.0511) Loss: 1.5654 (1.5516)\n",
            "TRAIN(212): [150/196] Batch: 0.0776 (0.0802) Data: 0.0460 (0.0505) Loss: 1.4855 (1.5527)\n",
            "TRAIN(212): [160/196] Batch: 0.0741 (0.0799) Data: 0.0540 (0.0503) Loss: 1.7151 (1.5543)\n",
            "TRAIN(212): [170/196] Batch: 0.0859 (0.0799) Data: 0.0415 (0.0500) Loss: 1.3687 (1.5541)\n",
            "TRAIN(212): [180/196] Batch: 0.0772 (0.0798) Data: 0.0490 (0.0496) Loss: 1.4688 (1.5549)\n",
            "TRAIN(212): [190/196] Batch: 0.0752 (0.0795) Data: 0.0602 (0.0496) Loss: 1.3881 (1.5530)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(212)         0:00:15         0:00:09         0:00:05          1.5535\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(213): [ 10/196] Batch: 0.0693 (0.1189) Data: 0.0537 (0.0805) Loss: 1.5500 (1.6347)\n",
            "TRAIN(213): [ 20/196] Batch: 0.0783 (0.0979) Data: 0.0506 (0.0656) Loss: 1.3028 (1.5808)\n",
            "TRAIN(213): [ 30/196] Batch: 0.0781 (0.0906) Data: 0.0536 (0.0600) Loss: 1.6016 (1.5774)\n",
            "TRAIN(213): [ 40/196] Batch: 0.0780 (0.0869) Data: 0.0556 (0.0574) Loss: 1.6113 (1.5674)\n",
            "TRAIN(213): [ 50/196] Batch: 0.0818 (0.0848) Data: 0.0485 (0.0559) Loss: 1.5415 (1.5614)\n",
            "TRAIN(213): [ 60/196] Batch: 0.0743 (0.0833) Data: 0.0504 (0.0546) Loss: 1.5686 (1.5631)\n",
            "TRAIN(213): [ 70/196] Batch: 0.0876 (0.0823) Data: 0.0501 (0.0539) Loss: 1.6364 (1.5677)\n",
            "TRAIN(213): [ 80/196] Batch: 0.0683 (0.0815) Data: 0.0568 (0.0532) Loss: 1.7728 (1.5690)\n",
            "TRAIN(213): [ 90/196] Batch: 0.0880 (0.0809) Data: 0.0496 (0.0529) Loss: 1.5107 (1.5675)\n",
            "TRAIN(213): [100/196] Batch: 0.0744 (0.0804) Data: 0.0489 (0.0526) Loss: 1.5890 (1.5737)\n",
            "TRAIN(213): [110/196] Batch: 0.0729 (0.0800) Data: 0.0520 (0.0522) Loss: 1.6757 (1.5728)\n",
            "TRAIN(213): [120/196] Batch: 0.0813 (0.0799) Data: 0.0419 (0.0514) Loss: 1.4267 (1.5711)\n",
            "TRAIN(213): [130/196] Batch: 0.0745 (0.0797) Data: 0.0483 (0.0510) Loss: 1.4199 (1.5650)\n",
            "TRAIN(213): [140/196] Batch: 0.0755 (0.0795) Data: 0.0473 (0.0504) Loss: 1.6140 (1.5661)\n",
            "TRAIN(213): [150/196] Batch: 0.0781 (0.0794) Data: 0.0445 (0.0499) Loss: 1.5330 (1.5633)\n",
            "TRAIN(213): [160/196] Batch: 0.0765 (0.0792) Data: 0.0554 (0.0497) Loss: 1.4964 (1.5617)\n",
            "TRAIN(213): [170/196] Batch: 0.0745 (0.0790) Data: 0.0516 (0.0495) Loss: 1.4541 (1.5579)\n",
            "TRAIN(213): [180/196] Batch: 0.0695 (0.0788) Data: 0.0548 (0.0494) Loss: 1.6022 (1.5599)\n",
            "TRAIN(213): [190/196] Batch: 0.0751 (0.0787) Data: 0.0603 (0.0494) Loss: 1.6990 (1.5622)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(213)         0:00:15         0:00:09         0:00:05          1.5626\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(214): [ 10/196] Batch: 0.0870 (0.1210) Data: 0.0477 (0.0821) Loss: 1.5105 (1.5451)\n",
            "TRAIN(214): [ 20/196] Batch: 0.0709 (0.0985) Data: 0.0545 (0.0672) Loss: 1.5169 (1.5495)\n",
            "TRAIN(214): [ 30/196] Batch: 0.0800 (0.0914) Data: 0.0496 (0.0616) Loss: 1.5999 (1.5353)\n",
            "TRAIN(214): [ 40/196] Batch: 0.0700 (0.0874) Data: 0.0556 (0.0587) Loss: 1.6573 (1.5380)\n",
            "TRAIN(214): [ 50/196] Batch: 0.0714 (0.0850) Data: 0.0631 (0.0568) Loss: 1.3894 (1.5495)\n",
            "TRAIN(214): [ 60/196] Batch: 0.0684 (0.0836) Data: 0.0601 (0.0553) Loss: 1.5331 (1.5595)\n",
            "TRAIN(214): [ 70/196] Batch: 0.0821 (0.0826) Data: 0.0528 (0.0543) Loss: 1.4083 (1.5587)\n",
            "TRAIN(214): [ 80/196] Batch: 0.0773 (0.0819) Data: 0.0502 (0.0533) Loss: 1.4914 (1.5595)\n",
            "TRAIN(214): [ 90/196] Batch: 0.0830 (0.0814) Data: 0.0413 (0.0522) Loss: 1.5573 (1.5611)\n",
            "TRAIN(214): [100/196] Batch: 0.0753 (0.0810) Data: 0.0576 (0.0513) Loss: 1.7648 (1.5617)\n",
            "TRAIN(214): [110/196] Batch: 0.0812 (0.0809) Data: 0.0468 (0.0510) Loss: 1.5782 (1.5588)\n",
            "TRAIN(214): [120/196] Batch: 0.0665 (0.0805) Data: 0.0587 (0.0503) Loss: 1.4447 (1.5580)\n",
            "TRAIN(214): [130/196] Batch: 0.0717 (0.0802) Data: 0.0549 (0.0502) Loss: 1.5909 (1.5653)\n",
            "TRAIN(214): [140/196] Batch: 0.0661 (0.0798) Data: 0.0614 (0.0500) Loss: 1.5732 (1.5674)\n",
            "TRAIN(214): [150/196] Batch: 0.0691 (0.0796) Data: 0.0545 (0.0500) Loss: 1.4778 (1.5680)\n",
            "TRAIN(214): [160/196] Batch: 0.0862 (0.0794) Data: 0.0506 (0.0500) Loss: 1.6977 (1.5651)\n",
            "TRAIN(214): [170/196] Batch: 0.0814 (0.0792) Data: 0.0555 (0.0499) Loss: 1.6443 (1.5666)\n",
            "TRAIN(214): [180/196] Batch: 0.0860 (0.0791) Data: 0.0502 (0.0498) Loss: 1.6477 (1.5654)\n",
            "TRAIN(214): [190/196] Batch: 0.0740 (0.0789) Data: 0.0626 (0.0498) Loss: 1.5690 (1.5649)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(214)         0:00:15         0:00:09         0:00:05          1.5671\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(215): [ 10/196] Batch: 0.0750 (0.1193) Data: 0.0494 (0.0816) Loss: 1.4243 (1.4940)\n",
            "TRAIN(215): [ 20/196] Batch: 0.0790 (0.0979) Data: 0.0516 (0.0667) Loss: 1.3911 (1.5096)\n",
            "TRAIN(215): [ 30/196] Batch: 0.0701 (0.0907) Data: 0.0550 (0.0610) Loss: 1.5913 (1.5138)\n",
            "TRAIN(215): [ 40/196] Batch: 0.0717 (0.0871) Data: 0.0555 (0.0581) Loss: 1.3968 (1.5110)\n",
            "TRAIN(215): [ 50/196] Batch: 0.0758 (0.0849) Data: 0.0571 (0.0564) Loss: 1.4833 (1.5169)\n",
            "TRAIN(215): [ 60/196] Batch: 0.0913 (0.0841) Data: 0.0322 (0.0543) Loss: 1.5202 (1.5248)\n",
            "TRAIN(215): [ 70/196] Batch: 0.0758 (0.0833) Data: 0.0540 (0.0529) Loss: 1.5681 (1.5205)\n",
            "TRAIN(215): [ 80/196] Batch: 0.1060 (0.0828) Data: 0.0335 (0.0519) Loss: 1.7539 (1.5296)\n",
            "TRAIN(215): [ 90/196] Batch: 0.0901 (0.0821) Data: 0.0473 (0.0507) Loss: 1.5316 (1.5368)\n",
            "TRAIN(215): [100/196] Batch: 0.0801 (0.0815) Data: 0.0504 (0.0505) Loss: 1.5947 (1.5409)\n",
            "TRAIN(215): [110/196] Batch: 0.0721 (0.0810) Data: 0.0552 (0.0502) Loss: 1.6884 (1.5429)\n",
            "TRAIN(215): [120/196] Batch: 0.0755 (0.0805) Data: 0.0611 (0.0503) Loss: 1.5651 (1.5404)\n",
            "TRAIN(215): [130/196] Batch: 0.0695 (0.0802) Data: 0.0549 (0.0502) Loss: 1.6021 (1.5459)\n",
            "TRAIN(215): [140/196] Batch: 0.0813 (0.0800) Data: 0.0500 (0.0500) Loss: 1.6462 (1.5501)\n",
            "TRAIN(215): [150/196] Batch: 0.0715 (0.0797) Data: 0.0558 (0.0501) Loss: 1.6259 (1.5521)\n",
            "TRAIN(215): [160/196] Batch: 0.0802 (0.0795) Data: 0.0536 (0.0500) Loss: 1.6241 (1.5526)\n",
            "TRAIN(215): [170/196] Batch: 0.0800 (0.0793) Data: 0.0507 (0.0499) Loss: 1.4532 (1.5537)\n",
            "TRAIN(215): [180/196] Batch: 0.0822 (0.0792) Data: 0.0505 (0.0497) Loss: 1.5180 (1.5546)\n",
            "TRAIN(215): [190/196] Batch: 0.0752 (0.0790) Data: 0.0612 (0.0498) Loss: 1.5283 (1.5564)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(215)         0:00:15         0:00:09         0:00:05          1.5584\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(216): [ 10/196] Batch: 0.0781 (0.1216) Data: 0.0485 (0.0841) Loss: 1.5334 (1.5899)\n",
            "TRAIN(216): [ 20/196] Batch: 0.0757 (0.0989) Data: 0.0567 (0.0675) Loss: 1.3982 (1.5860)\n",
            "TRAIN(216): [ 30/196] Batch: 0.0637 (0.0923) Data: 0.0514 (0.0603) Loss: 1.6575 (1.5950)\n",
            "TRAIN(216): [ 40/196] Batch: 0.0682 (0.0886) Data: 0.0532 (0.0562) Loss: 1.4909 (1.5925)\n",
            "TRAIN(216): [ 50/196] Batch: 0.0578 (0.0867) Data: 0.0500 (0.0541) Loss: 1.5661 (1.5920)\n",
            "TRAIN(216): [ 60/196] Batch: 0.0724 (0.0849) Data: 0.0580 (0.0526) Loss: 1.5874 (1.5792)\n",
            "TRAIN(216): [ 70/196] Batch: 0.0756 (0.0837) Data: 0.0517 (0.0520) Loss: 1.3767 (1.5712)\n",
            "TRAIN(216): [ 80/196] Batch: 0.0734 (0.0828) Data: 0.0501 (0.0515) Loss: 1.6130 (1.5664)\n",
            "TRAIN(216): [ 90/196] Batch: 0.0801 (0.0821) Data: 0.0510 (0.0513) Loss: 1.6434 (1.5690)\n",
            "TRAIN(216): [100/196] Batch: 0.0705 (0.0814) Data: 0.0616 (0.0511) Loss: 1.4809 (1.5679)\n",
            "TRAIN(216): [110/196] Batch: 0.0717 (0.0810) Data: 0.0542 (0.0510) Loss: 1.5300 (1.5664)\n",
            "TRAIN(216): [120/196] Batch: 0.0733 (0.0806) Data: 0.0548 (0.0508) Loss: 1.5653 (1.5660)\n",
            "TRAIN(216): [130/196] Batch: 0.0738 (0.0802) Data: 0.0622 (0.0507) Loss: 1.4654 (1.5673)\n",
            "TRAIN(216): [140/196] Batch: 0.0709 (0.0800) Data: 0.0611 (0.0507) Loss: 1.6426 (1.5651)\n",
            "TRAIN(216): [150/196] Batch: 0.0778 (0.0798) Data: 0.0531 (0.0505) Loss: 1.4905 (1.5625)\n",
            "TRAIN(216): [160/196] Batch: 0.0822 (0.0796) Data: 0.0504 (0.0503) Loss: 1.5377 (1.5600)\n",
            "TRAIN(216): [170/196] Batch: 0.0842 (0.0794) Data: 0.0468 (0.0501) Loss: 1.6473 (1.5606)\n",
            "TRAIN(216): [180/196] Batch: 0.0791 (0.0792) Data: 0.0548 (0.0500) Loss: 1.5655 (1.5637)\n",
            "TRAIN(216): [190/196] Batch: 0.0729 (0.0790) Data: 0.0635 (0.0500) Loss: 1.4842 (1.5640)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(216)         0:00:15         0:00:09         0:00:05          1.5631\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(217): [ 10/196] Batch: 0.0566 (0.1388) Data: 0.0497 (0.0945) Loss: 1.6110 (1.5385)\n",
            "TRAIN(217): [ 20/196] Batch: 0.0775 (0.1072) Data: 0.0578 (0.0724) Loss: 1.5737 (1.5445)\n",
            "TRAIN(217): [ 30/196] Batch: 0.0788 (0.0972) Data: 0.0486 (0.0643) Loss: 1.6458 (1.5621)\n",
            "TRAIN(217): [ 40/196] Batch: 0.0722 (0.0919) Data: 0.0537 (0.0605) Loss: 1.4473 (1.5727)\n",
            "TRAIN(217): [ 50/196] Batch: 0.0827 (0.0889) Data: 0.0489 (0.0579) Loss: 1.4860 (1.5673)\n",
            "TRAIN(217): [ 60/196] Batch: 0.0855 (0.0868) Data: 0.0489 (0.0561) Loss: 1.5690 (1.5554)\n",
            "TRAIN(217): [ 70/196] Batch: 0.0640 (0.0851) Data: 0.0615 (0.0552) Loss: 1.4518 (1.5510)\n",
            "TRAIN(217): [ 80/196] Batch: 0.0794 (0.0841) Data: 0.0510 (0.0544) Loss: 1.5949 (1.5572)\n",
            "TRAIN(217): [ 90/196] Batch: 0.0827 (0.0833) Data: 0.0487 (0.0536) Loss: 1.5654 (1.5576)\n",
            "TRAIN(217): [100/196] Batch: 0.0715 (0.0825) Data: 0.0548 (0.0530) Loss: 1.3705 (1.5496)\n",
            "TRAIN(217): [110/196] Batch: 0.0790 (0.0820) Data: 0.0551 (0.0526) Loss: 1.5600 (1.5493)\n",
            "TRAIN(217): [120/196] Batch: 0.0816 (0.0815) Data: 0.0566 (0.0524) Loss: 1.6256 (1.5505)\n",
            "TRAIN(217): [130/196] Batch: 0.0732 (0.0811) Data: 0.0544 (0.0522) Loss: 1.4663 (1.5502)\n",
            "TRAIN(217): [140/196] Batch: 0.0774 (0.0808) Data: 0.0517 (0.0520) Loss: 1.5385 (1.5506)\n",
            "TRAIN(217): [150/196] Batch: 0.0808 (0.0806) Data: 0.0400 (0.0514) Loss: 1.5927 (1.5545)\n",
            "TRAIN(217): [160/196] Batch: 0.0740 (0.0804) Data: 0.0582 (0.0509) Loss: 1.3944 (1.5553)\n",
            "TRAIN(217): [170/196] Batch: 0.0753 (0.0802) Data: 0.0508 (0.0506) Loss: 1.5486 (1.5581)\n",
            "TRAIN(217): [180/196] Batch: 0.0698 (0.0800) Data: 0.0492 (0.0502) Loss: 1.6462 (1.5605)\n",
            "TRAIN(217): [190/196] Batch: 0.0747 (0.0798) Data: 0.0609 (0.0502) Loss: 1.6155 (1.5601)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(217)         0:00:15         0:00:09         0:00:05          1.5607\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(218): [ 10/196] Batch: 0.0632 (0.1184) Data: 0.0603 (0.0814) Loss: 1.5109 (1.5735)\n",
            "TRAIN(218): [ 20/196] Batch: 0.0730 (0.0980) Data: 0.0537 (0.0674) Loss: 1.6272 (1.5477)\n",
            "TRAIN(218): [ 30/196] Batch: 0.0817 (0.0910) Data: 0.0477 (0.0608) Loss: 1.6596 (1.5615)\n",
            "TRAIN(218): [ 40/196] Batch: 0.0835 (0.0872) Data: 0.0490 (0.0575) Loss: 1.6349 (1.5623)\n",
            "TRAIN(218): [ 50/196] Batch: 0.0837 (0.0849) Data: 0.0545 (0.0560) Loss: 1.6955 (1.5674)\n",
            "TRAIN(218): [ 60/196] Batch: 0.0805 (0.0835) Data: 0.0493 (0.0548) Loss: 1.4665 (1.5645)\n",
            "TRAIN(218): [ 70/196] Batch: 0.0734 (0.0825) Data: 0.0478 (0.0536) Loss: 1.6826 (1.5740)\n",
            "TRAIN(218): [ 80/196] Batch: 0.0761 (0.0816) Data: 0.0609 (0.0529) Loss: 1.3241 (1.5710)\n",
            "TRAIN(218): [ 90/196] Batch: 0.0717 (0.0811) Data: 0.0553 (0.0522) Loss: 1.6944 (1.5719)\n",
            "TRAIN(218): [100/196] Batch: 0.0876 (0.0806) Data: 0.0486 (0.0517) Loss: 1.7049 (1.5737)\n",
            "TRAIN(218): [110/196] Batch: 0.0825 (0.0803) Data: 0.0492 (0.0515) Loss: 1.5753 (1.5770)\n",
            "TRAIN(218): [120/196] Batch: 0.0723 (0.0800) Data: 0.0506 (0.0509) Loss: 1.5998 (1.5733)\n",
            "TRAIN(218): [130/196] Batch: 0.0773 (0.0799) Data: 0.0508 (0.0505) Loss: 1.9415 (1.5719)\n",
            "TRAIN(218): [140/196] Batch: 0.0749 (0.0797) Data: 0.0547 (0.0501) Loss: 1.6527 (1.5722)\n",
            "TRAIN(218): [150/196] Batch: 0.0796 (0.0795) Data: 0.0515 (0.0500) Loss: 1.5251 (1.5705)\n",
            "TRAIN(218): [160/196] Batch: 0.0719 (0.0793) Data: 0.0558 (0.0500) Loss: 1.6768 (1.5708)\n",
            "TRAIN(218): [170/196] Batch: 0.0691 (0.0790) Data: 0.0619 (0.0500) Loss: 1.4581 (1.5681)\n",
            "TRAIN(218): [180/196] Batch: 0.0747 (0.0789) Data: 0.0557 (0.0500) Loss: 1.5267 (1.5650)\n",
            "TRAIN(218): [190/196] Batch: 0.0765 (0.0788) Data: 0.0618 (0.0500) Loss: 1.5474 (1.5636)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(218)         0:00:15         0:00:09         0:00:05          1.5621\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(219): [ 10/196] Batch: 0.0739 (0.1192) Data: 0.0496 (0.0842) Loss: 1.5345 (1.4879)\n",
            "TRAIN(219): [ 20/196] Batch: 0.0753 (0.0980) Data: 0.0501 (0.0673) Loss: 1.6158 (1.5093)\n",
            "TRAIN(219): [ 30/196] Batch: 0.0659 (0.0906) Data: 0.0583 (0.0620) Loss: 1.3393 (1.5049)\n",
            "TRAIN(219): [ 40/196] Batch: 0.0676 (0.0870) Data: 0.0603 (0.0588) Loss: 1.4554 (1.5071)\n",
            "TRAIN(219): [ 50/196] Batch: 0.0688 (0.0849) Data: 0.0560 (0.0568) Loss: 1.5559 (1.5115)\n",
            "TRAIN(219): [ 60/196] Batch: 0.0696 (0.0835) Data: 0.0546 (0.0551) Loss: 1.6159 (1.5166)\n",
            "TRAIN(219): [ 70/196] Batch: 0.0696 (0.0824) Data: 0.0568 (0.0541) Loss: 1.5319 (1.5219)\n",
            "TRAIN(219): [ 80/196] Batch: 0.0876 (0.0818) Data: 0.0436 (0.0533) Loss: 1.6916 (1.5289)\n",
            "TRAIN(219): [ 90/196] Batch: 0.0771 (0.0812) Data: 0.0580 (0.0526) Loss: 1.4467 (1.5318)\n",
            "TRAIN(219): [100/196] Batch: 0.0753 (0.0808) Data: 0.0572 (0.0524) Loss: 1.5934 (1.5387)\n",
            "TRAIN(219): [110/196] Batch: 0.0628 (0.0805) Data: 0.0505 (0.0515) Loss: 1.5872 (1.5379)\n",
            "TRAIN(219): [120/196] Batch: 0.0786 (0.0803) Data: 0.0541 (0.0509) Loss: 1.4954 (1.5440)\n",
            "TRAIN(219): [130/196] Batch: 0.0720 (0.0800) Data: 0.0605 (0.0508) Loss: 1.5046 (1.5422)\n",
            "TRAIN(219): [140/196] Batch: 0.0771 (0.0798) Data: 0.0530 (0.0505) Loss: 1.5578 (1.5441)\n",
            "TRAIN(219): [150/196] Batch: 0.0721 (0.0795) Data: 0.0547 (0.0504) Loss: 1.5911 (1.5454)\n",
            "TRAIN(219): [160/196] Batch: 0.0686 (0.0793) Data: 0.0560 (0.0503) Loss: 1.6485 (1.5461)\n",
            "TRAIN(219): [170/196] Batch: 0.0767 (0.0791) Data: 0.0538 (0.0502) Loss: 1.5342 (1.5468)\n",
            "TRAIN(219): [180/196] Batch: 0.0713 (0.0789) Data: 0.0619 (0.0503) Loss: 1.2779 (1.5457)\n",
            "TRAIN(219): [190/196] Batch: 0.0758 (0.0788) Data: 0.0605 (0.0502) Loss: 1.5691 (1.5454)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(219)         0:00:15         0:00:09         0:00:05          1.5470\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(220): [ 10/196] Batch: 0.0680 (0.1212) Data: 0.0543 (0.0848) Loss: 1.7121 (1.5688)\n",
            "TRAIN(220): [ 20/196] Batch: 0.0832 (0.0995) Data: 0.0469 (0.0668) Loss: 1.4640 (1.5575)\n",
            "TRAIN(220): [ 30/196] Batch: 0.0665 (0.0913) Data: 0.0594 (0.0609) Loss: 1.4340 (1.5455)\n",
            "TRAIN(220): [ 40/196] Batch: 0.0830 (0.0877) Data: 0.0553 (0.0580) Loss: 1.5246 (1.5407)\n",
            "TRAIN(220): [ 50/196] Batch: 0.0785 (0.0853) Data: 0.0539 (0.0562) Loss: 1.5165 (1.5455)\n",
            "TRAIN(220): [ 60/196] Batch: 0.0760 (0.0840) Data: 0.0496 (0.0544) Loss: 1.4656 (1.5463)\n",
            "TRAIN(220): [ 70/196] Batch: 0.0978 (0.0835) Data: 0.0373 (0.0528) Loss: 1.5533 (1.5425)\n",
            "TRAIN(220): [ 80/196] Batch: 0.0853 (0.0825) Data: 0.0514 (0.0521) Loss: 1.5330 (1.5449)\n",
            "TRAIN(220): [ 90/196] Batch: 0.0924 (0.0824) Data: 0.0277 (0.0510) Loss: 1.5040 (1.5445)\n",
            "TRAIN(220): [100/196] Batch: 0.0718 (0.0816) Data: 0.0538 (0.0508) Loss: 1.5080 (1.5496)\n",
            "TRAIN(220): [110/196] Batch: 0.0806 (0.0811) Data: 0.0507 (0.0508) Loss: 1.3554 (1.5441)\n",
            "TRAIN(220): [120/196] Batch: 0.0712 (0.0806) Data: 0.0563 (0.0505) Loss: 1.6741 (1.5455)\n",
            "TRAIN(220): [130/196] Batch: 0.0772 (0.0803) Data: 0.0521 (0.0503) Loss: 1.6609 (1.5458)\n",
            "TRAIN(220): [140/196] Batch: 0.0703 (0.0800) Data: 0.0545 (0.0502) Loss: 1.6748 (1.5454)\n",
            "TRAIN(220): [150/196] Batch: 0.0804 (0.0798) Data: 0.0525 (0.0503) Loss: 1.5159 (1.5455)\n",
            "TRAIN(220): [160/196] Batch: 0.0722 (0.0796) Data: 0.0556 (0.0502) Loss: 1.5532 (1.5435)\n",
            "TRAIN(220): [170/196] Batch: 0.0628 (0.0793) Data: 0.0609 (0.0501) Loss: 1.4822 (1.5448)\n",
            "TRAIN(220): [180/196] Batch: 0.0752 (0.0792) Data: 0.0546 (0.0499) Loss: 1.4608 (1.5459)\n",
            "TRAIN(220): [190/196] Batch: 0.0741 (0.0790) Data: 0.0615 (0.0499) Loss: 1.5109 (1.5482)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(220)         0:00:15         0:00:09         0:00:05          1.5485\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(221): [ 10/196] Batch: 0.0603 (0.1191) Data: 0.0599 (0.0845) Loss: 1.5528 (1.5254)\n",
            "TRAIN(221): [ 20/196] Batch: 0.0750 (0.0985) Data: 0.0490 (0.0668) Loss: 1.7153 (1.5396)\n",
            "TRAIN(221): [ 30/196] Batch: 0.0772 (0.0918) Data: 0.0531 (0.0603) Loss: 1.4923 (1.5338)\n",
            "TRAIN(221): [ 40/196] Batch: 0.0702 (0.0885) Data: 0.0550 (0.0558) Loss: 1.6896 (1.5440)\n",
            "TRAIN(221): [ 50/196] Batch: 0.0797 (0.0863) Data: 0.0514 (0.0536) Loss: 1.5926 (1.5473)\n",
            "TRAIN(221): [ 60/196] Batch: 0.0819 (0.0849) Data: 0.0407 (0.0520) Loss: 1.6843 (1.5485)\n",
            "TRAIN(221): [ 70/196] Batch: 0.0623 (0.0834) Data: 0.0612 (0.0513) Loss: 1.5844 (1.5536)\n",
            "TRAIN(221): [ 80/196] Batch: 0.0766 (0.0827) Data: 0.0502 (0.0508) Loss: 1.4569 (1.5551)\n",
            "TRAIN(221): [ 90/196] Batch: 0.0839 (0.0820) Data: 0.0528 (0.0506) Loss: 1.2698 (1.5531)\n",
            "TRAIN(221): [100/196] Batch: 0.0796 (0.0814) Data: 0.0484 (0.0503) Loss: 1.6298 (1.5571)\n",
            "TRAIN(221): [110/196] Batch: 0.0723 (0.0809) Data: 0.0560 (0.0501) Loss: 1.5007 (1.5591)\n",
            "TRAIN(221): [120/196] Batch: 0.0770 (0.0805) Data: 0.0547 (0.0499) Loss: 1.4822 (1.5575)\n",
            "TRAIN(221): [130/196] Batch: 0.0657 (0.0801) Data: 0.0619 (0.0499) Loss: 1.4459 (1.5558)\n",
            "TRAIN(221): [140/196] Batch: 0.0716 (0.0799) Data: 0.0553 (0.0497) Loss: 1.5884 (1.5575)\n",
            "TRAIN(221): [150/196] Batch: 0.0797 (0.0797) Data: 0.0494 (0.0498) Loss: 1.5577 (1.5590)\n",
            "TRAIN(221): [160/196] Batch: 0.0714 (0.0795) Data: 0.0557 (0.0496) Loss: 1.5232 (1.5575)\n",
            "TRAIN(221): [170/196] Batch: 0.0765 (0.0792) Data: 0.0618 (0.0497) Loss: 1.7078 (1.5572)\n",
            "TRAIN(221): [180/196] Batch: 0.0873 (0.0791) Data: 0.0509 (0.0497) Loss: 1.4549 (1.5578)\n",
            "TRAIN(221): [190/196] Batch: 0.0753 (0.0789) Data: 0.0612 (0.0497) Loss: 1.6816 (1.5591)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(221)         0:00:15         0:00:09         0:00:05          1.5584\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(222): [ 10/196] Batch: 0.0858 (0.1314) Data: 0.0423 (0.0880) Loss: 1.3837 (1.5412)\n",
            "TRAIN(222): [ 20/196] Batch: 0.0793 (0.1078) Data: 0.0372 (0.0654) Loss: 1.6357 (1.5500)\n",
            "TRAIN(222): [ 30/196] Batch: 0.0681 (0.0966) Data: 0.0577 (0.0597) Loss: 1.5020 (1.5471)\n",
            "TRAIN(222): [ 40/196] Batch: 0.0759 (0.0914) Data: 0.0627 (0.0573) Loss: 1.6090 (1.5472)\n",
            "TRAIN(222): [ 50/196] Batch: 0.0751 (0.0886) Data: 0.0489 (0.0551) Loss: 1.6230 (1.5349)\n",
            "TRAIN(222): [ 60/196] Batch: 0.0857 (0.0865) Data: 0.0522 (0.0539) Loss: 1.7325 (1.5455)\n",
            "TRAIN(222): [ 70/196] Batch: 0.0825 (0.0851) Data: 0.0504 (0.0533) Loss: 1.5712 (1.5445)\n",
            "TRAIN(222): [ 80/196] Batch: 0.0826 (0.0839) Data: 0.0530 (0.0527) Loss: 1.5680 (1.5443)\n",
            "TRAIN(222): [ 90/196] Batch: 0.0734 (0.0831) Data: 0.0529 (0.0521) Loss: 1.5378 (1.5437)\n",
            "TRAIN(222): [100/196] Batch: 0.0671 (0.0823) Data: 0.0540 (0.0516) Loss: 1.4340 (1.5453)\n",
            "TRAIN(222): [110/196] Batch: 0.0687 (0.0818) Data: 0.0559 (0.0513) Loss: 1.5875 (1.5492)\n",
            "TRAIN(222): [120/196] Batch: 0.0753 (0.0813) Data: 0.0532 (0.0510) Loss: 1.5444 (1.5440)\n",
            "TRAIN(222): [130/196] Batch: 0.0803 (0.0810) Data: 0.0514 (0.0507) Loss: 1.4515 (1.5439)\n",
            "TRAIN(222): [140/196] Batch: 0.0687 (0.0806) Data: 0.0575 (0.0506) Loss: 1.4911 (1.5457)\n",
            "TRAIN(222): [150/196] Batch: 0.0669 (0.0803) Data: 0.0574 (0.0504) Loss: 1.4915 (1.5473)\n",
            "TRAIN(222): [160/196] Batch: 0.0782 (0.0801) Data: 0.0505 (0.0501) Loss: 1.5651 (1.5509)\n",
            "TRAIN(222): [170/196] Batch: 0.0819 (0.0800) Data: 0.0472 (0.0496) Loss: 1.4594 (1.5520)\n",
            "TRAIN(222): [180/196] Batch: 0.0953 (0.0800) Data: 0.0375 (0.0493) Loss: 1.5595 (1.5524)\n",
            "TRAIN(222): [190/196] Batch: 0.0769 (0.0797) Data: 0.0594 (0.0491) Loss: 1.4580 (1.5516)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(222)         0:00:15         0:00:09         0:00:05          1.5524\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(223): [ 10/196] Batch: 0.0678 (0.1186) Data: 0.0523 (0.0815) Loss: 1.5566 (1.5490)\n",
            "TRAIN(223): [ 20/196] Batch: 0.0866 (0.0980) Data: 0.0486 (0.0665) Loss: 1.7038 (1.5709)\n",
            "TRAIN(223): [ 30/196] Batch: 0.0796 (0.0908) Data: 0.0491 (0.0605) Loss: 1.3359 (1.5447)\n",
            "TRAIN(223): [ 40/196] Batch: 0.0795 (0.0869) Data: 0.0612 (0.0575) Loss: 1.6230 (1.5617)\n",
            "TRAIN(223): [ 50/196] Batch: 0.0859 (0.0849) Data: 0.0490 (0.0557) Loss: 1.5742 (1.5628)\n",
            "TRAIN(223): [ 60/196] Batch: 0.0670 (0.0834) Data: 0.0567 (0.0543) Loss: 1.3185 (1.5646)\n",
            "TRAIN(223): [ 70/196] Batch: 0.0796 (0.0826) Data: 0.0490 (0.0534) Loss: 1.4852 (1.5662)\n",
            "TRAIN(223): [ 80/196] Batch: 0.0819 (0.0818) Data: 0.0503 (0.0529) Loss: 1.5245 (1.5683)\n",
            "TRAIN(223): [ 90/196] Batch: 0.0711 (0.0811) Data: 0.0573 (0.0524) Loss: 1.5260 (1.5668)\n",
            "TRAIN(223): [100/196] Batch: 0.0837 (0.0807) Data: 0.0478 (0.0520) Loss: 1.6575 (1.5663)\n",
            "TRAIN(223): [110/196] Batch: 0.0639 (0.0802) Data: 0.0610 (0.0518) Loss: 1.5539 (1.5649)\n",
            "TRAIN(223): [120/196] Batch: 0.0743 (0.0799) Data: 0.0577 (0.0518) Loss: 1.4770 (1.5657)\n",
            "TRAIN(223): [130/196] Batch: 0.0637 (0.0797) Data: 0.0517 (0.0512) Loss: 1.5022 (1.5651)\n",
            "TRAIN(223): [140/196] Batch: 0.0797 (0.0796) Data: 0.0489 (0.0509) Loss: 1.4500 (1.5612)\n",
            "TRAIN(223): [150/196] Batch: 0.0771 (0.0794) Data: 0.0524 (0.0506) Loss: 1.4081 (1.5598)\n",
            "TRAIN(223): [160/196] Batch: 0.0793 (0.0793) Data: 0.0507 (0.0501) Loss: 1.6077 (1.5625)\n",
            "TRAIN(223): [170/196] Batch: 0.0710 (0.0791) Data: 0.0557 (0.0500) Loss: 1.5510 (1.5635)\n",
            "TRAIN(223): [180/196] Batch: 0.0888 (0.0790) Data: 0.0484 (0.0500) Loss: 1.4230 (1.5634)\n",
            "TRAIN(223): [190/196] Batch: 0.0760 (0.0788) Data: 0.0620 (0.0502) Loss: 1.5407 (1.5622)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(223)         0:00:15         0:00:09         0:00:05          1.5616\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(224): [ 10/196] Batch: 0.0691 (0.1198) Data: 0.0534 (0.0818) Loss: 1.5505 (1.5713)\n",
            "TRAIN(224): [ 20/196] Batch: 0.0776 (0.0985) Data: 0.0487 (0.0653) Loss: 1.3789 (1.5483)\n",
            "TRAIN(224): [ 30/196] Batch: 0.0659 (0.0909) Data: 0.0568 (0.0599) Loss: 1.4880 (1.5618)\n",
            "TRAIN(224): [ 40/196] Batch: 0.0857 (0.0874) Data: 0.0489 (0.0570) Loss: 1.5709 (1.5471)\n",
            "TRAIN(224): [ 50/196] Batch: 0.0787 (0.0851) Data: 0.0542 (0.0548) Loss: 1.5416 (1.5523)\n",
            "TRAIN(224): [ 60/196] Batch: 0.0689 (0.0836) Data: 0.0557 (0.0539) Loss: 1.5732 (1.5510)\n",
            "TRAIN(224): [ 70/196] Batch: 0.0773 (0.0825) Data: 0.0603 (0.0532) Loss: 1.4933 (1.5485)\n",
            "TRAIN(224): [ 80/196] Batch: 0.0758 (0.0817) Data: 0.0616 (0.0526) Loss: 1.5666 (1.5503)\n",
            "TRAIN(224): [ 90/196] Batch: 0.0892 (0.0814) Data: 0.0370 (0.0518) Loss: 1.5851 (1.5463)\n",
            "TRAIN(224): [100/196] Batch: 0.0888 (0.0811) Data: 0.0418 (0.0509) Loss: 1.4840 (1.5419)\n",
            "TRAIN(224): [110/196] Batch: 0.0823 (0.0809) Data: 0.0415 (0.0501) Loss: 1.6636 (1.5463)\n",
            "TRAIN(224): [120/196] Batch: 0.0720 (0.0805) Data: 0.0526 (0.0495) Loss: 1.6518 (1.5474)\n",
            "TRAIN(224): [130/196] Batch: 0.0771 (0.0804) Data: 0.0489 (0.0491) Loss: 1.5804 (1.5489)\n",
            "TRAIN(224): [140/196] Batch: 0.0740 (0.0800) Data: 0.0610 (0.0493) Loss: 1.4973 (1.5519)\n",
            "TRAIN(224): [150/196] Batch: 0.0687 (0.0798) Data: 0.0608 (0.0493) Loss: 1.6337 (1.5524)\n",
            "TRAIN(224): [160/196] Batch: 0.0880 (0.0796) Data: 0.0502 (0.0493) Loss: 1.4298 (1.5520)\n",
            "TRAIN(224): [170/196] Batch: 0.0769 (0.0794) Data: 0.0561 (0.0493) Loss: 1.4587 (1.5502)\n",
            "TRAIN(224): [180/196] Batch: 0.0732 (0.0792) Data: 0.0553 (0.0493) Loss: 1.4996 (1.5468)\n",
            "TRAIN(224): [190/196] Batch: 0.0754 (0.0790) Data: 0.0617 (0.0494) Loss: 1.5585 (1.5490)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(224)         0:00:15         0:00:09         0:00:05          1.5497\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(225): [ 10/196] Batch: 0.0765 (0.1233) Data: 0.0541 (0.0833) Loss: 1.4703 (1.4847)\n",
            "TRAIN(225): [ 20/196] Batch: 0.0806 (0.1004) Data: 0.0471 (0.0666) Loss: 1.7438 (1.5303)\n",
            "TRAIN(225): [ 30/196] Batch: 0.0817 (0.0923) Data: 0.0496 (0.0612) Loss: 1.7498 (1.5414)\n",
            "TRAIN(225): [ 40/196] Batch: 0.0734 (0.0883) Data: 0.0530 (0.0580) Loss: 1.5700 (1.5497)\n",
            "TRAIN(225): [ 50/196] Batch: 0.0732 (0.0859) Data: 0.0539 (0.0558) Loss: 1.4952 (1.5456)\n",
            "TRAIN(225): [ 60/196] Batch: 0.0897 (0.0846) Data: 0.0384 (0.0539) Loss: 1.4694 (1.5421)\n",
            "TRAIN(225): [ 70/196] Batch: 0.0761 (0.0833) Data: 0.0574 (0.0528) Loss: 1.6956 (1.5475)\n",
            "TRAIN(225): [ 80/196] Batch: 0.0621 (0.0827) Data: 0.0504 (0.0516) Loss: 1.5828 (1.5492)\n",
            "TRAIN(225): [ 90/196] Batch: 0.0660 (0.0822) Data: 0.0525 (0.0509) Loss: 1.5036 (1.5493)\n",
            "TRAIN(225): [100/196] Batch: 0.0698 (0.0817) Data: 0.0562 (0.0505) Loss: 1.5727 (1.5495)\n",
            "TRAIN(225): [110/196] Batch: 0.0727 (0.0812) Data: 0.0608 (0.0504) Loss: 1.5725 (1.5476)\n",
            "TRAIN(225): [120/196] Batch: 0.0769 (0.0808) Data: 0.0524 (0.0503) Loss: 1.6921 (1.5492)\n",
            "TRAIN(225): [130/196] Batch: 0.0820 (0.0805) Data: 0.0570 (0.0504) Loss: 1.4725 (1.5476)\n",
            "TRAIN(225): [140/196] Batch: 0.0800 (0.0802) Data: 0.0475 (0.0502) Loss: 1.7340 (1.5502)\n",
            "TRAIN(225): [150/196] Batch: 0.0739 (0.0800) Data: 0.0521 (0.0499) Loss: 1.8511 (1.5501)\n",
            "TRAIN(225): [160/196] Batch: 0.0797 (0.0797) Data: 0.0511 (0.0498) Loss: 1.6088 (1.5507)\n",
            "TRAIN(225): [170/196] Batch: 0.0832 (0.0796) Data: 0.0487 (0.0498) Loss: 1.4416 (1.5533)\n",
            "TRAIN(225): [180/196] Batch: 0.0706 (0.0793) Data: 0.0543 (0.0498) Loss: 1.6546 (1.5549)\n",
            "TRAIN(225): [190/196] Batch: 0.0757 (0.0791) Data: 0.0602 (0.0499) Loss: 1.5758 (1.5554)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(225)         0:00:15         0:00:09         0:00:05          1.5552\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(226): [ 10/196] Batch: 0.0817 (0.1216) Data: 0.0477 (0.0841) Loss: 1.4393 (1.4972)\n",
            "TRAIN(226): [ 20/196] Batch: 0.0718 (0.0986) Data: 0.0604 (0.0674) Loss: 1.5813 (1.5226)\n",
            "TRAIN(226): [ 30/196] Batch: 0.0668 (0.0920) Data: 0.0459 (0.0599) Loss: 1.4781 (1.5333)\n",
            "TRAIN(226): [ 40/196] Batch: 0.0791 (0.0880) Data: 0.0562 (0.0563) Loss: 1.6721 (1.5376)\n",
            "TRAIN(226): [ 50/196] Batch: 0.0726 (0.0862) Data: 0.0560 (0.0541) Loss: 1.5902 (1.5363)\n",
            "TRAIN(226): [ 60/196] Batch: 0.0737 (0.0854) Data: 0.0337 (0.0527) Loss: 1.6821 (1.5428)\n",
            "TRAIN(226): [ 70/196] Batch: 0.0789 (0.0840) Data: 0.0521 (0.0518) Loss: 1.6867 (1.5460)\n",
            "TRAIN(226): [ 80/196] Batch: 0.0750 (0.0828) Data: 0.0619 (0.0515) Loss: 1.4288 (1.5437)\n",
            "TRAIN(226): [ 90/196] Batch: 0.0704 (0.0822) Data: 0.0538 (0.0512) Loss: 1.7012 (1.5461)\n",
            "TRAIN(226): [100/196] Batch: 0.0723 (0.0816) Data: 0.0618 (0.0510) Loss: 1.5113 (1.5438)\n",
            "TRAIN(226): [110/196] Batch: 0.0722 (0.0811) Data: 0.0556 (0.0509) Loss: 1.6597 (1.5470)\n",
            "TRAIN(226): [120/196] Batch: 0.0894 (0.0808) Data: 0.0477 (0.0507) Loss: 1.5059 (1.5478)\n",
            "TRAIN(226): [130/196] Batch: 0.0637 (0.0803) Data: 0.0609 (0.0505) Loss: 1.4678 (1.5457)\n",
            "TRAIN(226): [140/196] Batch: 0.0713 (0.0801) Data: 0.0554 (0.0503) Loss: 1.5768 (1.5450)\n",
            "TRAIN(226): [150/196] Batch: 0.0713 (0.0799) Data: 0.0546 (0.0501) Loss: 1.3958 (1.5440)\n",
            "TRAIN(226): [160/196] Batch: 0.0635 (0.0796) Data: 0.0622 (0.0500) Loss: 1.5010 (1.5427)\n",
            "TRAIN(226): [170/196] Batch: 0.0807 (0.0795) Data: 0.0478 (0.0499) Loss: 1.4568 (1.5440)\n",
            "TRAIN(226): [180/196] Batch: 0.0744 (0.0792) Data: 0.0617 (0.0499) Loss: 1.7154 (1.5451)\n",
            "TRAIN(226): [190/196] Batch: 0.0736 (0.0791) Data: 0.0619 (0.0499) Loss: 1.4968 (1.5452)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(226)         0:00:15         0:00:09         0:00:05          1.5445\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(227): [ 10/196] Batch: 0.0785 (0.1261) Data: 0.0416 (0.0844) Loss: 1.5059 (1.5196)\n",
            "TRAIN(227): [ 20/196] Batch: 0.0809 (0.1042) Data: 0.0397 (0.0636) Loss: 1.5800 (1.5518)\n",
            "TRAIN(227): [ 30/196] Batch: 0.0743 (0.0959) Data: 0.0491 (0.0550) Loss: 1.5342 (1.5629)\n",
            "TRAIN(227): [ 40/196] Batch: 0.0656 (0.0909) Data: 0.0604 (0.0534) Loss: 1.5485 (1.5572)\n",
            "TRAIN(227): [ 50/196] Batch: 0.0862 (0.0881) Data: 0.0539 (0.0525) Loss: 1.5603 (1.5516)\n",
            "TRAIN(227): [ 60/196] Batch: 0.0853 (0.0861) Data: 0.0508 (0.0517) Loss: 1.6013 (1.5531)\n",
            "TRAIN(227): [ 70/196] Batch: 0.0760 (0.0847) Data: 0.0541 (0.0514) Loss: 1.6046 (1.5544)\n",
            "TRAIN(227): [ 80/196] Batch: 0.0719 (0.0837) Data: 0.0538 (0.0511) Loss: 1.6835 (1.5646)\n",
            "TRAIN(227): [ 90/196] Batch: 0.0665 (0.0828) Data: 0.0624 (0.0508) Loss: 1.5830 (1.5673)\n",
            "TRAIN(227): [100/196] Batch: 0.0780 (0.0822) Data: 0.0567 (0.0507) Loss: 1.6124 (1.5632)\n",
            "TRAIN(227): [110/196] Batch: 0.0635 (0.0816) Data: 0.0611 (0.0503) Loss: 1.6400 (1.5655)\n",
            "TRAIN(227): [120/196] Batch: 0.0731 (0.0812) Data: 0.0543 (0.0504) Loss: 1.6149 (1.5673)\n",
            "TRAIN(227): [130/196] Batch: 0.0709 (0.0808) Data: 0.0544 (0.0502) Loss: 1.4787 (1.5646)\n",
            "TRAIN(227): [140/196] Batch: 0.0773 (0.0805) Data: 0.0533 (0.0500) Loss: 1.5886 (1.5619)\n",
            "TRAIN(227): [150/196] Batch: 0.0806 (0.0803) Data: 0.0494 (0.0498) Loss: 1.6637 (1.5623)\n",
            "TRAIN(227): [160/196] Batch: 0.0755 (0.0800) Data: 0.0483 (0.0497) Loss: 1.6659 (1.5620)\n",
            "TRAIN(227): [170/196] Batch: 0.0774 (0.0799) Data: 0.0557 (0.0495) Loss: 1.6203 (1.5602)\n",
            "TRAIN(227): [180/196] Batch: 0.0776 (0.0798) Data: 0.0477 (0.0490) Loss: 1.6279 (1.5599)\n",
            "TRAIN(227): [190/196] Batch: 0.0729 (0.0796) Data: 0.0614 (0.0489) Loss: 1.5542 (1.5587)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(227)         0:00:15         0:00:09         0:00:05          1.5587\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(228): [ 10/196] Batch: 0.0857 (0.1327) Data: 0.0325 (0.0847) Loss: 1.5249 (1.5457)\n",
            "TRAIN(228): [ 20/196] Batch: 0.0869 (0.1060) Data: 0.0451 (0.0646) Loss: 1.6767 (1.5663)\n",
            "TRAIN(228): [ 30/196] Batch: 0.0643 (0.0963) Data: 0.0480 (0.0574) Loss: 1.6015 (1.5680)\n",
            "TRAIN(228): [ 40/196] Batch: 0.0763 (0.0913) Data: 0.0522 (0.0549) Loss: 1.6376 (1.5742)\n",
            "TRAIN(228): [ 50/196] Batch: 0.0908 (0.0884) Data: 0.0467 (0.0538) Loss: 1.4776 (1.5723)\n",
            "TRAIN(228): [ 60/196] Batch: 0.0808 (0.0864) Data: 0.0468 (0.0528) Loss: 1.6381 (1.5705)\n",
            "TRAIN(228): [ 70/196] Batch: 0.0714 (0.0849) Data: 0.0553 (0.0520) Loss: 1.5747 (1.5712)\n",
            "TRAIN(228): [ 80/196] Batch: 0.0827 (0.0839) Data: 0.0503 (0.0515) Loss: 1.5337 (1.5679)\n",
            "TRAIN(228): [ 90/196] Batch: 0.0692 (0.0829) Data: 0.0549 (0.0511) Loss: 1.4445 (1.5649)\n",
            "TRAIN(228): [100/196] Batch: 0.0816 (0.0823) Data: 0.0499 (0.0507) Loss: 1.5703 (1.5636)\n",
            "TRAIN(228): [110/196] Batch: 0.0788 (0.0818) Data: 0.0520 (0.0508) Loss: 1.5313 (1.5675)\n",
            "TRAIN(228): [120/196] Batch: 0.0686 (0.0813) Data: 0.0556 (0.0507) Loss: 1.6104 (1.5693)\n",
            "TRAIN(228): [130/196] Batch: 0.0828 (0.0811) Data: 0.0417 (0.0501) Loss: 1.4964 (1.5658)\n",
            "TRAIN(228): [140/196] Batch: 0.0768 (0.0809) Data: 0.0531 (0.0496) Loss: 1.4484 (1.5656)\n",
            "TRAIN(228): [150/196] Batch: 0.0757 (0.0806) Data: 0.0576 (0.0493) Loss: 1.4688 (1.5650)\n",
            "TRAIN(228): [160/196] Batch: 0.0864 (0.0806) Data: 0.0351 (0.0492) Loss: 1.6661 (1.5677)\n",
            "TRAIN(228): [170/196] Batch: 0.0813 (0.0802) Data: 0.0532 (0.0492) Loss: 1.4203 (1.5662)\n",
            "TRAIN(228): [180/196] Batch: 0.0900 (0.0800) Data: 0.0478 (0.0492) Loss: 1.6112 (1.5675)\n",
            "TRAIN(228): [190/196] Batch: 0.0764 (0.0798) Data: 0.0607 (0.0493) Loss: 1.5481 (1.5678)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(228)         0:00:15         0:00:09         0:00:05          1.5663\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(229): [ 10/196] Batch: 0.0693 (0.1198) Data: 0.0542 (0.0822) Loss: 1.6018 (1.5996)\n",
            "TRAIN(229): [ 20/196] Batch: 0.0730 (0.0982) Data: 0.0550 (0.0661) Loss: 1.6147 (1.5763)\n",
            "TRAIN(229): [ 30/196] Batch: 0.0696 (0.0910) Data: 0.0546 (0.0604) Loss: 1.5159 (1.5432)\n",
            "TRAIN(229): [ 40/196] Batch: 0.0805 (0.0875) Data: 0.0497 (0.0575) Loss: 1.5044 (1.5525)\n",
            "TRAIN(229): [ 50/196] Batch: 0.0705 (0.0851) Data: 0.0565 (0.0554) Loss: 1.6170 (1.5492)\n",
            "TRAIN(229): [ 60/196] Batch: 0.0842 (0.0836) Data: 0.0538 (0.0546) Loss: 1.4637 (1.5488)\n",
            "TRAIN(229): [ 70/196] Batch: 0.0769 (0.0826) Data: 0.0533 (0.0540) Loss: 1.3924 (1.5437)\n",
            "TRAIN(229): [ 80/196] Batch: 0.0812 (0.0818) Data: 0.0513 (0.0534) Loss: 1.6245 (1.5460)\n",
            "TRAIN(229): [ 90/196] Batch: 0.0811 (0.0813) Data: 0.0468 (0.0528) Loss: 1.5751 (1.5442)\n",
            "TRAIN(229): [100/196] Batch: 0.0782 (0.0810) Data: 0.0389 (0.0520) Loss: 1.6602 (1.5432)\n",
            "TRAIN(229): [110/196] Batch: 0.0842 (0.0806) Data: 0.0426 (0.0513) Loss: 1.6415 (1.5447)\n",
            "TRAIN(229): [120/196] Batch: 0.0731 (0.0803) Data: 0.0490 (0.0506) Loss: 1.5110 (1.5456)\n",
            "TRAIN(229): [130/196] Batch: 0.0724 (0.0802) Data: 0.0457 (0.0501) Loss: 1.7451 (1.5487)\n",
            "TRAIN(229): [140/196] Batch: 0.0772 (0.0799) Data: 0.0558 (0.0500) Loss: 1.6019 (1.5490)\n",
            "TRAIN(229): [150/196] Batch: 0.0672 (0.0796) Data: 0.0584 (0.0499) Loss: 1.4589 (1.5477)\n",
            "TRAIN(229): [160/196] Batch: 0.0674 (0.0794) Data: 0.0617 (0.0499) Loss: 1.5328 (1.5449)\n",
            "TRAIN(229): [170/196] Batch: 0.0812 (0.0793) Data: 0.0491 (0.0497) Loss: 1.3579 (1.5431)\n",
            "TRAIN(229): [180/196] Batch: 0.0745 (0.0791) Data: 0.0581 (0.0498) Loss: 1.7379 (1.5465)\n",
            "TRAIN(229): [190/196] Batch: 0.0767 (0.0789) Data: 0.0611 (0.0499) Loss: 1.6143 (1.5482)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(229)         0:00:15         0:00:09         0:00:05          1.5483\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(230): [ 10/196] Batch: 0.0622 (0.1167) Data: 0.0491 (0.0780) Loss: 1.5904 (1.5793)\n",
            "TRAIN(230): [ 20/196] Batch: 0.0770 (0.0968) Data: 0.0513 (0.0644) Loss: 1.5502 (1.5921)\n",
            "TRAIN(230): [ 30/196] Batch: 0.0768 (0.0900) Data: 0.0521 (0.0598) Loss: 1.4054 (1.5772)\n",
            "TRAIN(230): [ 40/196] Batch: 0.0888 (0.0868) Data: 0.0484 (0.0570) Loss: 1.5939 (1.5868)\n",
            "TRAIN(230): [ 50/196] Batch: 0.0744 (0.0845) Data: 0.0548 (0.0556) Loss: 1.6185 (1.5778)\n",
            "TRAIN(230): [ 60/196] Batch: 0.0789 (0.0832) Data: 0.0497 (0.0544) Loss: 1.7150 (1.5750)\n",
            "TRAIN(230): [ 70/196] Batch: 0.0714 (0.0822) Data: 0.0560 (0.0535) Loss: 1.4734 (1.5799)\n",
            "TRAIN(230): [ 80/196] Batch: 0.0906 (0.0819) Data: 0.0377 (0.0526) Loss: 1.5911 (1.5755)\n",
            "TRAIN(230): [ 90/196] Batch: 0.0760 (0.0814) Data: 0.0441 (0.0515) Loss: 1.4950 (1.5711)\n",
            "TRAIN(230): [100/196] Batch: 0.0704 (0.0809) Data: 0.0517 (0.0507) Loss: 1.4897 (1.5701)\n",
            "TRAIN(230): [110/196] Batch: 0.0731 (0.0804) Data: 0.0536 (0.0505) Loss: 1.5681 (1.5707)\n",
            "TRAIN(230): [120/196] Batch: 0.0764 (0.0801) Data: 0.0556 (0.0504) Loss: 1.5082 (1.5666)\n",
            "TRAIN(230): [130/196] Batch: 0.0776 (0.0798) Data: 0.0549 (0.0503) Loss: 1.6317 (1.5688)\n",
            "TRAIN(230): [140/196] Batch: 0.0729 (0.0796) Data: 0.0498 (0.0502) Loss: 1.4732 (1.5678)\n",
            "TRAIN(230): [150/196] Batch: 0.0699 (0.0793) Data: 0.0543 (0.0501) Loss: 1.5670 (1.5696)\n",
            "TRAIN(230): [160/196] Batch: 0.0885 (0.0792) Data: 0.0492 (0.0501) Loss: 1.4693 (1.5669)\n",
            "TRAIN(230): [170/196] Batch: 0.0633 (0.0789) Data: 0.0625 (0.0499) Loss: 1.6458 (1.5685)\n",
            "TRAIN(230): [180/196] Batch: 0.0881 (0.0789) Data: 0.0490 (0.0498) Loss: 1.5487 (1.5658)\n",
            "TRAIN(230): [190/196] Batch: 0.0764 (0.0787) Data: 0.0618 (0.0499) Loss: 1.6938 (1.5622)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(230)         0:00:15         0:00:09         0:00:05          1.5608\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(231): [ 10/196] Batch: 0.0768 (0.1211) Data: 0.0478 (0.0835) Loss: 1.4019 (1.5420)\n",
            "TRAIN(231): [ 20/196] Batch: 0.0767 (0.0987) Data: 0.0516 (0.0671) Loss: 1.5292 (1.5511)\n",
            "TRAIN(231): [ 30/196] Batch: 0.0794 (0.0915) Data: 0.0521 (0.0614) Loss: 1.6934 (1.5477)\n",
            "TRAIN(231): [ 40/196] Batch: 0.0954 (0.0886) Data: 0.0380 (0.0576) Loss: 1.4535 (1.5483)\n",
            "TRAIN(231): [ 50/196] Batch: 0.0761 (0.0861) Data: 0.0487 (0.0550) Loss: 1.6501 (1.5613)\n",
            "TRAIN(231): [ 60/196] Batch: 0.0666 (0.0846) Data: 0.0675 (0.0533) Loss: 1.6282 (1.5622)\n",
            "TRAIN(231): [ 70/196] Batch: 0.0810 (0.0837) Data: 0.0505 (0.0524) Loss: 1.6475 (1.5607)\n",
            "TRAIN(231): [ 80/196] Batch: 0.0687 (0.0827) Data: 0.0573 (0.0519) Loss: 1.4828 (1.5660)\n",
            "TRAIN(231): [ 90/196] Batch: 0.0838 (0.0821) Data: 0.0455 (0.0515) Loss: 1.5569 (1.5654)\n",
            "TRAIN(231): [100/196] Batch: 0.0748 (0.0814) Data: 0.0613 (0.0510) Loss: 1.5679 (1.5611)\n",
            "TRAIN(231): [110/196] Batch: 0.0878 (0.0810) Data: 0.0510 (0.0509) Loss: 1.6085 (1.5626)\n",
            "TRAIN(231): [120/196] Batch: 0.0733 (0.0806) Data: 0.0540 (0.0506) Loss: 1.5197 (1.5597)\n",
            "TRAIN(231): [130/196] Batch: 0.0806 (0.0803) Data: 0.0495 (0.0504) Loss: 1.5963 (1.5580)\n",
            "TRAIN(231): [140/196] Batch: 0.0713 (0.0800) Data: 0.0549 (0.0502) Loss: 1.6310 (1.5561)\n",
            "TRAIN(231): [150/196] Batch: 0.0813 (0.0798) Data: 0.0518 (0.0500) Loss: 1.6512 (1.5579)\n",
            "TRAIN(231): [160/196] Batch: 0.0709 (0.0795) Data: 0.0561 (0.0499) Loss: 1.5032 (1.5563)\n",
            "TRAIN(231): [170/196] Batch: 0.0745 (0.0793) Data: 0.0621 (0.0499) Loss: 1.6274 (1.5555)\n",
            "TRAIN(231): [180/196] Batch: 0.0662 (0.0792) Data: 0.0617 (0.0499) Loss: 1.6146 (1.5599)\n",
            "TRAIN(231): [190/196] Batch: 0.0766 (0.0790) Data: 0.0612 (0.0500) Loss: 1.6249 (1.5619)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(231)         0:00:15         0:00:09         0:00:05          1.5614\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(232): [ 10/196] Batch: 0.0873 (0.1326) Data: 0.0434 (0.0917) Loss: 1.3992 (1.5499)\n",
            "TRAIN(232): [ 20/196] Batch: 0.0730 (0.1088) Data: 0.0475 (0.0658) Loss: 1.6147 (1.5777)\n",
            "TRAIN(232): [ 30/196] Batch: 0.0774 (0.0988) Data: 0.0485 (0.0579) Loss: 1.4204 (1.5467)\n",
            "TRAIN(232): [ 40/196] Batch: 0.0857 (0.0935) Data: 0.0375 (0.0547) Loss: 1.3371 (1.5408)\n",
            "TRAIN(232): [ 50/196] Batch: 0.0663 (0.0896) Data: 0.0609 (0.0534) Loss: 1.6092 (1.5416)\n",
            "TRAIN(232): [ 60/196] Batch: 0.0703 (0.0874) Data: 0.0612 (0.0529) Loss: 1.6179 (1.5423)\n",
            "TRAIN(232): [ 70/196] Batch: 0.0743 (0.0858) Data: 0.0620 (0.0526) Loss: 1.5709 (1.5530)\n",
            "TRAIN(232): [ 80/196] Batch: 0.0727 (0.0848) Data: 0.0545 (0.0521) Loss: 1.7469 (1.5506)\n",
            "TRAIN(232): [ 90/196] Batch: 0.0667 (0.0837) Data: 0.0601 (0.0518) Loss: 1.6279 (1.5558)\n",
            "TRAIN(232): [100/196] Batch: 0.0756 (0.0830) Data: 0.0565 (0.0517) Loss: 1.4315 (1.5499)\n",
            "TRAIN(232): [110/196] Batch: 0.0715 (0.0824) Data: 0.0537 (0.0514) Loss: 1.6134 (1.5485)\n",
            "TRAIN(232): [120/196] Batch: 0.0656 (0.0819) Data: 0.0614 (0.0513) Loss: 1.6428 (1.5510)\n",
            "TRAIN(232): [130/196] Batch: 0.0786 (0.0815) Data: 0.0541 (0.0512) Loss: 1.6063 (1.5526)\n",
            "TRAIN(232): [140/196] Batch: 0.0746 (0.0811) Data: 0.0622 (0.0510) Loss: 1.4897 (1.5540)\n",
            "TRAIN(232): [150/196] Batch: 0.0828 (0.0808) Data: 0.0549 (0.0508) Loss: 1.6757 (1.5514)\n",
            "TRAIN(232): [160/196] Batch: 0.0770 (0.0806) Data: 0.0475 (0.0505) Loss: 1.5290 (1.5498)\n",
            "TRAIN(232): [170/196] Batch: 0.0627 (0.0805) Data: 0.0483 (0.0502) Loss: 1.5555 (1.5487)\n",
            "TRAIN(232): [180/196] Batch: 0.0762 (0.0803) Data: 0.0508 (0.0498) Loss: 1.4643 (1.5493)\n",
            "TRAIN(232): [190/196] Batch: 0.0788 (0.0801) Data: 0.0610 (0.0497) Loss: 1.6286 (1.5526)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(232)         0:00:15         0:00:09         0:00:05          1.5513\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(233): [ 10/196] Batch: 0.0798 (0.1206) Data: 0.0485 (0.0837) Loss: 1.4445 (1.5405)\n",
            "TRAIN(233): [ 20/196] Batch: 0.0776 (0.0986) Data: 0.0506 (0.0672) Loss: 1.4512 (1.5434)\n",
            "TRAIN(233): [ 30/196] Batch: 0.0766 (0.0909) Data: 0.0616 (0.0620) Loss: 1.3931 (1.5332)\n",
            "TRAIN(233): [ 40/196] Batch: 0.0750 (0.0873) Data: 0.0605 (0.0592) Loss: 1.5150 (1.5401)\n",
            "TRAIN(233): [ 50/196] Batch: 0.0828 (0.0852) Data: 0.0554 (0.0573) Loss: 1.6621 (1.5479)\n",
            "TRAIN(233): [ 60/196] Batch: 0.0816 (0.0839) Data: 0.0485 (0.0556) Loss: 1.6800 (1.5525)\n",
            "TRAIN(233): [ 70/196] Batch: 0.0809 (0.0827) Data: 0.0562 (0.0547) Loss: 1.5288 (1.5532)\n",
            "TRAIN(233): [ 80/196] Batch: 0.0706 (0.0820) Data: 0.0537 (0.0539) Loss: 1.6720 (1.5543)\n",
            "TRAIN(233): [ 90/196] Batch: 0.0860 (0.0814) Data: 0.0520 (0.0533) Loss: 1.5435 (1.5543)\n",
            "TRAIN(233): [100/196] Batch: 0.0771 (0.0808) Data: 0.0546 (0.0529) Loss: 1.5013 (1.5558)\n",
            "TRAIN(233): [110/196] Batch: 0.0823 (0.0805) Data: 0.0522 (0.0524) Loss: 1.4872 (1.5539)\n",
            "TRAIN(233): [120/196] Batch: 0.0674 (0.0801) Data: 0.0571 (0.0524) Loss: 1.3925 (1.5521)\n",
            "TRAIN(233): [130/196] Batch: 0.0770 (0.0799) Data: 0.0494 (0.0518) Loss: 1.4542 (1.5539)\n",
            "TRAIN(233): [140/196] Batch: 0.0754 (0.0797) Data: 0.0507 (0.0513) Loss: 1.5449 (1.5553)\n",
            "TRAIN(233): [150/196] Batch: 0.0734 (0.0795) Data: 0.0493 (0.0507) Loss: 1.4795 (1.5558)\n",
            "TRAIN(233): [160/196] Batch: 0.0774 (0.0794) Data: 0.0507 (0.0502) Loss: 1.6518 (1.5543)\n",
            "TRAIN(233): [170/196] Batch: 0.0764 (0.0792) Data: 0.0553 (0.0500) Loss: 1.4522 (1.5532)\n",
            "TRAIN(233): [180/196] Batch: 0.0702 (0.0791) Data: 0.0576 (0.0499) Loss: 1.4260 (1.5532)\n",
            "TRAIN(233): [190/196] Batch: 0.0752 (0.0789) Data: 0.0605 (0.0500) Loss: 1.7117 (1.5557)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(233)         0:00:15         0:00:09         0:00:05          1.5558\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(234): [ 10/196] Batch: 0.0645 (0.1169) Data: 0.0598 (0.0846) Loss: 1.5647 (1.5545)\n",
            "TRAIN(234): [ 20/196] Batch: 0.0811 (0.0972) Data: 0.0528 (0.0685) Loss: 1.4376 (1.5225)\n",
            "TRAIN(234): [ 30/196] Batch: 0.0803 (0.0905) Data: 0.0484 (0.0614) Loss: 1.5796 (1.5427)\n",
            "TRAIN(234): [ 40/196] Batch: 0.0829 (0.0870) Data: 0.0509 (0.0578) Loss: 1.3804 (1.5580)\n",
            "TRAIN(234): [ 50/196] Batch: 0.0668 (0.0846) Data: 0.0569 (0.0559) Loss: 1.4714 (1.5572)\n",
            "TRAIN(234): [ 60/196] Batch: 0.0802 (0.0832) Data: 0.0558 (0.0550) Loss: 1.5476 (1.5525)\n",
            "TRAIN(234): [ 70/196] Batch: 0.0708 (0.0822) Data: 0.0585 (0.0541) Loss: 1.5255 (1.5520)\n",
            "TRAIN(234): [ 80/196] Batch: 0.0816 (0.0816) Data: 0.0502 (0.0533) Loss: 1.4579 (1.5555)\n",
            "TRAIN(234): [ 90/196] Batch: 0.0693 (0.0809) Data: 0.0614 (0.0528) Loss: 1.4640 (1.5596)\n",
            "TRAIN(234): [100/196] Batch: 0.0751 (0.0806) Data: 0.0527 (0.0523) Loss: 1.3654 (1.5614)\n",
            "TRAIN(234): [110/196] Batch: 0.0759 (0.0803) Data: 0.0489 (0.0515) Loss: 1.5816 (1.5638)\n",
            "TRAIN(234): [120/196] Batch: 0.0728 (0.0800) Data: 0.0492 (0.0508) Loss: 1.5527 (1.5620)\n",
            "TRAIN(234): [130/196] Batch: 0.0796 (0.0798) Data: 0.0475 (0.0505) Loss: 1.6981 (1.5641)\n",
            "TRAIN(234): [140/196] Batch: 0.0719 (0.0795) Data: 0.0545 (0.0502) Loss: 1.5848 (1.5641)\n",
            "TRAIN(234): [150/196] Batch: 0.0761 (0.0792) Data: 0.0615 (0.0500) Loss: 1.4601 (1.5610)\n",
            "TRAIN(234): [160/196] Batch: 0.0876 (0.0791) Data: 0.0495 (0.0500) Loss: 1.5940 (1.5600)\n",
            "TRAIN(234): [170/196] Batch: 0.0743 (0.0789) Data: 0.0553 (0.0500) Loss: 1.4008 (1.5564)\n",
            "TRAIN(234): [180/196] Batch: 0.0752 (0.0788) Data: 0.0618 (0.0500) Loss: 1.7587 (1.5577)\n",
            "TRAIN(234): [190/196] Batch: 0.0784 (0.0786) Data: 0.0612 (0.0499) Loss: 1.3944 (1.5590)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(234)         0:00:15         0:00:09         0:00:05          1.5575\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(235): [ 10/196] Batch: 0.0675 (0.1198) Data: 0.0556 (0.0854) Loss: 1.5602 (1.5219)\n",
            "TRAIN(235): [ 20/196] Batch: 0.0737 (0.0981) Data: 0.0596 (0.0695) Loss: 1.4317 (1.5330)\n",
            "TRAIN(235): [ 30/196] Batch: 0.0830 (0.0910) Data: 0.0541 (0.0634) Loss: 1.5677 (1.5360)\n",
            "TRAIN(235): [ 40/196] Batch: 0.0660 (0.0872) Data: 0.0627 (0.0598) Loss: 1.6357 (1.5547)\n",
            "TRAIN(235): [ 50/196] Batch: 0.0707 (0.0849) Data: 0.0621 (0.0578) Loss: 1.5542 (1.5531)\n",
            "TRAIN(235): [ 60/196] Batch: 0.0693 (0.0836) Data: 0.0565 (0.0566) Loss: 1.6962 (1.5557)\n",
            "TRAIN(235): [ 70/196] Batch: 0.0721 (0.0826) Data: 0.0549 (0.0550) Loss: 1.4262 (1.5595)\n",
            "TRAIN(235): [ 80/196] Batch: 0.0765 (0.0821) Data: 0.0484 (0.0534) Loss: 1.7631 (1.5651)\n",
            "TRAIN(235): [ 90/196] Batch: 0.0893 (0.0817) Data: 0.0380 (0.0517) Loss: 1.5538 (1.5673)\n",
            "TRAIN(235): [100/196] Batch: 0.0750 (0.0811) Data: 0.0495 (0.0509) Loss: 1.6530 (1.5666)\n",
            "TRAIN(235): [110/196] Batch: 0.0708 (0.0806) Data: 0.0617 (0.0505) Loss: 1.6041 (1.5648)\n",
            "TRAIN(235): [120/196] Batch: 0.0798 (0.0804) Data: 0.0498 (0.0503) Loss: 1.4294 (1.5634)\n",
            "TRAIN(235): [130/196] Batch: 0.0865 (0.0800) Data: 0.0526 (0.0503) Loss: 1.6190 (1.5653)\n",
            "TRAIN(235): [140/196] Batch: 0.0675 (0.0797) Data: 0.0611 (0.0503) Loss: 1.5589 (1.5625)\n",
            "TRAIN(235): [150/196] Batch: 0.0705 (0.0794) Data: 0.0616 (0.0503) Loss: 1.4779 (1.5615)\n",
            "TRAIN(235): [160/196] Batch: 0.0702 (0.0793) Data: 0.0557 (0.0502) Loss: 1.6843 (1.5647)\n",
            "TRAIN(235): [170/196] Batch: 0.0702 (0.0791) Data: 0.0530 (0.0500) Loss: 1.6854 (1.5671)\n",
            "TRAIN(235): [180/196] Batch: 0.0732 (0.0789) Data: 0.0550 (0.0500) Loss: 1.6338 (1.5680)\n",
            "TRAIN(235): [190/196] Batch: 0.0750 (0.0787) Data: 0.0630 (0.0500) Loss: 1.6618 (1.5675)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(235)         0:00:15         0:00:09         0:00:05          1.5658\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(236): [ 10/196] Batch: 0.0750 (0.1199) Data: 0.0451 (0.0820) Loss: 1.5742 (1.6004)\n",
            "TRAIN(236): [ 20/196] Batch: 0.0729 (0.0979) Data: 0.0496 (0.0666) Loss: 1.4765 (1.5692)\n",
            "TRAIN(236): [ 30/196] Batch: 0.0791 (0.0906) Data: 0.0496 (0.0611) Loss: 1.5537 (1.5643)\n",
            "TRAIN(236): [ 40/196] Batch: 0.0754 (0.0870) Data: 0.0537 (0.0575) Loss: 1.5288 (1.5575)\n",
            "TRAIN(236): [ 50/196] Batch: 0.0731 (0.0850) Data: 0.0504 (0.0544) Loss: 1.5839 (1.5567)\n",
            "TRAIN(236): [ 60/196] Batch: 0.0835 (0.0839) Data: 0.0403 (0.0520) Loss: 1.6843 (1.5624)\n",
            "TRAIN(236): [ 70/196] Batch: 0.0524 (0.0829) Data: 0.0553 (0.0506) Loss: 1.4015 (1.5607)\n",
            "TRAIN(236): [ 80/196] Batch: 0.0703 (0.0822) Data: 0.0555 (0.0498) Loss: 1.5090 (1.5583)\n",
            "TRAIN(236): [ 90/196] Batch: 0.0653 (0.0814) Data: 0.0609 (0.0497) Loss: 1.6372 (1.5619)\n",
            "TRAIN(236): [100/196] Batch: 0.0783 (0.0810) Data: 0.0525 (0.0496) Loss: 1.6947 (1.5615)\n",
            "TRAIN(236): [110/196] Batch: 0.0724 (0.0805) Data: 0.0574 (0.0497) Loss: 1.7373 (1.5641)\n",
            "TRAIN(236): [120/196] Batch: 0.0797 (0.0802) Data: 0.0498 (0.0494) Loss: 1.4864 (1.5661)\n",
            "TRAIN(236): [130/196] Batch: 0.0804 (0.0799) Data: 0.0510 (0.0493) Loss: 1.4853 (1.5640)\n",
            "TRAIN(236): [140/196] Batch: 0.0651 (0.0796) Data: 0.0588 (0.0492) Loss: 1.5001 (1.5570)\n",
            "TRAIN(236): [150/196] Batch: 0.0745 (0.0794) Data: 0.0509 (0.0491) Loss: 1.8169 (1.5573)\n",
            "TRAIN(236): [160/196] Batch: 0.0715 (0.0792) Data: 0.0548 (0.0489) Loss: 1.5165 (1.5568)\n",
            "TRAIN(236): [170/196] Batch: 0.0725 (0.0790) Data: 0.0547 (0.0489) Loss: 1.5408 (1.5569)\n",
            "TRAIN(236): [180/196] Batch: 0.0680 (0.0789) Data: 0.0551 (0.0488) Loss: 1.5433 (1.5554)\n",
            "TRAIN(236): [190/196] Batch: 0.0774 (0.0787) Data: 0.0604 (0.0489) Loss: 1.5262 (1.5563)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(236)         0:00:15         0:00:09         0:00:05          1.5546\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(237): [ 10/196] Batch: 0.0652 (0.1220) Data: 0.0497 (0.0784) Loss: 1.6980 (1.5890)\n",
            "TRAIN(237): [ 20/196] Batch: 0.0777 (0.1008) Data: 0.0489 (0.0624) Loss: 1.6065 (1.5728)\n",
            "TRAIN(237): [ 30/196] Batch: 0.0908 (0.0936) Data: 0.0387 (0.0561) Loss: 1.5174 (1.5529)\n",
            "TRAIN(237): [ 40/196] Batch: 0.0631 (0.0893) Data: 0.0473 (0.0530) Loss: 1.6924 (1.5636)\n",
            "TRAIN(237): [ 50/196] Batch: 0.0703 (0.0866) Data: 0.0545 (0.0513) Loss: 1.4714 (1.5620)\n",
            "TRAIN(237): [ 60/196] Batch: 0.0712 (0.0848) Data: 0.0556 (0.0508) Loss: 1.4767 (1.5570)\n",
            "TRAIN(237): [ 70/196] Batch: 0.0867 (0.0836) Data: 0.0498 (0.0507) Loss: 1.4314 (1.5570)\n",
            "TRAIN(237): [ 80/196] Batch: 0.0882 (0.0827) Data: 0.0498 (0.0506) Loss: 1.6374 (1.5550)\n",
            "TRAIN(237): [ 90/196] Batch: 0.0838 (0.0819) Data: 0.0540 (0.0504) Loss: 1.6667 (1.5580)\n",
            "TRAIN(237): [100/196] Batch: 0.0839 (0.0813) Data: 0.0522 (0.0501) Loss: 1.5939 (1.5575)\n",
            "TRAIN(237): [110/196] Batch: 0.0794 (0.0809) Data: 0.0488 (0.0501) Loss: 1.5095 (1.5611)\n",
            "TRAIN(237): [120/196] Batch: 0.0724 (0.0805) Data: 0.0544 (0.0498) Loss: 1.4652 (1.5623)\n",
            "TRAIN(237): [130/196] Batch: 0.0758 (0.0801) Data: 0.0606 (0.0496) Loss: 1.6322 (1.5634)\n",
            "TRAIN(237): [140/196] Batch: 0.0721 (0.0798) Data: 0.0539 (0.0497) Loss: 1.5909 (1.5651)\n",
            "TRAIN(237): [150/196] Batch: 0.0662 (0.0796) Data: 0.0598 (0.0496) Loss: 1.5251 (1.5632)\n",
            "TRAIN(237): [160/196] Batch: 0.0641 (0.0794) Data: 0.0593 (0.0494) Loss: 1.5055 (1.5649)\n",
            "TRAIN(237): [170/196] Batch: 0.0873 (0.0792) Data: 0.0492 (0.0494) Loss: 1.4938 (1.5627)\n",
            "TRAIN(237): [180/196] Batch: 0.0806 (0.0790) Data: 0.0493 (0.0493) Loss: 1.4746 (1.5633)\n",
            "TRAIN(237): [190/196] Batch: 0.0760 (0.0789) Data: 0.0591 (0.0492) Loss: 1.5957 (1.5635)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(237)         0:00:15         0:00:09         0:00:05          1.5647\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(238): [ 10/196] Batch: 0.0831 (0.1318) Data: 0.0392 (0.0809) Loss: 1.4602 (1.5633)\n",
            "TRAIN(238): [ 20/196] Batch: 0.0736 (0.1038) Data: 0.0501 (0.0636) Loss: 1.5615 (1.5480)\n",
            "TRAIN(238): [ 30/196] Batch: 0.0754 (0.0942) Data: 0.0615 (0.0593) Loss: 1.3521 (1.5356)\n",
            "TRAIN(238): [ 40/196] Batch: 0.0740 (0.0897) Data: 0.0611 (0.0568) Loss: 1.6073 (1.5396)\n",
            "TRAIN(238): [ 50/196] Batch: 0.0705 (0.0871) Data: 0.0552 (0.0551) Loss: 1.3948 (1.5347)\n",
            "TRAIN(238): [ 60/196] Batch: 0.0717 (0.0852) Data: 0.0553 (0.0539) Loss: 1.5606 (1.5432)\n",
            "TRAIN(238): [ 70/196] Batch: 0.0786 (0.0841) Data: 0.0485 (0.0528) Loss: 1.5683 (1.5462)\n",
            "TRAIN(238): [ 80/196] Batch: 0.0858 (0.0830) Data: 0.0505 (0.0524) Loss: 1.9296 (1.5532)\n",
            "TRAIN(238): [ 90/196] Batch: 0.0767 (0.0822) Data: 0.0545 (0.0520) Loss: 1.4413 (1.5536)\n",
            "TRAIN(238): [100/196] Batch: 0.0868 (0.0816) Data: 0.0479 (0.0518) Loss: 1.4545 (1.5568)\n",
            "TRAIN(238): [110/196] Batch: 0.0697 (0.0810) Data: 0.0546 (0.0513) Loss: 1.5285 (1.5518)\n",
            "TRAIN(238): [120/196] Batch: 0.0782 (0.0806) Data: 0.0572 (0.0512) Loss: 1.6365 (1.5523)\n",
            "TRAIN(238): [130/196] Batch: 0.0821 (0.0804) Data: 0.0491 (0.0508) Loss: 1.6306 (1.5559)\n",
            "TRAIN(238): [140/196] Batch: 0.0788 (0.0801) Data: 0.0520 (0.0505) Loss: 1.5030 (1.5567)\n",
            "TRAIN(238): [150/196] Batch: 0.0757 (0.0799) Data: 0.0492 (0.0502) Loss: 1.7460 (1.5545)\n",
            "TRAIN(238): [160/196] Batch: 0.0821 (0.0798) Data: 0.0444 (0.0498) Loss: 1.5096 (1.5539)\n",
            "TRAIN(238): [170/196] Batch: 0.0618 (0.0796) Data: 0.0515 (0.0494) Loss: 1.6021 (1.5544)\n",
            "TRAIN(238): [180/196] Batch: 0.0733 (0.0796) Data: 0.0495 (0.0490) Loss: 1.5618 (1.5548)\n",
            "TRAIN(238): [190/196] Batch: 0.0757 (0.0793) Data: 0.0606 (0.0491) Loss: 1.5685 (1.5556)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(238)         0:00:15         0:00:09         0:00:05          1.5544\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(239): [ 10/196] Batch: 0.0670 (0.1193) Data: 0.0565 (0.0840) Loss: 1.5812 (1.5442)\n",
            "TRAIN(239): [ 20/196] Batch: 0.0665 (0.0980) Data: 0.0565 (0.0674) Loss: 1.6619 (1.5260)\n",
            "TRAIN(239): [ 30/196] Batch: 0.0880 (0.0910) Data: 0.0481 (0.0611) Loss: 1.6760 (1.5442)\n",
            "TRAIN(239): [ 40/196] Batch: 0.0715 (0.0871) Data: 0.0554 (0.0582) Loss: 1.5951 (1.5561)\n",
            "TRAIN(239): [ 50/196] Batch: 0.0739 (0.0848) Data: 0.0614 (0.0568) Loss: 1.5846 (1.5520)\n",
            "TRAIN(239): [ 60/196] Batch: 0.0851 (0.0836) Data: 0.0469 (0.0554) Loss: 1.5452 (1.5532)\n",
            "TRAIN(239): [ 70/196] Batch: 0.0808 (0.0825) Data: 0.0498 (0.0544) Loss: 1.5397 (1.5545)\n",
            "TRAIN(239): [ 80/196] Batch: 0.0672 (0.0816) Data: 0.0601 (0.0536) Loss: 1.5025 (1.5576)\n",
            "TRAIN(239): [ 90/196] Batch: 0.0662 (0.0810) Data: 0.0625 (0.0531) Loss: 1.5517 (1.5547)\n",
            "TRAIN(239): [100/196] Batch: 0.0883 (0.0806) Data: 0.0488 (0.0527) Loss: 1.6365 (1.5549)\n",
            "TRAIN(239): [110/196] Batch: 0.0752 (0.0802) Data: 0.0491 (0.0523) Loss: 1.5933 (1.5542)\n",
            "TRAIN(239): [120/196] Batch: 0.0762 (0.0800) Data: 0.0460 (0.0515) Loss: 1.4727 (1.5529)\n",
            "TRAIN(239): [130/196] Batch: 0.0666 (0.0797) Data: 0.0551 (0.0510) Loss: 1.4047 (1.5500)\n",
            "TRAIN(239): [140/196] Batch: 0.0627 (0.0795) Data: 0.0537 (0.0506) Loss: 1.4679 (1.5486)\n",
            "TRAIN(239): [150/196] Batch: 0.0684 (0.0794) Data: 0.0518 (0.0502) Loss: 1.5132 (1.5517)\n",
            "TRAIN(239): [160/196] Batch: 0.0734 (0.0793) Data: 0.0462 (0.0500) Loss: 1.6220 (1.5526)\n",
            "TRAIN(239): [170/196] Batch: 0.0696 (0.0790) Data: 0.0613 (0.0501) Loss: 1.6095 (1.5519)\n",
            "TRAIN(239): [180/196] Batch: 0.0772 (0.0789) Data: 0.0552 (0.0499) Loss: 1.5044 (1.5538)\n",
            "TRAIN(239): [190/196] Batch: 0.0749 (0.0787) Data: 0.0609 (0.0499) Loss: 1.5060 (1.5546)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(239)         0:00:15         0:00:09         0:00:05          1.5549\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(240): [ 10/196] Batch: 0.0872 (0.1204) Data: 0.0483 (0.0852) Loss: 1.6713 (1.5871)\n",
            "TRAIN(240): [ 20/196] Batch: 0.0821 (0.0985) Data: 0.0476 (0.0671) Loss: 1.5669 (1.5801)\n",
            "TRAIN(240): [ 30/196] Batch: 0.0821 (0.0912) Data: 0.0492 (0.0606) Loss: 1.4814 (1.5608)\n",
            "TRAIN(240): [ 40/196] Batch: 0.0701 (0.0873) Data: 0.0564 (0.0574) Loss: 1.5374 (1.5698)\n",
            "TRAIN(240): [ 50/196] Batch: 0.0697 (0.0850) Data: 0.0564 (0.0555) Loss: 1.6845 (1.5583)\n",
            "TRAIN(240): [ 60/196] Batch: 0.0706 (0.0835) Data: 0.0548 (0.0547) Loss: 1.4813 (1.5543)\n",
            "TRAIN(240): [ 70/196] Batch: 0.0798 (0.0826) Data: 0.0489 (0.0538) Loss: 1.4600 (1.5574)\n",
            "TRAIN(240): [ 80/196] Batch: 0.0774 (0.0818) Data: 0.0517 (0.0530) Loss: 1.6799 (1.5606)\n",
            "TRAIN(240): [ 90/196] Batch: 0.0777 (0.0814) Data: 0.0470 (0.0524) Loss: 1.5633 (1.5597)\n",
            "TRAIN(240): [100/196] Batch: 0.0726 (0.0810) Data: 0.0481 (0.0514) Loss: 1.6798 (1.5604)\n",
            "TRAIN(240): [110/196] Batch: 0.0750 (0.0806) Data: 0.0503 (0.0506) Loss: 1.4911 (1.5620)\n",
            "TRAIN(240): [120/196] Batch: 0.0741 (0.0804) Data: 0.0496 (0.0500) Loss: 1.4691 (1.5618)\n",
            "TRAIN(240): [130/196] Batch: 0.0703 (0.0800) Data: 0.0608 (0.0501) Loss: 1.5720 (1.5585)\n",
            "TRAIN(240): [140/196] Batch: 0.0656 (0.0797) Data: 0.0630 (0.0500) Loss: 1.4613 (1.5604)\n",
            "TRAIN(240): [150/196] Batch: 0.0820 (0.0796) Data: 0.0491 (0.0499) Loss: 1.4207 (1.5618)\n",
            "TRAIN(240): [160/196] Batch: 0.0704 (0.0793) Data: 0.0531 (0.0498) Loss: 1.5207 (1.5626)\n",
            "TRAIN(240): [170/196] Batch: 0.0801 (0.0792) Data: 0.0508 (0.0496) Loss: 1.6270 (1.5653)\n",
            "TRAIN(240): [180/196] Batch: 0.0769 (0.0790) Data: 0.0551 (0.0495) Loss: 1.6311 (1.5649)\n",
            "TRAIN(240): [190/196] Batch: 0.0755 (0.0788) Data: 0.0618 (0.0496) Loss: 1.6823 (1.5623)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(240)         0:00:15         0:00:09         0:00:05          1.5631\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(241): [ 10/196] Batch: 0.0769 (0.1207) Data: 0.0486 (0.0858) Loss: 1.5661 (1.5648)\n",
            "TRAIN(241): [ 20/196] Batch: 0.0696 (0.0985) Data: 0.0542 (0.0681) Loss: 1.5853 (1.5484)\n",
            "TRAIN(241): [ 30/196] Batch: 0.0722 (0.0912) Data: 0.0546 (0.0623) Loss: 1.5716 (1.5499)\n",
            "TRAIN(241): [ 40/196] Batch: 0.0759 (0.0872) Data: 0.0629 (0.0589) Loss: 1.5569 (1.5525)\n",
            "TRAIN(241): [ 50/196] Batch: 0.0765 (0.0852) Data: 0.0601 (0.0565) Loss: 1.6335 (1.5490)\n",
            "TRAIN(241): [ 60/196] Batch: 0.0776 (0.0839) Data: 0.0507 (0.0552) Loss: 1.4246 (1.5522)\n",
            "TRAIN(241): [ 70/196] Batch: 0.0764 (0.0830) Data: 0.0523 (0.0533) Loss: 1.3904 (1.5536)\n",
            "TRAIN(241): [ 80/196] Batch: 0.0751 (0.0824) Data: 0.0482 (0.0521) Loss: 1.4061 (1.5552)\n",
            "TRAIN(241): [ 90/196] Batch: 0.0738 (0.0818) Data: 0.0491 (0.0512) Loss: 1.4037 (1.5494)\n",
            "TRAIN(241): [100/196] Batch: 0.0794 (0.0812) Data: 0.0509 (0.0507) Loss: 1.5587 (1.5531)\n",
            "TRAIN(241): [110/196] Batch: 0.0715 (0.0807) Data: 0.0625 (0.0505) Loss: 1.5644 (1.5552)\n",
            "TRAIN(241): [120/196] Batch: 0.0671 (0.0803) Data: 0.0592 (0.0503) Loss: 1.8618 (1.5567)\n",
            "TRAIN(241): [130/196] Batch: 0.0689 (0.0800) Data: 0.0557 (0.0503) Loss: 1.5950 (1.5559)\n",
            "TRAIN(241): [140/196] Batch: 0.0696 (0.0798) Data: 0.0557 (0.0500) Loss: 1.3633 (1.5549)\n",
            "TRAIN(241): [150/196] Batch: 0.0756 (0.0796) Data: 0.0509 (0.0498) Loss: 1.5707 (1.5559)\n",
            "TRAIN(241): [160/196] Batch: 0.0695 (0.0793) Data: 0.0617 (0.0498) Loss: 1.7401 (1.5560)\n",
            "TRAIN(241): [170/196] Batch: 0.0775 (0.0792) Data: 0.0570 (0.0498) Loss: 1.4687 (1.5537)\n",
            "TRAIN(241): [180/196] Batch: 0.0720 (0.0790) Data: 0.0553 (0.0497) Loss: 1.6488 (1.5529)\n",
            "TRAIN(241): [190/196] Batch: 0.0768 (0.0789) Data: 0.0603 (0.0497) Loss: 1.4833 (1.5536)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(241)         0:00:15         0:00:09         0:00:05          1.5542\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(242): [ 10/196] Batch: 0.0770 (0.1203) Data: 0.0500 (0.0833) Loss: 1.5722 (1.5649)\n",
            "TRAIN(242): [ 20/196] Batch: 0.0798 (0.0988) Data: 0.0472 (0.0667) Loss: 1.5482 (1.5656)\n",
            "TRAIN(242): [ 30/196] Batch: 0.0733 (0.0914) Data: 0.0571 (0.0599) Loss: 1.4948 (1.5658)\n",
            "TRAIN(242): [ 40/196] Batch: 0.0753 (0.0881) Data: 0.0502 (0.0552) Loss: 1.6261 (1.5631)\n",
            "TRAIN(242): [ 50/196] Batch: 0.0747 (0.0862) Data: 0.0560 (0.0537) Loss: 1.5332 (1.5607)\n",
            "TRAIN(242): [ 60/196] Batch: 0.0756 (0.0850) Data: 0.0481 (0.0520) Loss: 1.4864 (1.5618)\n",
            "TRAIN(242): [ 70/196] Batch: 0.0834 (0.0838) Data: 0.0485 (0.0513) Loss: 1.5491 (1.5664)\n",
            "TRAIN(242): [ 80/196] Batch: 0.0652 (0.0828) Data: 0.0583 (0.0509) Loss: 1.6481 (1.5660)\n",
            "TRAIN(242): [ 90/196] Batch: 0.0841 (0.0822) Data: 0.0470 (0.0504) Loss: 1.6002 (1.5674)\n",
            "TRAIN(242): [100/196] Batch: 0.0747 (0.0816) Data: 0.0519 (0.0501) Loss: 1.7052 (1.5616)\n",
            "TRAIN(242): [110/196] Batch: 0.0677 (0.0811) Data: 0.0608 (0.0499) Loss: 1.4677 (1.5564)\n",
            "TRAIN(242): [120/196] Batch: 0.0843 (0.0808) Data: 0.0503 (0.0497) Loss: 1.5147 (1.5583)\n",
            "TRAIN(242): [130/196] Batch: 0.0645 (0.0803) Data: 0.0620 (0.0497) Loss: 1.5778 (1.5584)\n",
            "TRAIN(242): [140/196] Batch: 0.0709 (0.0801) Data: 0.0535 (0.0496) Loss: 1.5091 (1.5576)\n",
            "TRAIN(242): [150/196] Batch: 0.0826 (0.0799) Data: 0.0490 (0.0496) Loss: 1.4923 (1.5564)\n",
            "TRAIN(242): [160/196] Batch: 0.0709 (0.0796) Data: 0.0623 (0.0496) Loss: 1.6398 (1.5568)\n",
            "TRAIN(242): [170/196] Batch: 0.0810 (0.0795) Data: 0.0523 (0.0495) Loss: 1.5170 (1.5566)\n",
            "TRAIN(242): [180/196] Batch: 0.0626 (0.0792) Data: 0.0618 (0.0497) Loss: 1.5981 (1.5576)\n",
            "TRAIN(242): [190/196] Batch: 0.0756 (0.0791) Data: 0.0596 (0.0497) Loss: 1.4658 (1.5576)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(242)         0:00:15         0:00:09         0:00:05          1.5581\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(243): [ 10/196] Batch: 0.0575 (0.1404) Data: 0.0439 (0.0923) Loss: 1.4411 (1.5110)\n",
            "TRAIN(243): [ 20/196] Batch: 0.0804 (0.1095) Data: 0.0512 (0.0697) Loss: 1.6497 (1.5313)\n",
            "TRAIN(243): [ 30/196] Batch: 0.0731 (0.0986) Data: 0.0497 (0.0621) Loss: 1.5545 (1.5390)\n",
            "TRAIN(243): [ 40/196] Batch: 0.0814 (0.0931) Data: 0.0499 (0.0589) Loss: 1.4525 (1.5418)\n",
            "TRAIN(243): [ 50/196] Batch: 0.0704 (0.0896) Data: 0.0574 (0.0570) Loss: 1.6473 (1.5523)\n",
            "TRAIN(243): [ 60/196] Batch: 0.0709 (0.0874) Data: 0.0537 (0.0554) Loss: 1.5422 (1.5650)\n",
            "TRAIN(243): [ 70/196] Batch: 0.0644 (0.0858) Data: 0.0588 (0.0545) Loss: 1.6244 (1.5666)\n",
            "TRAIN(243): [ 80/196] Batch: 0.0786 (0.0847) Data: 0.0538 (0.0538) Loss: 1.3696 (1.5633)\n",
            "TRAIN(243): [ 90/196] Batch: 0.0823 (0.0838) Data: 0.0550 (0.0532) Loss: 1.5998 (1.5667)\n",
            "TRAIN(243): [100/196] Batch: 0.0723 (0.0831) Data: 0.0536 (0.0524) Loss: 1.6323 (1.5684)\n",
            "TRAIN(243): [110/196] Batch: 0.0764 (0.0824) Data: 0.0601 (0.0523) Loss: 1.4419 (1.5669)\n",
            "TRAIN(243): [120/196] Batch: 0.0639 (0.0819) Data: 0.0615 (0.0522) Loss: 1.4682 (1.5627)\n",
            "TRAIN(243): [130/196] Batch: 0.0812 (0.0816) Data: 0.0471 (0.0519) Loss: 1.5845 (1.5602)\n",
            "TRAIN(243): [140/196] Batch: 0.0829 (0.0812) Data: 0.0503 (0.0516) Loss: 1.4729 (1.5615)\n",
            "TRAIN(243): [150/196] Batch: 0.0751 (0.0808) Data: 0.0569 (0.0515) Loss: 1.8672 (1.5681)\n",
            "TRAIN(243): [160/196] Batch: 0.0655 (0.0807) Data: 0.0513 (0.0511) Loss: 1.4149 (1.5659)\n",
            "TRAIN(243): [170/196] Batch: 0.0785 (0.0806) Data: 0.0511 (0.0507) Loss: 1.6968 (1.5680)\n",
            "TRAIN(243): [180/196] Batch: 0.0915 (0.0806) Data: 0.0321 (0.0501) Loss: 1.4050 (1.5674)\n",
            "TRAIN(243): [190/196] Batch: 0.0774 (0.0804) Data: 0.0606 (0.0498) Loss: 1.4505 (1.5679)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(243)         0:00:15         0:00:09         0:00:05          1.5675\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(244): [ 10/196] Batch: 0.0616 (0.1189) Data: 0.0600 (0.0784) Loss: 1.5420 (1.5281)\n",
            "TRAIN(244): [ 20/196] Batch: 0.0870 (0.0985) Data: 0.0494 (0.0651) Loss: 1.5253 (1.5511)\n",
            "TRAIN(244): [ 30/196] Batch: 0.0699 (0.0910) Data: 0.0561 (0.0597) Loss: 1.6291 (1.5606)\n",
            "TRAIN(244): [ 40/196] Batch: 0.0840 (0.0874) Data: 0.0511 (0.0571) Loss: 1.7223 (1.5686)\n",
            "TRAIN(244): [ 50/196] Batch: 0.0824 (0.0851) Data: 0.0504 (0.0558) Loss: 1.5610 (1.5604)\n",
            "TRAIN(244): [ 60/196] Batch: 0.0861 (0.0837) Data: 0.0490 (0.0549) Loss: 1.4258 (1.5538)\n",
            "TRAIN(244): [ 70/196] Batch: 0.0725 (0.0826) Data: 0.0550 (0.0541) Loss: 1.4640 (1.5510)\n",
            "TRAIN(244): [ 80/196] Batch: 0.0810 (0.0819) Data: 0.0468 (0.0531) Loss: 1.6099 (1.5478)\n",
            "TRAIN(244): [ 90/196] Batch: 0.0904 (0.0813) Data: 0.0464 (0.0528) Loss: 1.3779 (1.5440)\n",
            "TRAIN(244): [100/196] Batch: 0.0836 (0.0809) Data: 0.0418 (0.0520) Loss: 1.5545 (1.5487)\n",
            "TRAIN(244): [110/196] Batch: 0.0908 (0.0806) Data: 0.0446 (0.0511) Loss: 1.4918 (1.5517)\n",
            "TRAIN(244): [120/196] Batch: 0.1120 (0.0808) Data: 0.0248 (0.0502) Loss: 1.6217 (1.5519)\n",
            "TRAIN(244): [130/196] Batch: 0.1099 (0.0809) Data: 0.0419 (0.0492) Loss: 1.5280 (1.5519)\n",
            "TRAIN(244): [140/196] Batch: 0.0861 (0.0810) Data: 0.0291 (0.0482) Loss: 1.5932 (1.5506)\n",
            "TRAIN(244): [150/196] Batch: 0.0814 (0.0811) Data: 0.0475 (0.0479) Loss: 1.6153 (1.5528)\n",
            "TRAIN(244): [160/196] Batch: 0.0636 (0.0809) Data: 0.0581 (0.0476) Loss: 1.5073 (1.5517)\n",
            "TRAIN(244): [170/196] Batch: 0.0877 (0.0807) Data: 0.0489 (0.0476) Loss: 1.4395 (1.5496)\n",
            "TRAIN(244): [180/196] Batch: 0.0810 (0.0804) Data: 0.0519 (0.0475) Loss: 1.6178 (1.5511)\n",
            "TRAIN(244): [190/196] Batch: 0.0748 (0.0801) Data: 0.0632 (0.0476) Loss: 1.3500 (1.5505)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(244)         0:00:15         0:00:09         0:00:06          1.5497\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(245): [ 10/196] Batch: 0.0836 (0.1209) Data: 0.0512 (0.0867) Loss: 1.6809 (1.5402)\n",
            "TRAIN(245): [ 20/196] Batch: 0.0837 (0.0992) Data: 0.0469 (0.0687) Loss: 1.5674 (1.5482)\n",
            "TRAIN(245): [ 30/196] Batch: 0.0686 (0.0913) Data: 0.0558 (0.0622) Loss: 1.5268 (1.5529)\n",
            "TRAIN(245): [ 40/196] Batch: 0.0669 (0.0875) Data: 0.0606 (0.0589) Loss: 1.4848 (1.5540)\n",
            "TRAIN(245): [ 50/196] Batch: 0.0804 (0.0853) Data: 0.0544 (0.0571) Loss: 1.5559 (1.5543)\n",
            "TRAIN(245): [ 60/196] Batch: 0.0701 (0.0837) Data: 0.0612 (0.0558) Loss: 1.6181 (1.5541)\n",
            "TRAIN(245): [ 70/196] Batch: 0.0796 (0.0827) Data: 0.0538 (0.0551) Loss: 1.6802 (1.5521)\n",
            "TRAIN(245): [ 80/196] Batch: 0.0827 (0.0820) Data: 0.0497 (0.0541) Loss: 1.6119 (1.5517)\n",
            "TRAIN(245): [ 90/196] Batch: 0.0750 (0.0813) Data: 0.0552 (0.0536) Loss: 1.6086 (1.5481)\n",
            "TRAIN(245): [100/196] Batch: 0.0768 (0.0811) Data: 0.0554 (0.0528) Loss: 1.5609 (1.5491)\n",
            "TRAIN(245): [110/196] Batch: 0.0769 (0.0809) Data: 0.0404 (0.0519) Loss: 1.5400 (1.5483)\n",
            "TRAIN(245): [120/196] Batch: 0.0622 (0.0807) Data: 0.0497 (0.0512) Loss: 1.5874 (1.5475)\n",
            "TRAIN(245): [130/196] Batch: 0.0772 (0.0806) Data: 0.0512 (0.0506) Loss: 1.6176 (1.5464)\n",
            "TRAIN(245): [140/196] Batch: 0.0696 (0.0803) Data: 0.0544 (0.0503) Loss: 1.3676 (1.5449)\n",
            "TRAIN(245): [150/196] Batch: 0.0722 (0.0800) Data: 0.0620 (0.0502) Loss: 1.7785 (1.5482)\n",
            "TRAIN(245): [160/196] Batch: 0.0900 (0.0798) Data: 0.0505 (0.0501) Loss: 1.4627 (1.5477)\n",
            "TRAIN(245): [170/196] Batch: 0.0631 (0.0795) Data: 0.0612 (0.0499) Loss: 1.6463 (1.5525)\n",
            "TRAIN(245): [180/196] Batch: 0.0796 (0.0794) Data: 0.0497 (0.0499) Loss: 1.6003 (1.5534)\n",
            "TRAIN(245): [190/196] Batch: 0.0760 (0.0792) Data: 0.0613 (0.0498) Loss: 1.5124 (1.5515)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(245)         0:00:15         0:00:09         0:00:05          1.5526\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(246): [ 10/196] Batch: 0.0770 (0.1226) Data: 0.0481 (0.0837) Loss: 1.4570 (1.5480)\n",
            "TRAIN(246): [ 20/196] Batch: 0.0789 (0.0997) Data: 0.0524 (0.0668) Loss: 1.4546 (1.5705)\n",
            "TRAIN(246): [ 30/196] Batch: 0.0770 (0.0918) Data: 0.0542 (0.0606) Loss: 1.4212 (1.5687)\n",
            "TRAIN(246): [ 40/196] Batch: 0.0777 (0.0877) Data: 0.0613 (0.0578) Loss: 1.7430 (1.5736)\n",
            "TRAIN(246): [ 50/196] Batch: 0.0813 (0.0857) Data: 0.0498 (0.0556) Loss: 1.4548 (1.5713)\n",
            "TRAIN(246): [ 60/196] Batch: 0.0592 (0.0840) Data: 0.0554 (0.0540) Loss: 1.5307 (1.5708)\n",
            "TRAIN(246): [ 70/196] Batch: 0.0745 (0.0836) Data: 0.0475 (0.0528) Loss: 1.4360 (1.5691)\n",
            "TRAIN(246): [ 80/196] Batch: 0.0818 (0.0832) Data: 0.0483 (0.0514) Loss: 1.5392 (1.5679)\n",
            "TRAIN(246): [ 90/196] Batch: 0.0944 (0.0829) Data: 0.0417 (0.0509) Loss: 1.6172 (1.5734)\n",
            "TRAIN(246): [100/196] Batch: 0.0770 (0.0822) Data: 0.0485 (0.0502) Loss: 1.5697 (1.5716)\n",
            "TRAIN(246): [110/196] Batch: 0.0641 (0.0816) Data: 0.0585 (0.0501) Loss: 1.5953 (1.5758)\n",
            "TRAIN(246): [120/196] Batch: 0.0687 (0.0811) Data: 0.0565 (0.0501) Loss: 1.4159 (1.5665)\n",
            "TRAIN(246): [130/196] Batch: 0.0753 (0.0807) Data: 0.0612 (0.0502) Loss: 1.6098 (1.5636)\n",
            "TRAIN(246): [140/196] Batch: 0.0798 (0.0805) Data: 0.0551 (0.0500) Loss: 1.4556 (1.5646)\n",
            "TRAIN(246): [150/196] Batch: 0.0636 (0.0801) Data: 0.0612 (0.0499) Loss: 1.5092 (1.5644)\n",
            "TRAIN(246): [160/196] Batch: 0.0733 (0.0799) Data: 0.0564 (0.0498) Loss: 1.6251 (1.5668)\n",
            "TRAIN(246): [170/196] Batch: 0.0713 (0.0797) Data: 0.0535 (0.0497) Loss: 1.6978 (1.5680)\n",
            "TRAIN(246): [180/196] Batch: 0.0729 (0.0796) Data: 0.0542 (0.0497) Loss: 1.6945 (1.5683)\n",
            "TRAIN(246): [190/196] Batch: 0.0754 (0.0793) Data: 0.0618 (0.0497) Loss: 1.6334 (1.5706)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(246)         0:00:15         0:00:09         0:00:05          1.5703\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(247): [ 10/196] Batch: 0.0743 (0.1155) Data: 0.0486 (0.0823) Loss: 1.5465 (1.5691)\n",
            "TRAIN(247): [ 20/196] Batch: 0.0815 (0.0963) Data: 0.0478 (0.0666) Loss: 1.7430 (1.5564)\n",
            "TRAIN(247): [ 30/196] Batch: 0.0746 (0.0898) Data: 0.0483 (0.0609) Loss: 1.5577 (1.5463)\n",
            "TRAIN(247): [ 40/196] Batch: 0.0796 (0.0865) Data: 0.0527 (0.0568) Loss: 1.8174 (1.5582)\n",
            "TRAIN(247): [ 50/196] Batch: 0.0810 (0.0850) Data: 0.0488 (0.0546) Loss: 1.4344 (1.5504)\n",
            "TRAIN(247): [ 60/196] Batch: 0.0787 (0.0842) Data: 0.0304 (0.0528) Loss: 1.5316 (1.5542)\n",
            "TRAIN(247): [ 70/196] Batch: 0.0710 (0.0829) Data: 0.0566 (0.0517) Loss: 1.4756 (1.5534)\n",
            "TRAIN(247): [ 80/196] Batch: 0.0899 (0.0822) Data: 0.0466 (0.0515) Loss: 1.5092 (1.5556)\n",
            "TRAIN(247): [ 90/196] Batch: 0.0704 (0.0813) Data: 0.0635 (0.0513) Loss: 1.6428 (1.5572)\n",
            "TRAIN(247): [100/196] Batch: 0.0759 (0.0809) Data: 0.0546 (0.0510) Loss: 1.4066 (1.5588)\n",
            "TRAIN(247): [110/196] Batch: 0.0688 (0.0804) Data: 0.0617 (0.0510) Loss: 1.4642 (1.5566)\n",
            "TRAIN(247): [120/196] Batch: 0.0698 (0.0801) Data: 0.0618 (0.0509) Loss: 1.3936 (1.5550)\n",
            "TRAIN(247): [130/196] Batch: 0.0733 (0.0798) Data: 0.0544 (0.0508) Loss: 1.6180 (1.5552)\n",
            "TRAIN(247): [140/196] Batch: 0.0706 (0.0795) Data: 0.0610 (0.0507) Loss: 1.5940 (1.5602)\n",
            "TRAIN(247): [150/196] Batch: 0.0666 (0.0793) Data: 0.0606 (0.0508) Loss: 1.6269 (1.5616)\n",
            "TRAIN(247): [160/196] Batch: 0.0790 (0.0792) Data: 0.0488 (0.0505) Loss: 1.5388 (1.5586)\n",
            "TRAIN(247): [170/196] Batch: 0.0852 (0.0790) Data: 0.0525 (0.0504) Loss: 1.5754 (1.5582)\n",
            "TRAIN(247): [180/196] Batch: 0.0798 (0.0789) Data: 0.0520 (0.0502) Loss: 1.6641 (1.5585)\n",
            "TRAIN(247): [190/196] Batch: 0.0753 (0.0787) Data: 0.0615 (0.0502) Loss: 1.5450 (1.5568)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(247)         0:00:15         0:00:09         0:00:05          1.5577\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(248): [ 10/196] Batch: 0.0644 (0.1306) Data: 0.0493 (0.0861) Loss: 1.6178 (1.5126)\n",
            "TRAIN(248): [ 20/196] Batch: 0.0735 (0.1054) Data: 0.0526 (0.0667) Loss: 1.4717 (1.5130)\n",
            "TRAIN(248): [ 30/196] Batch: 0.0847 (0.0968) Data: 0.0431 (0.0574) Loss: 1.7464 (1.5368)\n",
            "TRAIN(248): [ 40/196] Batch: 0.0704 (0.0913) Data: 0.0573 (0.0554) Loss: 1.5536 (1.5385)\n",
            "TRAIN(248): [ 50/196] Batch: 0.0870 (0.0884) Data: 0.0508 (0.0544) Loss: 1.6471 (1.5467)\n",
            "TRAIN(248): [ 60/196] Batch: 0.0671 (0.0862) Data: 0.0578 (0.0535) Loss: 1.5211 (1.5484)\n",
            "TRAIN(248): [ 70/196] Batch: 0.0871 (0.0849) Data: 0.0505 (0.0530) Loss: 1.5803 (1.5436)\n",
            "TRAIN(248): [ 80/196] Batch: 0.0780 (0.0839) Data: 0.0488 (0.0522) Loss: 1.5396 (1.5474)\n",
            "TRAIN(248): [ 90/196] Batch: 0.0810 (0.0830) Data: 0.0470 (0.0516) Loss: 1.5954 (1.5506)\n",
            "TRAIN(248): [100/196] Batch: 0.0889 (0.0824) Data: 0.0473 (0.0513) Loss: 1.4208 (1.5507)\n",
            "TRAIN(248): [110/196] Batch: 0.0705 (0.0818) Data: 0.0548 (0.0509) Loss: 1.5556 (1.5538)\n",
            "TRAIN(248): [120/196] Batch: 0.0784 (0.0813) Data: 0.0541 (0.0508) Loss: 1.5747 (1.5571)\n",
            "TRAIN(248): [130/196] Batch: 0.0840 (0.0810) Data: 0.0493 (0.0505) Loss: 1.5120 (1.5580)\n",
            "TRAIN(248): [140/196] Batch: 0.0634 (0.0805) Data: 0.0614 (0.0504) Loss: 1.6138 (1.5580)\n",
            "TRAIN(248): [150/196] Batch: 0.0795 (0.0804) Data: 0.0491 (0.0502) Loss: 1.5038 (1.5565)\n",
            "TRAIN(248): [160/196] Batch: 0.0706 (0.0800) Data: 0.0550 (0.0501) Loss: 1.4766 (1.5562)\n",
            "TRAIN(248): [170/196] Batch: 0.0836 (0.0799) Data: 0.0415 (0.0498) Loss: 1.5593 (1.5583)\n",
            "TRAIN(248): [180/196] Batch: 0.0756 (0.0797) Data: 0.0555 (0.0497) Loss: 1.5955 (1.5565)\n",
            "TRAIN(248): [190/196] Batch: 0.0774 (0.0796) Data: 0.0583 (0.0496) Loss: 1.4708 (1.5545)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(248)         0:00:15         0:00:09         0:00:05          1.5550\n",
            "--------------------------------------------------------------------------------\n",
            "TRAIN(249): [ 10/196] Batch: 0.0743 (0.1232) Data: 0.0486 (0.0855) Loss: 1.6834 (1.5556)\n",
            "TRAIN(249): [ 20/196] Batch: 0.0760 (0.0996) Data: 0.0554 (0.0679) Loss: 1.5187 (1.5639)\n",
            "TRAIN(249): [ 30/196] Batch: 0.0732 (0.0918) Data: 0.0602 (0.0622) Loss: 1.5968 (1.5622)\n",
            "TRAIN(249): [ 40/196] Batch: 0.0868 (0.0881) Data: 0.0488 (0.0586) Loss: 1.4055 (1.5583)\n",
            "TRAIN(249): [ 50/196] Batch: 0.0790 (0.0857) Data: 0.0504 (0.0568) Loss: 1.5709 (1.5554)\n",
            "TRAIN(249): [ 60/196] Batch: 0.0772 (0.0841) Data: 0.0535 (0.0550) Loss: 1.4967 (1.5522)\n",
            "TRAIN(249): [ 70/196] Batch: 0.0750 (0.0830) Data: 0.0491 (0.0542) Loss: 1.6403 (1.5491)\n",
            "TRAIN(249): [ 80/196] Batch: 0.0733 (0.0822) Data: 0.0532 (0.0534) Loss: 1.4663 (1.5523)\n",
            "TRAIN(249): [ 90/196] Batch: 0.0696 (0.0815) Data: 0.0527 (0.0528) Loss: 1.7221 (1.5556)\n",
            "TRAIN(249): [100/196] Batch: 0.0729 (0.0809) Data: 0.0556 (0.0525) Loss: 1.4242 (1.5486)\n",
            "TRAIN(249): [110/196] Batch: 0.0826 (0.0806) Data: 0.0484 (0.0519) Loss: 1.3949 (1.5470)\n",
            "TRAIN(249): [120/196] Batch: 0.0682 (0.0802) Data: 0.0549 (0.0515) Loss: 1.5278 (1.5453)\n",
            "TRAIN(249): [130/196] Batch: 0.0780 (0.0799) Data: 0.0466 (0.0512) Loss: 1.6499 (1.5477)\n",
            "TRAIN(249): [140/196] Batch: 0.0746 (0.0797) Data: 0.0504 (0.0506) Loss: 1.5957 (1.5486)\n",
            "TRAIN(249): [150/196] Batch: 0.0784 (0.0798) Data: 0.0441 (0.0497) Loss: 1.5948 (1.5471)\n",
            "TRAIN(249): [160/196] Batch: 0.0775 (0.0796) Data: 0.0508 (0.0494) Loss: 1.5192 (1.5463)\n",
            "TRAIN(249): [170/196] Batch: 0.0648 (0.0793) Data: 0.0621 (0.0491) Loss: 1.4918 (1.5476)\n",
            "TRAIN(249): [180/196] Batch: 0.0811 (0.0792) Data: 0.0488 (0.0491) Loss: 1.5896 (1.5479)\n",
            "TRAIN(249): [190/196] Batch: 0.0761 (0.0790) Data: 0.0605 (0.0491) Loss: 1.4828 (1.5481)\n",
            "--------------------------------------------------------------------------------\n",
            "           Stage           Batch            Data           F+B+O            Loss\n",
            "--------------------------------------------------------------------------------\n",
            "      TRAIN(249)         0:00:15         0:00:09         0:00:05          1.5492\n",
            "--------------------------------------------------------------------------------\n",
            "Finished Training & Test at 1:04:52 ....\n"
          ]
        }
      ],
      "source": [
        "setup(args)\n",
        "run(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "WyXD5h_STYBI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "d4213f54-5d18-4799-e7bc-8bc06029c0c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      id_idx  label\n",
              "0          0     30\n",
              "1          1     33\n",
              "2          2     55\n",
              "3          3     51\n",
              "4          4     60\n",
              "...      ...    ...\n",
              "9995    9995     83\n",
              "9996    9996      3\n",
              "9997    9997     51\n",
              "9998    9998     66\n",
              "9999    9999     92\n",
              "\n",
              "[10000 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-5ec85de1-3f90-42cf-a5c8-ecbff5c944fb\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_idx</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>33</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>9995</td>\n",
              "      <td>83</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>9996</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>9997</td>\n",
              "      <td>51</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>9998</td>\n",
              "      <td>66</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>9999</td>\n",
              "      <td>92</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5ec85de1-3f90-42cf-a5c8-ecbff5c944fb')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-52d68958-ab4e-4435-937d-abba13f8f24a\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-52d68958-ab4e-4435-937d-abba13f8f24a')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-52d68958-ab4e-4435-937d-abba13f8f24a button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-5ec85de1-3f90-42cf-a5c8-ecbff5c944fb button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-5ec85de1-3f90-42cf-a5c8-ecbff5c944fb');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ],
      "source": [
        "# check submission.csv\n",
        "pd.read_csv(args.save_path / 'submission.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0HmP4ONPTZeK"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1iE1F3AERJnTFRSer9ThzD5l-ZpdKKTtT",
      "authorship_tag": "ABX9TyPEqkg+EudeQkB1vOllZxvg",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}